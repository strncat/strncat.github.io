<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-11T21:16:48-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 15: Inverse and Invertible Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps.html" rel="alternate" type="text/html" title="Lecture 15: Inverse and Invertible Linear Transformations" /><published>2024-08-07T01:01:36-07:00</published><updated>2024-08-07T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps.html"><![CDATA[<h4><b>Identity matrix and Kronecker delta</b></h4>
<p>The identity matrix:</p>
<div>
$$
\begin{align*}
I_n &amp;= 
\begin{pmatrix}
1 &amp; 0 &amp; \dotsb &amp; 0 \\
0 &amp; 1 &amp; \dotsb &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} \in M_{n \times n}
\end{align*}
$$
</div>
<p>\(I_n\) is the \(n \times n\) identity matrix. To construct the matrix, we have the following rule.</p>
<div>
$$
 \begin{equation*}
(I)_{ij} = \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j \end{cases}
 \end{equation*}
$$
</div>
<p>\(\delta_{ij}\) is the Kronecker delta.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Inverse Linear Transformation</b></h4>
<p>For \(A \in M_{m \times n}\)</p>
<div>
$$
\begin{align*}
AI_n &amp;= A \text{ and } I_mA = A
\end{align*}
$$
</div>
<p>Exercise: Suppose we have a finite basis \(\beta\) for \(V\), then</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta} = I_n
\end{align*}
$$
</div>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inverse of \(T: V \rightarrow W\) is a map \(S: W \rightarrow V\) such that
$$
\begin{align*}
S \circ T = I_V \text{ and } T \circ S = I_W
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<div>
$$
\begin{align*}
T &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
    &amp;(x, y) \rightarrow (y, -x)         
\end{align*}
$$
</div>
<p>has inverse</p>
<div>
$$
\begin{align*}
S &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
    &amp;(x, y) \rightarrow (-y, x)         
\end{align*}
$$
</div>
<p>Checking this is true by</p>
<div>
$$
\begin{align*}
S \circ T (x,y) &amp;= S(T(x, y)) = S(y, -x) = (x, y) \\
T \circ S (x,y) &amp;= T(S(x, y)) = T(-y, x) = (x, y) 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<div>
$$
\begin{align*}
V = P \\
W = \hat{P} = \{a_1x + a_2x^2 ... + a_kx^k\}    
\end{align*}
$$
</div>
<p>\(\hat{P}\) is the set of all polynomials without the constant term. We claim that \(\hat{P}\) is a subspace of \(P\). (TODO: verify?) <br />
Now consider the following linear map between \(P\) to \(\hat{P}\) where we multiply the polynomial by \(x\) (TODO: verify it’s linear?)</p>
<div>
$$
\begin{align*}
T &amp;: P \rightarrow \hat{P}  \\
&amp;a_0 + a_1x + ... + a_kx^k \rightarrow a_0x + a_1x^2 + ... + a_kx^{k+1} \\
&amp;f \rightarrow xf
\end{align*}
$$
</div>
<p>This map has an inverse</p>
<div>
$$
\begin{align*}
S &amp;: \hat{P} \rightarrow P  \\
&amp;a_1x + a_2x^2 + ... + a_kx^{k}  \rightarrow a_1 + a_2x + ... + a_kx^{k-1}  \\
&amp;f \rightarrow \frac{1}{x}f
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<h4><b>Example 4</b></h4>
<div>
$$
\begin{align*}
T &amp;: P \rightarrow P  \\
&amp;f \rightarrow f'
\end{align*}
$$
</div>
<p>This map has no inverse! (it is onto but not 1-1)
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
This leads us to the following theorem</p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be linear.
<ul>
	<li>\(T\) has an inverse if and only if \(T\) is 1-1 (\(N(T)=\{\bar{0}_V\)) and onto (\(R(T) = W\).</li>
	<li>If \(T\) has an inverse, then it is unique \((T^{-1})\).</li>
	<li>\(T^{-1}\) is linear.</li>
</ul>
</div>
<p><br />
The third property is not obvious and requires a proof.
<br />
Proof: Let \(T: V \rightarrow W\) be a linear map with inverse \(T^{-1}: W \rightarrow V\). We want to show that \(T^{-1}\) is linear. To do this, we need to show that</p>
<div>
$$
\begin{align*}
T^{-1}(w_1 + cw_2) = T^{-1}(w_1) + cT^{-1}(w_2).
\end{align*}
$$
</div>
<p>Because \(T\) has an inverse then \(T\) is onto. This means that \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1, v_2 \in V\). Moreover, since \(T\) is 1-1, then \(v_1 \neq v_2\). Now,</p>
<div>
$$
\begin{align*}
T^{-1}(w_1 + cw_2) &amp;= T^{-1}(T(v_1) + cT(v_2)) \\
 &amp;= T^{-1}(T(v_1 + cv_2)) \text{ (because $T$ is linear!)} \\
  &amp;= I(v_1 + cv_2) \text{ (because $T$ has an inverse!)} \\
  &amp;= v_1 + cv_2 \\
  &amp; = T^{-1}(w_1) + c T^{-1}(w_2)
\end{align*}
$$
</div>
<p>Therefore, \(T^{-1}\) is linear!
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow W\) be linear and invertible. If \(\beta\) is a basis for \(V\), then \(T(\beta)\) is a basis for \(W\).
</div>
<p><br />
Proof (in the case where \(V\) and \(W\) are finite dimensional spaces):<br />
Let \(T: V \rightarrow W\) be a linear and invertible map and suppose that \(\dim(V)=n\). Choose a basis \(\beta = \{v_1, ..., v_n\}\) for \(V\). We know that</p>
<div>
$$
\begin{align*}
T(\beta) = \{T(v_1),...,T(v_n)\}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Identity matrix and Kronecker delta The identity matrix: $$ \begin{align*} I_n &amp;= \begin{pmatrix} 1 &amp; 0 &amp; \dotsb &amp; 0 \\ 0 &amp; 1 &amp; \dotsb &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} \in M_{n \times n} \end{align*} $$ \(I_n\) is the \(n \times n\) identity matrix. To construct the matrix, we have the following rule. $$ \begin{equation*} (I)_{ij} = \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j \end{cases} \end{equation*} $$ \(\delta_{ij}\) is the Kronecker delta. Inverse Linear Transformation For \(A \in M_{m \times n}\) $$ \begin{align*} AI_n &amp;= A \text{ and } I_mA = A \end{align*} $$ Exercise: Suppose we have a finite basis \(\beta\) for \(V\), then $$ \begin{align*} [I_V]_{\beta}^{\beta} = I_n \end{align*} $$ Definition An inverse of \(T: V \rightarrow W\) is a map \(S: W \rightarrow V\) such that $$ \begin{align*} S \circ T = I_V \text{ and } T \circ S = I_W \end{align*} $$ Example 2 $$ \begin{align*} T &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (y, -x) \end{align*} $$ has inverse $$ \begin{align*} S &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (-y, x) \end{align*} $$ Checking this is true by $$ \begin{align*} S \circ T (x,y) &amp;= S(T(x, y)) = S(y, -x) = (x, y) \\ T \circ S (x,y) &amp;= T(S(x, y)) = T(-y, x) = (x, y) \end{align*} $$ Example 3 $$ \begin{align*} V = P \\ W = \hat{P} = \{a_1x + a_2x^2 ... + a_kx^k\} \end{align*} $$ \(\hat{P}\) is the set of all polynomials without the constant term. We claim that \(\hat{P}\) is a subspace of \(P\). (TODO: verify?) Now consider the following linear map between \(P\) to \(\hat{P}\) where we multiply the polynomial by \(x\) (TODO: verify it’s linear?) $$ \begin{align*} T &amp;: P \rightarrow \hat{P} \\ &amp;a_0 + a_1x + ... + a_kx^k \rightarrow a_0x + a_1x^2 + ... + a_kx^{k+1} \\ &amp;f \rightarrow xf \end{align*} $$ This map has an inverse $$ \begin{align*} S &amp;: \hat{P} \rightarrow P \\ &amp;a_1x + a_2x^2 + ... + a_kx^{k} \rightarrow a_1 + a_2x + ... + a_kx^{k-1} \\ &amp;f \rightarrow \frac{1}{x}f \end{align*} $$ Example 4 $$ \begin{align*} T &amp;: P \rightarrow P \\ &amp;f \rightarrow f' \end{align*} $$ This map has no inverse! (it is onto but not 1-1) This leads us to the following theorem Theorem Let \(T: V \rightarrow W\) be linear. \(T\) has an inverse if and only if \(T\) is 1-1 (\(N(T)=\{\bar{0}_V\)) and onto (\(R(T) = W\). If \(T\) has an inverse, then it is unique \((T^{-1})\). \(T^{-1}\) is linear. The third property is not obvious and requires a proof. Proof: Let \(T: V \rightarrow W\) be a linear map with inverse \(T^{-1}: W \rightarrow V\). We want to show that \(T^{-1}\) is linear. To do this, we need to show that $$ \begin{align*} T^{-1}(w_1 + cw_2) = T^{-1}(w_1) + cT^{-1}(w_2). \end{align*} $$ Because \(T\) has an inverse then \(T\) is onto. This means that \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1, v_2 \in V\). Moreover, since \(T\) is 1-1, then \(v_1 \neq v_2\). Now, $$ \begin{align*} T^{-1}(w_1 + cw_2) &amp;= T^{-1}(T(v_1) + cT(v_2)) \\ &amp;= T^{-1}(T(v_1 + cv_2)) \text{ (because $T$ is linear!)} \\ &amp;= I(v_1 + cv_2) \text{ (because $T$ has an inverse!)} \\ &amp;= v_1 + cv_2 \\ &amp; = T^{-1}(w_1) + c T^{-1}(w_2) \end{align*} $$ Therefore, \(T^{-1}\) is linear! Theorem Suppose \(T: V \rightarrow W\) be linear and invertible. If \(\beta\) is a basis for \(V\), then \(T(\beta)\) is a basis for \(W\). Proof (in the case where \(V\) and \(W\) are finite dimensional spaces): Let \(T: V \rightarrow W\) be a linear and invertible map and suppose that \(\dim(V)=n\). Choose a basis \(\beta = \{v_1, ..., v_n\}\) for \(V\). We know that $$ \begin{align*} T(\beta) = \{T(v_1),...,T(v_n)\} \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 14: Matrix Representation of Composition and Matrix Multiplication</title><link href="http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication.html" rel="alternate" type="text/html" title="Lecture 14: Matrix Representation of Composition and Matrix Multiplication" /><published>2024-08-06T01:01:36-07:00</published><updated>2024-08-06T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>The Composition of Linear Transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition
$$
\begin{align*}
T \circ S: X &amp;\rightarrow Z \\
x &amp;\rightarrow T(S(x))
\end{align*}
$$
</div>
<p><br />
Next, we will show that this transformation is also linear.
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) 
</div>
<p><br />
Proof: We want to show \((T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2).\). To see this notice that</p>
<div>
$$
\begin{align*}
(T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\
                     &amp;= T(S(x_1) + c+S(x_2)) \text{ (because $S$ is linear)} \\
                     &amp;= T(S(x_1)) + c+T(S(x_2)) \text{ (because $T$ is linear)} \\
                     &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2) \text{ (because $T$ is linear)}
					
					 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Matrix of Linear Composition</b></h4>
<p>Suppose now that \(x\), \(Y\) and \(Z\) are finite dimensional with bases \(\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}\) and \(\gamma = \{z_1,...,z_n\}\).
<br />
<br />
How are \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related?
<br />
<br />
To answer this question, we need to define matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Multiplication</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by
$$
\begin{align*}
(AB)_{ij} = \sum_k^n A_{ik}B_{kj}
\end{align*}
$$
Alternatively,
$$
\begin{align*}
AB &amp;= A(\bar{b}_1 ... \bar{b}_p) \\
   &amp;= (A\bar{b}_1 ... \bar{b}_p)
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix},
B =
\begin{pmatrix}
1 &amp; 2 \\
1 &amp; 2 \\
2 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
A\bar{b}_1 &amp;= 
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix}
*
\begin{pmatrix}
1 \\
1 \\
2 \\
\end{pmatrix} = 
1
\begin{pmatrix}
1  \\
4 
\end{pmatrix}
+
1
\begin{pmatrix}
2  \\
5 
\end{pmatrix}
+ 
\begin{pmatrix}
3 \\
6
\end{pmatrix}
=
\begin{pmatrix}
9 \\
21
\end{pmatrix}
\\
A\bar{b}_1 &amp;= 
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix}
*
\begin{pmatrix}
2 \\
2 \\
1 \\
\end{pmatrix} = 
\begin{pmatrix}
9 \\
24
\end{pmatrix} \\
AB &amp;= 
\begin{pmatrix}
9 &amp; 9 \\
21 &amp; 24
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Composition using Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} = [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<div>
$$
\begin{align*}
A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\
B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p
\end{align*}
$$
</div>
<p>The composition</p>
<div>
$$
\begin{align*}
[L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]_{\gamma}^{\beta} \circ [L_A]_{\beta}^{\alpha} \\
&amp;= BA = [L_{BA}]_{\gamma}^{\alpha}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<h4><b>Proof</b></h4>
<p>Let \(\alpha = \{x_1,...,x_n\}\). Then,</p>
<div>
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\
&amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\
&amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by the earlier theorem we proved)}\\
&amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\
&amp;= [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha}					 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let</p>
<div>
$$
\begin{align*}
T_d: P_3 &amp;\rightarrow P_2 \\
f &amp;\rightarrow f'			 
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
T_i: P_2 &amp;\rightarrow P_3 \\
f &amp;\rightarrow \int_0^x f(t)dt			 
\end{align*}
$$
</div>
<p>For standard bases \(\beta \{1,x,x^2, x^3\}\) of \(P_3\) and \(\gamma = \{1, x, x^2\}\) of \(P_2\). 
<br />
<br />
Extra notes: As a reminder, to find the matrix representative of of \(T_d\), we first apply the transformation on the vectors of \(\beta\)</p>
<div>
$$
\begin{align*}
T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. 		 
\end{align*}
$$
</div>
<p>and then we find the coordinates of these images with respect to \(\gamma\) which will be the column vectors of the matrix,</p>
<div>
$$
\begin{align*}
T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\
T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\
T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\
T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2).
\end{align*}
$$
</div>
<p>Therefore, \(T_d\) and \(T_i\) (can be found using the same method) are:</p>
<div>
$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}, [T_i]_{\gamma}^{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1/2 &amp; 0 \\
0 &amp; 0 &amp; 1/3
\end{pmatrix} = 
\end{align*}
$$
</div>
<p>To compose these matrices,</p>
<div>
$$
\begin{align*}
[T_i]_{\gamma}^{\beta}][T_d]^{\gamma}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} = 
\end{align*}
$$
</div>
<p>To verify, want to compute \([T_i \circ T_d]^{\beta}_{\beta}\) so</p>
<div>
$$
\begin{align*}
(T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\
                        &amp;= a_1x + a_2x^2 + a_3x^3
\end{align*}
$$
</div>
<p>So now,</p>
<div>
$$
\begin{align*}
[T_i \circ T_d]^{\beta}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} = 
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Multiplication Properties</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
<ol style="list-style-type:lower-alpha">
	<li>\(A(BC) = (AB)C\)</li>
	<li>\(A(B+C) = AB + AC\)</li>
	<li>Not commutative. In general \(AB \neq BA\)</li>
</ol>
</div>
<p><br />
<b>Proof (b):</b>
<br /> 
As a reminder we know that \((AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}\). We also know that \(B+C\) is just summing the matching coordinates from each matrix, \(B_{ij} + C_{ij}\). Now, expand \(A(B+C)\).</p>
<div>
$$
\begin{align*}
(A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\
             &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\
			 &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\
			 &amp;= (AB)_{ij} + (AC)_{ij} \\
			 &amp;= (AB + AC)_{ij} \\
			 &amp;= AB + AC
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The Composition of Linear Transformations Definition Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition $$ \begin{align*} T \circ S: X &amp;\rightarrow Z \\ x &amp;\rightarrow T(S(x)) \end{align*} $$ Next, we will show that this transformation is also linear. Theorem \(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) Proof: We want to show \((T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2).\). To see this notice that $$ \begin{align*} (T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\ &amp;= T(S(x_1) + c+S(x_2)) \text{ (because $S$ is linear)} \\ &amp;= T(S(x_1)) + c+T(S(x_2)) \text{ (because $T$ is linear)} \\ &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2) \text{ (because $T$ is linear)} \end{align*} $$ The Matrix of Linear Composition Suppose now that \(x\), \(Y\) and \(Z\) are finite dimensional with bases \(\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}\) and \(\gamma = \{z_1,...,z_n\}\). How are \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related? To answer this question, we need to define matrix multiplication. Matrix Multiplication Definition Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by $$ \begin{align*} (AB)_{ij} = \sum_k^n A_{ik}B_{kj} \end{align*} $$ Alternatively, $$ \begin{align*} AB &amp;= A(\bar{b}_1 ... \bar{b}_p) \\ &amp;= (A\bar{b}_1 ... \bar{b}_p) \end{align*} $$ Example $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}. \end{align*} $$ $$ \begin{align*} A\bar{b}_1 &amp;= \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} * \begin{pmatrix} 1 \\ 1 \\ 2 \\ \end{pmatrix} = 1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + \begin{pmatrix} 3 \\ 6 \end{pmatrix} = \begin{pmatrix} 9 \\ 21 \end{pmatrix} \\ A\bar{b}_1 &amp;= \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} * \begin{pmatrix} 2 \\ 2 \\ 1 \\ \end{pmatrix} = \begin{pmatrix} 9 \\ 24 \end{pmatrix} \\ AB &amp;= \begin{pmatrix} 9 &amp; 9 \\ 21 &amp; 24 \end{pmatrix} \end{align*} $$ Composition using Matrix Multiplication Theorem Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then $$ \begin{align*} [T \circ S]_{\alpha}^{\gamma} = [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha} \end{align*} $$ Example $$ \begin{align*} A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\ B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p \end{align*} $$ The composition $$ \begin{align*} [L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]_{\gamma}^{\beta} \circ [L_A]_{\beta}^{\alpha} \\ &amp;= BA = [L_{BA}]_{\gamma}^{\alpha} \end{align*} $$ Proof Let \(\alpha = \{x_1,...,x_n\}\). Then, $$ \begin{align*} [T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\ &amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\ &amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by the earlier theorem we proved)}\\ &amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\ &amp;= [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha} \end{align*} $$ Example Let $$ \begin{align*} T_d: P_3 &amp;\rightarrow P_2 \\ f &amp;\rightarrow f' \end{align*} $$ and $$ \begin{align*} T_i: P_2 &amp;\rightarrow P_3 \\ f &amp;\rightarrow \int_0^x f(t)dt \end{align*} $$ For standard bases \(\beta \{1,x,x^2, x^3\}\) of \(P_3\) and \(\gamma = \{1, x, x^2\}\) of \(P_2\). Extra notes: As a reminder, to find the matrix representative of of \(T_d\), we first apply the transformation on the vectors of \(\beta\) $$ \begin{align*} T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. \end{align*} $$ and then we find the coordinates of these images with respect to \(\gamma\) which will be the column vectors of the matrix, $$ \begin{align*} T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\ T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\ T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\ T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2). \end{align*} $$ Therefore, \(T_d\) and \(T_i\) (can be found using the same method) are: $$ \begin{align*} [T_d]^{\gamma}_{\beta} &amp;= \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix}, [T_i]_{\gamma}^{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 0 \\ 0 &amp; 0 &amp; 1/3 \end{pmatrix} = \end{align*} $$ To compose these matrices, $$ \begin{align*} [T_i]_{\gamma}^{\beta}][T_d]^{\gamma}_{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} = \end{align*} $$ To verify, want to compute \([T_i \circ T_d]^{\beta}_{\beta}\) so $$ \begin{align*} (T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\ &amp;= a_1x + a_2x^2 + a_3x^3 \end{align*} $$ So now, $$ \begin{align*} [T_i \circ T_d]^{\beta}_{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} = \end{align*} $$ Matrix Multiplication Properties Theorem \(A(BC) = (AB)C\) \(A(B+C) = AB + AC\) Not commutative. In general \(AB \neq BA\) Proof (b): As a reminder we know that \((AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}\). We also know that \(B+C\) is just summing the matching coordinates from each matrix, \(B_{ij} + C_{ij}\). Now, expand \(A(B+C)\). $$ \begin{align*} (A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\ &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\ &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\ &amp;= (AB)_{ij} + (AC)_{ij} \\ &amp;= (AB + AC)_{ij} \\ &amp;= AB + AC \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 13: More Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 13: More Linear Transformations" /><published>2024-08-05T01:01:36-07:00</published><updated>2024-08-05T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose we have \(A \in M_{m \times n}\) where \(A\) represents a linear map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\). If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p><br />
Proof: Let \(\beta = \{e_1,...,e_n\}\). where 
\(e_1 = \begin{pmatrix}
1 \\
0 \\
. \\
. \\
. \\
0 \\
\end{pmatrix}.\)
<br />
We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them  with respect to the basis \(\gamma\),</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma})
\end{align*}
$$
</div>
<p>But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1\) is just the first column of \(A\) since we only have 1 in the first coordinate of \(e_1\). Finally, writing the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) but that’s just the first column of \(A\) since it’s the standard basis. We use can use the same argument on the remaining columns to get</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p>So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformation as a Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases.<br />
For all \(v \in V\),
$$
\begin{align*}
[T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta}
\end{align*}
$$
\(T\) acts like matrix multiplication.
</div>
<p><br />
Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear.</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\
                &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\
				&amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\
             &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\
			 &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>The expression above is exactly matrix multiplication so we can re-write this as</p>
<div>
$$
\begin{align*}
a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
&amp;=
[T(v_1)]_{\gamma} + ... + [T(v_n)]_{\gamma} 
\begin{pmatrix}
a_1 \\
. \\
. \\
. \\
a_n \\
\end{pmatrix} \\
&amp;= [T]_{\beta}^{\gamma} [v]_{\beta}.
\end{align*}
$$
</div>
<p>This is exactly what we wanted to show.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Vector Space of Linear Transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given vector spaces \(V, W\) define
$$
\begin{align*}
\mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}.
\end{align*}
$$
</div>
<p><br />
FACT: The set of linear transformations \(\mathcal{L}(V, W)\) is a vector space. We can think of this as a way to get a new vector space from two vector spaces \(W\) and \(V\).
<br />
<br />
Note: This lecture contained the intro to the composition of linear transformations but I moved it to be with the next lecture.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem Suppose we have \(A \in M_{m \times n}\) where \(A\) represents a linear map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\). If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ Proof: Let \(\beta = \{e_1,...,e_n\}\). where \(e_1 = \begin{pmatrix} 1 \\ 0 \\ . \\ . \\ . \\ 0 \\ \end{pmatrix}.\) We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them with respect to the basis \(\gamma\), $$ \begin{align*} [L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma}) \end{align*} $$ But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1\) is just the first column of \(A\) since we only have 1 in the first coordinate of \(e_1\). Finally, writing the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) but that’s just the first column of \(A\) since it’s the standard basis. We use can use the same argument on the remaining columns to get $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). Linear Transformation as a Matrix Multiplication Theorem Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases. For all \(v \in V\), $$ \begin{align*} [T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta} \end{align*} $$ \(T\) acts like matrix multiplication. Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear. $$ \begin{align*} [T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\ &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\ &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \end{align*} $$ But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in $$ \begin{align*} [T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\ &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\ &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} \end{align*} $$ The expression above is exactly matrix multiplication so we can re-write this as $$ \begin{align*} a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} &amp;= [T(v_1)]_{\gamma} + ... + [T(v_n)]_{\gamma} \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \\ \end{pmatrix} \\ &amp;= [T]_{\beta}^{\gamma} [v]_{\beta}. \end{align*} $$ This is exactly what we wanted to show. The Vector Space of Linear Transformations Definition Given vector spaces \(V, W\) define $$ \begin{align*} \mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}. \end{align*} $$ FACT: The set of linear transformations \(\mathcal{L}(V, W)\) is a vector space. We can think of this as a way to get a new vector space from two vector spaces \(W\) and \(V\). Note: This lecture contained the intro to the composition of linear transformations but I moved it to be with the next lecture. References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Section 1.6: Exercise 11</title><link href="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 11" /><published>2024-08-04T01:01:36-07:00</published><updated>2024-08-04T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html"><![CDATA[<div class="ydiv">
1.6 Exercise 11
</div>
<div class="ybdiv">
Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\).
</div>
<p><br />
Proof: 
<br />
To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	a_1(u+v) + a_2(au) = \bar{0}
	\end{align*}
	$$
</div>
<p>is only satisfied by the trivial solution. We’ll re-arrange the terms as follows,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1u + a_1v + aa_2u  \\
	&amp;= (a_1 + aa_2)u + a_1v \\
	&amp;= (a_1 + aa_2)u + a_1v.
	\end{align*}
	$$
</div>
<p>From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis.
<br />
<br />
Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1(au) + a_2(bv) \\
	\bar{0} &amp;= (a_1a)u + (a_2b)v.
	\end{align*}
	$$
</div>
<p>This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.6 Exercise 11 Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\). Proof: To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation $$ \begin{align*} a_1(u+v) + a_2(au) = \bar{0} \end{align*} $$ is only satisfied by the trivial solution. We’ll re-arrange the terms as follows, $$ \begin{align*} \bar{0} &amp;= a_1u + a_1v + aa_2u \\ &amp;= (a_1 + aa_2)u + a_1v \\ &amp;= (a_1 + aa_2)u + a_1v. \end{align*} $$ From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis. Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms, $$ \begin{align*} \bar{0} &amp;= a_1(au) + a_2(bv) \\ \bar{0} &amp;= (a_1a)u + (a_2b)v. \end{align*} $$ This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 20" /><published>2024-08-03T01:01:36-07:00</published><updated>2024-08-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html"><![CDATA[<p>Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case?</p>
<div class="ydiv">
1.6 Exercise 20
</div>
<div class="ybdiv">
Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\).
<ol style="list-style-type:lower-alpha">
	<li>Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.)</li>
	<li>Prove \(S\) contains at least \(n\) vectors.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Similar to the proof of <a href="http://127.0.0.1:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html">theorem 1.9</a>, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\)</li>
	
	<li>If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains 
	at least \(n\) vectors by <a href="http://127.0.0.1:4000/jekyll/update/2024/08/02/1-6-corollary-2.html">Corollary 2</a>
	 from theorem 1.9. \(\blacksquare\)</li>
</ol>
<p><br />
The book provided the solution <a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_1_6.html">here</a>.
though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case? 1.6 Exercise 20 Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\). Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.) Prove \(S\) contains at least \(n\) vectors. Proof: Similar to the proof of theorem 1.9, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\) If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains at least \(n\) vectors by Corollary 2 from theorem 1.9. \(\blacksquare\) The book provided the solution here. though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Corollary 2</title><link href="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html" rel="alternate" type="text/html" title="Section 1.6: Corollary 2" /><published>2024-08-02T01:01:36-07:00</published><updated>2024-08-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html"><![CDATA[<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
	Let \(V\) be a vector space with dimension \(n\).
<ol style="list-style-type:lower-alpha">
	<li>Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\)</li>
	<li>Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\).</li>
	<li>Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\).</li>
</ol>
</div>
<p><br />
Proof: 
<br />
<br />
\((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Corollary 2 Let \(V\) be a vector space with dimension \(n\). Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\) Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\). Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\). Proof: \((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\) \(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\) \(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html" rel="alternate" type="text/html" title="Section 1.5: Exercise 21" /><published>2024-08-01T01:01:36-07:00</published><updated>2024-08-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html"><![CDATA[<div class="ydiv">
1.5 Exercise 21
</div>
<div class="ybdiv">
Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). 
</div>
<p><br />
Proof:
<br />
\(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}.
	\end{align*}
	$$
</div>
<p>Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\
	u &amp;= w \\
	\end{align*}
	$$
</div>
<p>We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\)
<br />
<br />
\(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\),</p>
<div>
	$$
	\begin{align*}
	v = a_1u_1 + a_2u_2 + ... + a_nu_n.
	\end{align*}
	$$
</div>
<p>And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows,</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Substituting for \(v\) in the previous equation,</p>
<div>
	$$
	\begin{align*}
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}.
	\end{align*}
	$$
</div>
<p>Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.5 Exercise 21 Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). Proof: \(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}. \end{align*} $$ Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\ u &amp;= w \\ \end{align*} $$ We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\) \(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\), $$ \begin{align*} v = a_1u_1 + a_2u_2 + ... + a_nu_n. \end{align*} $$ And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows, $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Substituting for \(v\) in the previous equation, $$ \begin{align*} &amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\ &amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}. \end{align*} $$ Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Theorem 1.7</title><link href="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html" rel="alternate" type="text/html" title="Section 1.5: Theorem 1.7" /><published>2024-07-31T01:01:36-07:00</published><updated>2024-07-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html"><![CDATA[<p>This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture.</p>
<div class="purdiv">
Theorem 1.7
</div>
<div class="purbdiv">
If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\).
</div>
<p><br />
Proof: 
<br />
<br />
\(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}.
	\end{align*}
	$$
</div>
<p>But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\
	v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\
	v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\
	\end{align*}
	$$
</div>
<p>This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\).
<br />
<br />
\(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Moving \(v\) to the other side, we see that,</p>
<div>
	$$
	\begin{align*}
	 b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}.
	\end{align*}
	$$
</div>
<p>We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture. Theorem 1.7 If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\). Proof: \(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}. \end{align*} $$ But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so $$ \begin{align*} \bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\ v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\ v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\ \end{align*} $$ This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\). \(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Moving \(v\) to the other side, we see that, $$ \begin{align*} b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}. \end{align*} $$ We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Theorem 1.9</title><link href="http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html" rel="alternate" type="text/html" title="Section 1.6: Theorem 1.9" /><published>2024-07-30T01:01:36-07:00</published><updated>2024-07-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html"><![CDATA[<p>This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet.</p>
<div class="purdiv">
Theorem 1.9
</div>
<div class="purbdiv">
If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis.
</div>
<p><br />
Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases</p>
<ol>
	<li>\(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done.</li>
	<li>\(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) </li>
</ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet. Theorem 1.9 If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis. Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases \(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done. \(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 12: Linear Transformations Continued</title><link href="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 12: Linear Transformations Continued" /><published>2024-07-29T01:01:36-07:00</published><updated>2024-07-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is onto if \(R(T) = W\) or
	$$
	\begin{align*}
	 \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is 1-1 if
	$$
	\begin{align*}
	 T(v_1) = T(v_2) \Rightarrow v_1 = v_2.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent 
<ul>
	<li> \(T\) is 1-1. </li>
	<li> \(T\) is onto. </li>
	<li> \(rank(T) = \dim(V)\). </li>
</ul>
</div>
<p><br />
Note here that \(rank(T) = \dim(R(T))\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Recall \(A \in M_{m \times n}\) defines a linear map</p>
<div>
	$$
	\begin{align*}
	L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\ 
	            &amp;\bar{x} \rightarrow A\bar{x} 
	\end{align*}
	$$
</div>
<p>We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example</p>
<div>
	$$
	\begin{align*}
	T_d: \ &amp;P_3 \rightarrow P_2 \\ 
	&amp;f \rightarrow f'
	\end{align*}
	$$
</div>
<p>and</p>
<div>
 	$$
 	\begin{align*}
 	T_i: \ &amp;P_2 \rightarrow P_3 \\ 
 	&amp;\ f \rightarrow \int_0^x f(t)dt
 	\end{align*}
 	$$
</div>
<p>But in order to show this, we will start with expressing vectors uniquely relative a basis in a vector space.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Coordinate Expression for a vector</b></h4>
<p>Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form,</p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n
	\end{align*}
	$$
</div>
<p>The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. 
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [v]_{\beta} = 
\begin{pmatrix}
a_1 \\
.  \\
. \\
. \\
a_n
\end{pmatrix}
\in \mathbf{R}^n
	\end{align*}
	$$
is the coordinate expression for \(v\) with respect to \(\beta\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\).
<br /></p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta} &amp;= 
	 \begin{pmatrix}
	 2 \\
	 1 \\
	 \end{pmatrix} \\
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis,</p>
<div>
	$$
	\begin{align*}
	 v = (2,1) = a_1(1,1) + a_2(1,-1).
\end{align*}
	 $$
</div>
<p>From this, we get</p>
<div>
	$$
	\begin{align*}
	 a_1 + a_2 &amp;= 2 \\
	 a_1 - a_2 &amp;= 1 \\
\end{align*}
	 $$
</div>
<p>Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are</p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix} \\
	 &amp;= 
	 \begin{pmatrix}
	 \frac{3}{2} \\
	 \frac{1}{2} \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>So we can think of \([\quad]_{\beta'}\) as a map:</p>
<div>
	$$
	\begin{align*}
	 [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance”
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional.
We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be the basis for \(V\) and let \(\gamma = \{w_1, ..., w_n\}\) be the basis for \(W\). For \(v \in V\), we have</p>
<div>
	$$
	\begin{align*}
	T(v) = a_1w_1 + ... + a_mw_m.
\end{align*}
$$
</div>
<p>(This is the image of \(v\) which can be expressed uniquely in terms of the vectors of the basis \(\gamma\) because we’re in the vector space \(W\) now.) \(v\) itself is a linear combination of the basis vectors of \(\beta\). So for each \(v_j\) we have,</p>
<div>
 	$$
 	\begin{align*}
 	T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m.
 \end{align*}
 $$
 </div>
<p>From this, we now have this definition,
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [T]_{\beta}^{\gamma} = (a_{ij}) \text{ where } i=1,...,m, j=1,...,n
	\end{align*}
	$$
is the matrix representation of \(T\) with respect to the bases \(\beta,\gamma\).
</div>
<p><br />
Remark: The column vectors of \(T\) are the images of the</p>
<div>
	$$
\begin{align*}
	 [T]_{\beta}^{\gamma} = 
\begin{pmatrix}
| &amp; &amp; |\\ 
[T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\
| &amp; &amp; |
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For all \(v \in V\), \([T(v)]_{\gamma} = [T(v)]_{\beta}^{\gamma}[v]_{\beta}\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(T_d: P_3 \rightarrow P_2, f \rightarrow f'\). 
<br />
\(P_3\) has a basis</p>
<div>
	$$
\begin{align*}
\beta = \{1, x, x^2, x^3\}.
\end{align*}
$$
</div>
<p>and \(P_2\) has a basis,</p>
<div>
	$$
\begin{align*}
\gamma = \{1, x, x^2\}.
\end{align*}
$$
</div>
<p>We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so,</p>
<div>
	$$
\begin{align*}
T_d(1) &amp;= 0 \\
T_d(x) &amp;= x \\
T_d(x^2) &amp;= 2x \\
T_d(x^3) &amp;= 3x^2
\end{align*}
$$
</div>
<p>Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\).</p>
<div>
	$$
\begin{align*}
[T_d(1)]_{\gamma} &amp;=  [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x)]_{\gamma} &amp;=  [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
1 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x^2)]_{\gamma} &amp;=  [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
2 \\
0
\end{pmatrix} \\
[T_d(x^3)]_{\gamma} &amp;=  [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>So the final matrix is</p>
<div>
	$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>As an example, suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix</p>
<div>
	$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}
\begin{pmatrix}
5 \\ 
4 \\
0 \\
3
\end{pmatrix} = 
\begin{pmatrix}
4 \\
0 \\
9
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this.</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(T: V \rightarrow W\) is onto if \(R(T) = W\) or $$ \begin{align*} \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w. \end{align*} $$ Definition \(T: V \rightarrow W\) is 1-1 if $$ \begin{align*} T(v_1) = T(v_2) \Rightarrow v_1 = v_2. \end{align*} $$ Theorem If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\) Theorem Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent \(T\) is 1-1. \(T\) is onto. \(rank(T) = \dim(V)\). Note here that \(rank(T) = \dim(R(T))\). Matrix Representation of linear transformations Recall \(A \in M_{m \times n}\) defines a linear map $$ \begin{align*} L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\ &amp;\bar{x} \rightarrow A\bar{x} \end{align*} $$ We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example $$ \begin{align*} T_d: \ &amp;P_3 \rightarrow P_2 \\ &amp;f \rightarrow f' \end{align*} $$ and $$ \begin{align*} T_i: \ &amp;P_2 \rightarrow P_3 \\ &amp;\ f \rightarrow \int_0^x f(t)dt \end{align*} $$ But in order to show this, we will start with expressing vectors uniquely relative a basis in a vector space. Coordinate Expression for a vector Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form, $$ \begin{align*} v = a_1v_1 + ... + a_nv_n \end{align*} $$ The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. Definition $$ \begin{align*} [v]_{\beta} = \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \end{pmatrix} \in \mathbf{R}^n \end{align*} $$ is the coordinate expression for \(v\) with respect to \(\beta\). Example Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\). $$ \begin{align*} [v]_{\beta} &amp;= \begin{pmatrix} 2 \\ 1 \\ \end{pmatrix} \\ [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \end{align*} $$ These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis, $$ \begin{align*} v = (2,1) = a_1(1,1) + a_2(1,-1). \end{align*} $$ From this, we get $$ \begin{align*} a_1 + a_2 &amp;= 2 \\ a_1 - a_2 &amp;= 1 \\ \end{align*} $$ Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are $$ \begin{align*} [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \\ &amp;= \begin{pmatrix} \frac{3}{2} \\ \frac{1}{2} \\ \end{pmatrix} \end{align*} $$ So we can think of \([\quad]_{\beta'}\) as a map: $$ \begin{align*} [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n \end{align*} $$ The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance” Matrix Representation of linear transformations Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional. We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be the basis for \(V\) and let \(\gamma = \{w_1, ..., w_n\}\) be the basis for \(W\). For \(v \in V\), we have $$ \begin{align*} T(v) = a_1w_1 + ... + a_mw_m. \end{align*} $$ (This is the image of \(v\) which can be expressed uniquely in terms of the vectors of the basis \(\gamma\) because we’re in the vector space \(W\) now.) \(v\) itself is a linear combination of the basis vectors of \(\beta\). So for each \(v_j\) we have, $$ \begin{align*} T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m. \end{align*} $$ From this, we now have this definition, Definition $$ \begin{align*} [T]_{\beta}^{\gamma} = (a_{ij}) \text{ where } i=1,...,m, j=1,...,n \end{align*} $$ is the matrix representation of \(T\) with respect to the bases \(\beta,\gamma\). Remark: The column vectors of \(T\) are the images of the $$ \begin{align*} [T]_{\beta}^{\gamma} = \begin{pmatrix} | &amp; &amp; |\\ [T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\ | &amp; &amp; | \end{pmatrix} \end{align*} $$ Theorem For all \(v \in V\), \([T(v)]_{\gamma} = [T(v)]_{\beta}^{\gamma}[v]_{\beta}\) Example Let \(T_d: P_3 \rightarrow P_2, f \rightarrow f'\). \(P_3\) has a basis $$ \begin{align*} \beta = \{1, x, x^2, x^3\}. \end{align*} $$ and \(P_2\) has a basis, $$ \begin{align*} \gamma = \{1, x, x^2\}. \end{align*} $$ We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so, $$ \begin{align*} T_d(1) &amp;= 0 \\ T_d(x) &amp;= x \\ T_d(x^2) &amp;= 2x \\ T_d(x^3) &amp;= 3x^2 \end{align*} $$ Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\). $$ \begin{align*} [T_d(1)]_{\gamma} &amp;= [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix} \\ [T_d(x)]_{\gamma} &amp;= [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \\ [T_d(x^2)]_{\gamma} &amp;= [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = \begin{pmatrix} 0 \\ 2 \\ 0 \end{pmatrix} \\ [T_d(x^3)]_{\gamma} &amp;= [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = \begin{pmatrix} 0 \\ 0 \\ 3 \end{pmatrix}. \end{align*} $$ So the final matrix is $$ \begin{align*} [T_d]^{\gamma}_{\beta} &amp;= \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix}. \end{align*} $$ As an example, suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix} \begin{pmatrix} 5 \\ 4 \\ 0 \\ 3 \end{pmatrix} = \begin{pmatrix} 4 \\ 0 \\ 9 \end{pmatrix}. \end{align*} $$ This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this.]]></summary></entry></feed>