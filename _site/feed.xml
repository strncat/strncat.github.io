<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-14T20:17:13-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 17: Change of Coordinates and Matrix Representations</title><link href="http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates.html" rel="alternate" type="text/html" title="Lecture 17: Change of Coordinates and Matrix Representations" /><published>2024-08-09T01:01:36-07:00</published><updated>2024-08-09T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates.html"><![CDATA[<p>Given a vector space \(V\) with two finite basis \(\beta\) and \(\beta'\). For \(v \in V\), what is the relationship between \([v]_{\beta}\) and \([v]_{\beta'}\)?
<br />
<br />
Starting with \([v]_{\beta'}\), we want to derive an expression for \([v]_{\beta}\). We can take the</p>
<div>
$$
\begin{align*}
[v]_{\beta} &amp;= [I_V(v)]_{\beta'} \\
           &amp;= [I_V]_{\beta}^{\beta'}[v]_{\beta} \\
\end{align*}
$$
</div>
<p>In the first line we just re-wrote the vector so now the identity map is applied on \(v\) which doesn’t change \(v\). Since \(I_V\) is a linear map, we can use Theorem 2.14 to re-write this as the second line. We compute the matrix \([I_V]_{\beta}^{\beta'}\) by applying \(I_V\) on the basis vectors of \(\beta\). But since it’s the identity transformation then the vectors in \(\beta\) will not change. We then find the coordinates of each vector when written as a linear combination of the vectors of basis \(\beta'\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = P_1\), \(\beta = \{1, x\}\) and \(\beta' = \{1+x, 1-x\}\). Then to compute the matrix \([I_V]_{\beta}^{\beta'}\), we want to apply the map on the basis \(\beta\) and then write these relative to the basis \(\beta'\) (meaning for each vector, find its coordinates when written as a linear combination of the vectors of the basis \(\beta'\)).</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'} &amp;= 
\begin{pmatrix}
[I_V(1)]_{\beta'} &amp; [I_V(x)]_{\beta'}
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
[1]_{\beta'} &amp; [x]_{\beta'}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Now we need to find the coordinates when 1 is written as a linear combination of the vectors of basis \(\beta'\) so</p>
<div>
$$
\begin{align*}
1 &amp;= a(1 + x) + b(1 - x) \\
1 &amp;= a + ax + b - bx \\
1 &amp;= a + b + x(a - b) \\
\end{align*}
$$
</div>
<p>from this we see that \(a = b = 1/2\). Similarly we need to find the coordinates of \(x\)</p>
<div>
$$
\begin{align*}
x &amp;= a(1 + x) + b(1 - x) \\
0 &amp;= a + ax + b - bx - x \\
0 &amp;= a + b + x(a - b - 1) \\
\end{align*}
$$
</div>
<p>A solution to this is \(a = 1/2\) and \(b = -1/2\). So now we have</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'} &amp;= 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \\
\frac{1}{2} &amp; -\frac{1}{2}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We can now use this matrix to transform any vector in \(V\) written with respect to \(\beta\) to a vector written with respect to \(\beta'\). Given some polynomial in \(V\),</p>
<div>
$$
\begin{align*}
[a_0 + a_1x]_{\beta'} = 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \\
\frac{1}{2} &amp; -\frac{1}{2}
\end{pmatrix}
\begin{pmatrix}
a_0 \\
a_1
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{2}(a_0 + a_1) \\
\frac{1}{2}(a_0 - a_1)
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Inverse of the Change of Coordinates Matrix</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	$$
	\begin{align*}
([I_V]_{\beta}^{\beta'})^{-1} = [I_V]^{\beta}_{\beta'}
    \end{align*}
	$$
</div>
<p><br />
Proof: To prove that a matrix is an inverse of another matrix, we need to show that their product is the identity matrix so</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'}[I_V]^{\beta}_{\beta'} &amp;= [I_V \circ I_V]^{\beta'}_{\beta'} \text{ (Theorem 2.11)}\\
 &amp;= [I_V]^{\beta'}_{\beta'} \\
 &amp;= I
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Change of Bases</b></h4>
<p>Question: Given \(T: V \rightarrow W\) and finite bases \(\beta, \beta'\) for \(V\) and \(\gamma, \gamma'\) for \(W\), how are \([T]^{\gamma}_{\beta}\) and \([T]^{\gamma'}_{\beta'}\) related?
<br />
<br />
l
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Given a vector space \(V\) with two finite basis \(\beta\) and \(\beta'\). For \(v \in V\), what is the relationship between \([v]_{\beta}\) and \([v]_{\beta'}\)? Starting with \([v]_{\beta'}\), we want to derive an expression for \([v]_{\beta}\). We can take the $$ \begin{align*} [v]_{\beta} &amp;= [I_V(v)]_{\beta'} \\ &amp;= [I_V]_{\beta}^{\beta'}[v]_{\beta} \\ \end{align*} $$ In the first line we just re-wrote the vector so now the identity map is applied on \(v\) which doesn’t change \(v\). Since \(I_V\) is a linear map, we can use Theorem 2.14 to re-write this as the second line. We compute the matrix \([I_V]_{\beta}^{\beta'}\) by applying \(I_V\) on the basis vectors of \(\beta\). But since it’s the identity transformation then the vectors in \(\beta\) will not change. We then find the coordinates of each vector when written as a linear combination of the vectors of basis \(\beta'\). Example Let \(V = P_1\), \(\beta = \{1, x\}\) and \(\beta' = \{1+x, 1-x\}\). Then to compute the matrix \([I_V]_{\beta}^{\beta'}\), we want to apply the map on the basis \(\beta\) and then write these relative to the basis \(\beta'\) (meaning for each vector, find its coordinates when written as a linear combination of the vectors of the basis \(\beta'\)). $$ \begin{align*} [I_V]_{\beta}^{\beta'} &amp;= \begin{pmatrix} [I_V(1)]_{\beta'} &amp; [I_V(x)]_{\beta'} \end{pmatrix} \\ &amp;= \begin{pmatrix} [1]_{\beta'} &amp; [x]_{\beta'} \end{pmatrix}. \end{align*} $$ Now we need to find the coordinates when 1 is written as a linear combination of the vectors of basis \(\beta'\) so $$ \begin{align*} 1 &amp;= a(1 + x) + b(1 - x) \\ 1 &amp;= a + ax + b - bx \\ 1 &amp;= a + b + x(a - b) \\ \end{align*} $$ from this we see that \(a = b = 1/2\). Similarly we need to find the coordinates of \(x\) $$ \begin{align*} x &amp;= a(1 + x) + b(1 - x) \\ 0 &amp;= a + ax + b - bx - x \\ 0 &amp;= a + b + x(a - b - 1) \\ \end{align*} $$ A solution to this is \(a = 1/2\) and \(b = -1/2\). So now we have $$ \begin{align*} [I_V]_{\beta}^{\beta'} &amp;= \begin{pmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; -\frac{1}{2} \end{pmatrix}. \end{align*} $$ We can now use this matrix to transform any vector in \(V\) written with respect to \(\beta\) to a vector written with respect to \(\beta'\). Given some polynomial in \(V\), $$ \begin{align*} [a_0 + a_1x]_{\beta'} = \begin{pmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; -\frac{1}{2} \end{pmatrix} \begin{pmatrix} a_0 \\ a_1 \end{pmatrix} = \begin{pmatrix} \frac{1}{2}(a_0 + a_1) \\ \frac{1}{2}(a_0 - a_1) \end{pmatrix} \end{align*} $$ The Inverse of the Change of Coordinates Matrix Theorem $$ \begin{align*} ([I_V]_{\beta}^{\beta'})^{-1} = [I_V]^{\beta}_{\beta'} \end{align*} $$ Proof: To prove that a matrix is an inverse of another matrix, we need to show that their product is the identity matrix so $$ \begin{align*} [I_V]_{\beta}^{\beta'}[I_V]^{\beta}_{\beta'} &amp;= [I_V \circ I_V]^{\beta'}_{\beta'} \text{ (Theorem 2.11)}\\ &amp;= [I_V]^{\beta'}_{\beta'} \\ &amp;= I \end{align*} $$ Change of Bases Question: Given \(T: V \rightarrow W\) and finite bases \(\beta, \beta'\) for \(V\) and \(\gamma, \gamma'\) for \(W\), how are \([T]^{\gamma}_{\beta}\) and \([T]^{\gamma'}_{\beta'}\) related? l References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 16: Inverse and Invertible Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/08/lec16-inverse-and-invertible-matrices.html" rel="alternate" type="text/html" title="Lecture 16: Inverse and Invertible Matrices" /><published>2024-08-08T01:01:36-07:00</published><updated>2024-08-08T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/08/lec16-inverse-and-invertible-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/08/lec16-inverse-and-invertible-matrices.html"><![CDATA[<p>Recall that \(\mathcal{L}(\mathbf{R}^n, \mathbf{R}^m) = \{L_A: A \in M_{m \times n}\}\). (The vector space of linear transformations)
<br />
<br />
Question: For what \(A \in M_{m \times n}\) does \(L_A\) have an inverse?
<br />
<br />
By Corollary of Theorem 2, we require \(n = m\). 
<br />
<br />
Moreover, consider the following map</p>
<div>
$$
\begin{align*}
&amp;L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\
&amp;\bar{x} \rightarrow A\bar{x}
\end{align*}
$$
</div>
<p>For this map to be invertible there must be a map</p>
<div>
$$
\begin{align*}
(L_A)^{-1}: \mathbf{R}^n \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>such that</p>
<div>
$$
\begin{align*}
(L_A)^{-1} \circ L_A = I_{\mathbf{R}^n} = L_A \circ (L_A)^{-1}
\end{align*}
$$
</div>
<p>This map is linear since the inverse of a linear map is linear. Since it is linear then we can represent it with a matrix so let \((L_A)^{-1} = L_B\) for some \(B \in M_{n \times n}\) and so</p>
<div>
$$
\begin{align*}
L_B \circ L_A &amp;= I_{\mathbf{R}^n} = L_A \circ L_B \\
L_{BA} &amp;= L_{I_n} = L_{AB} \\
BA &amp;= I_n = AB.
\end{align*}
$$
</div>
<p>Based on this, we have this definition</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is invertible if \(\exists B \in M_{n \times n}\) such that \(BA = I_n = AB\)
</div>
<p><br />
Remark: The inverse of \(A\) is unique if it exists.
<br />
Proof: Suppose \(BA = I_n = AB\) and \(CA = I_n = AC\). We need to show that \(C = B\). To do this,</p>
<div>
$$
\begin{align*}
CA &amp;= I_n \\
(CA)B &amp;= (I_n)B \\
C(AB) &amp;= B \\
C(I_n) &amp;= B \\
C &amp; B.
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Conditions for An Invertible Matrix</b></h4>
<p>So when is \(A\) invertible? 
<br />
<br />
\(A\) is invertible if and only if</p>
<ul>
	<li>\(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^n\) is invertible.</li>
	<li>\(L_A\) is 1-1 and onto. (you only need one as consequence of the dimension theorem since the dimension of the domain and the codomain are the same)</li>
	<li>\(L_A\) is 1-1. (see above or theorem 2.5)</li>
	<li>\(N(L_A) = \{\bar{0}\}\). The above is equivalent (proved in homework I think) to saying that the null space is only the zero vector.</li>
	<li>\(\{\bar{x} \ | \ A\bar{x} = \bar{0} = \{\bar{0}\}\). This is just the null space. We can check/settle this by putting the matrix in a row echelon form!</li>
	<li>a REF of \(A\) has leading entries in each column.</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation for an Inverse Linear Transformation</b></h4>
<p>Suppose \(T: V \rightarrow W\) is linear. Let \(\beta\) be a finite basis for \(V\) and \(\gamma\) be a finite basis for \(W\). 
<br />
<br />
We know that \(T\) has a matrix representative from \(\beta\) to \(\gamma\), \([T]_{\beta}^{\gamma}\). Therefore, the inverse of \(T\) will have a matrix representative instead from \(\gamma\) to \(\beta\), \([T]_{\gamma}^{\beta}\). We want to know what the relationship is between these two matrices.
<br />
<br />
Claim: If \(T: V \rightarrow W\) is invertible, then</p>
<div>
$$
\begin{align*}
[T^{-1}]_{\gamma}^{\beta} &amp;= ([T]_{\beta}^{\gamma})^{-1}
\end{align*}
$$
</div>
<p>To see this, let’s multiply both matrices. By defintion multiplying these matrices is the composition of the matrices from \(\beta\) to \(\beta\) (proved in the last lecture?)</p>
<div>
$$
\begin{align*}
[T^{-1}]_{\gamma}^{\beta}[T]^{\gamma}_{\beta} &amp;= [T^{-1} \circ T]_{\beta}^{\beta} \\
                              &amp;= [I_V]_{\beta}^{\beta} \\
							  &amp;= I_n
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation for an Inverse Linear Transformation</b></h4>
<p>If \(A\) is invertible, how do you find an its inverse?
<br />
<br />
We need \(B\) such that \(AB = I_n\)
<br />
<br />
We can think of \(AB\) as multiplying the columns of \(B\) by \(A\) and so</p>
<div>
$$
\begin{align*}
AB &amp;= I_n \\
A(\bar{b_1} ... \bar{b_n}) &amp;= (e_1 ... e_n) \\
A\bar{b_1} ... A\bar{b_n} &amp;= (e_1 ... e_n).
\end{align*}
$$
</div>
<p>This is equivalent to solving the following \(n\) equations in \(n\) variables.</p>
<div>
$$
\begin{align*}
Ab_1 &amp;= e_1 \\
... \\
Ab_n &amp;= e_n.
\end{align*}
$$
</div>
<p>We can solve this all at once because if you notice here, \(A\) is the same in every equation. We do this by grouping this in the following way</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
A &amp; | &amp; e_1 &amp; ... &amp; e_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>We then put this matrix in row reduced echelon form to get \((RREF(A) | B)\). Because we have \(n\) entries, then if \(RREF(A) = I_n\), then \(B = A^{-1}\). If \(RREF(A) \neq I_n\), then \(A\) is not invertible.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(A = \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{pmatrix}\). Find if \(A^{-1}\) exists.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\
3 &amp; 4 &amp; | &amp; 0 &amp; 1
\end{pmatrix}
R_2 \rightarrow -3R_1 + R_2
\begin{pmatrix}
1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}\\
\begin{pmatrix}
1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}
R_1 \rightarrow R_2 + R_1
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}\\
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}
R_2 \rightarrow -\frac{1}{2}R_2
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\
0 &amp; 1 &amp; | &amp; \frac{3}{2} &amp; -\frac{1}{2}
\end{pmatrix}\\
\end{align*}
$$
</div>
<p>From this we see that \(A\) is invertible and its inverse is \(\begin{pmatrix}
-2 &amp; 1 \\
\frac{3}{2} &amp; -\frac{1}{2}
\end{pmatrix}\)
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Special Rule for 2 by 2 Matrices</b></h4>
<p>In fact, any 2 by 2 matrix \(\begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix}\) is invertible if \(ad - bc = 0\). In which case</p>
<div>
$$
\begin{align*}
A^{-1} = \frac{1}{ad - bc}
\begin{pmatrix}
d &amp; -b \\
-c &amp; a
\end{pmatrix}
\end{align*}
$$
</div>
<p>Proof: We will show that this is true by using the same procedure from the last example. We will also assume that \(a \neq 0\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
c &amp; d &amp; | &amp; 0 &amp; 1
\end{pmatrix}
R_2 \rightarrow -\frac{c}{a}R_1 + R_2
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1
\end{pmatrix}\\
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1
\end{pmatrix}
R_2 \rightarrow \frac{1}{a}R_2
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
0 &amp; ad - bc &amp; | &amp; -c &amp; a
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that if \(ad - bc = 0\), then the system for the last column is inconsistent and \(A\) has no inverse. We can proceed until we get to</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; \frac{d}{ad - bc} &amp; \frac{-b}{ad - bc} \\
0 &amp; 1 &amp; | &amp; \frac{-c}{ad - bc} &amp; \frac{a}{ad - bc}
\end{pmatrix}
\end{align*}
$$
</div>
<p>And so this will be the inverse. We still need to settle the case when \(a = 0\). (Exercise)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recall that \(\mathcal{L}(\mathbf{R}^n, \mathbf{R}^m) = \{L_A: A \in M_{m \times n}\}\). (The vector space of linear transformations) Question: For what \(A \in M_{m \times n}\) does \(L_A\) have an inverse? By Corollary of Theorem 2, we require \(n = m\). Moreover, consider the following map $$ \begin{align*} &amp;L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\ &amp;\bar{x} \rightarrow A\bar{x} \end{align*} $$ For this map to be invertible there must be a map $$ \begin{align*} (L_A)^{-1}: \mathbf{R}^n \rightarrow \mathbf{R}^n \end{align*} $$ such that $$ \begin{align*} (L_A)^{-1} \circ L_A = I_{\mathbf{R}^n} = L_A \circ (L_A)^{-1} \end{align*} $$ This map is linear since the inverse of a linear map is linear. Since it is linear then we can represent it with a matrix so let \((L_A)^{-1} = L_B\) for some \(B \in M_{n \times n}\) and so $$ \begin{align*} L_B \circ L_A &amp;= I_{\mathbf{R}^n} = L_A \circ L_B \\ L_{BA} &amp;= L_{I_n} = L_{AB} \\ BA &amp;= I_n = AB. \end{align*} $$ Based on this, we have this definition Definition \(A \in M_{n \times n}\) is invertible if \(\exists B \in M_{n \times n}\) such that \(BA = I_n = AB\) Remark: The inverse of \(A\) is unique if it exists. Proof: Suppose \(BA = I_n = AB\) and \(CA = I_n = AC\). We need to show that \(C = B\). To do this, $$ \begin{align*} CA &amp;= I_n \\ (CA)B &amp;= (I_n)B \\ C(AB) &amp;= B \\ C(I_n) &amp;= B \\ C &amp; B. \end{align*} $$ Conditions for An Invertible Matrix So when is \(A\) invertible? \(A\) is invertible if and only if \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^n\) is invertible. \(L_A\) is 1-1 and onto. (you only need one as consequence of the dimension theorem since the dimension of the domain and the codomain are the same) \(L_A\) is 1-1. (see above or theorem 2.5) \(N(L_A) = \{\bar{0}\}\). The above is equivalent (proved in homework I think) to saying that the null space is only the zero vector. \(\{\bar{x} \ | \ A\bar{x} = \bar{0} = \{\bar{0}\}\). This is just the null space. We can check/settle this by putting the matrix in a row echelon form! a REF of \(A\) has leading entries in each column. Matrix Representation for an Inverse Linear Transformation Suppose \(T: V \rightarrow W\) is linear. Let \(\beta\) be a finite basis for \(V\) and \(\gamma\) be a finite basis for \(W\). We know that \(T\) has a matrix representative from \(\beta\) to \(\gamma\), \([T]_{\beta}^{\gamma}\). Therefore, the inverse of \(T\) will have a matrix representative instead from \(\gamma\) to \(\beta\), \([T]_{\gamma}^{\beta}\). We want to know what the relationship is between these two matrices. Claim: If \(T: V \rightarrow W\) is invertible, then $$ \begin{align*} [T^{-1}]_{\gamma}^{\beta} &amp;= ([T]_{\beta}^{\gamma})^{-1} \end{align*} $$ To see this, let’s multiply both matrices. By defintion multiplying these matrices is the composition of the matrices from \(\beta\) to \(\beta\) (proved in the last lecture?) $$ \begin{align*} [T^{-1}]_{\gamma}^{\beta}[T]^{\gamma}_{\beta} &amp;= [T^{-1} \circ T]_{\beta}^{\beta} \\ &amp;= [I_V]_{\beta}^{\beta} \\ &amp;= I_n \end{align*} $$ Matrix Representation for an Inverse Linear Transformation If \(A\) is invertible, how do you find an its inverse? We need \(B\) such that \(AB = I_n\) We can think of \(AB\) as multiplying the columns of \(B\) by \(A\) and so $$ \begin{align*} AB &amp;= I_n \\ A(\bar{b_1} ... \bar{b_n}) &amp;= (e_1 ... e_n) \\ A\bar{b_1} ... A\bar{b_n} &amp;= (e_1 ... e_n). \end{align*} $$ This is equivalent to solving the following \(n\) equations in \(n\) variables. $$ \begin{align*} Ab_1 &amp;= e_1 \\ ... \\ Ab_n &amp;= e_n. \end{align*} $$ We can solve this all at once because if you notice here, \(A\) is the same in every equation. We do this by grouping this in the following way $$ \begin{align*} \begin{pmatrix} A &amp; | &amp; e_1 &amp; ... &amp; e_n \end{pmatrix} \end{align*} $$ We then put this matrix in row reduced echelon form to get \((RREF(A) | B)\). Because we have \(n\) entries, then if \(RREF(A) = I_n\), then \(B = A^{-1}\). If \(RREF(A) \neq I_n\), then \(A\) is not invertible. Example Let \(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\). Find if \(A^{-1}\) exists. $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\ 3 &amp; 4 &amp; | &amp; 0 &amp; 1 \end{pmatrix} R_2 \rightarrow -3R_1 + R_2 \begin{pmatrix} 1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix}\\ \begin{pmatrix} 1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix} R_1 \rightarrow R_2 + R_1 \begin{pmatrix} 1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix}\\ \begin{pmatrix} 1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix} R_2 \rightarrow -\frac{1}{2}R_2 \begin{pmatrix} 1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\ 0 &amp; 1 &amp; | &amp; \frac{3}{2} &amp; -\frac{1}{2} \end{pmatrix}\\ \end{align*} $$ From this we see that \(A\) is invertible and its inverse is \(\begin{pmatrix} -2 &amp; 1 \\ \frac{3}{2} &amp; -\frac{1}{2} \end{pmatrix}\) Special Rule for 2 by 2 Matrices In fact, any 2 by 2 matrix \(\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\) is invertible if \(ad - bc = 0\). In which case $$ \begin{align*} A^{-1} = \frac{1}{ad - bc} \begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix} \end{align*} $$ Proof: We will show that this is true by using the same procedure from the last example. We will also assume that \(a \neq 0\). $$ \begin{align*} \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ c &amp; d &amp; | &amp; 0 &amp; 1 \end{pmatrix} R_2 \rightarrow -\frac{c}{a}R_1 + R_2 \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ 0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1 \end{pmatrix}\\ \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ 0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1 \end{pmatrix} R_2 \rightarrow \frac{1}{a}R_2 \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ 0 &amp; ad - bc &amp; | &amp; -c &amp; a \end{pmatrix} \end{align*} $$ Note here that if \(ad - bc = 0\), then the system for the last column is inconsistent and \(A\) has no inverse. We can proceed until we get to $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; | &amp; \frac{d}{ad - bc} &amp; \frac{-b}{ad - bc} \\ 0 &amp; 1 &amp; | &amp; \frac{-c}{ad - bc} &amp; \frac{a}{ad - bc} \end{pmatrix} \end{align*} $$ And so this will be the inverse. We still need to settle the case when \(a = 0\). (Exercise) References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 15: Inverse and Invertible Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps.html" rel="alternate" type="text/html" title="Lecture 15: Inverse and Invertible Linear Transformations" /><published>2024-08-07T01:01:36-07:00</published><updated>2024-08-07T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps.html"><![CDATA[<h4><b>Identity matrix and Kronecker delta</b></h4>
<p>The identity matrix:</p>
<div>
$$
\begin{align*}
I_n &amp;= 
\begin{pmatrix}
1 &amp; 0 &amp; \dotsb &amp; 0 \\
0 &amp; 1 &amp; \dotsb &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} \in M_{n \times n}
\end{align*}
$$
</div>
<p>\(I_n\) is the \(n \times n\) identity matrix. To construct the matrix, we have the following rule.</p>
<div>
$$
 \begin{equation*}
(I)_{ij} = \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j \end{cases}
 \end{equation*}
$$
</div>
<p>\(\delta_{ij}\) is the Kronecker delta.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Inverse Linear Transformation</b></h4>
<p>For \(A \in M_{m \times n}\)</p>
<div>
$$
\begin{align*}
AI_n &amp;= A \text{ and } I_mA = A
\end{align*}
$$
</div>
<p>Exercise: Suppose we have a finite basis \(\beta\) for \(V\), then</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta} = I_n
\end{align*}
$$
</div>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inverse of \(T: V \rightarrow W\) is a map \(S: W \rightarrow V\) such that
$$
\begin{align*}
S \circ T = I_V \text{ and } T \circ S = I_W
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<div>
$$
\begin{align*}
T &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
    &amp;(x, y) \rightarrow (y, -x)         
\end{align*}
$$
</div>
<p>has inverse</p>
<div>
$$
\begin{align*}
S &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
    &amp;(x, y) \rightarrow (-y, x)         
\end{align*}
$$
</div>
<p>Checking this is true by</p>
<div>
$$
\begin{align*}
S \circ T (x,y) &amp;= S(T(x, y)) = S(y, -x) = (x, y) \\
T \circ S (x,y) &amp;= T(S(x, y)) = T(-y, x) = (x, y) 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<div>
$$
\begin{align*}
V = P \\
W = \hat{P} = \{a_1x + a_2x^2 ... + a_kx^k\}    
\end{align*}
$$
</div>
<p>\(\hat{P}\) is the set of all polynomials without the constant term. We claim that \(\hat{P}\) is a subspace of \(P\). (TODO: verify?) <br />
Now consider the following linear map between \(P\) to \(\hat{P}\) where we multiply the polynomial by \(x\) (TODO: verify it’s linear?)</p>
<div>
$$
\begin{align*}
T &amp;: P \rightarrow \hat{P}  \\
&amp;a_0 + a_1x + ... + a_kx^k \rightarrow a_0x + a_1x^2 + ... + a_kx^{k+1} \\
&amp;f \rightarrow xf
\end{align*}
$$
</div>
<p>This map has an inverse</p>
<div>
$$
\begin{align*}
S &amp;: \hat{P} \rightarrow P  \\
&amp;a_1x + a_2x^2 + ... + a_kx^{k}  \rightarrow a_1 + a_2x + ... + a_kx^{k-1}  \\
&amp;f \rightarrow \frac{1}{x}f
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<div>
$$
\begin{align*}
T &amp;: P \rightarrow P  \\
&amp;f \rightarrow f'
\end{align*}
$$
</div>
<p>This map has no inverse! (it is onto but not 1-1)
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
This leads us to the following theorem</p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be linear.
<ul>
	<li>\(T\) has an inverse if and only if \(T\) is 1-1 (\(N(T)=\{\bar{0}_V\)) and onto (\(R(T) = W\).</li>
	<li>If \(T\) has an inverse, then it is unique \((T^{-1})\).</li>
	<li>\(T^{-1}\) is linear.</li>
</ul>
</div>
<p><br />
The third property is not obvious and requires a proof.
<br />
Proof: Let \(T: V \rightarrow W\) be a linear map with inverse \(T^{-1}: W \rightarrow V\). We want to show that \(T^{-1}\) is linear. To do this, we need to show that</p>
<div>
$$
\begin{align*}
T^{-1}(w_1 + cw_2) = T^{-1}(w_1) + cT^{-1}(w_2).
\end{align*}
$$
</div>
<p>Because \(T\) has an inverse then \(T\) is onto. This means that \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1, v_2 \in V\). Moreover, since \(T\) is 1-1, then \(v_1 \neq v_2\). Now,</p>
<div>
$$
\begin{align*}
T^{-1}(w_1 + cw_2) &amp;= T^{-1}(T(v_1) + cT(v_2)) \\
 &amp;= T^{-1}(T(v_1 + cv_2)) \text{ (because $T$ is linear!)} \\
  &amp;= I(v_1 + cv_2) \text{ (because $T$ has an inverse!)} \\
  &amp;= v_1 + cv_2 \\
  &amp; = T^{-1}(w_1) + c T^{-1}(w_2)
\end{align*}
$$
</div>
<p>Therefore, \(T^{-1}\) is linear!
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow W\) be linear and invertible. If \(\beta\) is a basis for \(V\), then \(T(\beta)\) is a basis for \(W\).
</div>
<p><br />
Proof (in the case where \(V\) and \(W\) are finite dimensional spaces):<br />
Let \(T: V \rightarrow W\) be a linear and invertible map and suppose that \(\dim(V)=n\). Choose a basis \(\beta = \{v_1, ..., v_n\}\) for \(V\). Consider the set of images,</p>
<div>
$$
\begin{align*}
T(\beta) = \{T(v_1),...,T(v_n)\}.
\end{align*}
$$
</div>
<p>We need to show that this set is a basis which means that we need to show that it spans \(W\) and so \(Span(T(\beta)) = W\) and that it is a linearly independent set. To show that it spans \(W\), we need for any \(w \in W\), scalars \(a_1,...,a_n\) such that</p>
<div>
$$
\begin{align*}
w = a_1T(v_1) + ... + a_nT(v_n).
\end{align*}
$$
</div>
<p>But we know that \(w = T(v)\) for some \(v \in V\) since \(T\) is onto. We also know that \(\beta\) is a basis for \(V\) and so we can write \(v\) as a linear combination of the vectors in \(\beta\) for some scalars \(a_1,...,a_n\).</p>
<div>
$$
\begin{align*}
v = a_1v_1 + ... + a_nv_n.
\end{align*}
$$
</div>
<p>And now because \(T\) is linear, we can do the following</p>
<div>
$$
\begin{align*}
w &amp;= T(v) \\
  &amp;= T(a_1v_1 + ... + a_nv_n) \\
  &amp; = a_1T(v_1) + ... + a_nT(v_n).
\end{align*}
$$
</div>
<p>Which is what we wanted to show. To show that the vectors in \(T(\beta)\) are linearly independent, we need to show that the solution to</p>
<div>
$$
\begin{align*}
a_1T(v_1) + ... + a_nT(v_n) = \bar{0}_W.
\end{align*}
$$
</div>
<p>is the trivial solution which means that \(a_1=0,..,a_n=0\). We can use the linearity of \(T\) to show that</p>
<div>
$$
\begin{align*}
\bar{0}_W &amp;= a_1T(v_1) + ... + a_nT(v_n) \\
 &amp;= T(a_1v_1 + ... + a_nv_n).
\end{align*}
$$
</div>
<p>But \(T\) is 1-1 and so this implies that \(a_1v_1 + ... + a_nv_n\) must be \(\bar{0}_V\). Therefore, we must have \(a_1=0,...,a_n=0\) as required.
<br />
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be linear. If \(\dim(V) = n\) and \(T\) is invertible, then 
$$
\begin{align*}
\dim W = \dim V = n
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Isomorphism</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(V\) and \(W\) are isomorphic if there is an invertible linear map \(T: V \rightarrow W\). Such a map \(T\) is called isomorphism.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>\(\mathbf{R}^3\) and \(P_2\) are isomorphic. To see this, we need an invertible map from one to the other. The maps</p>
<div>
$$
\begin{align*}
&amp;T : \mathbf{R}^3 \rightarrow P_2  \\
&amp;(a_1,a_2,a_3) \rightarrow a_1 + a_2x + a_3x^2
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
&amp;U : \mathbf{R}^3 \rightarrow P_2  \\
&amp;(a_1,a_2,a_3) \rightarrow a_3 + (a_1 + a_2)x + (a_1 + a_2)x^2
\end{align*}
$$
</div>
<p>are both isomorphisms.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<div>
$$
\begin{align*}
&amp;U : P \rightarrow \hat{P}  \\
&amp;f \rightarrow xf
\end{align*}
$$
</div>
<p>is an isomorphism and so \(P\) and \(\hat{P}\) are isomorphic.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Criterion for Isomorphic Finite Dimensional Vector Spaces</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) is finite dimensional, then \(W\) is isomorphic to \(V\) if and only if \(\dim W = \dim V\).
</div>
<p><br />
Proof: 
\(\Leftarrow\): Suppose that \(\dim V = \dim W\). We want to show that they are isomorphic. This means that there exists some invertible map from one to the other. So let \(\beta = \{v_1,...,v_n\}\) be a basis for \(V\) and \(\alpha = \{w_1, ...,w_n\}\) be a basis for \(W\).
<br />
<br />
Define the map \(T: W \rightarrow V\) by \([T]_{\alpha}^{\beta} = I_n\). This \(T\) works. Why?</p>
<div>
$$
\begin{align*}
v_i &amp;= T(w_i) \\
       &amp;= T(a_1w_1 + ... + a_nw_n) \\
	   &amp;= a_1v_1 + ... + a_nv_n.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Identity matrix and Kronecker delta The identity matrix: $$ \begin{align*} I_n &amp;= \begin{pmatrix} 1 &amp; 0 &amp; \dotsb &amp; 0 \\ 0 &amp; 1 &amp; \dotsb &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} \in M_{n \times n} \end{align*} $$ \(I_n\) is the \(n \times n\) identity matrix. To construct the matrix, we have the following rule. $$ \begin{equation*} (I)_{ij} = \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j \end{cases} \end{equation*} $$ \(\delta_{ij}\) is the Kronecker delta. Inverse Linear Transformation For \(A \in M_{m \times n}\) $$ \begin{align*} AI_n &amp;= A \text{ and } I_mA = A \end{align*} $$ Exercise: Suppose we have a finite basis \(\beta\) for \(V\), then $$ \begin{align*} [I_V]_{\beta}^{\beta} = I_n \end{align*} $$ Definition An inverse of \(T: V \rightarrow W\) is a map \(S: W \rightarrow V\) such that $$ \begin{align*} S \circ T = I_V \text{ and } T \circ S = I_W \end{align*} $$ Example 2 $$ \begin{align*} T &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (y, -x) \end{align*} $$ has inverse $$ \begin{align*} S &amp;: \mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (-y, x) \end{align*} $$ Checking this is true by $$ \begin{align*} S \circ T (x,y) &amp;= S(T(x, y)) = S(y, -x) = (x, y) \\ T \circ S (x,y) &amp;= T(S(x, y)) = T(-y, x) = (x, y) \end{align*} $$ Example 3 $$ \begin{align*} V = P \\ W = \hat{P} = \{a_1x + a_2x^2 ... + a_kx^k\} \end{align*} $$ \(\hat{P}\) is the set of all polynomials without the constant term. We claim that \(\hat{P}\) is a subspace of \(P\). (TODO: verify?) Now consider the following linear map between \(P\) to \(\hat{P}\) where we multiply the polynomial by \(x\) (TODO: verify it’s linear?) $$ \begin{align*} T &amp;: P \rightarrow \hat{P} \\ &amp;a_0 + a_1x + ... + a_kx^k \rightarrow a_0x + a_1x^2 + ... + a_kx^{k+1} \\ &amp;f \rightarrow xf \end{align*} $$ This map has an inverse $$ \begin{align*} S &amp;: \hat{P} \rightarrow P \\ &amp;a_1x + a_2x^2 + ... + a_kx^{k} \rightarrow a_1 + a_2x + ... + a_kx^{k-1} \\ &amp;f \rightarrow \frac{1}{x}f \end{align*} $$ Example 4 $$ \begin{align*} T &amp;: P \rightarrow P \\ &amp;f \rightarrow f' \end{align*} $$ This map has no inverse! (it is onto but not 1-1) This leads us to the following theorem Theorem Let \(T: V \rightarrow W\) be linear. \(T\) has an inverse if and only if \(T\) is 1-1 (\(N(T)=\{\bar{0}_V\)) and onto (\(R(T) = W\). If \(T\) has an inverse, then it is unique \((T^{-1})\). \(T^{-1}\) is linear. The third property is not obvious and requires a proof. Proof: Let \(T: V \rightarrow W\) be a linear map with inverse \(T^{-1}: W \rightarrow V\). We want to show that \(T^{-1}\) is linear. To do this, we need to show that $$ \begin{align*} T^{-1}(w_1 + cw_2) = T^{-1}(w_1) + cT^{-1}(w_2). \end{align*} $$ Because \(T\) has an inverse then \(T\) is onto. This means that \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1, v_2 \in V\). Moreover, since \(T\) is 1-1, then \(v_1 \neq v_2\). Now, $$ \begin{align*} T^{-1}(w_1 + cw_2) &amp;= T^{-1}(T(v_1) + cT(v_2)) \\ &amp;= T^{-1}(T(v_1 + cv_2)) \text{ (because $T$ is linear!)} \\ &amp;= I(v_1 + cv_2) \text{ (because $T$ has an inverse!)} \\ &amp;= v_1 + cv_2 \\ &amp; = T^{-1}(w_1) + c T^{-1}(w_2) \end{align*} $$ Therefore, \(T^{-1}\) is linear! Theorem Suppose \(T: V \rightarrow W\) be linear and invertible. If \(\beta\) is a basis for \(V\), then \(T(\beta)\) is a basis for \(W\). Proof (in the case where \(V\) and \(W\) are finite dimensional spaces): Let \(T: V \rightarrow W\) be a linear and invertible map and suppose that \(\dim(V)=n\). Choose a basis \(\beta = \{v_1, ..., v_n\}\) for \(V\). Consider the set of images, $$ \begin{align*} T(\beta) = \{T(v_1),...,T(v_n)\}. \end{align*} $$ We need to show that this set is a basis which means that we need to show that it spans \(W\) and so \(Span(T(\beta)) = W\) and that it is a linearly independent set. To show that it spans \(W\), we need for any \(w \in W\), scalars \(a_1,...,a_n\) such that $$ \begin{align*} w = a_1T(v_1) + ... + a_nT(v_n). \end{align*} $$ But we know that \(w = T(v)\) for some \(v \in V\) since \(T\) is onto. We also know that \(\beta\) is a basis for \(V\) and so we can write \(v\) as a linear combination of the vectors in \(\beta\) for some scalars \(a_1,...,a_n\). $$ \begin{align*} v = a_1v_1 + ... + a_nv_n. \end{align*} $$ And now because \(T\) is linear, we can do the following $$ \begin{align*} w &amp;= T(v) \\ &amp;= T(a_1v_1 + ... + a_nv_n) \\ &amp; = a_1T(v_1) + ... + a_nT(v_n). \end{align*} $$ Which is what we wanted to show. To show that the vectors in \(T(\beta)\) are linearly independent, we need to show that the solution to $$ \begin{align*} a_1T(v_1) + ... + a_nT(v_n) = \bar{0}_W. \end{align*} $$ is the trivial solution which means that \(a_1=0,..,a_n=0\). We can use the linearity of \(T\) to show that $$ \begin{align*} \bar{0}_W &amp;= a_1T(v_1) + ... + a_nT(v_n) \\ &amp;= T(a_1v_1 + ... + a_nv_n). \end{align*} $$ But \(T\) is 1-1 and so this implies that \(a_1v_1 + ... + a_nv_n\) must be \(\bar{0}_V\). Therefore, we must have \(a_1=0,...,a_n=0\) as required. Corollary Let \(T: V \rightarrow W\) be linear. If \(\dim(V) = n\) and \(T\) is invertible, then $$ \begin{align*} \dim W = \dim V = n \end{align*} $$ Isomorphism Definition \(V\) and \(W\) are isomorphic if there is an invertible linear map \(T: V \rightarrow W\). Such a map \(T\) is called isomorphism. Example 4 \(\mathbf{R}^3\) and \(P_2\) are isomorphic. To see this, we need an invertible map from one to the other. The maps $$ \begin{align*} &amp;T : \mathbf{R}^3 \rightarrow P_2 \\ &amp;(a_1,a_2,a_3) \rightarrow a_1 + a_2x + a_3x^2 \end{align*} $$ and $$ \begin{align*} &amp;U : \mathbf{R}^3 \rightarrow P_2 \\ &amp;(a_1,a_2,a_3) \rightarrow a_3 + (a_1 + a_2)x + (a_1 + a_2)x^2 \end{align*} $$ are both isomorphisms. Example 5 $$ \begin{align*} &amp;U : P \rightarrow \hat{P} \\ &amp;f \rightarrow xf \end{align*} $$ is an isomorphism and so \(P\) and \(\hat{P}\) are isomorphic. Criterion for Isomorphic Finite Dimensional Vector Spaces Theorem If \(V\) is finite dimensional, then \(W\) is isomorphic to \(V\) if and only if \(\dim W = \dim V\). Proof: \(\Leftarrow\): Suppose that \(\dim V = \dim W\). We want to show that they are isomorphic. This means that there exists some invertible map from one to the other. So let \(\beta = \{v_1,...,v_n\}\) be a basis for \(V\) and \(\alpha = \{w_1, ...,w_n\}\) be a basis for \(W\). Define the map \(T: W \rightarrow V\) by \([T]_{\alpha}^{\beta} = I_n\). This \(T\) works. Why? $$ \begin{align*} v_i &amp;= T(w_i) \\ &amp;= T(a_1w_1 + ... + a_nw_n) \\ &amp;= a_1v_1 + ... + a_nv_n. \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 14: Matrix Representation of Composition and Matrix Multiplication</title><link href="http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication.html" rel="alternate" type="text/html" title="Lecture 14: Matrix Representation of Composition and Matrix Multiplication" /><published>2024-08-06T01:01:36-07:00</published><updated>2024-08-06T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>The Composition of Linear Transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition
$$
\begin{align*}
T \circ S: X &amp;\rightarrow Z \\
x &amp;\rightarrow T(S(x))
\end{align*}
$$
</div>
<p><br />
Next, we will show that this transformation is also linear.
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) 
</div>
<p><br />
Proof: We want to show \((T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2).\). To see this notice that</p>
<div>
$$
\begin{align*}
(T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\
                     &amp;= T(S(x_1) + c+S(x_2)) \text{ (because $S$ is linear)} \\
                     &amp;= T(S(x_1)) + c+T(S(x_2)) \text{ (because $T$ is linear)} \\
                     &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2) \text{ (because $T$ is linear)}
					
					 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Matrix of Linear Composition</b></h4>
<p>Suppose now that \(x\), \(Y\) and \(Z\) are finite dimensional with bases \(\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}\) and \(\gamma = \{z_1,...,z_n\}\).
<br />
<br />
How are \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related?
<br />
<br />
To answer this question, we need to define matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Multiplication</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by
$$
\begin{align*}
(AB)_{ij} = \sum_k^n A_{ik}B_{kj}
\end{align*}
$$
Alternatively,
$$
\begin{align*}
AB &amp;= A(\bar{b}_1 ... \bar{b}_p) \\
   &amp;= (A\bar{b}_1 ... \bar{b}_p)
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix},
B =
\begin{pmatrix}
1 &amp; 2 \\
1 &amp; 2 \\
2 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
A\bar{b}_1 &amp;= 
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix}
*
\begin{pmatrix}
1 \\
1 \\
2 \\
\end{pmatrix} = 
1
\begin{pmatrix}
1  \\
4 
\end{pmatrix}
+
1
\begin{pmatrix}
2  \\
5 
\end{pmatrix}
+ 
\begin{pmatrix}
3 \\
6
\end{pmatrix}
=
\begin{pmatrix}
9 \\
21
\end{pmatrix}
\\
A\bar{b}_1 &amp;= 
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix}
*
\begin{pmatrix}
2 \\
2 \\
1 \\
\end{pmatrix} = 
\begin{pmatrix}
9 \\
24
\end{pmatrix} \\
AB &amp;= 
\begin{pmatrix}
9 &amp; 9 \\
21 &amp; 24
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Composition using Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} = [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<div>
$$
\begin{align*}
A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\
B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p
\end{align*}
$$
</div>
<p>The composition</p>
<div>
$$
\begin{align*}
[L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]_{\gamma}^{\beta} \circ [L_A]_{\beta}^{\alpha} \\
&amp;= BA = [L_{BA}]_{\gamma}^{\alpha}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<h4><b>Proof</b></h4>
<p>Let \(\alpha = \{x_1,...,x_n\}\). Then,</p>
<div>
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\
&amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\
&amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by the earlier theorem we proved)}\\
&amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\
&amp;= [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha}					 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let</p>
<div>
$$
\begin{align*}
T_d: P_3 &amp;\rightarrow P_2 \\
f &amp;\rightarrow f'			 
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
T_i: P_2 &amp;\rightarrow P_3 \\
f &amp;\rightarrow \int_0^x f(t)dt			 
\end{align*}
$$
</div>
<p>For standard bases \(\beta \{1,x,x^2, x^3\}\) of \(P_3\) and \(\gamma = \{1, x, x^2\}\) of \(P_2\). 
<br />
<br />
Extra notes: As a reminder, to find the matrix representative of of \(T_d\), we first apply the transformation on the vectors of \(\beta\)</p>
<div>
$$
\begin{align*}
T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. 		 
\end{align*}
$$
</div>
<p>and then we find the coordinates of these images with respect to \(\gamma\) which will be the column vectors of the matrix,</p>
<div>
$$
\begin{align*}
T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\
T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\
T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\
T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2).
\end{align*}
$$
</div>
<p>Therefore, \(T_d\) and \(T_i\) (can be found using the same method) are:</p>
<div>
$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}, [T_i]_{\gamma}^{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1/2 &amp; 0 \\
0 &amp; 0 &amp; 1/3
\end{pmatrix} = 
\end{align*}
$$
</div>
<p>To compose these matrices,</p>
<div>
$$
\begin{align*}
[T_i]_{\gamma}^{\beta}][T_d]^{\gamma}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} = 
\end{align*}
$$
</div>
<p>To verify, want to compute \([T_i \circ T_d]^{\beta}_{\beta}\) so</p>
<div>
$$
\begin{align*}
(T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\
                        &amp;= a_1x + a_2x^2 + a_3x^3
\end{align*}
$$
</div>
<p>So now,</p>
<div>
$$
\begin{align*}
[T_i \circ T_d]^{\beta}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} = 
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Multiplication Properties</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
<ol style="list-style-type:lower-alpha">
	<li>\(A(BC) = (AB)C\)</li>
	<li>\(A(B+C) = AB + AC\)</li>
	<li>Not commutative. In general \(AB \neq BA\)</li>
</ol>
</div>
<p><br />
<b>Proof (b):</b>
<br /> 
As a reminder we know that \((AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}\). We also know that \(B+C\) is just summing the matching coordinates from each matrix, \(B_{ij} + C_{ij}\). Now, expand \(A(B+C)\).</p>
<div>
$$
\begin{align*}
(A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\
             &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\
			 &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\
			 &amp;= (AB)_{ij} + (AC)_{ij} \\
			 &amp;= (AB + AC)_{ij} \\
			 &amp;= AB + AC
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The Composition of Linear Transformations Definition Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition $$ \begin{align*} T \circ S: X &amp;\rightarrow Z \\ x &amp;\rightarrow T(S(x)) \end{align*} $$ Next, we will show that this transformation is also linear. Theorem \(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) Proof: We want to show \((T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2).\). To see this notice that $$ \begin{align*} (T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\ &amp;= T(S(x_1) + c+S(x_2)) \text{ (because $S$ is linear)} \\ &amp;= T(S(x_1)) + c+T(S(x_2)) \text{ (because $T$ is linear)} \\ &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2) \text{ (because $T$ is linear)} \end{align*} $$ The Matrix of Linear Composition Suppose now that \(x\), \(Y\) and \(Z\) are finite dimensional with bases \(\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}\) and \(\gamma = \{z_1,...,z_n\}\). How are \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related? To answer this question, we need to define matrix multiplication. Matrix Multiplication Definition Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by $$ \begin{align*} (AB)_{ij} = \sum_k^n A_{ik}B_{kj} \end{align*} $$ Alternatively, $$ \begin{align*} AB &amp;= A(\bar{b}_1 ... \bar{b}_p) \\ &amp;= (A\bar{b}_1 ... \bar{b}_p) \end{align*} $$ Example $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}. \end{align*} $$ $$ \begin{align*} A\bar{b}_1 &amp;= \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} * \begin{pmatrix} 1 \\ 1 \\ 2 \\ \end{pmatrix} = 1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + \begin{pmatrix} 3 \\ 6 \end{pmatrix} = \begin{pmatrix} 9 \\ 21 \end{pmatrix} \\ A\bar{b}_1 &amp;= \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} * \begin{pmatrix} 2 \\ 2 \\ 1 \\ \end{pmatrix} = \begin{pmatrix} 9 \\ 24 \end{pmatrix} \\ AB &amp;= \begin{pmatrix} 9 &amp; 9 \\ 21 &amp; 24 \end{pmatrix} \end{align*} $$ Composition using Matrix Multiplication Theorem Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then $$ \begin{align*} [T \circ S]_{\alpha}^{\gamma} = [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha} \end{align*} $$ Example $$ \begin{align*} A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\ B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p \end{align*} $$ The composition $$ \begin{align*} [L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]_{\gamma}^{\beta} \circ [L_A]_{\beta}^{\alpha} \\ &amp;= BA = [L_{BA}]_{\gamma}^{\alpha} \end{align*} $$ Proof Let \(\alpha = \{x_1,...,x_n\}\). Then, $$ \begin{align*} [T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\ &amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\ &amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by the earlier theorem we proved)}\\ &amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\ &amp;= [T]_{\gamma}^{\beta} \circ [S]_{\beta}^{\alpha} \end{align*} $$ Example Let $$ \begin{align*} T_d: P_3 &amp;\rightarrow P_2 \\ f &amp;\rightarrow f' \end{align*} $$ and $$ \begin{align*} T_i: P_2 &amp;\rightarrow P_3 \\ f &amp;\rightarrow \int_0^x f(t)dt \end{align*} $$ For standard bases \(\beta \{1,x,x^2, x^3\}\) of \(P_3\) and \(\gamma = \{1, x, x^2\}\) of \(P_2\). Extra notes: As a reminder, to find the matrix representative of of \(T_d\), we first apply the transformation on the vectors of \(\beta\) $$ \begin{align*} T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. \end{align*} $$ and then we find the coordinates of these images with respect to \(\gamma\) which will be the column vectors of the matrix, $$ \begin{align*} T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\ T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\ T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\ T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2). \end{align*} $$ Therefore, \(T_d\) and \(T_i\) (can be found using the same method) are: $$ \begin{align*} [T_d]^{\gamma}_{\beta} &amp;= \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix}, [T_i]_{\gamma}^{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 0 \\ 0 &amp; 0 &amp; 1/3 \end{pmatrix} = \end{align*} $$ To compose these matrices, $$ \begin{align*} [T_i]_{\gamma}^{\beta}][T_d]^{\gamma}_{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} = \end{align*} $$ To verify, want to compute \([T_i \circ T_d]^{\beta}_{\beta}\) so $$ \begin{align*} (T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\ &amp;= a_1x + a_2x^2 + a_3x^3 \end{align*} $$ So now, $$ \begin{align*} [T_i \circ T_d]^{\beta}_{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} = \end{align*} $$ Matrix Multiplication Properties Theorem \(A(BC) = (AB)C\) \(A(B+C) = AB + AC\) Not commutative. In general \(AB \neq BA\) Proof (b): As a reminder we know that \((AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}\). We also know that \(B+C\) is just summing the matching coordinates from each matrix, \(B_{ij} + C_{ij}\). Now, expand \(A(B+C)\). $$ \begin{align*} (A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\ &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\ &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\ &amp;= (AB)_{ij} + (AC)_{ij} \\ &amp;= (AB + AC)_{ij} \\ &amp;= AB + AC \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 13: More Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 13: More Linear Transformations" /><published>2024-08-05T01:01:36-07:00</published><updated>2024-08-05T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose we have \(A \in M_{m \times n}\) where \(A\) represents a linear map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\). If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p><br />
Proof: Let \(\beta = \{e_1,...,e_n\}\). where 
\(e_1 = \begin{pmatrix}
1 \\
0 \\
. \\
. \\
. \\
0 \\
\end{pmatrix}.\)
<br />
We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them  with respect to the basis \(\gamma\),</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma})
\end{align*}
$$
</div>
<p>But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1\) is just the first column of \(A\) since we only have 1 in the first coordinate of \(e_1\). Finally, writing the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) but that’s just the first column of \(A\) since it’s the standard basis. We use can use the same argument on the remaining columns to get</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p>So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformation as a Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases.<br />
For all \(v \in V\),
$$
\begin{align*}
[T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta}
\end{align*}
$$
\(T\) acts like matrix multiplication.
</div>
<p><br />
Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear.</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\
                &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\
				&amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\
             &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\
			 &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>The expression above is exactly matrix multiplication so we can re-write this as</p>
<div>
$$
\begin{align*}
a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
&amp;=
[T(v_1)]_{\gamma} + ... + [T(v_n)]_{\gamma} 
\begin{pmatrix}
a_1 \\
. \\
. \\
. \\
a_n \\
\end{pmatrix} \\
&amp;= [T]_{\beta}^{\gamma} [v]_{\beta}.
\end{align*}
$$
</div>
<p>This is exactly what we wanted to show.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Vector Space of Linear Transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given vector spaces \(V, W\) define
$$
\begin{align*}
\mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}.
\end{align*}
$$
</div>
<p><br />
FACT: The set of linear transformations \(\mathcal{L}(V, W)\) is a vector space. We can think of this as a way to get a new vector space from two vector spaces \(W\) and \(V\).
<br />
<br />
Note: This lecture contained the intro to the composition of linear transformations but I moved it to be with the next lecture.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem Suppose we have \(A \in M_{m \times n}\) where \(A\) represents a linear map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\). If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ Proof: Let \(\beta = \{e_1,...,e_n\}\). where \(e_1 = \begin{pmatrix} 1 \\ 0 \\ . \\ . \\ . \\ 0 \\ \end{pmatrix}.\) We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them with respect to the basis \(\gamma\), $$ \begin{align*} [L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma}) \end{align*} $$ But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1\) is just the first column of \(A\) since we only have 1 in the first coordinate of \(e_1\). Finally, writing the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) but that’s just the first column of \(A\) since it’s the standard basis. We use can use the same argument on the remaining columns to get $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). Linear Transformation as a Matrix Multiplication Theorem Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases. For all \(v \in V\), $$ \begin{align*} [T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta} \end{align*} $$ \(T\) acts like matrix multiplication. Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear. $$ \begin{align*} [T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\ &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\ &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \end{align*} $$ But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in $$ \begin{align*} [T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\ &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\ &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} \end{align*} $$ The expression above is exactly matrix multiplication so we can re-write this as $$ \begin{align*} a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} &amp;= [T(v_1)]_{\gamma} + ... + [T(v_n)]_{\gamma} \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \\ \end{pmatrix} \\ &amp;= [T]_{\beta}^{\gamma} [v]_{\beta}. \end{align*} $$ This is exactly what we wanted to show. The Vector Space of Linear Transformations Definition Given vector spaces \(V, W\) define $$ \begin{align*} \mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}. \end{align*} $$ FACT: The set of linear transformations \(\mathcal{L}(V, W)\) is a vector space. We can think of this as a way to get a new vector space from two vector spaces \(W\) and \(V\). Note: This lecture contained the intro to the composition of linear transformations but I moved it to be with the next lecture. References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Section 1.6: Exercise 11</title><link href="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 11" /><published>2024-08-04T01:01:36-07:00</published><updated>2024-08-04T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html"><![CDATA[<div class="ydiv">
1.6 Exercise 11
</div>
<div class="ybdiv">
Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\).
</div>
<p><br />
Proof: 
<br />
To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	a_1(u+v) + a_2(au) = \bar{0}
	\end{align*}
	$$
</div>
<p>is only satisfied by the trivial solution. We’ll re-arrange the terms as follows,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1u + a_1v + aa_2u  \\
	&amp;= (a_1 + aa_2)u + a_1v \\
	&amp;= (a_1 + aa_2)u + a_1v.
	\end{align*}
	$$
</div>
<p>From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis.
<br />
<br />
Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1(au) + a_2(bv) \\
	\bar{0} &amp;= (a_1a)u + (a_2b)v.
	\end{align*}
	$$
</div>
<p>This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.6 Exercise 11 Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\). Proof: To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation $$ \begin{align*} a_1(u+v) + a_2(au) = \bar{0} \end{align*} $$ is only satisfied by the trivial solution. We’ll re-arrange the terms as follows, $$ \begin{align*} \bar{0} &amp;= a_1u + a_1v + aa_2u \\ &amp;= (a_1 + aa_2)u + a_1v \\ &amp;= (a_1 + aa_2)u + a_1v. \end{align*} $$ From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis. Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms, $$ \begin{align*} \bar{0} &amp;= a_1(au) + a_2(bv) \\ \bar{0} &amp;= (a_1a)u + (a_2b)v. \end{align*} $$ This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 20" /><published>2024-08-03T01:01:36-07:00</published><updated>2024-08-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html"><![CDATA[<p>Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case?</p>
<div class="ydiv">
1.6 Exercise 20
</div>
<div class="ybdiv">
Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\).
<ol style="list-style-type:lower-alpha">
	<li>Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.)</li>
	<li>Prove \(S\) contains at least \(n\) vectors.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Similar to the proof of <a href="http://127.0.0.1:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html">theorem 1.9</a>, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\)</li>
	
	<li>If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains 
	at least \(n\) vectors by <a href="http://127.0.0.1:4000/jekyll/update/2024/08/02/1-6-corollary-2.html">Corollary 2</a>
	 from theorem 1.9. \(\blacksquare\)</li>
</ol>
<p><br />
The book provided the solution <a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_1_6.html">here</a>.
though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case? 1.6 Exercise 20 Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\). Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.) Prove \(S\) contains at least \(n\) vectors. Proof: Similar to the proof of theorem 1.9, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\) If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains at least \(n\) vectors by Corollary 2 from theorem 1.9. \(\blacksquare\) The book provided the solution here. though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Corollary 2</title><link href="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html" rel="alternate" type="text/html" title="Section 1.6: Corollary 2" /><published>2024-08-02T01:01:36-07:00</published><updated>2024-08-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html"><![CDATA[<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
	Let \(V\) be a vector space with dimension \(n\).
<ol style="list-style-type:lower-alpha">
	<li>Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\)</li>
	<li>Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\).</li>
	<li>Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\).</li>
</ol>
</div>
<p><br />
Proof: 
<br />
<br />
\((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Corollary 2 Let \(V\) be a vector space with dimension \(n\). Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\) Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\). Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\). Proof: \((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\) \(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\) \(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html" rel="alternate" type="text/html" title="Section 1.5: Exercise 21" /><published>2024-08-01T01:01:36-07:00</published><updated>2024-08-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html"><![CDATA[<div class="ydiv">
1.5 Exercise 21
</div>
<div class="ybdiv">
Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). 
</div>
<p><br />
Proof:
<br />
\(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}.
	\end{align*}
	$$
</div>
<p>Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\
	u &amp;= w \\
	\end{align*}
	$$
</div>
<p>We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\)
<br />
<br />
\(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\),</p>
<div>
	$$
	\begin{align*}
	v = a_1u_1 + a_2u_2 + ... + a_nu_n.
	\end{align*}
	$$
</div>
<p>And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows,</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Substituting for \(v\) in the previous equation,</p>
<div>
	$$
	\begin{align*}
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}.
	\end{align*}
	$$
</div>
<p>Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.5 Exercise 21 Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). Proof: \(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}. \end{align*} $$ Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\ u &amp;= w \\ \end{align*} $$ We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\) \(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\), $$ \begin{align*} v = a_1u_1 + a_2u_2 + ... + a_nu_n. \end{align*} $$ And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows, $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Substituting for \(v\) in the previous equation, $$ \begin{align*} &amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\ &amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}. \end{align*} $$ Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Theorem 1.7</title><link href="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html" rel="alternate" type="text/html" title="Section 1.5: Theorem 1.7" /><published>2024-07-31T01:01:36-07:00</published><updated>2024-07-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html"><![CDATA[<p>This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture.</p>
<div class="purdiv">
Theorem 1.7
</div>
<div class="purbdiv">
If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\).
</div>
<p><br />
Proof: 
<br />
<br />
\(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}.
	\end{align*}
	$$
</div>
<p>But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\
	v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\
	v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\
	\end{align*}
	$$
</div>
<p>This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\).
<br />
<br />
\(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Moving \(v\) to the other side, we see that,</p>
<div>
	$$
	\begin{align*}
	 b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}.
	\end{align*}
	$$
</div>
<p>We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture. Theorem 1.7 If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\). Proof: \(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}. \end{align*} $$ But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so $$ \begin{align*} \bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\ v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\ v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\ \end{align*} $$ This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\). \(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Moving \(v\) to the other side, we see that, $$ \begin{align*} b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}. \end{align*} $$ We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). References: Linear Algebra 5th Edition]]></summary></entry></feed>