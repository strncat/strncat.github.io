<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-19T16:23:25-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 9: Basis Vectors and The Replacement Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem.html" rel="alternate" type="text/html" title="Lecture 9: Basis Vectors and The Replacement Theorem" /><published>2024-07-26T01:01:36-07:00</published><updated>2024-07-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem.html"><![CDATA[<p>From the previous lecture, we know that a subset \(\beta \subset V\) is a basis if \(\beta\) is linearly independent and if \(Span(\beta) = V\). Moreover, we learned that for any vector \(u \in V\), we can uniquely express \(u\) interms of the elements in \(\beta\). In this lecture, we’ll start with some standard examples of basis.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>In \(\mathbf{R}^n\), let</p>
<div>
	$$
	\begin{align*}
	 e_1 = (1,0,...,0), e_2=(0,1,0,...,0),...,e_n=(0,...,1)
	\end{align*}
	$$
</div>
<p>\(\beta = \{e_1,e_2,...,e_n\}\) is the standard basis of \(\mathbf{R}^n\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>In the vector space of polynomials of degree at most \(n\) (\(P_n\)), the standard basis is \(\beta = \{1, x, x^2, ..., x^n\}\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Recall the space of all sequences, \(V = \{\{a_n\}\}\) where \(\{a_n\}\) is a sequence. Let \(e_j\) be the sequence</p>
<div>
	$$
	\begin{align*}
	 0,0,...,0,1,0,0....,0,...
	\end{align*}
	$$
</div>
<p>Where the \(j\)th term is the term 1 above. Then, the standard basis is \(\beta = \{e_1, e_2, ....\}\). This basis has infinitely many terms unlike the previous two examples.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>The vector space of all polynomials (\(P\)). The standard basis is \(\beta = \{1, x, x^2, x^3, ...\}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>\(\mathcal{F}(\mathbf{R})\) has a basis … hard to describe but it exists!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) has a finite generating set, then \(V\) has a finite basis.
</div>
<p><br />
Proof: This follows from the Refinement Theorem. If \(\{u_1,...,u_k\}\) is a finite generating set, then we can find a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and has span \(Span(\{u_{i1},...,u_{ij}\}) = V\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) has a finite basis, then any basis of \(V\) has the same number of elements.
</div>
<p><br />
Proof: This proofs needs another result coming next.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (Replacement Theorem)
</div>
<div class="purbdiv">
Suppose \(\mathcal{S} = \{s_1,...,s_n\}\) generates \(V\). If \(\ \mathcal{U} = \{u_1,...,u_k\}\) is linearly independent, then \(k \leq n\) and there is a subset \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) such that \(Span(\mathcal{U} \cup \mathcal{T}) = V\).
</div>
<p><br />
Notes: So here, \(\mathcal{U}\) is a linearly independent subset of \(V\). But this doesn’t mean that it’s a basis because it might need some additional vectors added to it. If we know another set \(S\) that generates \(V\), then there is a subset \(\mathcal{T} \subset \mathcal{S}\) such that the span of both \(\mathcal{T}\) and \(\mathcal{U}\) will generate \(V\).
<br />
<br />
<b>Proof</b>: By induction on \(k\).
<br />
Base case: \(k = 0\). This means that \(\mathcal{U} = \emptyset\). The empty set is linearly independent and \(k \leq n\). Also, \(n - k = n\) and We can take \(\mathcal{T} = \mathcal{S}\). We know that \(\mathcal{S}\) generates \(V\), so \(\mathcal{U} \cup \mathcal{T}\) generates \(V\) as required.
<br /><br />
Inductive Step: Assume that the theorem is true for \(j\). We need to show that it’s true for \(j+1\). So suppose \(\mathcal{U}_{j+1} = \{u_1, ..., u_{j+1}\}\) is linearly independent. Specifically, we need to show that:</p>
<ul>
	<li>\(j + 1 \leq n\)</li>
	<li>There exists a subset \(\mathcal{T}_{j+1} \in \mathcal{S}\) of size \(n - (j+1)\) such that \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\).</li>
</ul>
<p>Throw the \(j+1\)th element away. So now we have \(\mathcal{U}_j = \{u_1,...,u_j\}\) which is linearly independent by theorem (from the book: If \(S_1 \subseteq S_2\) and \(S_2\) is linearly independent then, \(S_1\) must be linearly independent). 
<br />
By the inductive hypothesis, we know,</p>
<ul>
	<li>\(j \leq n\)</li>
	<li>There exists a subset \(\mathcal{T}_{j} = \{s_1,...,s_{n-j}\}\) has \(n-j\) elements such that \(Span(\{u_1,...,u_j,s_1,...,s_{n-j}\}) = Span(\mathcal{U}_{j} \cup \mathcal{T}_{j}) = V\).</li>
</ul>
<p>But we know that \(u_{j+1}\) is in \(V\). Since \(Span(\mathcal{U}_{j} \cup \mathcal{T}_{j})\) generates \(V\), then we can write \(u_{j+1}\) in terms of the elements in the span.</p>
<div>
	$$
	\begin{align*}
	 u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}.
	\end{align*}
	$$
</div>
<p>Note here that \(b_1,...b_{n-j}\) can’t be all zeros (because if they are, then \(u_{j+1}\) can be written as a linear combination of the elements of \(\mathcal{U}_{j}\) alone. But that’s not possible since we said that the set \(\mathcal{U}_{j+1}\) is linearly independent.). Without the loss of generality, let \(b_{n-j}\) be non-zero. This means that,</p>
<div>
	$$
	\begin{align*}
	 n - j &amp;\geq 1 \\
	 n &amp;\geq j+1
	\end{align*}
	$$
</div>
<p>as desired. Now, we need to satisfy the second condition and find a subset of \(n-(j+1)\) elements such that the span of \(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}\) generates \(V\). We can’t choose the subset \(\mathcal{T}_{j}\) since it has \(n-j\) elements and we need \(n-j-1\) elements. So the strategy is to remove one element from \(\mathcal{T}_{j}\). 
<br />
<br />
Since we said earlier that \(b_{n-j}\) is not zero along with the inductive hypothesis, \(u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}\), we can re-write this as,</p>
<div>
	$$
	\begin{align*}
	 s_{n-j} = -\frac{1}{b_{n-j}} (a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}+u_{j+1}).
	\end{align*}
	$$
</div>
<p>basically as a linear combination of the other elements from the inductive hypothesis equation. This implies that \(s_{n-j} \in Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\). Let \(\mathcal{T}_{j+1} = s_1,...,s_{n-j-1}\). The last thing to prove is that (\(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1})) = V\). We will do this in two steps. First we will prove that,</p>
<div>
	$$
	\begin{align*}
	 Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}).
	\end{align*}
	$$
</div>
<p>These spans are equal which means that adding \(s_{n-j}\) to the span, didn’t increase the span. This is because we showed earlier that \(s_{n-j}\) is a linear combination of all the other elements \(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}\). Moreover,</p>
<div>
	$$
	\begin{align*}
	Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}).
	\end{align*}
	$$
</div>
<p>This is true because we also showed that \(u_{j+1}\) is a linear combinations of the elements \(\{u_1,...u_{j},s_1,...,s_{n-j}\}\). Therefore from the previous two equations, we can conclude that</p>
<div>
	$$
	\begin{align*}
	Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}).
	\end{align*}
	$$
</div>
<p>But we know that \(Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}) = V\) from the inductive hypothesis. Therefore, \(Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\) also generates \(V\) and so \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[From the previous lecture, we know that a subset \(\beta \subset V\) is a basis if \(\beta\) is linearly independent and if \(Span(\beta) = V\). Moreover, we learned that for any vector \(u \in V\), we can uniquely express \(u\) interms of the elements in \(\beta\). In this lecture, we’ll start with some standard examples of basis. Example 1 In \(\mathbf{R}^n\), let $$ \begin{align*} e_1 = (1,0,...,0), e_2=(0,1,0,...,0),...,e_n=(0,...,1) \end{align*} $$ \(\beta = \{e_1,e_2,...,e_n\}\) is the standard basis of \(\mathbf{R}^n\). Example 2 In the vector space of polynomials of degree at most \(n\) (\(P_n\)), the standard basis is \(\beta = \{1, x, x^2, ..., x^n\}\). Example 3 Recall the space of all sequences, \(V = \{\{a_n\}\}\) where \(\{a_n\}\) is a sequence. Let \(e_j\) be the sequence $$ \begin{align*} 0,0,...,0,1,0,0....,0,... \end{align*} $$ Where the \(j\)th term is the term 1 above. Then, the standard basis is \(\beta = \{e_1, e_2, ....\}\). This basis has infinitely many terms unlike the previous two examples. Example 4 The vector space of all polynomials (\(P\)). The standard basis is \(\beta = \{1, x, x^2, x^3, ...\}\). Example 5 \(\mathcal{F}(\mathbf{R})\) has a basis … hard to describe but it exists! Theorem If \(V\) has a finite generating set, then \(V\) has a finite basis. Proof: This follows from the Refinement Theorem. If \(\{u_1,...,u_k\}\) is a finite generating set, then we can find a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and has span \(Span(\{u_{i1},...,u_{ij}\}) = V\). Theorem If \(V\) has a finite basis, then any basis of \(V\) has the same number of elements. Proof: This proofs needs another result coming next. Theorem (Replacement Theorem) Suppose \(\mathcal{S} = \{s_1,...,s_n\}\) generates \(V\). If \(\ \mathcal{U} = \{u_1,...,u_k\}\) is linearly independent, then \(k \leq n\) and there is a subset \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) such that \(Span(\mathcal{U} \cup \mathcal{T}) = V\). Notes: So here, \(\mathcal{U}\) is a linearly independent subset of \(V\). But this doesn’t mean that it’s a basis because it might need some additional vectors added to it. If we know another set \(S\) that generates \(V\), then there is a subset \(\mathcal{T} \subset \mathcal{S}\) such that the span of both \(\mathcal{T}\) and \(\mathcal{U}\) will generate \(V\). Proof: By induction on \(k\). Base case: \(k = 0\). This means that \(\mathcal{U} = \emptyset\). The empty set is linearly independent and \(k \leq n\). Also, \(n - k = n\) and We can take \(\mathcal{T} = \mathcal{S}\). We know that \(\mathcal{S}\) generates \(V\), so \(\mathcal{U} \cup \mathcal{T}\) generates \(V\) as required. Inductive Step: Assume that the theorem is true for \(j\). We need to show that it’s true for \(j+1\). So suppose \(\mathcal{U}_{j+1} = \{u_1, ..., u_{j+1}\}\) is linearly independent. Specifically, we need to show that: \(j + 1 \leq n\) There exists a subset \(\mathcal{T}_{j+1} \in \mathcal{S}\) of size \(n - (j+1)\) such that \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). Throw the \(j+1\)th element away. So now we have \(\mathcal{U}_j = \{u_1,...,u_j\}\) which is linearly independent by theorem (from the book: If \(S_1 \subseteq S_2\) and \(S_2\) is linearly independent then, \(S_1\) must be linearly independent). By the inductive hypothesis, we know, \(j \leq n\) There exists a subset \(\mathcal{T}_{j} = \{s_1,...,s_{n-j}\}\) has \(n-j\) elements such that \(Span(\{u_1,...,u_j,s_1,...,s_{n-j}\}) = Span(\mathcal{U}_{j} \cup \mathcal{T}_{j}) = V\). But we know that \(u_{j+1}\) is in \(V\). Since \(Span(\mathcal{U}_{j} \cup \mathcal{T}_{j})\) generates \(V\), then we can write \(u_{j+1}\) in terms of the elements in the span. $$ \begin{align*} u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}. \end{align*} $$ Note here that \(b_1,...b_{n-j}\) can’t be all zeros (because if they are, then \(u_{j+1}\) can be written as a linear combination of the elements of \(\mathcal{U}_{j}\) alone. But that’s not possible since we said that the set \(\mathcal{U}_{j+1}\) is linearly independent.). Without the loss of generality, let \(b_{n-j}\) be non-zero. This means that, $$ \begin{align*} n - j &amp;\geq 1 \\ n &amp;\geq j+1 \end{align*} $$ as desired. Now, we need to satisfy the second condition and find a subset of \(n-(j+1)\) elements such that the span of \(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}\) generates \(V\). We can’t choose the subset \(\mathcal{T}_{j}\) since it has \(n-j\) elements and we need \(n-j-1\) elements. So the strategy is to remove one element from \(\mathcal{T}_{j}\). Since we said earlier that \(b_{n-j}\) is not zero along with the inductive hypothesis, \(u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}\), we can re-write this as, $$ \begin{align*} s_{n-j} = -\frac{1}{b_{n-j}} (a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}+u_{j+1}). \end{align*} $$ basically as a linear combination of the other elements from the inductive hypothesis equation. This implies that \(s_{n-j} \in Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\). Let \(\mathcal{T}_{j+1} = s_1,...,s_{n-j-1}\). The last thing to prove is that (\(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1})) = V\). We will do this in two steps. First we will prove that, $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}). \end{align*} $$ These spans are equal which means that adding \(s_{n-j}\) to the span, didn’t increase the span. This is because we showed earlier that \(s_{n-j}\) is a linear combination of all the other elements \(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}\). Moreover, $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}). \end{align*} $$ This is true because we also showed that \(u_{j+1}\) is a linear combinations of the elements \(\{u_1,...u_{j},s_1,...,s_{n-j}\}\). Therefore from the previous two equations, we can conclude that $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}). \end{align*} $$ But we know that \(Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}) = V\) from the inductive hypothesis. Therefore, \(Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\) also generates \(V\) and so \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). \(\blacksquare\) References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 8: More on Linear Dependance</title><link href="http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance.html" rel="alternate" type="text/html" title="Lecture 8: More on Linear Dependance" /><published>2024-07-25T01:01:36-07:00</published><updated>2024-07-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \( \{u_1,...,u_k\} \subset V\) is linearly dependent, then there exists one \(u_j\) can be expressed as a linear combination of the others.
</div>
<p><br />
Proof:
Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. This means that for some (not all zero) scalars \(a_1,...,a_k\), we must have</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + ... + a_ju_j + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p>Without the loss of generality that \(a_j \neq 0\) for some \(j\). Multiply the equation by \(1/a_j\)</p>
<div>
	$$
	\begin{align*}
	\frac{1}{a_j}(a_1u_1 + ... + a_ju_j + ... + a_ku_k) &amp;= \frac{1}{a_j}\bar{0} \\
	\frac{a_1}{a_j}u_1 + ... + u_j + ... + \frac{a_k}{a_j}u_k &amp;= \bar{0} \\
	-\frac{a_1}{a_j}u_1 - ... - \frac{a_k}{a_j}u_k &amp;= u_j
	\end{align*}
	$$
</div>
<p>Therefore, \(u_j\) is a linear combination of the other vectors as we wanted to show. \(\blacksquare\)
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (Refinement Theorem)
</div>
<div class="purbdiv">
Suppose \( \{u_1,...,u_k\} \subset V\) is linearly dependent. There is a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and satisfies
	$$
	\begin{align*}
	Span(\{u_{i1},...,u_{il}\}) = Span(\{u_1,...,u_k\}).
	\end{align*}
	$$
</div>
<p><br />
Proof:
Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. By the previous theorem, there exists some \(j\) where \(u_j\) is a linear combination of the other vectors in the set. So,</p>
<div>
	$$
	\begin{align*} 
	u_j = b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k.
	\end{align*}
	$$
</div>
<p>Now, delete \(u_j\) from this set. We claim that</p>
<div>
	$$
	\begin{align*} 
	Span(\{u_1,...,u_j,...,u_k\}) = Span(\{u_1,...\hat{u_j},...,u_k\}).
	\end{align*}
	$$
</div>
<p>(the hat symbol means that the variable has been deleted). To show this, we will show that the two sets are subsets of each others which will imply that the two sets are equal.
<br /><br />
Now, given a vector \(\bar{u} = a_1u_1 + ... + a_{j}u_{j} + ... + a_ku_k\) in \(Span(\{u_1,...,u_j,...,u_k\})\), we want to show that \(\bar{u} \in Span(\{u_1,...\hat{u_j},...,u_k\})\). But since we established that \(u_j\) is a linear combination of the other vectors so substitute \(u_j\) in \(\bar{u}\) as follows,</p>
<div>
	$$
	\begin{align*}
	&amp;= a_1u_1 + ... + a_{j}(b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k) + ... + a_ku_k \\
	&amp;= (a_1+ a_jb_1)u_1 + ... + (a_k+a_jb_k)u_k.
	\end{align*}
	$$
</div>
<p>From this we see that given a vector in \(Span(\{u_1,...,u_j,...,u_k\})\), it is also in \(Span(\{u_1,...\hat{u_j},...,u_k\})\). For the other direction, it’s trivial. If we have a vector \(\bar{u} = a_1u_1 + ... + 0u_{j} + ... + a_ku_k\), then it is in \(Span(\{u_1,...,u_j,...,u_k\})\). Therefore, the two spans are equal.
<br />
<br />
But now, if \(\{u_1,...,u_{j-1},u_{j+1},...,u_k\}\) is linearly independent, we then stop. Otherwise we find another \(u_j\) to throw out. This process will stop since we started with a finite number of vectors.
<br />
<br />
Remark: To find \(u_j\), we start with \(u_1\) and ask/settle the question (Q1) can you \(u_1\) as linear combinations of \(u_2, ... ,u_k\). If the answer is yes set \(u_j = u_1\). If the answer is No, then ask (Q2) can \(u_2\) be written as a linear combination of \(u_1, ... , u_k\)?
<br />
<br />
Conclusion: we can refine a finite subset \(\{u_1,...,u_k\}\) to obtain a linearly independent subset \(\{u_{i1},...,u_{il}\}\) with the same span.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(B \subset V\) is a basis of \(V\) if
	<ol>
		<li>\(B\) is linearly independent.</li>
		<li>\(Span(B) = V\). (\(B\) generates \(V\))</li>
	</ol>
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Every vector space has a basis.
</div>
<p><br />
Proof in 1.7.
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(\beta \subset V\) is a basis then every \(u \in V\) can be expressed in a unique way as an element of \(Span(\beta)\).
</div>
<p><br />
Proof: Let \(u \in V\). Let (\beta \subset V) be a basis for (V). We can express \(u\) as</p>
<div>
	$$
	\begin{align*}
	 u = a_1u_1 + ... + a_ku_k.
	\end{align*}
	$$
</div>
<p>for \(u_1, ..., u_k \in \beta\) and \(a_1, ....,a_k \in \mathbf{R}\). We claim that this is the only way to express \(u\) in terms of the elements in \(\beta\). To see why, suppose for the sake of contradiction that it is not the only way. This means that we can also express \(u\) as</p>
<div>
 	$$
 	\begin{align*}
 	 u = b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}.
 	\end{align*}
 	$$
</div>
<p>But we know that \(u - u = \bar{0}\). Evaluating \(u-u\),</p>
<div>
	$$
	\begin{align*}
	 u - u &amp;= (a_1u_1 + ... + a_ku_k) - (b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\
	\bar{0} &amp;= (a_1-b_1)u_1 + ... + (a_k-b_k)u_k - (b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\
	\end{align*}
	$$
</div>
<p>We also know that \(\beta\) is linearly independent. So for the linear combination above, all the coefficients must be 0. Therefore, we must have,</p>
<div>
	$$
	a_1 = b_1, a_2 = b_2, ..., a_k=b_k, b_{k+1}=0, b_{l} = 0.
	$$
</div>
<p>This is exactly the first representation of \(u\) which is a contradiction and so \(u\) can only be uniquely expressed in terms of the elements of \(\beta\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem If \( \{u_1,...,u_k\} \subset V\) is linearly dependent, then there exists one \(u_j\) can be expressed as a linear combination of the others. Proof: Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. This means that for some (not all zero) scalars \(a_1,...,a_k\), we must have $$ \begin{align*} a_1u_1 + ... + a_ju_j + ... + a_ku_k = \bar{0}. \end{align*} $$ Without the loss of generality that \(a_j \neq 0\) for some \(j\). Multiply the equation by \(1/a_j\) $$ \begin{align*} \frac{1}{a_j}(a_1u_1 + ... + a_ju_j + ... + a_ku_k) &amp;= \frac{1}{a_j}\bar{0} \\ \frac{a_1}{a_j}u_1 + ... + u_j + ... + \frac{a_k}{a_j}u_k &amp;= \bar{0} \\ -\frac{a_1}{a_j}u_1 - ... - \frac{a_k}{a_j}u_k &amp;= u_j \end{align*} $$ Therefore, \(u_j\) is a linear combination of the other vectors as we wanted to show. \(\blacksquare\) Theorem (Refinement Theorem) Suppose \( \{u_1,...,u_k\} \subset V\) is linearly dependent. There is a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and satisfies $$ \begin{align*} Span(\{u_{i1},...,u_{il}\}) = Span(\{u_1,...,u_k\}). \end{align*} $$ Proof: Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. By the previous theorem, there exists some \(j\) where \(u_j\) is a linear combination of the other vectors in the set. So, $$ \begin{align*} u_j = b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k. \end{align*} $$ Now, delete \(u_j\) from this set. We claim that $$ \begin{align*} Span(\{u_1,...,u_j,...,u_k\}) = Span(\{u_1,...\hat{u_j},...,u_k\}). \end{align*} $$ (the hat symbol means that the variable has been deleted). To show this, we will show that the two sets are subsets of each others which will imply that the two sets are equal. Now, given a vector \(\bar{u} = a_1u_1 + ... + a_{j}u_{j} + ... + a_ku_k\) in \(Span(\{u_1,...,u_j,...,u_k\})\), we want to show that \(\bar{u} \in Span(\{u_1,...\hat{u_j},...,u_k\})\). But since we established that \(u_j\) is a linear combination of the other vectors so substitute \(u_j\) in \(\bar{u}\) as follows, $$ \begin{align*} &amp;= a_1u_1 + ... + a_{j}(b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k) + ... + a_ku_k \\ &amp;= (a_1+ a_jb_1)u_1 + ... + (a_k+a_jb_k)u_k. \end{align*} $$ From this we see that given a vector in \(Span(\{u_1,...,u_j,...,u_k\})\), it is also in \(Span(\{u_1,...\hat{u_j},...,u_k\})\). For the other direction, it’s trivial. If we have a vector \(\bar{u} = a_1u_1 + ... + 0u_{j} + ... + a_ku_k\), then it is in \(Span(\{u_1,...,u_j,...,u_k\})\). Therefore, the two spans are equal. But now, if \(\{u_1,...,u_{j-1},u_{j+1},...,u_k\}\) is linearly independent, we then stop. Otherwise we find another \(u_j\) to throw out. This process will stop since we started with a finite number of vectors. Remark: To find \(u_j\), we start with \(u_1\) and ask/settle the question (Q1) can you \(u_1\) as linear combinations of \(u_2, ... ,u_k\). If the answer is yes set \(u_j = u_1\). If the answer is No, then ask (Q2) can \(u_2\) be written as a linear combination of \(u_1, ... , u_k\)? Conclusion: we can refine a finite subset \(\{u_1,...,u_k\}\) to obtain a linearly independent subset \(\{u_{i1},...,u_{il}\}\) with the same span. Definition \(B \subset V\) is a basis of \(V\) if \(B\) is linearly independent. \(Span(B) = V\). (\(B\) generates \(V\)) Theorem Every vector space has a basis. Proof in 1.7. Theorem If \(\beta \subset V\) is a basis then every \(u \in V\) can be expressed in a unique way as an element of \(Span(\beta)\). Proof: Let \(u \in V\). Let (\beta \subset V) be a basis for (V). We can express \(u\) as $$ \begin{align*} u = a_1u_1 + ... + a_ku_k. \end{align*} $$ for \(u_1, ..., u_k \in \beta\) and \(a_1, ....,a_k \in \mathbf{R}\). We claim that this is the only way to express \(u\) in terms of the elements in \(\beta\). To see why, suppose for the sake of contradiction that it is not the only way. This means that we can also express \(u\) as $$ \begin{align*} u = b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}. \end{align*} $$ But we know that \(u - u = \bar{0}\). Evaluating \(u-u\), $$ \begin{align*} u - u &amp;= (a_1u_1 + ... + a_ku_k) - (b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\ \bar{0} &amp;= (a_1-b_1)u_1 + ... + (a_k-b_k)u_k - (b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\ \end{align*} $$ We also know that \(\beta\) is linearly independent. So for the linear combination above, all the coefficients must be 0. Therefore, we must have, $$ a_1 = b_1, a_2 = b_2, ..., a_k=b_k, b_{k+1}=0, b_{l} = 0. $$ This is exactly the first representation of \(u\) which is a contradiction and so \(u\) can only be uniquely expressed in terms of the elements of \(\beta\). References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 7: Linear Dependance</title><link href="http://localhost:4000/jekyll/update/2024/07/24/lec07-linear-dependance.html" rel="alternate" type="text/html" title="Lecture 7: Linear Dependance" /><published>2024-07-24T01:01:36-07:00</published><updated>2024-07-24T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/24/lec07-linear-dependance</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/24/lec07-linear-dependance.html"><![CDATA[<p>In the previous lecture, we saw that for a subset \(S \subset V\) (where \(V\) is a vector space) that the span of \(S\) (\(Span(S)\)) is a subspace of \(V\) and so \(Span(S)\) is a vector space. We also solved the question of whether a particular vector \(w\) belongs to \(Span(S)\) by answering the question of whether we can write \(w\) as a linear combinations of the elements of \(S\). When \(S\) contains a finite collection of elements \(\{u_1, u_2, u_3\}\), we did this by writing \(w\) as linear combination of the elements of \(S\) as follows,</p>
<div>
$$
\begin{align*}
w = x_1 (u_1) + x_2 (u_2) +...+ x_k(u_k).
\end{align*}
$$
</div>
<p>Writing \(w\) as a linear combinations above requires solving the above system of linear equations with \(k\) variables \((x_1, x_2,...,x_k)\). If there is a solution to the system, then we know \(w\) can be written as a linear combination of the elements of \(S\) and it belongs to the span of \(S\). If the system doesn’t have a solution, then this means that \(w\) can’t be written as a linear combination of the elements and so \(w\) doesn’t belong to the span of \(S\).
<br />
<br /> 
But this question can be reversed meaning we can answer this question in terms of the span of a matrix. So if we’re given a linear system of equations with matrix \(A\),</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
a_{11} &amp; a_{12} &amp; ... &amp; a_{1n}  \\
. &amp;  &amp;  &amp; . \\
. &amp;  &amp;  &amp;  . \\
. &amp;  &amp;  &amp;  . \\
a_{m1} &amp; a_{m2} &amp; ... &amp; a_{mn}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>If we view this matrix as a collection of \(n-\)column vectors in \(R^m\) instead. The span of these vectors is a subspace of \(\mathbf{R}^m\). We call this subspace the <b>Column Space of \(A\) \((Col(A))\)</b>.
<br />
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given a vector
\(
\bar{x} = 
\begin{pmatrix}
x_1 \\
x_2 \\
.  \\
. \\
x_n
\end{pmatrix}.
\in \mathbf{R}^n
\), we define the following operation
\begin{align*}
A\bar{x} = 
\begin{pmatrix}
a_{11} &amp; ... &amp; a_{1n}  \\
. &amp;  &amp;  . \\
. &amp;  &amp;   . \\
. &amp;  &amp;   . \\
a_{m1} &amp; ... &amp; a_{mn}
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2 \\
.  \\
. \\
x_n
\end{pmatrix}
= x_1
\begin{pmatrix}
a_{11}   \\
.  \\
. \\
. \\
a_{m1} 
\end{pmatrix}
+ 
...
+ 
x_n
\begin{pmatrix}
a_{1n} \\
.  \\
. \\
. \\
a_{mn} 
\end{pmatrix}.
\end{align*}
This product is in the column space of \(A\) because it is a linear combinations of the columns of \(A\).
</div>
<p><br />
Note that this product or operations works if the vector has as many entries as the columns of \(A\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>The following matrix has three column vectors in \(\mathbf{R}^2\). The product below is a linear combination of the columns of the matrix.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6
\end{pmatrix}
\begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
&amp;=
1
\begin{pmatrix}
1 \\
4
\end{pmatrix}
+
1
\begin{pmatrix}
2 \\
5
\end{pmatrix}
+
1
\begin{pmatrix}
3 \\
6
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
6 \\
15
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>An Observation</b></h4>
<p>Given \(\bar{b} \in \mathbf{R}^m\) and viewing the \(\bar{x}\) as a variable, the equation</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
A\bar{x} = \bar{b}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>is equivalent to the linear system with augmented matrix \((A\bar{b})\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Test</b></h4>
<p>Let’s verify the above observation. Let 
\(\begin{align*}
A =
\begin{pmatrix}
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6
\end{pmatrix}
,
\bar{b} = 
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix}
\end{align*}\)
Then,</p>
<div>
$$
\begin{align*}
A\bar{x} &amp;= \bar{b} \\
\begin{pmatrix}
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2 \\
x_3 \\
\end{pmatrix}
&amp;=
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix} \\
x_1
\begin{pmatrix}
1 \\
4
\end{pmatrix}
+
x_2
\begin{pmatrix}
2 \\
5
\end{pmatrix}
+
x_3
\begin{pmatrix}
3 \\
6
\end{pmatrix}
&amp;=
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix} \\
\begin{pmatrix}
1x_1 + 2x_2 + 3x_3 \\
4x_1 + 5x_2 + 6x_3
\end{pmatrix}
&amp;=
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix}
\end{align*}
$$
</div>
<p>This is a linear system of equations with augmented matrix</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3  &amp; \sqrt{2} \\
	4 &amp; 5 &amp; 6 &amp; \pi
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>\(A\bar{x} = \bar{b}\) is consistent if and only if \(\bar{b} \in Col(A)\). This again means that answering the question of whether \(\bar{b}\) is in the span of columns of \(A\) is the same as answering if the system has a solution.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Dependence and Linear Independence</b></h4>
<p>Suppose \(W\) is a subspace of \(V\). We know that spans are subspaces but is \(W\) a span of some elements? or what is the smallest number \(k\) such that \(W\) can be written as</p>
<div>
	$$
	\begin{align*}
	W = Span = \{(w_1, w_2, ..., w_k)\}
	\end{align*}
	$$
</div>
<p>The answer to this question could be that there is no such \(k\) (for example if \(W\) is a subspace of the vector space of all continuous functions).</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A subset \( \{u_1, u_2, u_3,...\} \subset V\) is <b>linearly dependent</b> if there are scalars \(a_1, ... a_k\) not all zero such that 
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p><br />
Note here that <b>not</b> linearly dependent is equivalent to <b>linearly independent</b>. In particular the set \(\{u_1, u_2, u_3,...\}\) is linearly independent if and only if</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p>is true only when \(a_1 = 0, a_2 = 0, ... a_k = 0\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 0</b></h4>
<p>Consider \(\{\bar{0}\}\). This set is linearly dependent because we can choose a scalar $a_1 \neq 0$ such that \(a_1\bar{0} = \bar{0}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Given a vector \(u\) such that \(u \neq \{\bar{0}\}\), then \(\{u\}\) is linearly independent.
<br />
<br />
Proof: We need to prove that \(au = \bar{0}\) only if \(a = 0\). Now suppose that \(au = \bar{0}\). We have two cases. Case one is when \(a = 0\) and we’re done. Case two is when \(a \neq 0\). If \(a \neq 0\), then</p>
<div>
	$$
	\begin{align*}
	 \frac{1}{a}au &amp;= \frac{1}{a}\bar{0} \\
	 u &amp;= \bar{0}
	\end{align*}
	$$
</div>
<p>But we know that \(u \neq \bar{0}\). Therefore \(a\) must be zero and we’re done.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Given a vector \(u_1, u_2\), prove that \(\{u_1, u_2\}\) is linearly dependent if and only if one vector is a scalar multiple of the other.
<br />
<br />
Proof: For the forward direction, suppose that \(\{u_1, u_2\}\) is linearly dependent. This means that there are scalars (a_1, a_2) not all zero such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 &amp;= \bar{0}
	\end{align*}
	$$
</div>
<p>Without the loss of generality, suppose that \(a_1 \neq 0\). Then,</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 &amp;= \bar{0} \\
	a_1u_1 &amp;= - a_2u_2 \\
	u_1 &amp;= -\frac{a_2}{a_1}u_2.
	\end{align*}
	$$
</div>
<p>For the backward direction, suppose that one vector can be written as a scalar multiple of the other. Without the loss of generality, suppose it is \(u_1\). Then \(u_1 = cu_2\) where \(c \neq 0\), We can re-write this as follows,</p>
<div>
	$$
	\begin{align*}
	u_1 &amp;= cu_2 \\
	u_1 + (-c)u_2 &amp;= \bar{0}
	\end{align*}
	$$
</div>
<p>From this we see that there are scalars \(a_1 = 1\) and \(a_2 = -c\), not all zero such that \(a_1u_1 + a_2u_2 = \bar{0}\) which means that they are linearly dependent as we wanted to show.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Determine the set of the following vectors:</p>
<div>
	$$
	\begin{align*}
	\{u_1, u_2, u_3\} = \{(−1, 1, 2), (1, 2, 1), (5, 1, −4) \} 
	\end{align*}
	$$
</div>
<p>is linearly dependent.
<br />
<br />
The set of vectors is linearly independent \(a_1u_1 + a_2u_2 + a_3u_3 = \bar{0}\) implies that \(a_1=0, a_2=0\) and \(a_3=0\). So,</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + a_3u_3 &amp;= \bar{0} \\
	a_1(−1, 1, 2) + a_2(1, 2, 1) + a_3(5, 1, −4) &amp;= (0,0,0) \\
	(-a_1+a_2+5a_3, a_2+2a_2+a_3, 2a_1+2a_2-4a_3) &amp;= (0,0,0).
	\end{align*}
	$$
</div>
<p>This is equivalent to solving the following system:</p>
<div>
	$$
	\begin{align*}
	-a_1 + a_2 + 5a_3 &amp;= 0 \\
	a_2+2a_2+a_3 &amp;= 0 \\
	2a_1+2a_2-4a_3 &amp;= 0.
	\end{align*}
	$$
</div>
<p>\(\{u_1, u_2, u_3\}\) is linearly independent if and only if this system has the trivial solution \((0,0,0)\). \((0,0,0)\) will always be a solution. It’s why it is called the trivial solution. So the question now is how do we know from the REF matrix, whether we have the \((0,0,0)\) solution or a non-zero solution?
<br />
<br />
If the matrix has no column without a leading entry (besides last) then we have a unique solution and the set is linearly independent. If the matrix has a column (besides the last) with no leading entry then we have infinitely many solutions (so not just the zero vector) and the set is linearly dependent. So we want to make sure that all columns have leading entries.</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	-1 &amp; 1 &amp; 5  &amp; 0 \\
	1 &amp; 2 &amp; 1 &amp; 0 \\
	2 &amp; 2 &amp; -4 &amp; 0
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>This will eventually be</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	-1 &amp; 1 &amp; 5  &amp; 0 \\
	0 &amp; 3 &amp; 6 &amp; 0 \\
	0 &amp; 0 &amp; 0 &amp; 0
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>The third column has no leading entry so there are infinitely many solutions besides the zero solution this set is linearly dependent.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Consider \(\{ \sin(x), \cos(x)\} \in F(\mathbf{R})\). Is this set linearly dependent or independant?
<br />
<br />
From the definition if \(\sin(x)\) was a non-zero scalar multiple of \(cos(x)\), in other words \(\sin(x) = a\cos(x)\) for some non-zero scalar \(a\), then they are linearly dependent. But two functions are equal when they take the same value at every point.
<br />
<br />
Suppose we choose the point \(x = 0\), then we know that \(\sin(0) = 0\) but \(a\cos(0)=a\). Therefore, these functions are not equal and so the set is linearly independent.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In the previous lecture, we saw that for a subset \(S \subset V\) (where \(V\) is a vector space) that the span of \(S\) (\(Span(S)\)) is a subspace of \(V\) and so \(Span(S)\) is a vector space. We also solved the question of whether a particular vector \(w\) belongs to \(Span(S)\) by answering the question of whether we can write \(w\) as a linear combinations of the elements of \(S\). When \(S\) contains a finite collection of elements \(\{u_1, u_2, u_3\}\), we did this by writing \(w\) as linear combination of the elements of \(S\) as follows, $$ \begin{align*} w = x_1 (u_1) + x_2 (u_2) +...+ x_k(u_k). \end{align*} $$ Writing \(w\) as a linear combinations above requires solving the above system of linear equations with \(k\) variables \((x_1, x_2,...,x_k)\). If there is a solution to the system, then we know \(w\) can be written as a linear combination of the elements of \(S\) and it belongs to the span of \(S\). If the system doesn’t have a solution, then this means that \(w\) can’t be written as a linear combination of the elements and so \(w\) doesn’t belong to the span of \(S\). But this question can be reversed meaning we can answer this question in terms of the span of a matrix. So if we’re given a linear system of equations with matrix \(A\), $$ \begin{align*} \begin{pmatrix} a_{11} &amp; a_{12} &amp; ... &amp; a_{1n} \\ . &amp; &amp; &amp; . \\ . &amp; &amp; &amp; . \\ . &amp; &amp; &amp; . \\ a_{m1} &amp; a_{m2} &amp; ... &amp; a_{mn} \end{pmatrix}. \end{align*} $$ If we view this matrix as a collection of \(n-\)column vectors in \(R^m\) instead. The span of these vectors is a subspace of \(\mathbf{R}^m\). We call this subspace the Column Space of \(A\) \((Col(A))\). Definition Given a vector \( \bar{x} = \begin{pmatrix} x_1 \\ x_2 \\ . \\ . \\ x_n \end{pmatrix}. \in \mathbf{R}^n \), we define the following operation \begin{align*} A\bar{x} = \begin{pmatrix} a_{11} &amp; ... &amp; a_{1n} \\ . &amp; &amp; . \\ . &amp; &amp; . \\ . &amp; &amp; . \\ a_{m1} &amp; ... &amp; a_{mn} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ . \\ . \\ x_n \end{pmatrix} = x_1 \begin{pmatrix} a_{11} \\ . \\ . \\ . \\ a_{m1} \end{pmatrix} + ... + x_n \begin{pmatrix} a_{1n} \\ . \\ . \\ . \\ a_{mn} \end{pmatrix}. \end{align*} This product is in the column space of \(A\) because it is a linear combinations of the columns of \(A\). Note that this product or operations works if the vector has as many entries as the columns of \(A\). Example 1 The following matrix has three column vectors in \(\mathbf{R}^2\). The product below is a linear combination of the columns of the matrix. $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} &amp;= 1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + 1 \begin{pmatrix} 3 \\ 6 \end{pmatrix}\\ &amp;= \begin{pmatrix} 6 \\ 15 \end{pmatrix}. \end{align*} $$ An Observation Given \(\bar{b} \in \mathbf{R}^m\) and viewing the \(\bar{x}\) as a variable, the equation $$ \begin{align*} \begin{pmatrix} A\bar{x} = \bar{b} \end{pmatrix}. \end{align*} $$ is equivalent to the linear system with augmented matrix \((A\bar{b})\). Test Let’s verify the above observation. Let \(\begin{align*} A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} , \bar{b} = \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \end{align*}\) Then, $$ \begin{align*} A\bar{x} &amp;= \bar{b} \\ \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ \end{pmatrix} &amp;= \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \\ x_1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + x_2 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + x_3 \begin{pmatrix} 3 \\ 6 \end{pmatrix} &amp;= \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \\ \begin{pmatrix} 1x_1 + 2x_2 + 3x_3 \\ 4x_1 + 5x_2 + 6x_3 \end{pmatrix} &amp;= \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \end{align*} $$ This is a linear system of equations with augmented matrix $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; \sqrt{2} \\ 4 &amp; 5 &amp; 6 &amp; \pi \end{pmatrix} \end{align*} $$ \(A\bar{x} = \bar{b}\) is consistent if and only if \(\bar{b} \in Col(A)\). This again means that answering the question of whether \(\bar{b}\) is in the span of columns of \(A\) is the same as answering if the system has a solution. Linear Dependence and Linear Independence Suppose \(W\) is a subspace of \(V\). We know that spans are subspaces but is \(W\) a span of some elements? or what is the smallest number \(k\) such that \(W\) can be written as $$ \begin{align*} W = Span = \{(w_1, w_2, ..., w_k)\} \end{align*} $$ The answer to this question could be that there is no such \(k\) (for example if \(W\) is a subspace of the vector space of all continuous functions). Definition A subset \( \{u_1, u_2, u_3,...\} \subset V\) is linearly dependent if there are scalars \(a_1, ... a_k\) not all zero such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}. \end{align*} $$ Note here that not linearly dependent is equivalent to linearly independent. In particular the set \(\{u_1, u_2, u_3,...\}\) is linearly independent if and only if $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}. \end{align*} $$ is true only when \(a_1 = 0, a_2 = 0, ... a_k = 0\). Example 0 Consider \(\{\bar{0}\}\). This set is linearly dependent because we can choose a scalar $a_1 \neq 0$ such that \(a_1\bar{0} = \bar{0}\). Example 1 Given a vector \(u\) such that \(u \neq \{\bar{0}\}\), then \(\{u\}\) is linearly independent. Proof: We need to prove that \(au = \bar{0}\) only if \(a = 0\). Now suppose that \(au = \bar{0}\). We have two cases. Case one is when \(a = 0\) and we’re done. Case two is when \(a \neq 0\). If \(a \neq 0\), then $$ \begin{align*} \frac{1}{a}au &amp;= \frac{1}{a}\bar{0} \\ u &amp;= \bar{0} \end{align*} $$ But we know that \(u \neq \bar{0}\). Therefore \(a\) must be zero and we’re done. Example 2 Given a vector \(u_1, u_2\), prove that \(\{u_1, u_2\}\) is linearly dependent if and only if one vector is a scalar multiple of the other. Proof: For the forward direction, suppose that \(\{u_1, u_2\}\) is linearly dependent. This means that there are scalars (a_1, a_2) not all zero such that $$ \begin{align*} a_1u_1 + a_2u_2 &amp;= \bar{0} \end{align*} $$ Without the loss of generality, suppose that \(a_1 \neq 0\). Then, $$ \begin{align*} a_1u_1 + a_2u_2 &amp;= \bar{0} \\ a_1u_1 &amp;= - a_2u_2 \\ u_1 &amp;= -\frac{a_2}{a_1}u_2. \end{align*} $$ For the backward direction, suppose that one vector can be written as a scalar multiple of the other. Without the loss of generality, suppose it is \(u_1\). Then \(u_1 = cu_2\) where \(c \neq 0\), We can re-write this as follows, $$ \begin{align*} u_1 &amp;= cu_2 \\ u_1 + (-c)u_2 &amp;= \bar{0} \end{align*} $$ From this we see that there are scalars \(a_1 = 1\) and \(a_2 = -c\), not all zero such that \(a_1u_1 + a_2u_2 = \bar{0}\) which means that they are linearly dependent as we wanted to show. Example 3 Determine the set of the following vectors: $$ \begin{align*} \{u_1, u_2, u_3\} = \{(−1, 1, 2), (1, 2, 1), (5, 1, −4) \} \end{align*} $$ is linearly dependent. The set of vectors is linearly independent \(a_1u_1 + a_2u_2 + a_3u_3 = \bar{0}\) implies that \(a_1=0, a_2=0\) and \(a_3=0\). So, $$ \begin{align*} a_1u_1 + a_2u_2 + a_3u_3 &amp;= \bar{0} \\ a_1(−1, 1, 2) + a_2(1, 2, 1) + a_3(5, 1, −4) &amp;= (0,0,0) \\ (-a_1+a_2+5a_3, a_2+2a_2+a_3, 2a_1+2a_2-4a_3) &amp;= (0,0,0). \end{align*} $$ This is equivalent to solving the following system: $$ \begin{align*} -a_1 + a_2 + 5a_3 &amp;= 0 \\ a_2+2a_2+a_3 &amp;= 0 \\ 2a_1+2a_2-4a_3 &amp;= 0. \end{align*} $$ \(\{u_1, u_2, u_3\}\) is linearly independent if and only if this system has the trivial solution \((0,0,0)\). \((0,0,0)\) will always be a solution. It’s why it is called the trivial solution. So the question now is how do we know from the REF matrix, whether we have the \((0,0,0)\) solution or a non-zero solution? If the matrix has no column without a leading entry (besides last) then we have a unique solution and the set is linearly independent. If the matrix has a column (besides the last) with no leading entry then we have infinitely many solutions (so not just the zero vector) and the set is linearly dependent. So we want to make sure that all columns have leading entries. $$ \begin{align*} \begin{pmatrix} -1 &amp; 1 &amp; 5 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 0 \\ 2 &amp; 2 &amp; -4 &amp; 0 \end{pmatrix} \end{align*} $$ This will eventually be $$ \begin{align*} \begin{pmatrix} -1 &amp; 1 &amp; 5 &amp; 0 \\ 0 &amp; 3 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The third column has no leading entry so there are infinitely many solutions besides the zero solution this set is linearly dependent. Example 4 Consider \(\{ \sin(x), \cos(x)\} \in F(\mathbf{R})\). Is this set linearly dependent or independant? From the definition if \(\sin(x)\) was a non-zero scalar multiple of \(cos(x)\), in other words \(\sin(x) = a\cos(x)\) for some non-zero scalar \(a\), then they are linearly dependent. But two functions are equal when they take the same value at every point. Suppose we choose the point \(x = 0\), then we know that \(\sin(0) = 0\) but \(a\cos(0)=a\). Therefore, these functions are not equal and so the set is linearly independent. References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 2: Echelon Form and Reduced Echelon Form</title><link href="http://localhost:4000/jekyll/update/2024/07/23/elementary-matrices.html" rel="alternate" type="text/html" title="Lecture 2: Echelon Form and Reduced Echelon Form" /><published>2024-07-23T01:01:36-07:00</published><updated>2024-07-23T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/23/elementary-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/23/elementary-matrices.html"><![CDATA[<p>From lecture 2, we know that there are 3 types of elementary row operations:</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A\) be an \(m \times n\) matrix. Any one of the following three operations on the rows of \(A\) is called an <b>elementary row operation</b>:
<ol>
<li> interchanging any two rows of \(A\).</li>
<li> multiplying any row of \(A\) by a non-zero scalar.</li>
<li> adding any scalar multiple of a row of \(A\) to another row.</li>
</ol>
</div>
<p><br />
What’s more interesting is the next definition:
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of type 1, 2 or 3. according to whether the elementary operation performed on \(I_n\) is a type 1, 2 or 3 operation respectively.
</div>
<p><br />
What does the definition above mean? If we interchange the first two rows of \(I_3\), then this will produce the elementary matrix:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>But why this is useful? This is useful because if we have some matrix \(A\) and want to interchange the first two rows. We can perform this operation by instead multiply this new matrix above by \(A\). So we can turn our elementary row operations into a matrix multiplication! The next theorem states this fact.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 3.1
</div>
<div class="purbdiv">
Let \(A \in M_{m \times n}(F)\), and suppose that \(B\) is obtained from \(A\) by performing an elementary row operation. Then there exists an \(m \times n\) elementary matrix \(E\) such that \(B = EA\). In fact, \(E\) is obtained from \(I_m\) by performing the same elementary row operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m\) matrix, then \(EA\) is the matrix obtained from \(A\) by performing the same elementary row operation as that which produces \(E\) from \(I_m\).
</div>
<p><br />
This confirms what we said earlier. If we have a matrix \(A\) and we want to interchange the first two rows then we can first generate the matrix \(E\) from \(I_m\) by interchanging the first two rows:</p>
<div>
$$
\begin{align*}
E =
\begin{pmatrix}
0 &amp; 1 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>And then multiplying \(E\) by \(A\) to produce the same matrix produced from interchanging the first two rows in \(A\).  Finally we have this nice theorem to confirm that this elementary matrix is invertible!</p>
<div class="purdiv">
Theorem 3.2
</div>
<div class="purbdiv">
Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type.
</div>
<p><br />
<b>Proof:</b>
Let \(E\) be an elementary \(n \times n\) matrix. We know that that \(E\) was obtained by some elementary row operation on \(I_n\). We can reverse the steps used to transform \(I_n\) to obtain \(E\) in order to get back \(I_n\). To get \(I_n\) back, we had to use the same type of elementary row operation. By the previous theorem (3.1), this elementary row operation can be done using an elementary matrix \(\bar{E}\) such that \(\bar{E}E = I_n\). Therefore, \(E\) is invertible and \(E^{-1} = \bar{E}\) (By 2.4 Exercise 10!). \(\blacksquare\)</p>

<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241">Linear Algebra 5th Edition by Stephen Friedberg, Arnold Insel, Lawrence Spence</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[From lecture 2, we know that there are 3 types of elementary row operations: Definition Let \(A\) be an \(m \times n\) matrix. Any one of the following three operations on the rows of \(A\) is called an elementary row operation: interchanging any two rows of \(A\). multiplying any row of \(A\) by a non-zero scalar. adding any scalar multiple of a row of \(A\) to another row. What’s more interesting is the next definition: Definition An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of type 1, 2 or 3. according to whether the elementary operation performed on \(I_n\) is a type 1, 2 or 3 operation respectively. What does the definition above mean? If we interchange the first two rows of \(I_3\), then this will produce the elementary matrix: $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ But why this is useful? This is useful because if we have some matrix \(A\) and want to interchange the first two rows. We can perform this operation by instead multiply this new matrix above by \(A\). So we can turn our elementary row operations into a matrix multiplication! The next theorem states this fact. Theorem 3.1 Let \(A \in M_{m \times n}(F)\), and suppose that \(B\) is obtained from \(A\) by performing an elementary row operation. Then there exists an \(m \times n\) elementary matrix \(E\) such that \(B = EA\). In fact, \(E\) is obtained from \(I_m\) by performing the same elementary row operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m\) matrix, then \(EA\) is the matrix obtained from \(A\) by performing the same elementary row operation as that which produces \(E\) from \(I_m\). This confirms what we said earlier. If we have a matrix \(A\) and we want to interchange the first two rows then we can first generate the matrix \(E\) from \(I_m\) by interchanging the first two rows: $$ \begin{align*} E = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ And then multiplying \(E\) by \(A\) to produce the same matrix produced from interchanging the first two rows in \(A\). Finally we have this nice theorem to confirm that this elementary matrix is invertible! Theorem 3.2 Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type. Proof: Let \(E\) be an elementary \(n \times n\) matrix. We know that that \(E\) was obtained by some elementary row operation on \(I_n\). We can reverse the steps used to transform \(I_n\) to obtain \(E\) in order to get back \(I_n\). To get \(I_n\) back, we had to use the same type of elementary row operation. By the previous theorem (3.1), this elementary row operation can be done using an elementary matrix \(\bar{E}\) such that \(\bar{E}E = I_n\). Therefore, \(E\) is invertible and \(E^{-1} = \bar{E}\) (By 2.4 Exercise 10!). \(\blacksquare\)]]></summary></entry><entry><title type="html">Lecture 3: Gaussian Elimination</title><link href="http://localhost:4000/jekyll/update/2024/07/22/lec03-gaussian-elimination.html" rel="alternate" type="text/html" title="Lecture 3: Gaussian Elimination" /><published>2024-07-22T01:01:36-07:00</published><updated>2024-07-22T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/22/lec03-gaussian-elimination</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/22/lec03-gaussian-elimination.html"><![CDATA[<p>Gaussian Elimination consists of two passes. The forward pass is used to put the augmented matrix of linear equations into a Row Echelon Form. At this point, it will be clear if this system is inconsistent or has infinitely many solutions. The second stage of the algorithm is to do a backward pass on the augmented matrix to put it into a Reduced Row Echelon Form.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Suppose we have the following augmented matrix:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 2 &amp; 3 \\
1 &amp; 1 &amp; 1 &amp; 1 \\
3 &amp; 2 &amp; 1 &amp; 2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>The first stage of the algorithm is the forward pass. We want to put this matrix into Row Echelon Form which means we’ll want non-zero leading entries and zero out the entries below them. In row 1 above, we have a zero, so we’ll swap row 1 and 2.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
3 &amp; 2 &amp; 1 &amp; 2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next we want to zero out any entries below the leading entry. So we want to get rid of the 3 in column 3. To do that, we’ll multiply row 1 by -3 and add it to row 3. We write this as \(R_3 \rightarrow -3R_1 + R_3\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
0 &amp; -1 &amp; -2 &amp; -1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next, we want to ignore row 1 and move to the next row. In row 2, the next leading entry is in the most left non-zero column which is column 2. We’ll repeat the same process by zeroing the entries below it. This means we’ll want to get rid of the -1 in row 3. So we write \(R_3 \rightarrow R_2 + R_3\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We repeat this process until we have no more rows. At the end of this process, the matrix will be in Row Echelon Form. Notice here that this system is inconsistent because we have a leading entry in the very last column. 
<br />
<br />
The next phase of the algorithm is a backward pass to put the matrix in Reduced Row Echelon Form by making the leading entries 1 and zeroing out the non-leading entries in each column. We want to do this phase from right to left. For the last row, we need to multiply row 3 by 1/2 to make it 1.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next, we’ll zero out the remaining entries above it by performing the operations \(R_2 \rightarrow -3R_3 + R_2\) and \(R_1 \rightarrow -R_3 + R_1\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We then want to focus on the next leading entry in row 2 which happens to be in column 2. The leading entry is 1 already so we only need to zero out the 1 above it by performing \(R_1 = -R_2 + R_1\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Now this matrix is in Reduced Row Echelon Form.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Find all the solutions to the following system</p>
<div>
$$
\begin{align*}
  x_1 + 2x_2 - x_3  &amp;= 1 \\
  x_1 + x_2 - 2x_3  &amp;= 0 \\
  5x_1 + 8x_2 + x_3 &amp;= 1
\end{align*}
$$
</div>
<p>We’ll put the equations in an augmented matrix form:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
1 &amp; 1 &amp; 2 &amp; 0 \\
5 &amp; 8 &amp; 1 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next, we’ll perform Gaussian Elimination by first performing the forward pass which will put the matrix in Row Echelon. We’ll make the top most left entry non-zero and zero out the remaining entries below it. So we’ll perform the operations \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; -2 &amp; 6 &amp; -4
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We’ll repeat the same process for the next leading entry in row 2 which is -1. We’ll zero out the entries below it by applying \(R_3 \rightarrow R_3 - 2R_2\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; 0 &amp; 0 &amp; -2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Here we should know that that since there is a leading entry in the last column then this system is inconsistent and the system has no solutions and so we’re done.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Find all the solutions to the following system</p>
<div>
$$
\begin{align*}
x_1 + 2x_2 - x_3  &amp;= 1 \\
x_1 + x_2 - 2x_3 &amp;= 0 \\
5x_1 + 8x_2 + x_3 &amp;= 3
\end{align*}
$$
</div>
<p>We’ll put the equations in an augmented matrix form:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
1 &amp; 1 &amp; 2 &amp; 0 \\
5 &amp; 8 &amp; 1 &amp; 3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Step one is the forward pass. we’ll start with the top left most entry and make it a non-zero. it is already a non-zero entry. then we’ll zero out the remaining entries in that column by applying \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; -2 &amp; 6 &amp; -2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We’ll repeat this for the next leading entry in row 2 which is -1 and then zero out the below rows. We’ll apply \(R_3 \rightarrow R_3 - 2R_2\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We have the leading entries 1 and -1. We’re in Row Echelon Form. We can describe the solution set now. but first we want to put it in Reduced Row Echelon Form. so we’ll do the backward pass.
<br />
<br />
We’ll go right to left in the backward pass, making leading entries 1 and zeroing out the remaining entries in those columns. The first leading entry we’ll work on is -1 in row 2. We’ll make it a 1 by multiplying row 2 by -1 so \(R_2 \rightarrow -R_2\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; 1 &amp; -3 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>And then we need to zero out the remaining entries in the same column above that leading entry so apply \(R_1 = -2R_2 + R_1\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 5 &amp; -1 \\
0 &amp; 1 &amp; -3 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We finally are in RREF (columns with leading entries are zeroed out except for the leading entries and the leading entries are all 1s). We can now use the convention to use a variable to parameterize the solution set for the columns with no leading entries so let \(x_3 = t\). Based on this, we’ll see that</p>
<div>
$$
\begin{align*}
x_1 &amp;= -1-5t \\
x_2 &amp;= 1 + 3t
\end{align*}
$$
</div>
<p>Finally, we can write the solution set as</p>
<div>
$$
\begin{align*}
\{(-1-5t, 1+3t, t) | t \in \mathbf{R}\}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Gaussian Elimination consists of two passes. The forward pass is used to put the augmented matrix of linear equations into a Row Echelon Form. At this point, it will be clear if this system is inconsistent or has infinitely many solutions. The second stage of the algorithm is to do a backward pass on the augmented matrix to put it into a Reduced Row Echelon Form. Example 1 Suppose we have the following augmented matrix: $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 2 &amp; 3 \\ 1 &amp; 1 &amp; 1 &amp; 1 \\ 3 &amp; 2 &amp; 1 &amp; 2 \end{pmatrix}. \end{align*} $$ The first stage of the algorithm is the forward pass. We want to put this matrix into Row Echelon Form which means we’ll want non-zero leading entries and zero out the entries below them. In row 1 above, we have a zero, so we’ll swap row 1 and 2. $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 3 &amp; 2 &amp; 1 &amp; 2 \end{pmatrix}. \end{align*} $$ Next we want to zero out any entries below the leading entry. So we want to get rid of the 3 in column 3. To do that, we’ll multiply row 1 by -3 and add it to row 3. We write this as \(R_3 \rightarrow -3R_1 + R_3\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 0 &amp; -1 &amp; -2 &amp; -1 \end{pmatrix}. \end{align*} $$ Next, we want to ignore row 1 and move to the next row. In row 2, the next leading entry is in the most left non-zero column which is column 2. We’ll repeat the same process by zeroing the entries below it. This means we’ll want to get rid of the -1 in row 3. So we write \(R_3 \rightarrow R_2 + R_3\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}. \end{align*} $$ We repeat this process until we have no more rows. At the end of this process, the matrix will be in Row Echelon Form. Notice here that this system is inconsistent because we have a leading entry in the very last column. The next phase of the algorithm is a backward pass to put the matrix in Reduced Row Echelon Form by making the leading entries 1 and zeroing out the non-leading entries in each column. We want to do this phase from right to left. For the last row, we need to multiply row 3 by 1/2 to make it 1. $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ Next, we’ll zero out the remaining entries above it by performing the operations \(R_2 \rightarrow -3R_3 + R_2\) and \(R_1 \rightarrow -R_3 + R_1\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ We then want to focus on the next leading entry in row 2 which happens to be in column 2. The leading entry is 1 already so we only need to zero out the 1 above it by performing \(R_1 = -R_2 + R_1\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ Now this matrix is in Reduced Row Echelon Form. Example 2 Find all the solutions to the following system $$ \begin{align*} x_1 + 2x_2 - x_3 &amp;= 1 \\ x_1 + x_2 - 2x_3 &amp;= 0 \\ 5x_1 + 8x_2 + x_3 &amp;= 1 \end{align*} $$ We’ll put the equations in an augmented matrix form: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 1 &amp; 1 &amp; 2 &amp; 0 \\ 5 &amp; 8 &amp; 1 &amp; 1 \end{pmatrix}. \end{align*} $$ Next, we’ll perform Gaussian Elimination by first performing the forward pass which will put the matrix in Row Echelon. We’ll make the top most left entry non-zero and zero out the remaining entries below it. So we’ll perform the operations \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; -2 &amp; 6 &amp; -4 \end{pmatrix}. \end{align*} $$ We’ll repeat the same process for the next leading entry in row 2 which is -1. We’ll zero out the entries below it by applying \(R_3 \rightarrow R_3 - 2R_2\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; -2 \end{pmatrix}. \end{align*} $$ Here we should know that that since there is a leading entry in the last column then this system is inconsistent and the system has no solutions and so we’re done. Example 3 Find all the solutions to the following system $$ \begin{align*} x_1 + 2x_2 - x_3 &amp;= 1 \\ x_1 + x_2 - 2x_3 &amp;= 0 \\ 5x_1 + 8x_2 + x_3 &amp;= 3 \end{align*} $$ We’ll put the equations in an augmented matrix form: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 1 &amp; 1 &amp; 2 &amp; 0 \\ 5 &amp; 8 &amp; 1 &amp; 3 \end{pmatrix}. \end{align*} $$ Step one is the forward pass. we’ll start with the top left most entry and make it a non-zero. it is already a non-zero entry. then we’ll zero out the remaining entries in that column by applying \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; -2 &amp; 6 &amp; -2 \end{pmatrix}. \end{align*} $$ We’ll repeat this for the next leading entry in row 2 which is -1 and then zero out the below rows. We’ll apply \(R_3 \rightarrow R_3 - 2R_2\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ We have the leading entries 1 and -1. We’re in Row Echelon Form. We can describe the solution set now. but first we want to put it in Reduced Row Echelon Form. so we’ll do the backward pass. We’ll go right to left in the backward pass, making leading entries 1 and zeroing out the remaining entries in those columns. The first leading entry we’ll work on is -1 in row 2. We’ll make it a 1 by multiplying row 2 by -1 so \(R_2 \rightarrow -R_2\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; 1 &amp; -3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ And then we need to zero out the remaining entries in the same column above that leading entry so apply \(R_1 = -2R_2 + R_1\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; 5 &amp; -1 \\ 0 &amp; 1 &amp; -3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ We finally are in RREF (columns with leading entries are zeroed out except for the leading entries and the leading entries are all 1s). We can now use the convention to use a variable to parameterize the solution set for the columns with no leading entries so let \(x_3 = t\). Based on this, we’ll see that $$ \begin{align*} x_1 &amp;= -1-5t \\ x_2 &amp;= 1 + 3t \end{align*} $$ Finally, we can write the solution set as $$ \begin{align*} \{(-1-5t, 1+3t, t) | t \in \mathbf{R}\}. \end{align*} $$ References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 2: Echelon Form and Reduced Echelon Form</title><link href="http://localhost:4000/jekyll/update/2024/07/21/lec02-rref.html" rel="alternate" type="text/html" title="Lecture 2: Echelon Form and Reduced Echelon Form" /><published>2024-07-21T01:01:36-07:00</published><updated>2024-07-21T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/21/lec02-rref</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/21/lec02-rref.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A matrix is in Row Echelon Form (REF) if
<ol style="list-style-type:lower-alpha">
<li> All zero rows are below all nonzero rows.</li>
<li> The leading entry of of each row to be to the right of the leading entry of the row above.</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>For example, this matrix is in Row Echelon Form:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4  \\
0 &amp; 1 &amp; 1 &amp; 1  \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A matrix is in Reduced Row Echelon Form (RREF) if it is in REF and it satisfies the following two additional properties:
c) The leading entries are all 1.
d) The leading entries are the only non-zero entries in their column
<ol style="list-style-type:lower-alpha">
<li> The leading entries are all 1..</li>
<li> The leading entries are the only non-zero entries in their column.</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The same matrix from example 1 is not in RREF because column 2 for example still has non-zero entries above its leading entry.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4  \\
0 &amp; 1 &amp; 1 &amp; 1  \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
Every matrix can be put in RREF by a finite sequence of elementary row operations.
<ol>
	<li>\(R_i \leftrightarrow R_j \) </li>
    <li> \(cR_i\) where \(c \neq 0\)</li>
    <li> \(R_i \rightarrow R_i + cR_j\) </li>
</ol>
</div>
<p><br />
Proof: by construction (Gaussian Elimination) … TODO?
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
The reduced RREF form of a matrix is unique. (Note row echelon form is not unique).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 4
</div>
<div class="purbdiv">
The solution set of a linear system with augmented matrix in reduced row echelon form is easily described in a standard way.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Consider the system with augmented matrix as follows:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>The last row has a single non-zero entry in the last column. This tells us that this system will not have a solution. To see this, translate back the system of linear equations</p>
<div>
$$
\begin{array}{ll}
x_1 \quad -x_3 &amp;=0 \\ 
\quad x_2 &amp;=0 \\ 
\quad \quad \quad \quad 0 &amp;= 1
\end{array}
$$
</div>
<p>We can see here that \(0=1\) has no solutions.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Consider the system with augmented matrix as follows:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>The last row has has all zeros. This tells us that we will have infinitely many solutions. To see this, translate back the system of linear equations</p>
<div>
$$
\begin{array}{ll}
x_1 \quad -x_3 &amp;=0 \\ 
\quad x_2 &amp;=0 \\ 
\quad \quad \quad \quad 0 &amp;= 0
\end{array}
$$
</div>
<p>This time we have infinitely many solutions. The convention is to look for a column without a leading entry like Column 3. The convention is to use the third variable \(x_3\) to parameterize the solution set. so set \(x_3 = t\), then</p>
<div>
$$
\begin{align*}
x_3 &amp;= t \\
x_1 &amp;= t \\
x_2 &amp;= 0.
\end{align*}
$$
</div>
<p>The solution set is \((x_1,x_2,x_3) = \{(t, t, 0) | t \in \mathbf{R}\}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>Consider the system with augmented matrix as follows:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; 1 &amp; 0 &amp; 2  \\
0 &amp; 0 &amp; 1 &amp; 3 &amp; 0 &amp; 3  \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 4  \\
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This is a system of three equations in 5 unknowns. Column one is \(x_1\), column two is \(x_2\) and so on. This matrix is also in Row Reduced Echelon Form because the leading entries are 1 and the leading entries are the only non-zero entries in their columns. Also, we don’t have a leading entry in the very last column so we do have a solution. This translates to the system of linear:</p>
<div>
$$
\begin{array}{ll}
x_1 + 2x_2 \quad \quad + x_4 &amp;= 2 \\ 
\ \ \ \ \ \ \ \quad \quad \quad x_3 + 3x_4 &amp;= 3 \\ 
\quad \quad \quad \quad \quad \quad \quad \quad \quad x_5 &amp;= 4
\end{array}
$$
</div>
<p>We have infinitely many solutions. So we’ll go by the convention which is to look for the columns without a leading entries and assign them variables. Here, columns 2 and 4 have no leading entries so set \(x_2 = t_1\) and \(x_4 = t_2\). This means that \(x_3 = 3 - 3t_2\) and \(x_1 = 2 - 2t_1 - t_2\). Writing this a solution vector:</p>
<div>
$$
\begin{align*}
\{(2-2t_1-t_2, t_1, 3-3t_2, t_2, 4) | t_1, t_2 \in \mathbf{R} \}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
a system of linear equations has no solutions, exactly one solution (no parameters), or infinitely many solutions.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A matrix is in Row Echelon Form (REF) if All zero rows are below all nonzero rows. The leading entry of of each row to be to the right of the leading entry of the row above. Example 1 For example, this matrix is in Row Echelon Form: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ Definition A matrix is in Reduced Row Echelon Form (RREF) if it is in REF and it satisfies the following two additional properties: c) The leading entries are all 1. d) The leading entries are the only non-zero entries in their column The leading entries are all 1.. The leading entries are the only non-zero entries in their column. Example 2 The same matrix from example 1 is not in RREF because column 2 for example still has non-zero entries above its leading entry. $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ Theorem 2 Every matrix can be put in RREF by a finite sequence of elementary row operations. \(R_i \leftrightarrow R_j \) \(cR_i\) where \(c \neq 0\) \(R_i \rightarrow R_i + cR_j\) Proof: by construction (Gaussian Elimination) … TODO? Theorem 3 The reduced RREF form of a matrix is unique. (Note row echelon form is not unique). Theorem 4 The solution set of a linear system with augmented matrix in reduced row echelon form is easily described in a standard way. Example 3 Consider the system with augmented matrix as follows: $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ The last row has a single non-zero entry in the last column. This tells us that this system will not have a solution. To see this, translate back the system of linear equations $$ \begin{array}{ll} x_1 \quad -x_3 &amp;=0 \\ \quad x_2 &amp;=0 \\ \quad \quad \quad \quad 0 &amp;= 1 \end{array} $$ We can see here that \(0=1\) has no solutions. Example 4 Consider the system with augmented matrix as follows: $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ The last row has has all zeros. This tells us that we will have infinitely many solutions. To see this, translate back the system of linear equations $$ \begin{array}{ll} x_1 \quad -x_3 &amp;=0 \\ \quad x_2 &amp;=0 \\ \quad \quad \quad \quad 0 &amp;= 0 \end{array} $$ This time we have infinitely many solutions. The convention is to look for a column without a leading entry like Column 3. The convention is to use the third variable \(x_3\) to parameterize the solution set. so set \(x_3 = t\), then $$ \begin{align*} x_3 &amp;= t \\ x_1 &amp;= t \\ x_2 &amp;= 0. \end{align*} $$ The solution set is \((x_1,x_2,x_3) = \{(t, t, 0) | t \in \mathbf{R}\}\). Example 5 Consider the system with augmented matrix as follows: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; 1 &amp; 0 &amp; 2 \\ 0 &amp; 0 &amp; 1 &amp; 3 &amp; 0 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 4 \\ \end{pmatrix}. \end{align*} $$ This is a system of three equations in 5 unknowns. Column one is \(x_1\), column two is \(x_2\) and so on. This matrix is also in Row Reduced Echelon Form because the leading entries are 1 and the leading entries are the only non-zero entries in their columns. Also, we don’t have a leading entry in the very last column so we do have a solution. This translates to the system of linear: $$ \begin{array}{ll} x_1 + 2x_2 \quad \quad + x_4 &amp;= 2 \\ \ \ \ \ \ \ \ \quad \quad \quad x_3 + 3x_4 &amp;= 3 \\ \quad \quad \quad \quad \quad \quad \quad \quad \quad x_5 &amp;= 4 \end{array} $$ We have infinitely many solutions. So we’ll go by the convention which is to look for the columns without a leading entries and assign them variables. Here, columns 2 and 4 have no leading entries so set \(x_2 = t_1\) and \(x_4 = t_2\). This means that \(x_3 = 3 - 3t_2\) and \(x_1 = 2 - 2t_1 - t_2\). Writing this a solution vector: $$ \begin{align*} \{(2-2t_1-t_2, t_1, 3-3t_2, t_2, 4) | t_1, t_2 \in \mathbf{R} \}. \end{align*} $$ Corollary a system of linear equations has no solutions, exactly one solution (no parameters), or infinitely many solutions. References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 6: Span of a Subset</title><link href="http://localhost:4000/jekyll/update/2024/07/20/lec06-span-of-a-subset.html" rel="alternate" type="text/html" title="Lecture 6: Span of a Subset" /><published>2024-07-20T01:01:36-07:00</published><updated>2024-07-20T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/20/lec06-span-of-a-subset</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/20/lec06-span-of-a-subset.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A linear combination of the vectors \(u_1,...,u_k\) in \(V\) is a vector of the form
$$
\begin{align*}
a_1u_1 + a_2u_2 + ... + a_ku_k, \quad \text{where $a_1, a_2, ... a_k \in \mathbf{R}$}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>\((4,3) \in \mathbf{R}^2\). This vector can be written as</p>
<div>
$$
\begin{align*}
(4,3) = 4(1,0) + 3(0,1).
\end{align*}
$$
</div>
<p>So \((4,3)\) is a linear combination of \((1,0)\) and \((0,1)\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>\(x^3 + 3x + \pi \in P_3\). in a linear combination of \(x^3, x^2, x, 1\) with \(a_1 = 1, a_2 = 0, a_3 = 3, a_4 = \pi\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Span of of a Subset</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(V\) be a vector space and let \(S \in V\) be a subset. The span of \(S\) is the set of all linear combinations of elements in \(S\).
$$
\begin{align*}
Span(S) = \{a_1u_1 + a_2u_2 + ... + a_ku_k \quad|\quad a_1, a_2, ... a_k \in \mathbf{R}, u_1, u_2,...,u_k \in S\}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Let \(S = \{\bar{0}\}\), then \(Span(S) = \{\bar{0}\}.\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Let \(S = V\), then \(Span(S) = V.\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>Let \(S=\{(1,0),(0,1)\}\). \(S \subset \mathbf{R}^2\). Then,</p>
<div>
$$
\begin{align*}
Span(S) &amp;= \{a_1(1,0) + a_2(0,1) \ | \ a_1, a_2 \in \mathbf{R}\} \\
        &amp;= \{(a_1, a_2) \ | \ a_1, a_2 \in \mathbf{R}\} \\
		&amp;= \mathbf{R}^2.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>Let \(S=\{(1,m)\}\). \(S \subset \mathbf{R}^2\). Then,</p>
<div>
$$
\begin{align*}
Span(S) &amp;= \{a_1(1,m) \ | \ a_1 \in \mathbf{R}\} \\
        &amp;= \{(a_1, ma_1) \ | \ a_1 \in \mathbf{R}\} \\
		&amp;= L_m.
\end{align*}
$$
</div>
<p>This is the line through the origin with slope \(m\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 7</b></h4>
<p>Let \(S=\{(1,0,0),(0,0,1)\}\). \(S \subset \mathbf{R}^3\). Then,</p>
<div>
$$
\begin{align*}
Span(S) &amp;= \{a_1(1,0,0) + a_2(0,0,1) \ | \ a_1, a_2 \in \mathbf{R}\} \\
        &amp;= \{(a_1, 0, a_2) \ | \ a_1, a_2 \in \mathbf{R}\} \\
\end{align*}
$$
</div>
<p>This is the \(xz\)-plane in \(\mathbf{R}^3\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 
</div>
<div class="purbdiv">
For any nonempty subset \(S \subset V\), \(Span(S)\) is a subspace of \(V\).
</div>
<p><br />
Proof: Let \(S \subset V\) where \(V\) is a vector space. By definition we know that \(Span(S)\)</p>
<div>
$$
\begin{align*}
\{a_1u_1 + a_2u_2 + ... + a_ku_k \ | \ a_1, a_2, ... a_k \in \mathbf{R}, u_1, u_2,...u_k \in S\}
\end{align*}
$$
</div>
<p>We will show that \(Span(S)\) is a subspace of (V) by verifying the three Subspace conditions.</p>
<ol style="list-style-type:lower-alpha">

<li>We need to show that \(\bar{0} \in Span(S)\). \(S\) is nonempty so we can choose some vector \(u \in S\). Since all linear combinations of \(u\) are in \(Span(S)\), then \(0u \in Span(S)\). But \(0u = \bar{0}\). Therefore, \(\bar{0} \in Span(S)\) and we're done.</li>


<li>We need to show that \(Span(S)\) is closed under addition. Suppose \(u, v \in Span(S)\). We need to show that \(u + v \in Span(S)\). We can write \(u\) and \(v\) as follows 
<div>
$$
\begin{align*}
u &amp;= a_1u_1 + a_2u_2 + ... + a_ku_k \\
v &amp;= b_1u_1 + b_2u_2 + ... + b_ku_k.
\end{align*}
$$
</div>
Then the sum can be written as,
<div>
$$
\begin{align*}
u+v &amp;= (a_1+b_1)u_1 + (a_2+b_2)u_2 + ... + (a_k+b_k)u_k.
\end{align*}
$$
</div>
So \(u+v \in Span(S)\) as we wanted to show.
</li>

<li>We need to show that \(Span(S)\) is closed under scalar multiplication. Similar to the previous argument we can see that
<div>
$$
\begin{align*}
cu &amp;= (ca_1)u_1 + (ca_2)u_2 + ... + (ca_k)u_k.
\end{align*}
$$
</div>
So \(cu \in Span(S)\) as required.
</li>
</ol>
<p>From (a), (b), (c), we can conclude that \(Span(S)\) is a subspace of \(V\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 7</b></h4>
<p>Is \(x^3 - 3x + 5\) in \(Span(\{x^3 + 2x^2 - x + 1, x^3 + 3x^2 - 1\})\)?
<br />
<br />
For \(x^3 - 3x + 5\) to be in the span, this means that \(x^3 - 3x + 5\) can be written as a linear combinations of what’s inside the set. In other words, there exists \(a_1, a_2 \in \mathbf{R}\) such that</p>
<div>
$$
\begin{align*}
x^3 - 3x + 5 &amp;= a_1(x^3 + 2x^2 - x + 1) + a_2(x^3 + 3x^2 - 1) \\
			 &amp;= (a_1+a_2)x^3 + (2a_1 - 3a_3)x^2 + (-a_1)x + (a_1-a_2)
\end{align*}
$$
</div>
<p>These equations are the same when the coefficients are equal. So equivalently we can solve the following system of equations:</p>
<div>
$$
\begin{align*}
a_1 + a_2 = 1 \\
2a_2 + 3a_2 = 0 \\
-a_1 = -3 \\
a_1 - a_2 = 5.
\end{align*}
$$
</div>
<p>We can use Gaussian Elimination to solve this system by doing a forward pass followed by a backward pass.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1  \\
2 &amp; 3 &amp; 0  \\
-1 &amp; 0 &amp; -3 \\
1 &amp; -1 &amp; 5
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We’ll put the matrix in Row Echelon Form. Starting with zeroing out the entries below the first leading entry in the first column.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1  \\
0 &amp; 1 &amp; -2  \\
0 &amp; 0 &amp; -2 \\
0 &amp; -2 &amp; 4
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Continuing with the next leading entry to get.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1  \\
0 &amp; 1 &amp; -2  \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This tells us that there is a solution so the answer is yes, we can write it \(x^3 -3x + 5\) is in the span above. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(S \subset V\) generates \(V\) if \(Span(S)=V\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 8</b></h4>
<p>The set \(\{(1,0),(0,1)\}\) generates \(\mathbf{R}^2\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 9</b></h4>
<p>The following set</p>
<div>
$$
\begin{align*}
\left\{
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{pmatrix},
\begin{pmatrix}
0 &amp; 0 \\
0 &amp; 1
\end{pmatrix},
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix}
\right\}
\end{align*}
$$
</div>
<p>generates the vector space of 2x2 symmetric matrices. How do we go about proving this?
<br />
Consider the matrix:</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}
\end{pmatrix},
\end{align*}
$$
</div>
<p>If \(A\) was symmetric, then we must have \(a_{12} = a_{21}\) or \(a_{ij} = a_{ji}\) if \(i \neq j\). This means we can re-write \(A\) as</p>
<div>
$$
\begin{align*}
a_{11}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{pmatrix}
+ a_{12}
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix}
+ a_{22}
\begin{pmatrix}
0 &amp; 0 \\
0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A linear combination of the vectors \(u_1,...,u_k\) in \(V\) is a vector of the form $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_ku_k, \quad \text{where $a_1, a_2, ... a_k \in \mathbf{R}$}. \end{align*} $$ Example 1 \((4,3) \in \mathbf{R}^2\). This vector can be written as $$ \begin{align*} (4,3) = 4(1,0) + 3(0,1). \end{align*} $$ So \((4,3)\) is a linear combination of \((1,0)\) and \((0,1)\). Example 2 \(x^3 + 3x + \pi \in P_3\). in a linear combination of \(x^3, x^2, x, 1\) with \(a_1 = 1, a_2 = 0, a_3 = 3, a_4 = \pi\). Span of of a Subset Definition Let \(V\) be a vector space and let \(S \in V\) be a subset. The span of \(S\) is the set of all linear combinations of elements in \(S\). $$ \begin{align*} Span(S) = \{a_1u_1 + a_2u_2 + ... + a_ku_k \quad|\quad a_1, a_2, ... a_k \in \mathbf{R}, u_1, u_2,...,u_k \in S\}. \end{align*} $$ Example 3 Let \(S = \{\bar{0}\}\), then \(Span(S) = \{\bar{0}\}.\) Example 4 Let \(S = V\), then \(Span(S) = V.\) Example 5 Let \(S=\{(1,0),(0,1)\}\). \(S \subset \mathbf{R}^2\). Then, $$ \begin{align*} Span(S) &amp;= \{a_1(1,0) + a_2(0,1) \ | \ a_1, a_2 \in \mathbf{R}\} \\ &amp;= \{(a_1, a_2) \ | \ a_1, a_2 \in \mathbf{R}\} \\ &amp;= \mathbf{R}^2. \end{align*} $$ Example 6 Let \(S=\{(1,m)\}\). \(S \subset \mathbf{R}^2\). Then, $$ \begin{align*} Span(S) &amp;= \{a_1(1,m) \ | \ a_1 \in \mathbf{R}\} \\ &amp;= \{(a_1, ma_1) \ | \ a_1 \in \mathbf{R}\} \\ &amp;= L_m. \end{align*} $$ This is the line through the origin with slope \(m\). Example 7 Let \(S=\{(1,0,0),(0,0,1)\}\). \(S \subset \mathbf{R}^3\). Then, $$ \begin{align*} Span(S) &amp;= \{a_1(1,0,0) + a_2(0,0,1) \ | \ a_1, a_2 \in \mathbf{R}\} \\ &amp;= \{(a_1, 0, a_2) \ | \ a_1, a_2 \in \mathbf{R}\} \\ \end{align*} $$ This is the \(xz\)-plane in \(\mathbf{R}^3\). Theorem For any nonempty subset \(S \subset V\), \(Span(S)\) is a subspace of \(V\). Proof: Let \(S \subset V\) where \(V\) is a vector space. By definition we know that \(Span(S)\) $$ \begin{align*} \{a_1u_1 + a_2u_2 + ... + a_ku_k \ | \ a_1, a_2, ... a_k \in \mathbf{R}, u_1, u_2,...u_k \in S\} \end{align*} $$ We will show that \(Span(S)\) is a subspace of (V) by verifying the three Subspace conditions.]]></summary></entry><entry><title type="html">Lecture 5: Subspaces</title><link href="http://localhost:4000/jekyll/update/2024/07/19/lec05-subspaces.html" rel="alternate" type="text/html" title="Lecture 5: Subspaces" /><published>2024-07-19T01:01:36-07:00</published><updated>2024-07-19T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/19/lec05-subspaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/19/lec05-subspaces.html"><![CDATA[<p>Consider \(\mathbf{R}^2 = \{(x,y) | x,y \in \mathbf{R}\}\) with component wise addition and component wise scalar multiplication defined in the previous lecture. This is a vector space. Now, consider a line \(L_m\) through the origin.</p>
<div>
$$
\begin{align*}
L_m &amp;= \{(x,y) | y = mx\} = \{(x, mx) | x \in \mathbf{R}\}.
\end{align*}
$$
</div>
<p>If we add two vectors in \(L_m\), the result is in \(L_m\).</p>
<div>
$$
\begin{align*}
(x, mx) + (\tilde{x}, m\tilde{x}) = (x + \tilde{x}, mx + m\tilde{x}) 
                                  &amp;= (x + \tilde{x}, m(x + \tilde{x})).
\end{align*}
$$
</div>
<p>Similarly, scalar multiplication also preserves \(L_m\). So \(L_m\) seems to inherit the same structure of a vector space from \(\mathbf{R}^2\). Let’s introduce the following definition</p>
<div class="bdiv">
  Definition
</div>
<div class="bbdiv">
  Let \(W\) be a subset of a vector space \(V\). \(W\) is a subspace if
  <ol style="list-style-type:lower-alpha">
      <li>\(\bar{0} \in W\).</li>
	  <li>For any \(w_1, w_2 \in W\). \(w_1 + w_2 \in W\). (\(W\) is closed under addition)</li>
      <li>For any \(c \in \mathbf{R}\) and \(w \in W\), \(cw \in W\). (\(W\) is closed under scalar multiplication).</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1: \([0, 1]\)</b></h4>
<p>Let \(W = [0, 1] \subset \mathbf{R}\). It is not closed under addition. For example, for \(w_1 = 1\) and \(w_2 = 1\), \(w_1 + w_2 \notin \mathbf{R}\). It is not closed under scalar multiplication either. \(\bar{0} \in [0,1]\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2: \(\mathbf{Z}\)</b></h4>
<p>\(\bar{0} \in [0,1]\). It is closed under addition. But it is not closed under scalar multiplication because we defined scalar multiplication as \(\{c(x, y) = (cx, cy) | c \in \mathbf{R}\}\). (Remember that the vector spaces we’re defining are over \(\mathbf{R}\)).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Subspaces are Vector Spaces</b></h4>
<div class="purdiv">
  Theorem
</div>
<div class="purbdiv">
  If \(W \subset V\) is a subspace and \(V\) is a vector space, then \(W\) is a vector space with the operations are inherited from \(V\).
</div>
<p><br />
<b>Proof:</b>
Let \(V\) be a vector space and \(W \subset V\). Since \(V\) is a vector space, then we know it satisfies conditions (1)-(8) of vector spaces. We also know that \(W\) satisfies properties (a)-(c) since it’s a subspace of \(V\). To verify that \(W\) is a vector space, we need to show that it satisfies conditions (1)-(8) of vector spaces.
<br />
<br />
Conditions (1)-(2) and (5)-(8) are inherited from \(V\). For condition (3), we need a \(\bar{0}\). But we already we know that there exists a zero element since it’s property (1) in the subspaces definition. For property (4), we need to prove that for each \(w \in W\), there exists a \(z \in W\) such that \(w + z = \bar{0}\). But we know there is a \(u \in V\) such that \(w + u = \bar{0}\) since \(V\) is a vector space. We also know that \(u = (-1)w\) (In the previous lecture we proved that \(u\) is unique). But \(u=(-1)w\) must also be in \(W\) since \(W\) is closed under scalar multiplication.
Therefore \(W\) is a vector space as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Show that \(W = \{(t_1 + t_2, t_1 - t_2, t_1) | t_1, t_2 \in \mathbf{R}\}\) is a vector space.
<br />
<br />
Proof: \(W\) is a subset of \(\mathbf{R}^3\) so by the previous theorem, it suffices to show that \(W\) is a subspace of \(\mathbf{R}^3\). For condition one, set \(t_1\) and \(t_2\) to 0 to get \(\bar{0} = (0,0,0) \in W\). For any \(w \in W\), we see that \(w + \bar{0} = w + (0,0,0) = w\) as required. For condition 2, let \(w_1, w_2 \in W\).</p>
<div>
$$
\begin{align*}
w_1 + w_2 &amp;= (t_1 + t_2, t_1 - t_2, t_1) + (s_1 + s_2, s_1 - s_2, s_1) \\
          &amp;= (t_1 + t_2 + s_1 + s_2, t_1 - t_2 + s_1 + s_2, t_1 + s_1) \\
		  &amp;= ((t_1 + s_1) + (t_2 + s_2), (t_1 + s_1) - (t_2 + s_2), (t_1 + s_1)).
\end{align*}
$$
</div>
<p>Therefore, \(w_1 + w_2 \in W\). For condition 3, let \(w \in W\) and \(c \in \mathbf{R}\),</p>
<div>
$$
\begin{align*}
cw &amp;= c(t_1 + t_2, t_1 - t_2, t_1) \\
   &amp;= ( c(t_1 + t_2), ct_1 - ct_2, ct_1).
\end{align*}
$$
</div>
<p>This is also clearly in \(W\), Therefore, we can conclude that \(W\) is a vector space. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Are Solution Sets Vector Spaces?</b></h4>
<p>From the previous example, \(W\) has the form of a solution set of a linear system so does this mean that every solution set is a vector space? The answer is no. Consider \(W' = \{t_1 + t_2, t_1 - t_2, t_1 + 1\}\). \(W'\) is not a subspace of \(\mathbf{R}^3\). We don’t have a zero vector \(\bar{0}\) in \(W\). To show that, we need to prove that there isn’t a solution for the system \(\{t_1 + t_2, t_1 - t_2, t_1 + 1\}\) where \(t_1+t_2 = 0\), \(t_1 - t_2 = 0\) and \(t_1 + 1 = 0\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4: Polynomials of Degree at Most \(k\)</b></h4>
<p>Consider \(P_k\) which is the set of all polynomials of degree at most \(k\). \(P_k\) is a subspace of \(P_{k+1}\). To show this, we know that \(P_{k+1}\) is a vector space so now we just need to prove the three conditions of subspaces. The zero polynomial is in \(P_k\). For condition 2, if we take any two polynomials in \(k\), then</p>
<div>
$$
\begin{align*}
(a_kx^k + a_{k-1}x^{k-1} + ... + a_0) + (b_kx^k + b_{k-1}x^{k-1} + ... + b_0) = \\
(a_k + b_k)x^k + (a_{k-1}+b_{k-1})x^{k-1} + ... + (a_0+b_0).
\end{align*}
$$
</div>
<p>The result is in \(P_k\) which means that \(P_k\) is closed under addition. For condition 3, take a polynomial in \(P_k\) and a scalar \(c \in \mathbf{R}\), then</p>
<div>
$$
\begin{align*}
c(a_kx^k + a_{k-1}x^{k-1} + ... + a_0) =  (ca_k)x^k + (ca_{k-1})x^{k-1} + ... + (ca_0).
\end{align*}
$$
</div>
<p>This is also in \(P_k\) which means that \(P_k\) is closed under scalar multiplication. Therefore, \(P_k\) is a subspace of \(P_{k+1}\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4: Continuous Functions</b></h4>
<p>The claim is that \(C^0(\mathbf{R})\) is a subspace of \(F(\mathbf{R})\). For the zero vector, \(\bar{0}(x) = 0\) is a continuous function so it belongs to \(C^0(\mathbf{R})\). For condition 2, if \(f\) and \(g\) are continuous then we know that \(f+g\) is also continuous (from calculus, needs to be proved). For condition 3, a scalar multiplied by \(f\) is also continuous. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5: Continuous Functions</b></h4>
<p>Consider the map transpose: \(M_{m \times n} \rightarrow M_{n \times m}\).</p>
<div>
$$
\begin{align*}
A = (a_{ij}) \rightarrow A^t = (a_{ij} = a_{ji}).
\end{align*}
$$
</div>
<p>Example:</p>
<div>
$$
\begin{align*}
\begin{bmatrix}
a &amp; b \\
c &amp; d
\end{bmatrix}^t
= 
\begin{bmatrix}
a &amp; c \\
b &amp; d
\end{bmatrix}
\end{align*}
$$
</div>
<div class="bdiv">
  Definition
</div>
<div class="bbdiv">
  \(A \in M_{n \times n}\) is symmetric if \(A = A^t\).
</div>
<p><br />
We can show that the set of symmetric matrices in \(M_{n \times n}\) is a subspace. Let the set of symmetric matrices in \(M_{n \times n}\) be \(S\). To show that \(S\) is a subspace, we’ll verify the three conditions</p>
<ol style="list-style-type:lower-alpha">
<li>\(\bar{0} = \begin{bmatrix} 0... &amp;... 0 \\ 0... &amp;... 0 \end{bmatrix}^t\) is symmetric and therefore it is in \(S\)</li>
<li>The sum of two symmetric matrices is symmetric and therefore it is in \(S\) and so \(S\) is closed under addition.
The sum is symmetric because given two symmetric matrices \(A\) and \(B\), their component wise sum is \(a_{ij}+b_{ij}\). But then we know that
<div>
$$
\begin{align*}
a_{ij} + b_{ij} = a_{ji} + b_{ji}.
\end{align*}
$$
</div>
Since both matrices are symmetric. But this implies that \(A+B = (A+B)^t\).
</li>
<li>For any \(c \in \mathbf{R}\) and \(s_1 \in S\), \(cS\) is a symmetric matrix and therefore it is in \(S\). So (\(S\) is closed under scalar multiplication).</li>
</ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Consider \(\mathbf{R}^2 = \{(x,y) | x,y \in \mathbf{R}\}\) with component wise addition and component wise scalar multiplication defined in the previous lecture. This is a vector space. Now, consider a line \(L_m\) through the origin. $$ \begin{align*} L_m &amp;= \{(x,y) | y = mx\} = \{(x, mx) | x \in \mathbf{R}\}. \end{align*} $$ If we add two vectors in \(L_m\), the result is in \(L_m\). $$ \begin{align*} (x, mx) + (\tilde{x}, m\tilde{x}) = (x + \tilde{x}, mx + m\tilde{x}) &amp;= (x + \tilde{x}, m(x + \tilde{x})). \end{align*} $$ Similarly, scalar multiplication also preserves \(L_m\). So \(L_m\) seems to inherit the same structure of a vector space from \(\mathbf{R}^2\). Let’s introduce the following definition Definition Let \(W\) be a subset of a vector space \(V\). \(W\) is a subspace if \(\bar{0} \in W\). For any \(w_1, w_2 \in W\). \(w_1 + w_2 \in W\). (\(W\) is closed under addition) For any \(c \in \mathbf{R}\) and \(w \in W\), \(cw \in W\). (\(W\) is closed under scalar multiplication). Example 1: \([0, 1]\) Let \(W = [0, 1] \subset \mathbf{R}\). It is not closed under addition. For example, for \(w_1 = 1\) and \(w_2 = 1\), \(w_1 + w_2 \notin \mathbf{R}\). It is not closed under scalar multiplication either. \(\bar{0} \in [0,1]\). Example 2: \(\mathbf{Z}\) \(\bar{0} \in [0,1]\). It is closed under addition. But it is not closed under scalar multiplication because we defined scalar multiplication as \(\{c(x, y) = (cx, cy) | c \in \mathbf{R}\}\). (Remember that the vector spaces we’re defining are over \(\mathbf{R}\)). Subspaces are Vector Spaces Theorem If \(W \subset V\) is a subspace and \(V\) is a vector space, then \(W\) is a vector space with the operations are inherited from \(V\). Proof: Let \(V\) be a vector space and \(W \subset V\). Since \(V\) is a vector space, then we know it satisfies conditions (1)-(8) of vector spaces. We also know that \(W\) satisfies properties (a)-(c) since it’s a subspace of \(V\). To verify that \(W\) is a vector space, we need to show that it satisfies conditions (1)-(8) of vector spaces. Conditions (1)-(2) and (5)-(8) are inherited from \(V\). For condition (3), we need a \(\bar{0}\). But we already we know that there exists a zero element since it’s property (1) in the subspaces definition. For property (4), we need to prove that for each \(w \in W\), there exists a \(z \in W\) such that \(w + z = \bar{0}\). But we know there is a \(u \in V\) such that \(w + u = \bar{0}\) since \(V\) is a vector space. We also know that \(u = (-1)w\) (In the previous lecture we proved that \(u\) is unique). But \(u=(-1)w\) must also be in \(W\) since \(W\) is closed under scalar multiplication. Therefore \(W\) is a vector space as we wanted to show. \(\blacksquare\) Example 3 Show that \(W = \{(t_1 + t_2, t_1 - t_2, t_1) | t_1, t_2 \in \mathbf{R}\}\) is a vector space. Proof: \(W\) is a subset of \(\mathbf{R}^3\) so by the previous theorem, it suffices to show that \(W\) is a subspace of \(\mathbf{R}^3\). For condition one, set \(t_1\) and \(t_2\) to 0 to get \(\bar{0} = (0,0,0) \in W\). For any \(w \in W\), we see that \(w + \bar{0} = w + (0,0,0) = w\) as required. For condition 2, let \(w_1, w_2 \in W\). $$ \begin{align*} w_1 + w_2 &amp;= (t_1 + t_2, t_1 - t_2, t_1) + (s_1 + s_2, s_1 - s_2, s_1) \\ &amp;= (t_1 + t_2 + s_1 + s_2, t_1 - t_2 + s_1 + s_2, t_1 + s_1) \\ &amp;= ((t_1 + s_1) + (t_2 + s_2), (t_1 + s_1) - (t_2 + s_2), (t_1 + s_1)). \end{align*} $$ Therefore, \(w_1 + w_2 \in W\). For condition 3, let \(w \in W\) and \(c \in \mathbf{R}\), $$ \begin{align*} cw &amp;= c(t_1 + t_2, t_1 - t_2, t_1) \\ &amp;= ( c(t_1 + t_2), ct_1 - ct_2, ct_1). \end{align*} $$ This is also clearly in \(W\), Therefore, we can conclude that \(W\) is a vector space. \(\blacksquare\) Are Solution Sets Vector Spaces? From the previous example, \(W\) has the form of a solution set of a linear system so does this mean that every solution set is a vector space? The answer is no. Consider \(W' = \{t_1 + t_2, t_1 - t_2, t_1 + 1\}\). \(W'\) is not a subspace of \(\mathbf{R}^3\). We don’t have a zero vector \(\bar{0}\) in \(W\). To show that, we need to prove that there isn’t a solution for the system \(\{t_1 + t_2, t_1 - t_2, t_1 + 1\}\) where \(t_1+t_2 = 0\), \(t_1 - t_2 = 0\) and \(t_1 + 1 = 0\). Example 4: Polynomials of Degree at Most \(k\) Consider \(P_k\) which is the set of all polynomials of degree at most \(k\). \(P_k\) is a subspace of \(P_{k+1}\). To show this, we know that \(P_{k+1}\) is a vector space so now we just need to prove the three conditions of subspaces. The zero polynomial is in \(P_k\). For condition 2, if we take any two polynomials in \(k\), then $$ \begin{align*} (a_kx^k + a_{k-1}x^{k-1} + ... + a_0) + (b_kx^k + b_{k-1}x^{k-1} + ... + b_0) = \\ (a_k + b_k)x^k + (a_{k-1}+b_{k-1})x^{k-1} + ... + (a_0+b_0). \end{align*} $$ The result is in \(P_k\) which means that \(P_k\) is closed under addition. For condition 3, take a polynomial in \(P_k\) and a scalar \(c \in \mathbf{R}\), then $$ \begin{align*} c(a_kx^k + a_{k-1}x^{k-1} + ... + a_0) = (ca_k)x^k + (ca_{k-1})x^{k-1} + ... + (ca_0). \end{align*} $$ This is also in \(P_k\) which means that \(P_k\) is closed under scalar multiplication. Therefore, \(P_k\) is a subspace of \(P_{k+1}\). Example 4: Continuous Functions The claim is that \(C^0(\mathbf{R})\) is a subspace of \(F(\mathbf{R})\). For the zero vector, \(\bar{0}(x) = 0\) is a continuous function so it belongs to \(C^0(\mathbf{R})\). For condition 2, if \(f\) and \(g\) are continuous then we know that \(f+g\) is also continuous (from calculus, needs to be proved). For condition 3, a scalar multiplied by \(f\) is also continuous. Example 5: Continuous Functions Consider the map transpose: \(M_{m \times n} \rightarrow M_{n \times m}\). $$ \begin{align*} A = (a_{ij}) \rightarrow A^t = (a_{ij} = a_{ji}). \end{align*} $$ Example: $$ \begin{align*} \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}^t = \begin{bmatrix} a &amp; c \\ b &amp; d \end{bmatrix} \end{align*} $$ Definition \(A \in M_{n \times n}\) is symmetric if \(A = A^t\). We can show that the set of symmetric matrices in \(M_{n \times n}\) is a subspace. Let the set of symmetric matrices in \(M_{n \times n}\) be \(S\). To show that \(S\) is a subspace, we’ll verify the three conditions \(\bar{0} = \begin{bmatrix} 0... &amp;... 0 \\ 0... &amp;... 0 \end{bmatrix}^t\) is symmetric and therefore it is in \(S\) The sum of two symmetric matrices is symmetric and therefore it is in \(S\) and so \(S\) is closed under addition. The sum is symmetric because given two symmetric matrices \(A\) and \(B\), their component wise sum is \(a_{ij}+b_{ij}\). But then we know that $$ \begin{align*} a_{ij} + b_{ij} = a_{ji} + b_{ji}. \end{align*} $$ Since both matrices are symmetric. But this implies that \(A+B = (A+B)^t\). For any \(c \in \mathbf{R}\) and \(s_1 \in S\), \(cS\) is a symmetric matrix and therefore it is in \(S\). So (\(S\) is closed under scalar multiplication). References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 4: Vector Spaces</title><link href="http://localhost:4000/jekyll/update/2024/07/17/lec04-vector-spaces.html" rel="alternate" type="text/html" title="Lecture 4: Vector Spaces" /><published>2024-07-17T01:01:36-07:00</published><updated>2024-07-17T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/17/lec04-vector-spaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/17/lec04-vector-spaces.html"><![CDATA[<div class="bdiv">
  Definition
</div>
<div class="bbdiv">
  A <i>vector space</i> is a set \(V\) with two operations: addition: \(V \times V \rightarrow V\), and a scalar multiplication: \(\mathbf{R} \times V \rightarrow V\), such that the following properties hold:
  <ol>
      <li>\(u + v = v + u\) for all \(u,v \in V\).</li>
	  <li> \((u + v) + w = u + (v + w)\) for all \(u, v, w \in V\).</li>
      <li>There exists an element \(\bar{0} \in V\) such that \(v + \bar{0} = v\) for all \(v \in V\).</li>
	  <li>For all \(v \in V\), there exists \(w \in V\) such that \(v + w = \bar{0}\). </li>
      <li>\(1v = v\) for all \(v \in V\)</li>
	  <li>\(a(bv) = a(bv)\) for all \(v \in V\) and for all \(a, b \in \mathbf{F}\) </li>
	  <li>\(a(u + v) = au + av\) for all \(u, v \in V\) and for all \(a \in \mathbf{F}\)</li>
      <li>\((a + b)v = av + bv\) for all \(u, v \in V\) and for all \(a, b \in \mathbf{F}\)</li>
</ol>
</div>
<p><br />
For property (4), we don’t call it \(-v\) yet because we didn’t prove yet if it’s unique.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1: \(\mathbf{R}\)</b></h4>
<p>\(\mathbf{R}\) is a vector space equipped with the usual addition and scalar multiplication. The number 0 is the zero vector. We can additionally verify that all the 8 properties are true.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2: A Set of Matrices</b></h4>
<p>The set of \(m\) by \(n\) matrices (\(M_{m \times n}\)) equipped with component wise addition such that</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f \\
\end{pmatrix}
+
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
\end{pmatrix}
=
\begin{pmatrix}
1+a &amp; 2+b &amp; 3+c \\
4+d &amp; 5+e &amp; 6+f \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>and component wise scalar multiplication such that</p>
<div>
$$
\begin{align*}
c
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6 \\
\end{pmatrix}
=
\begin{pmatrix}
1c &amp; 2c &amp; 3c \\
4c &amp; 5c &amp; 6c \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>is a vector space.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3: Sets of Functions</b></h4>
<p>let \(S\) be a nonempty set. For example \(S = \mathbf{R}\), or \(S = \{\pi, \pi^2\}\), \(S = \{\)atoms in the universe\(\}\). Basically any non-empty set. 
Now consider \(F(S) = \{f: S \rightarrow \mathbf{R}\}\), the set of all functions or mappings from \(S\) to \(\mathbf{R}\). One way to think of this is the all the ways we can label the elements in the set \(S\) with real numbers.
<br /><br />
Define addition as \((f+g)(s) = f(s) + g(s)\) for all \(s \in S\). So addition of functions works as addition of their values and produces a real number which is what we want. Define scalar multiplication as \(cf(s) =c(f(s))\) for all \(s \in S\). 
<br /><br />
\(F(S)\) is a vector space. It satisfies all 8 conditions. For example. The zero vector in this space is \(\bar{0}(s) = 0\) for all \(s \in S\). Note also that \(C^1(\mathbf{R})\) (the functions where with continues derivatives) is a subset of \(C^0(\mathbf{R})\) (the set of continuous functions) which is a subset of \(F(S)\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4: The Set of all Sequences</b></h4>
<p>Consider the set of all natural numbers \(\mathbf{N}\) and the set of functions \(F(\mathbf{N}) = \{\sigma: \mathbf{N} \rightarrow \mathbf{R}\). \(\sigma\) is a function that takes a natural number and assigns it a real number. But</p>
<div>
$$
\begin{align*}
    \sigma(1), \sigma(2), \sigma(3), ...
\end{align*}
$$
</div>
<p>is a sequence. So we’re giving the set of sequences the structure of a vector space. Let \(\sigma(1) =a_1, \sigma(2) = a_2, ...\) and so on. So now we can write the sequence as</p>
<div>
$$
\begin{align*}
    a_1, a_2, a_3 .... = \{a_n\}
\end{align*}
$$
</div>
<p>Let \(V = \{\) sequences \(\{a_n\}\) is a vector space. Define the addition of two sequences as \(\{a_n\} + \{b_n\} = \{a_n + b_n\}\). Adding the terms one by one. Define scalar multiplication as \(c\{a_n\} = \{ca_n\}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5: The Set of Polynomials</b></h4>
<div class="bdiv">
  Definition: Degree of a Polynomial
</div>
<div class="bbdiv">
Let \(f\) be defined as follows,
$$
\begin{align*}
    f(x) = a_nx^n + a_{n-1}x^{n-1}+...+a_1x+a_0.
\end{align*}
$$
the degree of \(f\) is the largest \(k\) such that \(x^k\) appears in \(f\) with \(a_k \neq 0\).
</div>
<p><br />
Let \(P_n = \{\) polynomials \(f(x)\) of degree at most \(n\}\).<br />
Define the addition operation as follows,</p>
<div>
$$
\begin{align*}
f(x)+g(x) = (a_nx^n + ...+a_0) + (b_nx^n + ...+b_0) = (a_n+b_n)x^n + ...+(a_0+b_0). 
\end{align*}
$$
</div>
<p>and define scalar multiplication as</p>
<div>
$$
\begin{align*}
cf(x) = ca_nx^n + ca_{n-1}x^{n-1} +...ca_0.
\end{align*}
$$
</div>
<p>\(P_n\) is a vector space. The zero vector is the function \(f\bar{0} = 0 = 0x^n + .... + 0\).
Question: why did we define the polynomials to have at most \(n\) and not just \(n\)? because take \((X^5 + 1)\) and \((-x^5 + 9)\). The addition of these two will generate a 0 and so we have to say at most.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Additional Vector Space Results</b></h4>
<div class="purdiv">
  Theorem
</div>
<div class="purbdiv">
  Let \(u, v, w\) be elements of a vector space \(V\). if \(u + w = v + w\), then \(u = v\).
</div>
<p><br />
<b>Proof:</b>
Let \(V\) be a vector space and \(u, v, w\) be elements in \(V\). By property (4) there is a \(z \in V\) such that</p>
<div>
$$
\begin{align*}
    w + z = \bar{0} 
\end{align*}
$$
</div>
<p>We also know that by property (3) that</p>
<div>
$$
\begin{align*}
    u = u + \bar{0}.
\end{align*}
$$
</div>
<p>But \(\bar{0} = w+z\) and so</p>
<div>
$$
\begin{align*}
u &amp;= u + \bar{0} \\
  &amp;= u + (w + z) \\
  &amp;= (u + w) + z \quad \text{ (by property (2)) } \\
  &amp;= (v + w) + z \quad \text { (by the hypothesis)} \\
  &amp;= v + (w + z) \quad \text{ (by property (2))} \\
  &amp;= v + \bar{0} \\
  &amp;= v \quad
\end{align*}
$$
</div>
<p>Therefore \(u = v\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
  Theorem
</div>
<div class="purbdiv">
  The zero vector in a vector space is unique.
</div>
<p><br />
<b>Proof:</b>
Suppose \(V\) is a vector space. Now suppose for the sake of contradiction that \(\bar{0}\) and \(\bar{0}'\) are additive inverses of \(V\) where \(\bar{0} \neq \bar{0}'\). This means that</p>
<div>
$$
\begin{align*}
\bar{0}' &amp;= \bar{0}' + \bar{0} \quad \text{(By property 3)}\\
         &amp;= \bar{0} + \bar{0}' \quad \text{By property 1}\\
		 &amp;= \bar{0}. \quad \text{(by property 3)}
\end{align*}
$$
</div>
<p>Therefore, \(\bar{0} = \bar{0}'\) which is a contradiction and the zero vector must be unique. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
  Theorem
</div>
<div class="purbdiv">
   For all \(v \in V\), there exists a \(w \in V\) such that \(v + w = \bar{0}\). This \(w\) is unique. We call \(w = -v\).
</div>
<p><br />
<b>Proof:</b> 
Suppose \(V\) is a vector space. Let \(v \in V\). Suppose for the sake of contradiction that \(w\) is not unique and there exists two additive inverses \(w\) and \(w'\) such that \(w \neq w'\). Then</p>
<div>
$$
\begin{align*}
w &amp;= w + 0  \quad \text{By property 3}\\
  &amp;= w + (v + w')  \quad \text{By property 3}\\
  &amp;= (w + v) + w'  \quad \text{By property 1}\\
  &amp;= 0 + w'  \quad \text{By the hypothesis}\\
  &amp;= w'.  \quad \text{By property 3}\\
\end{align*}
$$
</div>
<p>Since \(w = w'\), we can conclude that \(w\) is a unique additive inverse. \(\blacksquare\)
<br />
<br />
Two additional implications mentioned in the class is that \(w = (-1)v\) and \(0v = \bar{0}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6: A Non Example</b></h4>
<p>Consider the set \(\mathbf{R}^2\) equipped with a different set of operations. Let’s define addition as</p>
<div>
$$
\begin{align*}
   (a_1, a_2) + (b_1, b_2) = (a_1+b_1, a_2*b_2).
\end{align*}
$$
</div>
<p>and scalar multiplication.</p>
<div>
$$
\begin{align*}
    c(a_1, a_2) = (ca_1, a_2).
\end{align*}
$$
</div>

<p>Is this a vector space? No. why?</p>

<p>What is the zero vector is this space?</p>
<div>
$$
\begin{align*}
    \bar{0} = (0, 1)
\end{align*}
$$
</div>
<p>because for every vector \(v\), we want \(v + \bar{0} = v\). \((0, 1)\) works here because</p>
<div>
$$
\begin{align*}
(a_1,a_2)+(0,1) = (a_1+0, a_2*1) = (a_1,a_2)
\end{align*}
$$
</div>
<p><br />
The claim is that property 4 can’t be true. Let \(v = (0,0)\). There is no \((a_1, a_2) \in \mathbf{R}^2\) such that 
\(v + (a_1, a_2) = \bar{0}\). To see this,</p>
<div>
$$
\begin{align*}
    (0,0) + (a_1, a_2) = (a_1+0, 0*a_2) = (a_1, 0).
\end{align*}
$$
</div>
<p>so it can never be equal to (0, 1).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
<li>Linear Algebra Done Right for the last two proofs</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A vector space is a set \(V\) with two operations: addition: \(V \times V \rightarrow V\), and a scalar multiplication: \(\mathbf{R} \times V \rightarrow V\), such that the following properties hold: \(u + v = v + u\) for all \(u,v \in V\). \((u + v) + w = u + (v + w)\) for all \(u, v, w \in V\). There exists an element \(\bar{0} \in V\) such that \(v + \bar{0} = v\) for all \(v \in V\). For all \(v \in V\), there exists \(w \in V\) such that \(v + w = \bar{0}\). \(1v = v\) for all \(v \in V\) \(a(bv) = a(bv)\) for all \(v \in V\) and for all \(a, b \in \mathbf{F}\) \(a(u + v) = au + av\) for all \(u, v \in V\) and for all \(a \in \mathbf{F}\) \((a + b)v = av + bv\) for all \(u, v \in V\) and for all \(a, b \in \mathbf{F}\) For property (4), we don’t call it \(-v\) yet because we didn’t prove yet if it’s unique. Example 1: \(\mathbf{R}\) \(\mathbf{R}\) is a vector space equipped with the usual addition and scalar multiplication. The number 0 is the zero vector. We can additionally verify that all the 8 properties are true. Example 2: A Set of Matrices The set of \(m\) by \(n\) matrices (\(M_{m \times n}\)) equipped with component wise addition such that $$ \begin{align*} \begin{pmatrix} a &amp; b &amp; c \\ d &amp; e &amp; f \\ \end{pmatrix} + \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ \end{pmatrix} = \begin{pmatrix} 1+a &amp; 2+b &amp; 3+c \\ 4+d &amp; 5+e &amp; 6+f \\ \end{pmatrix} \end{align*} $$ and component wise scalar multiplication such that $$ \begin{align*} c \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ \end{pmatrix} = \begin{pmatrix} 1c &amp; 2c &amp; 3c \\ 4c &amp; 5c &amp; 6c \\ \end{pmatrix} \end{align*} $$ is a vector space. Example 3: Sets of Functions let \(S\) be a nonempty set. For example \(S = \mathbf{R}\), or \(S = \{\pi, \pi^2\}\), \(S = \{\)atoms in the universe\(\}\). Basically any non-empty set. Now consider \(F(S) = \{f: S \rightarrow \mathbf{R}\}\), the set of all functions or mappings from \(S\) to \(\mathbf{R}\). One way to think of this is the all the ways we can label the elements in the set \(S\) with real numbers. Define addition as \((f+g)(s) = f(s) + g(s)\) for all \(s \in S\). So addition of functions works as addition of their values and produces a real number which is what we want. Define scalar multiplication as \(cf(s) =c(f(s))\) for all \(s \in S\). \(F(S)\) is a vector space. It satisfies all 8 conditions. For example. The zero vector in this space is \(\bar{0}(s) = 0\) for all \(s \in S\). Note also that \(C^1(\mathbf{R})\) (the functions where with continues derivatives) is a subset of \(C^0(\mathbf{R})\) (the set of continuous functions) which is a subset of \(F(S)\). Example 4: The Set of all Sequences Consider the set of all natural numbers \(\mathbf{N}\) and the set of functions \(F(\mathbf{N}) = \{\sigma: \mathbf{N} \rightarrow \mathbf{R}\). \(\sigma\) is a function that takes a natural number and assigns it a real number. But $$ \begin{align*} \sigma(1), \sigma(2), \sigma(3), ... \end{align*} $$ is a sequence. So we’re giving the set of sequences the structure of a vector space. Let \(\sigma(1) =a_1, \sigma(2) = a_2, ...\) and so on. So now we can write the sequence as $$ \begin{align*} a_1, a_2, a_3 .... = \{a_n\} \end{align*} $$ Let \(V = \{\) sequences \(\{a_n\}\) is a vector space. Define the addition of two sequences as \(\{a_n\} + \{b_n\} = \{a_n + b_n\}\). Adding the terms one by one. Define scalar multiplication as \(c\{a_n\} = \{ca_n\}\). Example 5: The Set of Polynomials Definition: Degree of a Polynomial Let \(f\) be defined as follows, $$ \begin{align*} f(x) = a_nx^n + a_{n-1}x^{n-1}+...+a_1x+a_0. \end{align*} $$ the degree of \(f\) is the largest \(k\) such that \(x^k\) appears in \(f\) with \(a_k \neq 0\). Let \(P_n = \{\) polynomials \(f(x)\) of degree at most \(n\}\). Define the addition operation as follows, $$ \begin{align*} f(x)+g(x) = (a_nx^n + ...+a_0) + (b_nx^n + ...+b_0) = (a_n+b_n)x^n + ...+(a_0+b_0). \end{align*} $$ and define scalar multiplication as $$ \begin{align*} cf(x) = ca_nx^n + ca_{n-1}x^{n-1} +...ca_0. \end{align*} $$ \(P_n\) is a vector space. The zero vector is the function \(f\bar{0} = 0 = 0x^n + .... + 0\). Question: why did we define the polynomials to have at most \(n\) and not just \(n\)? because take \((X^5 + 1)\) and \((-x^5 + 9)\). The addition of these two will generate a 0 and so we have to say at most. Additional Vector Space Results Theorem Let \(u, v, w\) be elements of a vector space \(V\). if \(u + w = v + w\), then \(u = v\). Proof: Let \(V\) be a vector space and \(u, v, w\) be elements in \(V\). By property (4) there is a \(z \in V\) such that $$ \begin{align*} w + z = \bar{0} \end{align*} $$ We also know that by property (3) that $$ \begin{align*} u = u + \bar{0}. \end{align*} $$ But \(\bar{0} = w+z\) and so $$ \begin{align*} u &amp;= u + \bar{0} \\ &amp;= u + (w + z) \\ &amp;= (u + w) + z \quad \text{ (by property (2)) } \\ &amp;= (v + w) + z \quad \text { (by the hypothesis)} \\ &amp;= v + (w + z) \quad \text{ (by property (2))} \\ &amp;= v + \bar{0} \\ &amp;= v \quad \end{align*} $$ Therefore \(u = v\) as we wanted to show. \(\blacksquare\) Theorem The zero vector in a vector space is unique. Proof: Suppose \(V\) is a vector space. Now suppose for the sake of contradiction that \(\bar{0}\) and \(\bar{0}'\) are additive inverses of \(V\) where \(\bar{0} \neq \bar{0}'\). This means that $$ \begin{align*} \bar{0}' &amp;= \bar{0}' + \bar{0} \quad \text{(By property 3)}\\ &amp;= \bar{0} + \bar{0}' \quad \text{By property 1}\\ &amp;= \bar{0}. \quad \text{(by property 3)} \end{align*} $$ Therefore, \(\bar{0} = \bar{0}'\) which is a contradiction and the zero vector must be unique. \(\blacksquare\) Theorem For all \(v \in V\), there exists a \(w \in V\) such that \(v + w = \bar{0}\). This \(w\) is unique. We call \(w = -v\). Proof: Suppose \(V\) is a vector space. Let \(v \in V\). Suppose for the sake of contradiction that \(w\) is not unique and there exists two additive inverses \(w\) and \(w'\) such that \(w \neq w'\). Then $$ \begin{align*} w &amp;= w + 0 \quad \text{By property 3}\\ &amp;= w + (v + w') \quad \text{By property 3}\\ &amp;= (w + v) + w' \quad \text{By property 1}\\ &amp;= 0 + w' \quad \text{By the hypothesis}\\ &amp;= w'. \quad \text{By property 3}\\ \end{align*} $$ Since \(w = w'\), we can conclude that \(w\) is a unique additive inverse. \(\blacksquare\) Two additional implications mentioned in the class is that \(w = (-1)v\) and \(0v = \bar{0}\). Example 6: A Non Example Consider the set \(\mathbf{R}^2\) equipped with a different set of operations. Let’s define addition as $$ \begin{align*} (a_1, a_2) + (b_1, b_2) = (a_1+b_1, a_2*b_2). \end{align*} $$ and scalar multiplication. $$ \begin{align*} c(a_1, a_2) = (ca_1, a_2). \end{align*} $$]]></summary></entry><entry><title type="html">Traversing Diagonals of a Grid</title><link href="http://localhost:4000/jekyll/update/2024/07/15/traversal-diagonals.html" rel="alternate" type="text/html" title="Traversing Diagonals of a Grid" /><published>2024-07-15T01:01:36-07:00</published><updated>2024-07-15T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/15/traversal-diagonals</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/15/traversal-diagonals.html"><![CDATA[<p>This is kind of a misleading title but I didn’t know what to call it. But what I wanted here to print each and every “diagonal” of this grid in both the left and right direction. For example, if left to right, then we’ll have</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/1.png" width="35%" class="center" /></p>
<p>And we’ll print</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="mi">1</span> <span class="mi">6</span> <span class="mi">11</span>
<span class="mi">2</span> <span class="mi">7</span> <span class="mi">12</span>
<span class="mi">3</span> <span class="mi">8</span>
<span class="mi">4</span>
<span class="mi">5</span> <span class="mi">10</span>
<span class="mi">9</span></code></pre></figure>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Approach</b></h4>
<p>We’re going to divide this problem into two subproblems. In the first subproblem, we’ll print the diagonals starting from the cells in the first column below.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/2.png" width="35%" class="center" /></p>
<p>In the second subproblem, we’ll print the remaining diagonals starting from the cells in the first row but skipping the first overlapping element with the first column since we did that in the first subproblem.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/6.png" width="32%" class="center" /></p>

<!------------------------------------------------------------------------------------>
<h4><b>First Subproblem</b></h4>
<p>We’ll iterate over the first column. From the first cell, we’ll print the following diagonal.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/3.png" width="32%" class="center" /></p>
<p>Next, we’ll move to the next cell in the column and print the diagonal starting from it.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/4.png" width="32%" class="center" /></p>
<p>And finally the last cell in the column.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/5.png" width="32%" class="center" /></p>
<p>This is process is captured in the following code:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="c1">// left to right direction in an m by n grid</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// print the diagonal</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">k</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
        <span class="n">k</span><span class="o">++</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>

<p>The above code will output</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="mi">1</span> <span class="mi">6</span> <span class="mi">11</span>
<span class="mi">5</span> <span class="mi">10</span>
<span class="mi">9</span></code></pre></figure>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Second Subproblem</b></h4>
<p>We’ll iterate over the row column starting at the second cell and print the diagonal from there.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/7.png" width="32%" class="center" /></p>
<p>We’ll move on to the third cell and print the diagonal again.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/8.png" width="32%" class="center" /></p>
<p>Finally, we’ll print the very last diagonal.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/9.png" width="32%" class="center" /></p>
<p>This process is captured in the following code:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// print the diagonal</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"d: "</span><span class="p">);</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">s</span><span class="p">]);</span>
        <span class="n">i</span><span class="o">++</span><span class="p">;</span>
        <span class="n">s</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span></code></pre></figure>

<p>which will print the following</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="mi">2</span> <span class="mi">7</span> <span class="mi">12</span>
<span class="mi">3</span> <span class="mi">8</span>
<span class="mi">4</span></code></pre></figure>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Right to Left Direction</b></h4>
<p>What about the complete other direction (right to left) below. How do we print these diagonals?</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/competitive-programming/traversal-d/10.png" width="32%" class="center" /></p>
<p>We’ll do the same thing, divide the problem into two subproblems and tackle each separately. This is captured in the following code:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="c1">// right to left</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
        <span class="n">k</span><span class="o">--</span><span class="p">;</span>
        <span class="n">j</span><span class="o">++</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
        <span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">s</span><span class="p">]);</span>
            <span class="n">i</span><span class="o">--</span><span class="p">;</span>
            <span class="n">s</span><span class="o">++</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="p">}</span></code></pre></figure>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is kind of a misleading title but I didn’t know what to call it. But what I wanted here to print each and every “diagonal” of this grid in both the left and right direction. For example, if left to right, then we’ll have And we’ll print 1 6 11 2 7 12 3 8 4 5 10 9 Approach We’re going to divide this problem into two subproblems. In the first subproblem, we’ll print the diagonals starting from the cells in the first column below. In the second subproblem, we’ll print the remaining diagonals starting from the cells in the first row but skipping the first overlapping element with the first column since we did that in the first subproblem.]]></summary></entry></feed>