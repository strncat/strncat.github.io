<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="nemo&apos;s notebook" />
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">nemo&#39;s notebook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
  
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 12/13: Matrix Representation of Linear Transformations</h1>
    <!--
    <p class="post-meta">
      <time class="dt-published" datetime="2024-08-05T04:01:36-04:00" itemprop="datePublished">Aug 5, 2024
      </time></p>
     -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Recall \(A \in M_{m \times n}\) defines a linear map</p>
<div>
	$$
	\begin{align*}
	L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\ 
	            &amp;\bar{x} \rightarrow A\bar{x} 
	\end{align*}
	$$
</div>
<p>We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces can be represented by a matrix. For example the following linear transformations</p>
<div>
	$$
	\begin{align*}
	T_d: \ &amp;P_3 \rightarrow P_2 \\ 
	&amp;f \rightarrow f'
	\end{align*}
	$$
</div>
<p>and</p>
<div>
 	$$
 	\begin{align*}
 	T_i: \ &amp;P_2 \rightarrow P_3 \\ 
 	&amp;\ f \rightarrow \int_0^x f(t)dt
 	\end{align*}
 	$$
</div>
<p>are examples of linear transformations with finite dimensional vector spaces that can be represented by a matrix. But in order to show this, we will start with expressing vectors uniquely relative a basis in a vector space.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Coordinate Expression for a vector</b></h4>
<p>Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form,</p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n
	\end{align*}
	$$
</div>
<p>The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. 
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [v]_{\beta} = 
\begin{pmatrix}
a_1 \\
.  \\
. \\
. \\
a_n
\end{pmatrix}
\in \mathbf{R}^n
	\end{align*}
	$$
is the coordinate expression for \(v\) with respect to \(\beta\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\).
<br /></p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta} &amp;= 
	 \begin{pmatrix}
	 2 \\
	 1 \\
	 \end{pmatrix} \\
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis,</p>
<div>
	$$
	\begin{align*}
	 v = (2,1) = a_1(1,1) + a_2(1,-1).
\end{align*}
	 $$
</div>
<p>From this, we get</p>
<div>
	$$
	\begin{align*}
	 a_1 + a_2 &amp;= 2 \\
	 a_1 - a_2 &amp;= 1 \\
\end{align*}
	 $$
</div>
<p>Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are</p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix} \\
	 &amp;= 
	 \begin{pmatrix}
	 \frac{3}{2} \\
	 \frac{1}{2} \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>So we can think of \([\quad]_{\beta'}\) as a map:</p>
<div>
	$$
	\begin{align*}
	 [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance”
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional.
We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be the basis for \(V\) and let \(\gamma = \{w_1, ..., w_n\}\) be the basis for \(W\). For \(v \in V\), we have</p>
<div>
	$$
	\begin{align*}
	T(v) = a_1w_1 + ... + a_mw_m.
\end{align*}
$$
</div>
<p>(This is the image of \(v\) which can be expressed uniquely in terms of the vectors of the basis \(\gamma\) because we’re in the vector space \(W\) now.) \(v\) itself is a linear combination of the basis vectors of \(\beta\). So for each \(v_j\) we have,</p>
<div>
 	$$
 	\begin{align*}
 	T(v_j) &amp;= a_{1j}w_1 + ... + a_{mj}w_m \\
	&amp;= \sum_i^m a_{ij}w_i \text{ for $j = 1,2,..,n.$}
 \end{align*}
 $$
 </div>
<p>From this, we now have this definition,
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	Using the above notation, let the matrix \(A\) defined as \(A_{ij} = a_ij\) be the <b>matrix representation of T</b> in the ordered \(\beta\) and \(\gamma\) and we write
	$$
	\begin{align*}
	 [T]_{\beta}^{\gamma} = A
	\end{align*}
	$$
</div>
<p><br />
Remark: The column vectors of \(T\) are the images of the basis vectors \(\beta\) written with respect to \(\gamma\).</p>
<div>
	$$
\begin{align*}
	 [T]_{\beta}^{\gamma} = 
\begin{pmatrix}
| &amp; &amp; |\\ 
[T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\
| &amp; &amp; |
\end{pmatrix}
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let</p>
<div>
	$$
\begin{align*}
T_d: \ &amp;P_3 \rightarrow P_2 \\
        &amp;f \rightarrow f'. 
\end{align*}
$$
</div>
<p>Let \(\beta = \{1, x, x^2, x^3\}\) be the standard basis of \(P_3\). and \(\gamma = \{1, x, x^2\}\) be the standard basis of \(P_2\). We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so,</p>
<div>
	$$
\begin{align*}
T_d(1) &amp;= 0 \\
T_d(x) &amp;= x \\
T_d(x^2) &amp;= 2x \\
T_d(x^3) &amp;= 3x^2
\end{align*}
$$
</div>
<p>Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\).</p>
<div>
	$$
\begin{align*}
[T_d(1)]_{\gamma} &amp;=  [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x)]_{\gamma} &amp;=  [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
1 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x^2)]_{\gamma} &amp;=  [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
2 \\
0
\end{pmatrix} \\
[T_d(x^3)]_{\gamma} &amp;=  [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>So the final matrix is</p>
<div>
	$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2.15(a)
</div>
<div class="purbdiv">
Let \(A \in M_{m \times n}\) and let
$$
\begin{align*}
L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\
&amp;\bar{x} \rightarrow A\bar{x}.
\end{align*}
$$
If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p><br />
(Note: We earlier said that any linear transformation between finite dimensional vector spaces can be represented by a matrix. \(L_A\) is a linear transformation. So what is the matrix representation of it? The claim is that as long as you use the standard basis for \(\mathbf{R}^n\) and \(\mathbf{R}^m\), then the representation is just \(A\) itself!)
<br />
<br />
Proof: \(\beta\) is the standard basis of \(\mathbf{R}^n\) so \(\beta = \{e_1,...,e_n\}\) where 
\(e_1 = \begin{pmatrix}
1 &amp; 0 &amp; ... &amp; 0
\end{pmatrix}^t\)
<br />
We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them with respect to the basis \(\gamma\),</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma})
\end{align*}
$$
</div>
<p>But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1 = \bar{a}_1e_{11}+\bar{a}_2e_{12}+...+\bar{a}_ne_{1n}\) where \(\bar{a}_1,...,\bar{a}_n\) are the columns of \(A\). But all the coefficients of \(e_1\) are zero except for the first one. So \(Ae_1 = 1\bar{a}_1\) which is just the first column of \(A\). Finally, the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) is just the first column of \(A\) since it’s the standard basis. Similarly, we can use the same argument to find out the remaining columns and so</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A. \blacksquare
\end{align*}
$$
</div>
<p><br />
So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformation as a Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem 2.14
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases.<br />
For all \(v \in V\),
$$
\begin{align*}
[T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta}
\end{align*}
$$
\(T\) acts like matrix multiplication.
</div>
<p><br />
Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear.</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\
                &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\
				&amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\
             &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\
			 &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>The expression above is exactly matrix multiplication so we can re-write this as</p>
<div>
$$
\begin{align*}
a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
&amp;=
\begin{pmatrix}
[T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} 
\end{pmatrix}
\begin{pmatrix}
a_1 \\
. \\
. \\
. \\
a_n \\
\end{pmatrix} \\
&amp;= [T]_{\beta}^{\gamma} [v]_{\beta}.
\end{align*}
$$
</div>
<p>This is exactly what we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let’s continue the same example from earlier where we found the matrix representative for the linear transformation</p>
<div>
	$$
\begin{align*}
T_d: \ &amp;P_3 \rightarrow P_2 \\
        &amp;f \rightarrow f'. 
\end{align*}
$$
</div>
<p>Suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix</p>
<div>
	$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}
\begin{pmatrix}
5 \\ 
4 \\
0 \\
3
\end{pmatrix} = 
\begin{pmatrix}
4 \\
0 \\
9
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>


  </div><div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /jekyll/update/2024/08/05/lec13-more-linear-transformations.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier =  /jekyll/update/2024/08/05/lec13-more-linear-transformations; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://strncat-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/jekyll/update/2024/08/05/lec13-more-linear-transformations.html" hidden></a>
</article>
		 
      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">nemo&#39;s notebook</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">nemo&#39;s notebook</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>personal study notes</p>
      </div>
    </div>

  </div>

</footer>
-->

	
  </body>

</html>
