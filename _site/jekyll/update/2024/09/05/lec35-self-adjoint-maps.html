<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="nemo&apos;s notebook" />
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">nemo&#39;s notebook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
  
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 35: Self Adjoint Maps</h1>
    <!--
    <p class="post-meta">
      <time class="dt-published" datetime="2024-09-05T01:01:36-07:00" itemprop="datePublished">Sep 5, 2024
      </time></p>
     -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the last two lectures we studied adjoint maps where an adjoint map is defined as follows
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that
$$
\begin{align*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and we also studied a special class of adjoint maps called normal adjoint maps where \(T\) is called normal if \(T \circ T^* = T^* \circ T\). We also studied properties of these normals maps. For example if \(T\) is normal then,</p>
<div>
$$
\begin{align*}
\Vert T(x) \Vert &amp;= \Vert T^*(x) \Vert \\
T(x) = \lambda x \ &amp;\implies \ T^*(x) = \bar{\lambda}x
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>Next we will study another special class of adjoint maps called self adjoint maps defined below
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow V\) is self-adjoint if
$$
\begin{align*}
T^* = T
\end{align*}
$$
</div>
<p><br />
Note here that self adjoint implies that \(T\) is normal. The converse is not true (rotation matrix is an example)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let \(A \in M_{n \times n}(\mathbf{R})\) and \(A_{ij} = A_{ji} \ \forall i,j\), then A (L_A) is self-adjoint.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose that \(V\) is a finite dimensional inner product space where \(W \subseteq V\) is a subspace.</p>
<div>
$$
\begin{align*}
T = \text{proj}_W \ : \ V &amp;\rightarrow V \\
                    x = w + z &amp;\rightarrow w
\end{align*}
$$
</div>
<p>So if you take a vector \(x \in V\), we know we can decompose it into two vectors \(w \in W\) and \(z \in W^{\perp}\). This map just produces the part that is in \(W\). We claim that \(\text{proj}_W\) is self-adjoint.
<br />
<br />
<b>Proof</b>
<br />
Take \(x_1, x_2 \in V\). We need to show that</p>
<div>
$$
\begin{align*}
\langle T(x_1), x_2 \rangle = \langle x_1, T(x_2) \rangle
\end{align*}
$$
</div>
<p>We know that \(x_1 = w_1 + z_1\) and \(x_2 = w_2 + z_2\) for some unique vectors \(w_1, w_2 \in W\) and \(z_1, z_2 \in W^{\perp}\). Then,</p>
<div>
$$
\begin{align*}
\langle T(x_1), x_2 \rangle &amp;= \langle T(w_1 + z_1), x_2 \rangle \\
                            &amp;= \langle w_1, w_2 + z_2 \rangle \\
							&amp;= \langle w_1, w_2 \rangle + \langle w_1, z_2 \rangle \\
							&amp;= \langle w_1, w_2 \rangle \quad \text{(because $w_1 \in W$ and $z_2 \in W^{\perp}$)}\\
\end{align*}
$$
</div>
<p>At this point, we recognize that \(w_2 = T(x_2)\) but we still have \(w_1\) and want to reach \(x_1\). Notice that \(x_1 = w_1 + z_1\). Moreover, \(\langle z_1, w_2 \rangle = 0\) So</p>
<div>
$$
\begin{align*}
\langle T(x_1), x_2\rangle &amp;= \langle w_1, w_2 \rangle \phantom{ \quad \text{(because $w_1 \in W$ and $z_2 \in W^{\perp}$)}}\\ 
                            &amp;=  \langle w_1, w_2 \rangle +  \langle z_1, w_2 \rangle \\
							&amp;= \langle x_1, w_2 \rangle \\
							&amp;= \langle x_1, T(x_2) \rangle 
\end{align*}
$$
</div>
<p>as we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Self-adjoint Maps are Diagonalizable</b></h4>
<p>Today’s goal is to prove that self adjoint maps are diagonalizable. Note here that when a matrix \(A\) is symmetric where \(A_{ij} = A_{ji}\), then \(A\) is self-adjoint. This implies \(A\) is diagonalizable and so  \(\det(A - tI_n)\) splits which is really useful to know!
<br />
<br />
Question: What is the diagonal form of the project map \(\text{proj}_W\)? because eigenvectors get mapped to a multiple of themselves, the projection of the vector is either all in \(W\) or all in \(Z\) and you get zero from the projection. Therefore, we notice here that the eigenvalues are 0s and 1s.
<br />
<br />
Proving that self adjoint maps are diagonalizable, requires a few things along the way so we will next prove the results that we need in order to prove that they’re diagonalizable.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvalues of Self-adjoint Maps</b></h4>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
If \(T: V \rightarrow V\) where \(V\) is over \(\mathbf{C}\) is self-adjoint, then all eigenvalues are real.
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Since \(T\) is self-adjoint, then \(T\) is normal. Then for any \(\lambda\),</p>
<div>
$$
\begin{align*}
T(x) &amp;= \lambda x \\
T^*(x) &amp;= \bar{\lambda} x \quad \text{because $T$ is normal} 
\end{align*}
$$
</div>
<p>But \(T = T^*\) since \(T\) is self-adjoint. Therefore,</p>
<div>
$$
\begin{align*}
\lambda x = \bar{\lambda} x \\
\implies \lambda = \bar{\lambda} 
\end{align*}
$$
</div>
<p>as we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Do Self-adjoint Maps have Eigenvalues?</b></h4>
<p>So eigenvalues are real but are there eigenvalues?
<br />
<br />
If \(V\) is a vector space over \(\mathbf{C}\), then \(T: V \rightarrow V\) always has eigenvalues. It doesn’t matter if \(T\) is self-adjoint or not. The characteristic polynomial \(\det([T]_{\beta}^{\beta} - tI_n)\) with complex entries always splits! (fact from algebra). 
<br />
What if \(V\) was over \(\mathbf{R}?\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
If \(T: V \rightarrow V\) where \(V\) is over \(\mathbf{R}\) is self-adjoint, then \(T\) has at least one eigenvalue.
</div>
<p><br />
An example is the rotation matrix. It is normal but not self-adjoint and it doesn’t have real eigenvalues.
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
Let \(\beta\) be an orthonormal basis of \(V\). We need to show that \(\det([T]_{\beta}^{\beta} - tI_n) = 0\) has a real root. Let \(A = [T]^{\beta}_{\beta}\). Then</p>
<div>
$$
\begin{align*}
A = [T]^{\beta}_{\beta} &amp;= [T^*]^{\beta}_{\beta} \quad \text{(Because $$T$$ is self-adjoint)} \\
                    &amp;= ([T]^{\beta}_{\beta})^* \quad \text{(Proof in last lecture)} \\
					&amp;= A^* \\
					&amp;= A^t \quad \text{(because $$V$$ is over $$\mathbf{R}$$)}
\end{align*}
$$
</div>
<p>So \(A\) is symmetric. The idea is to apply theorem 1 which states that if \(V\) is over \(\mathbf{C}\), then we have real eigenvalues. So consider the following map</p>
<div>
$$
\begin{align*}
L_A \ : \ &amp;\mathbf{C}^n \rightarrow \mathbf{C}^n \\
          &amp; z \rightarrow Az
\end{align*}
$$
</div>
<p>We know that \(L_A\) is self adjoint so \((L_A)^* = L_{A^*} = L_{A^t} = L_A\). So the fact from algebra, \(L_A\) has an eigenvalue. By theorem 1, this eigenvalue must be real. Therefore, \(\det([T]_{\beta}^{\beta} - tI_n) = 0\) has a real root. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors of a Self-adjoint Map</b></h4>
<p>What can we say about the eigenvectors of a linear self-adjoint map? 
<br /></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\)
</div>
<p><br />
<b>Proof</b>
<br />
We’re given that we have one at least eigenvalue but we want to prove that we have an orthonormal basis of eigenvectors. Having one eigenvalue gives us a base case. So let’s do this by Induction on \(\dim V = n\).
<br />
<br />
Base Case: \(n = 1\): we have a map from \(\mathbf{F}\) to \(\mathbf{F}\). A linear map is just multiplying by a scalar.</p>
<div>
$$
\begin{align*}
T \ : \ &amp;\mathbf{F} \rightarrow \mathbf{F} \\
          &amp; x \rightarrow ax
\end{align*}
$$
</div>
<p>This map is self-adjoint. Every value of \(a\) is an eigenvalue since it produces a multiple of \(x\) as long as it’s not zero. Therefore, just choose any \(a \neq 0\). The orthonormal basis will then be \(\{\frac{x_0}{|x_0|}\}\)
<br />
<br />
Inductive Case: Assume this is true for \(n\). <br />
Let \(T: V \rightarrow V\) with \(\dim(V)=n\), be self-adjoint. 
<br />
<br />
By theorem 2, \(T\) has a real eigenvalue \(\lambda_1\) and so \(T(v_1) = \lambda_1 v_1\). Assume \(v_1\) has length 1 (We can normalize otherwise). So now at this point we have our first vector in the orthonormal basis \(\beta\). How do we get our second vector?
<br />
<br />
We know that the second vector must be orthogonal to the first vector. This means that it lies in the orthogonal complement of of where \(v_1\) lies. So Set \(W = \{ v_1 \}^{\perp}\). \(W\) has dimension \(n-1\) because we took away a dimension. But we can’t yet apply the inductive hypothesis on \(W\) because we need a self-adjoint map on \(W\). What could this map be?
<br />
<br />
We know the eigenvectors in the basis \(\beta\) need to be eigenvectors of \(T\) so this map has to be related to \(T\)? So the hope is that \(T\) restricts to a map on \(W\). This happens as we know if \(W\) is \(T\) invariant. For that to happen, we need to show that \(T(W) \subseteq W\). 
<br /><br />
To show that \(T(W) \subseteq W\), suppose we’re given \(w \in W\), we want to show that \(T(w) \in W\). But we know that \(W = \{ v_1 \}^{\perp}\). So we want to show that \(\langle T(w), v_1 \rangle = 0\).</p>
<div>
$$
\begin{align*}
\langle T(w), v_1 \rangle &amp;= \langle w, T(v_1) \rangle \quad \text{(because $T$ is adjoint)}\\
                      &amp;= \langle w, \lambda_1 v_1 \rangle \quad \text{(because $v_1$ is an eigenvector)}\\
					  &amp;= \lambda_1 \langle w, v_1 \rangle \\
					  &amp;= 0 \quad \text{(because $w$ is in the orthogonal complement of $v_1$)}\\
					  
\end{align*}
$$
</div>
<p>So now we know that \(W\) is \(T\)-invariant and so we have \(T_W: W \rightarrow W\). We still need two things before we can apply the inductive hypothesis. We need \(W\) to be an inner product space and we also need \(T_W\) to be self-adjoint. But \(W\) is a subspace of \(V\) so it inherits the inner product from \(V\). Moreover, \(T_W\) is a self-adjoint map. To see why, notice that</p>
<div>
$$
\begin{align*}
\langle T_W(w_1), w_2 \rangle &amp;= \langle T(w_1), w_2 \rangle  \\
                              &amp;= \langle w_1, T(w_2) \rangle \\
							  &amp;= \langle w_1, T_W(w_2) \rangle
					  
\end{align*}
$$
</div>
<p>So now we can apply the inductive hypothesis to conclude that \(W\) has an orthonormal basis, \(\beta_W = \{v_2,...,v_n\}\) consisting of eigenvectors of \(T_W\). But we know that eigenvectors of a restriction are also eigenvectors of \(T\) itself. so \(\beta = \{v_1,v_2,...,v_n\} and we are done.\)\ \blacksquare$$
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
\(T\) is diagonalizable.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>


  </div><div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /jekyll/update/2024/09/05/lec35-self-adjoint-maps.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier =  /jekyll/update/2024/09/05/lec35-self-adjoint-maps; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://strncat-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/jekyll/update/2024/09/05/lec35-self-adjoint-maps.html" hidden></a>
</article>
		 
      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">nemo&#39;s notebook</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">nemo&#39;s notebook</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>personal study notes</p>
      </div>
    </div>

  </div>

</footer>
-->

	
  </body>

</html>
