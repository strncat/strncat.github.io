<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-26T20:49:53-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Section 1.6: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 20" /><published>2024-08-03T01:01:36-07:00</published><updated>2024-08-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html"><![CDATA[<p>Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case?</p>
<div class="ydiv">
1.6 Exercise 20
</div>
<div class="ybdiv">
Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\).
<ol style="list-style-type:lower-alpha">
	<li>Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.)</li>
	<li>Prove \(S\) contains at least \(n\) vectors.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Similar to the proof of <a href="http://127.0.0.1:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html">theorem 1.9</a>, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\)</li>
	
	<li>If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains 
	at least \(n\) vectors by <a href="http://127.0.0.1:4000/jekyll/update/2024/08/02/1-6-corollary-2.html">Corollary 2</a>
	 from theorem 1.9. \(\blacksquare\)</li>
</ol>
<p><br />
The book provided the solution <a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_1_6.html">here</a>.
though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case? 1.6 Exercise 20 Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\). Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.) Prove \(S\) contains at least \(n\) vectors. Proof: Similar to the proof of theorem 1.9, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\) If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains at least \(n\) vectors by Corollary 2 from theorem 1.9. \(\blacksquare\) The book provided the solution here. though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Corollary 2</title><link href="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html" rel="alternate" type="text/html" title="Section 1.6: Corollary 2" /><published>2024-08-02T01:01:36-07:00</published><updated>2024-08-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html"><![CDATA[<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
	Let \(V\) be a vector space with dimension \(n\).
<ol style="list-style-type:lower-alpha">
	<li>Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\)</li>
	<li>Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\).</li>
	<li>Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\).</li>
</ol>
</div>
<p><br />
Proof: 
<br />
<br />
\((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Corollary 2 Let \(V\) be a vector space with dimension \(n\). Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\) Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\). Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\). Proof: \((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\) \(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\) \(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html" rel="alternate" type="text/html" title="Section 1.5: Exercise 21" /><published>2024-08-01T01:01:36-07:00</published><updated>2024-08-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html"><![CDATA[<div class="ydiv">
1.5 Exercise 21
</div>
<div class="ybdiv">
Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). 
</div>
<p><br />
Proof: 
<br />
<br />
\(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}.
	\end{align*}
	$$
</div>
<p>Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\
	u &amp;= w \\
	\end{align*}
	$$
</div>
<p>We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\)
<br />
<br />
\(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\),</p>
<div>
	$$
	\begin{align*}
	v = a_1u_1 + a_2u_2 + ... + a_nu_n.
	\end{align*}
	$$
</div>
<p>And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows,</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Substituting for \(v\) in the previous equation,</p>
<div>
	$$
	\begin{align*}
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}.
	\end{align*}
	$$
</div>
<p>Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.5 Exercise 21 Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). Proof: \(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}. \end{align*} $$ Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\ u &amp;= w \\ \end{align*} $$ We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\) \(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\), $$ \begin{align*} v = a_1u_1 + a_2u_2 + ... + a_nu_n. \end{align*} $$ And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows, $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Substituting for \(v\) in the previous equation, $$ \begin{align*} &amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\ &amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}. \end{align*} $$ Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\). References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Theorem 1.7</title><link href="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html" rel="alternate" type="text/html" title="Section 1.5: Theorem 1.7" /><published>2024-07-31T01:01:36-07:00</published><updated>2024-07-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html"><![CDATA[<p>This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture.</p>
<div class="purdiv">
Theorem 1.7
</div>
<div class="purbdiv">
If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\).
</div>
<p><br />
Proof: 
<br />
<br />
\(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}.
	\end{align*}
	$$
</div>
<p>But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\
	v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\
	v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\
	\end{align*}
	$$
</div>
<p>This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\).
<br />
<br />
\(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Moving \(v\) to the other side, we see that,</p>
<div>
	$$
	\begin{align*}
	 b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}.
	\end{align*}
	$$
</div>
<p>We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture. Theorem 1.7 If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\). Proof: \(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}. \end{align*} $$ But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so $$ \begin{align*} \bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\ v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\ v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\ \end{align*} $$ This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\). \(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Moving \(v\) to the other side, we see that, $$ \begin{align*} b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}. \end{align*} $$ We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Theorem 1.9</title><link href="http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html" rel="alternate" type="text/html" title="Section 1.6: Theorem 1.9" /><published>2024-07-30T01:01:36-07:00</published><updated>2024-07-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html"><![CDATA[<p>This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet.</p>
<div class="purdiv">
Theorem 1.9
</div>
<div class="purbdiv">
If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis.
</div>
<p><br />
Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases</p>
<ol>
	<li>\(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done.</li>
	<li>\(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) </li>
</ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet. Theorem 1.9 If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis. Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases \(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done. \(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 12: Linear Transformations Continued</title><link href="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 12: Linear Transformations Continued" /><published>2024-07-29T01:01:36-07:00</published><updated>2024-07-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is onto if \(R(T) = W\) or
	$$
	\begin{align*}
	 \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is 1-1 if
	$$
	\begin{align*}
	 T(v_1) = T(v_2) \Rightarrow v_1 = v_2.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent 
<ul>
	<li> \(T\) is 1-1. </li>
	<li> \(T\) is onto. </li>
	<li> \(rank(T) = \dim(V)\). </li>
</ul>
</div>
<p><br />
Note here that \(rank(T) = \dim(R(T))\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Recall \(A \in M_{m \times n}\) defines a linear map</p>
<div>
	$$
	\begin{align*}
	L_A:\mathbf{R}^n \rightarrow \mathbf{R}^m, \ \bar{x} \rightarrow A\bar{x} 
	\end{align*}
	$$
</div>
<p>We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example \(T_d: P_3 \rightarrow P_2, \ f \rightarrow f'\) and \(T_i: P_2 \rightarrow P_3, \ f \rightarrow \int_0^x f(t)dt\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Coordinate Expression for a vector</b></h4>
<p>Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. 
<br />
<br />
But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every $v \in V\(,\)v$$ can be expressed uniquely in the form,</p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n
	\end{align*}
	$$
</div>
<p>The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. 
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [v]_{\beta} = 
\begin{pmatrix}
a_1 \\
.  \\
. \\
. \\
a_n
\end{pmatrix}
\in \mathbf{R}^n
	\end{align*}
	$$
is the coordinate expression for \(v\) with respect to \(\beta\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\).
<br /></p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta} &amp;= 
	 \begin{pmatrix}
	 2 \\
	 1 \\
	 \end{pmatrix} \\
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis,</p>
<div>
	$$
	\begin{align*}
	 v = (2,1) = a_1(1,1) + a_2(1,-1).
\end{align*}
	 $$
</div>
<p>From this, we get</p>
<div>
	$$
	\begin{align*}
	 a_1 + a_2 &amp;= 2 \\
	 a_1 - a_2 &amp;= 1 \\
\end{align*}
	 $$
</div>
<p>Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are</p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix} \\
	 &amp;= 
	 \begin{pmatrix}
	 \frac{3}{2} \\
	 \frac{1}{2} \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>So we can think of \([\quad]_{\beta'}\) as a map:</p>
<div>
	$$
	\begin{align*}
	 [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance”
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional.
We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix.
<br />
<br />
Let \(\beta = \{v_1, ..., v_n\}\) be a basis for \(V\) and \(\gamma = \{w_1, ..., w_n\}\). For \(v \in V\), we have</p>
<div>
	$$
	\begin{align*}
	T(v) = a_1w_1 + ... + a_mw_m.
\end{align*}
$$
</div>
<p>For \(v_j\), we have,</p>
<div>
 	$$
 	\begin{align*}
 	T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m.
 \end{align*}
 $$
 </div>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(T: V \rightarrow W\) is onto if \(R(T) = W\) or $$ \begin{align*} \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w. \end{align*} $$ Definition \(T: V \rightarrow W\) is 1-1 if $$ \begin{align*} T(v_1) = T(v_2) \Rightarrow v_1 = v_2. \end{align*} $$ Theorem If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\) Theorem Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent \(T\) is 1-1. \(T\) is onto. \(rank(T) = \dim(V)\). Note here that \(rank(T) = \dim(R(T))\). Matrix Representation of linear transformations Recall \(A \in M_{m \times n}\) defines a linear map $$ \begin{align*} L_A:\mathbf{R}^n \rightarrow \mathbf{R}^m, \ \bar{x} \rightarrow A\bar{x} \end{align*} $$ We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example \(T_d: P_3 \rightarrow P_2, \ f \rightarrow f'\) and \(T_i: P_2 \rightarrow P_3, \ f \rightarrow \int_0^x f(t)dt\). Coordinate Expression for a vector Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every $v \in V\(,\)v$$ can be expressed uniquely in the form, $$ \begin{align*} v = a_1v_1 + ... + a_nv_n \end{align*} $$ The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. Definition $$ \begin{align*} [v]_{\beta} = \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \end{pmatrix} \in \mathbf{R}^n \end{align*} $$ is the coordinate expression for \(v\) with respect to \(\beta\). Example Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\). $$ \begin{align*} [v]_{\beta} &amp;= \begin{pmatrix} 2 \\ 1 \\ \end{pmatrix} \\ [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \end{align*} $$ These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis, $$ \begin{align*} v = (2,1) = a_1(1,1) + a_2(1,-1). \end{align*} $$ From this, we get $$ \begin{align*} a_1 + a_2 &amp;= 2 \\ a_1 - a_2 &amp;= 1 \\ \end{align*} $$ Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are $$ \begin{align*} [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \\ &amp;= \begin{pmatrix} \frac{3}{2} \\ \frac{1}{2} \\ \end{pmatrix} \end{align*} $$ So we can think of \([\quad]_{\beta'}\) as a map: $$ \begin{align*} [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n \end{align*} $$ The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance” Matrix Representation of linear transformations Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional. We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be a basis for \(V\) and \(\gamma = \{w_1, ..., w_n\}\). For \(v \in V\), we have $$ \begin{align*} T(v) = a_1w_1 + ... + a_mw_m. \end{align*} $$ For \(v_j\), we have, $$ \begin{align*} T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m. \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 11: Null Space, Range, and Dimension Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem.html" rel="alternate" type="text/html" title="Lecture 11: Null Space, Range, and Dimension Theorem" /><published>2024-07-28T01:01:36-07:00</published><updated>2024-07-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The null space (kernel) of \(T\) is 
	$$
	\begin{align*}
	 N(T) = \{v \in V \ | \ T(v) = \bar{0}_W\} \subset V
	\end{align*}
	$$
The range (image) of \(T\) is
	$$
	\begin{align*}
	 R(T) = \{T(v) \ | v \in V \} \subset W
	\end{align*}
	$$
</div>
<p><br />
Notes: So the null space is any vector that the transformation turns it into the zero vector. The range just like the definition is the set of the images of all the vectors in \(V\). If all the vectors have an image, then the range is equal to the codomain and that’s when the transformation is onto.
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>\(N(T^b_a) = \{\)functions in \(C^0(\mathbf{R})\) whos average over \([a,b]\) is \(0\}\). In other words, it’s the set of all functions whose definite integral is 0. 
<br />
<br />
\(R(T^b_a) = \mathbf{R}\). Here we want all functions whose definite integral is some constant/some real number. In other words, this will be the entire codomain (\(\mathbf{R}\)). We call this map \((T^b_a)\) onto.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \( \ T: V \rightarrow W\) is linear then \(N(T)\) is a subspace of \(V\) and \(R(T)\) is a subspace of \(W\).
</div>
<p><br />
Proof:
<br />
We need to verify the three subspaces properties. To prove that \(R(T)\) is a subspace:</p>
<ul>
<li>We need \(\bar{0}_W \in R(T)\). So we need the zero vector to be in the range. This means that we need a \(v \in V\) such that \(T(v) = \bar{0}_W\). Since \(T\) is linear then \(T(\bar{0}_v) = \bar{0}_W\). (We proved this in the previous lecture) </li>
<li>We need \(R(T)\) to be closed under addition. Suppose \(w_1, w_2 \in R(T)\), then \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1\) and \(v_2\). Therefore,
	<div>
		$$
		\begin{align*}
		 w_1 + w_2 &amp;= T(v_1) + T(v_2) \\
		           &amp;= T(v_1 + v_2) \ \text{because $$T$$ is linear}
		\end{align*}
		$$
	</div>
By definition of a vector space \(v_1 + v_2 \in V\), so the image of the sum \(w_1 + w_2 \in R(T)\) as required.
</li>
<li> We need \(R(T)\) to be closed under multiplication. Suppose \(w_1 \in R(T)\) and \(c\) is a scalar. We need \(cw_1 \in R(T)\). Since \(w_1 \in R(T)\), then there must be a vector \(v_1 \in V\) such that \(w_1 = T(v_1)\). Notice now,
	<div>
		$$
		\begin{align*}
		 w_1 &amp;= T(v_1) \\
		 cw_1 &amp;= cT(v_1) \\
         &amp;= T(cv_1) \ \text{because $$T$$ is linear}
		\end{align*}
		$$
	</div>
By definition of a vector space \(cv_1 \in V\), so the image of the product \(cv_1 \in R(T)\) as required.

</li>
</ul>
<p>From this, we conclude that the \(R(T)\) is a subspace of \(W\).
Proving that \(N(T)\) is a subspace is an exercise.
<br />
<br /></p>
<div class="purdiv">
Theorem (Dimension Theorem)
</div>
<div class="purbdiv">
If \( \ T: V \rightarrow W\) is linear and \(V\) is finite dimensional, then
		$$
		\begin{align*}
		 \dim(N(T)) + \dim(R(T)) = \dim(V).
		\end{align*}
		$$
</div>
<p><br />
We know that \(N(T)\) is a subspace of \(V\). This means that \(\dim(N(T)) \leq \dim(V)\). This theorem tells us that the difference \(\dim(V) - \dim(N(T))\) is the dimension of \(R(T)\). Even if \(W\) is an infinite dimensional space, we know from linearity, it is finite dimensional. Typically, \(\dim(R(T))\) is called the rank of \(T\) and \(\dim(N(T))\) is called the nullity of \(T\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Consider the map \(T: \mathbf{R}^n \rightarrow \mathbf{R}^n\) where</p>
<div>
	$$
	\begin{align*}
	(x_1,...,x_n) \rightarrow (x_1,...,x_m,0,...,0) \quad (m &lt; n)
	\end{align*}
	$$
</div>
<p>The null space is the set of vectors where their images are the zero vector. This means that given some vector \(v=(x_1,...,x_n)\), we want all of \((x_1,...x_m)\) to be zero since we already know anything after \(m\) is zero by the definition of the map. Therefore,</p>
<div>
	$$
	\begin{align*}
	N(T) = \{(x_1,...,x_n) \ | \ x_1=0,...,x_m=0\}
	\end{align*}
	$$
</div>
<p>Therefore, \(\dim(N(T)) = n - m\). For the range of \(T\),</p>
<div>
	$$
	\begin{align*}
	R(T) &amp;= \{(x_1,...,x_m,0,...,0)\} \\
	 \dim(R(T)) &amp;= m
	\end{align*}
	$$
</div>
<p>From this we see that,</p>
<div>
	$$
	\begin{align*}
	 \dim(R(T)) + \dim(N(T)) &amp;= m + n - m \\ 
	                        &amp;= n \\ 
	                        &amp;= \dim(\mathbf{R}^n)
	\end{align*}
	$$
</div>
<h4><b>Proof</b></h4>
<p>Let \(T: V \rightarrow W\) be a linear transformation and \(V\) be finite dimensional. Let \(\dim(V) = n\). Let \(\dim(N(T) = k\). We know that \(N(T)\) is a subspace of \(V\). Therefore, \(k \leq n\). Since \(\dim(N(T)) = k\), this means that any basis of the null space will have \(k\) elements. So let \(\beta_N = \{u_1,...,u_k\}\) be a basis for \(N(T)\).
<br />
<br />
Claim 1: We can extend the basis \(\beta_N\) by \(n-k\) vectors to form a basis of \(V\). To see this, let \(\beta\) be a basis for \(V\). We will use the replacement theorem, setting \(\mathcal{S} = \beta\) and \(\mathcal{U} = \beta_N\). (TODO: How do we know that \(\beta_N\) is linearly independent?) So we can add \(n-k\) elements of \(\beta\) to \(\beta_N\) so the resulting set generates \(V\). Let this resulting set be \(\beta_V\).</p>
<div>
	$$
	\begin{align*}
	Span(\beta_V) &amp;= Span(\{u_1,...,u_k,v_1,...v_{n-k}\}) 
	              &amp;= V.
	\end{align*}
	$$
</div>
<p>We claim that \(\beta_V\) is a basis for \(V\). How do we know this? We know that it spans \(V\). Moreover, it has \(k+n-k = n\) elements which is the dimension of \(V\). To see that the vectors are linearly independent, suppose that it’s not. Since \(\beta_V\) generates \(V\), then by the refinement theorem, we can take away an element and still have a span that generates \(V\). But then if do take an element out, this means that we’ll have \(n-1\) elements in \(\beta_V\). This is a contradiction since we need at least \(n\) elements to span \(V\).
<br />
<br />
The second claim is that \(\{T(v_1),...,T(v_{n-k})\}\) is a basis for \(R(T)\). If we prove this, then we’ll be done since,</p>
<div>
	$$
	\begin{align*}
	 \dim(N(T)) + \dim(R(T)) &amp;= k + (n-k) \\ 
	                        &amp;= n \\ 
	                        &amp;= \dim(V).
	\end{align*}
	$$
</div>
<p>So now we need to prove that \(\{T(v_1),...,T(v_{n-k})\}\) is a basis. To do so, we need to prove that it generates \(R(T)\) and so \(Span(\{T(v_1),...,T(v_{n-k})\}) = R(T)\). Additionally, we need to prove that the vectors are linearly independent. To see that it generates \((R)\), we know that \(R(T) = \{T(v) \ | \ v \in V\}\). Furthermore, we know that any vector \(v \in V\) an be written as a linear combinations in terms of the elements in \(\beta_V\) and we know that \(T\) is linear. So now we can re-write the definition as,</p>
<div>
	$$
	\begin{align*}
	R(T) &amp;= \{T(v) \ | \ v \in V\} \\
	     &amp;= \{T(a_1u_1 + ... + a_ku_k + b_1v_1 + ... + b_{n-k}v_{n-k}) \ | \ a_1,...,a_k \in \mathbf{R}, b_1,...b_{n-k}\in \mathbf{R} \}. \\
	     &amp;= \{a_1T(u_1) + ... + a_kT(u_k) + b_1T(v_1) + ... + b_{n-k}T(v_{n-k}) \} \\
		 &amp;= \{b_1T(v_1) + ... + b_{n-k}T(v_{n-k})\} \quad \text{because $u_1,...u_k$ are in the null space ($T(u_i)=0_W$)} \\
		 &amp;= Span(\{T(v_1),...,T(v_{n-k})\}).
	\end{align*}
	$$
</div>
<p>Next, to see that the set is linearly independent, we need to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	 a_1T(v_1) + ... + a_{n-k}T(v_{n-k}) = \bar{0}_W.
	\end{align*}
	$$
</div>
<p>implies that \(a_1 = 0, ..., a_{n-k} = 0\). To see this, we’ll use the linearity of \(T\) where the image of a linear combination is the same as the linear combination of the images to get,</p>
<div>
	$$
	\begin{align*}
	 a_1T(v_1) + ... + a_{n-k}T(v_{n-k}) &amp;= \bar{0}_W \\
	 aT(a_1v_1 + ... + a_{n-k}v_{n-k})  &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p>If this equation holds, this means that \(a_1v_1 + ... + a_{n-k}v_{n-k}\) is in the null space \(N(T)\) by definition since it’s image is the zero vector. We defined previously \(\beta_N\) to be the basis for the null space. This means that</p>
<div>
	$$
	\begin{align*}
	 \{a_1v_1 + ... + a_{n-k}v_{n-k} &amp;\in Span(\{u_1,...,u_k\}) \\
	 &amp;= Span(\beta_N).
	\end{align*}
	$$
</div>
<p>We claim that this must imply that \(a_1 = 0, ..., a_{n-k} = 0\). Why? Suppose not, then the linear combination \(a_1v_1 + ... + a_{n-k}v_{n-k}\) would be a linear combination of the elements \(u_1,...,u_k\). But this mean that we’ll have a linear combination of all the \(v\) vectors and all the \(u\) vectors equal to the zero vector,</p>
<div>
	$$
	\begin{align*}
	 a_1v_1 + ... + a_{n-k}v_{n-k} &amp;= b_1u_1 + ... + b_ku_k 
	  a_1v_1 + ... + a_{n-k}v_{n-k} - b_1u_1 + ... + b_ku_k &amp;= 0
	\end{align*}
	$$
</div>
<p>But all the \(u\) and \(v\) vectors \(\{u_1,...,u_k,v_1,...,v_{n-k}\}\) are part of a basis, \(\beta_V\). So any linear combination of these vectors equaling the zero vector must imply that the coefficients are zero because otherwise they are linearly dependent and this is a contradiction. \(\blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The null space (kernel) of \(T\) is $$ \begin{align*} N(T) = \{v \in V \ | \ T(v) = \bar{0}_W\} \subset V \end{align*} $$ The range (image) of \(T\) is $$ \begin{align*} R(T) = \{T(v) \ | v \in V \} \subset W \end{align*} $$ Notes: So the null space is any vector that the transformation turns it into the zero vector. The range just like the definition is the set of the images of all the vectors in \(V\). If all the vectors have an image, then the range is equal to the codomain and that’s when the transformation is onto. Example \(N(T^b_a) = \{\)functions in \(C^0(\mathbf{R})\) whos average over \([a,b]\) is \(0\}\). In other words, it’s the set of all functions whose definite integral is 0. \(R(T^b_a) = \mathbf{R}\). Here we want all functions whose definite integral is some constant/some real number. In other words, this will be the entire codomain (\(\mathbf{R}\)). We call this map \((T^b_a)\) onto. Theorem If \( \ T: V \rightarrow W\) is linear then \(N(T)\) is a subspace of \(V\) and \(R(T)\) is a subspace of \(W\). Proof: We need to verify the three subspaces properties. To prove that \(R(T)\) is a subspace: We need \(\bar{0}_W \in R(T)\). So we need the zero vector to be in the range. This means that we need a \(v \in V\) such that \(T(v) = \bar{0}_W\). Since \(T\) is linear then \(T(\bar{0}_v) = \bar{0}_W\). (We proved this in the previous lecture) We need \(R(T)\) to be closed under addition. Suppose \(w_1, w_2 \in R(T)\), then \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1\) and \(v_2\). Therefore, $$ \begin{align*} w_1 + w_2 &amp;= T(v_1) + T(v_2) \\ &amp;= T(v_1 + v_2) \ \text{because $$T$$ is linear} \end{align*} $$ By definition of a vector space \(v_1 + v_2 \in V\), so the image of the sum \(w_1 + w_2 \in R(T)\) as required. We need \(R(T)\) to be closed under multiplication. Suppose \(w_1 \in R(T)\) and \(c\) is a scalar. We need \(cw_1 \in R(T)\). Since \(w_1 \in R(T)\), then there must be a vector \(v_1 \in V\) such that \(w_1 = T(v_1)\). Notice now, $$ \begin{align*} w_1 &amp;= T(v_1) \\ cw_1 &amp;= cT(v_1) \\ &amp;= T(cv_1) \ \text{because $$T$$ is linear} \end{align*} $$ By definition of a vector space \(cv_1 \in V\), so the image of the product \(cv_1 \in R(T)\) as required.]]></summary></entry><entry><title type="html">Lecture 10: Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 10: Linear Transformations" /><published>2024-07-27T01:01:36-07:00</published><updated>2024-07-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations.html"><![CDATA[<div class="purdiv">
Definition
</div>
<div class="purbdiv">
A map \(T\) from a vector space \(V\) to a vector space \(W\), \(T: V \rightarrow W\) is linear for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\) if
<ol>
	<li>\(T(v_1+v_2) = T(v_1) + T(v_2))\)</li>
	<li>\(T(cv_1) = cT(v_1)\)</li>
</ol>
</div>
<p><br />
Remark: The two conditions can be combined together and so \(T: V \rightarrow W\) is linear above if and only if \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\) for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\).
<br />
<br />
Proof: 
<br />
\(\Rightarrow\): Assume \(T\) is linear. Then,</p>
<div>
	$$
	\begin{align*}
	 T(v_1 + cv_2) &amp;= T(v_1 + cv_2) \\ 
	              &amp;= T(v_1) + T(cv_2) \text{ (By property (1))} \\
				  &amp;= T(v_1) + cT(v_2) \text{ (By property (2))}
	\end{align*}
	$$
</div>
<p>\(\Leftarrow\): Assume \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). Then to show that property (1) is true, notice that</p>
<div>
	$$
	\begin{align*}
	 T(v_1 + v_2) &amp;= T(v_1 + (1)v_2) \\
	              &amp;= T(v_1) + (1)T(v_2) \\
				  &amp;= T(v_1) + T(v_2).
	\end{align*}
	$$
</div>
<p>And to see that property (2) is true, notice that</p>
<div>
	$$
	\begin{align*}
	 T(cv_1) &amp;= T(bar{0}_V + cv_1) \\
	         &amp;= T(\bar{0}_V) + cT(v_1)
	\end{align*}
	$$
</div>
<p>To finish the proof we want to additionally show that \(T(\bar{0}_V) = \bar{0}_W\). How do we do this? We can only use the assumption that \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). To do this notice that,</p>
<div>
	$$
	\begin{align*}
	 \bar{0}_W + T(\bar{0}_V) &amp;= T(\bar{0}_V) \text{ (We're just adding the zero vector)} \\
	         &amp;= T(\bar{0}_V + (1)\bar{0}_V) \text{ (using (c))} \\
			 &amp;= T(\bar{0}_V) + (1)T(\bar{0}_V) \text{ (using (c))} \\
			 &amp;= T(\bar{0}_V) + T(\bar{0}_V) \\
	\bar{0}_W &amp;= T(\bar{0}_V) 
			 
	\end{align*}
	$$
</div>
<p>Another way to do this is the following</p>
<div>
	$$
	\begin{align*}
	 T(\bar{0}_V) &amp;= T(v_1 - v_1) \\
	         &amp;= T(v_1 + (-1)v_1) \\
			 &amp;= T(v_1) + (-1)T(v_1) \\
			 &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p><br />
Remark 2: Suppose we have a linear transformation \(T: V \rightarrow W\)</p>
<div>
	$$
	\begin{align*}
	 T(a_1u_1 + ... + a_ku_k) &amp;= T(a_1u_1 + ... + a_{k-1}u_{k-1}) + a_kT(u_k) \text{ Using property (3)}\\
	 &amp;= a_1T(u_1) + ... + a_kT(u_k)
	\end{align*}
	$$
</div>
<p><br />
This is crucial because this says that the image of a linear combination with coefficients \(a_1, ... a_k\) is again a linear combination in the new vector space with coefficients \(a_1,...a_k\) except that it’s a linear combination of the image of the original vectors.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>For \(V, W\), the map \(T_0: V \rightarrow W, u \rightarrow \bar{0}_W\) is linear.
<br />
<br />
We need to verify that it is linear by verifying \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). This is easy because for any vectors \(v_1, v_2\),</p>
<div>
	$$
	\begin{align*}
	 T_0(v_1 + cv_2) &amp;= \bar{0}_W.
	 \end{align*}
	$$
</div>
<p>Moreover, we also have</p>
<div>
	$$
	\begin{align*}
	 T_0(v_1) + T_0(cv_2) &amp;= \bar{0}_W + c\bar{0}_W \\
	                      &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p>The two sides are equal and so \(T_0\) is linear.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The map \(I_V: V \rightarrow V, u \rightarrow u\) is linear as well.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>The map \(T: \mathbf{R}^2 \rightarrow \mathbf{R}^2, (x,y) \rightarrow (-y,x)\) is linear as well. (a rotation by 90 degrees, todo: add pic). To see why it’s linear, notice that</p>
<div>
	$$
	\begin{align*}
	 T((x_1,y_1) + c(x_2,y_2)) &amp;= T((x_1 + cx_2, y_1 + cy_2)) \text{ (add the two vectors to get one vector)}\\ 
	 &amp;= (-y_1-cy_2, x_1+cx_2).
	 \end{align*}
	$$
</div>
<p>Moreover, notice that</p>
<div>
	$$
	\begin{align*}
	 T((x_1,y_1)) + T((cx_2, cy_2)) &amp;= (-y_1, x_1) + (-cy_2, cx_2) \\
	                                &amp;= (-y_1-cy_2, x_1 + cx_2).
	\end{align*}
	$$
</div>
<p>Both sides are equal and so the transformation is linear.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>The map \(T: P \rightarrow P, f(x) \rightarrow f'(x)\) is linear. Note here that the map The map \(T (P_n \rightarrow P_{n-1}): f(x) \rightarrow f'(x)\) is different because the domain and codomain are different here! this is crucial. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>Let \(A \in M_{m \times n}\). The map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\) is linear. Remember here \(A\bar{x}\) are the linear combinations of the column vectors of \(A\) with the coefficients being the entries of \(\bar{x}\). The crucial thing here is that if \(V\) and \(W\) are both finite dimensional can be represented with this kind of transformation (matrix). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>For \(a &lt; b\), define the map \(T_a^b: C^0(\mathbf{R}) \rightarrow \mathbf{R}, f \rightarrow \int_a^b f(x)dx\) (\(C^0\) is the set of continuous functions on \(\mathbf{R})\)). 
<br />
<br />
Recall the dimension of \(\mathbf{R}\) is 1 and the dimension of \(C^0\) is infinte because the set of all polynomials (which has dimension infinity) is a subset of the set of continuous functions. Therefore, the set of continuous function has dimension infinity as well. This mapping goes from an infinite dimensional space to a finite dimensional space.
<br />
<br />
To prove that this mapping in linear, we notice that</p>
<div>
	$$
	\begin{align*}
	 T_a^b(f + cg) &amp;= \int_a^b (f(x) + cg(x))dx \\
	          &amp;= \int_a^b f(x)dx + c \int_a^b g(x)dx \ \text{ (By Calculus)} \\
	 &amp;= T_a^b(f) + cT_a^b(g).
	\end{align*}
	$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A map \(T\) from a vector space \(V\) to a vector space \(W\), \(T: V \rightarrow W\) is linear for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\) if \(T(v_1+v_2) = T(v_1) + T(v_2))\) \(T(cv_1) = cT(v_1)\) Remark: The two conditions can be combined together and so \(T: V \rightarrow W\) is linear above if and only if \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\) for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\). Proof: \(\Rightarrow\): Assume \(T\) is linear. Then, $$ \begin{align*} T(v_1 + cv_2) &amp;= T(v_1 + cv_2) \\ &amp;= T(v_1) + T(cv_2) \text{ (By property (1))} \\ &amp;= T(v_1) + cT(v_2) \text{ (By property (2))} \end{align*} $$ \(\Leftarrow\): Assume \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). Then to show that property (1) is true, notice that $$ \begin{align*} T(v_1 + v_2) &amp;= T(v_1 + (1)v_2) \\ &amp;= T(v_1) + (1)T(v_2) \\ &amp;= T(v_1) + T(v_2). \end{align*} $$ And to see that property (2) is true, notice that $$ \begin{align*} T(cv_1) &amp;= T(bar{0}_V + cv_1) \\ &amp;= T(\bar{0}_V) + cT(v_1) \end{align*} $$ To finish the proof we want to additionally show that \(T(\bar{0}_V) = \bar{0}_W\). How do we do this? We can only use the assumption that \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). To do this notice that, $$ \begin{align*} \bar{0}_W + T(\bar{0}_V) &amp;= T(\bar{0}_V) \text{ (We're just adding the zero vector)} \\ &amp;= T(\bar{0}_V + (1)\bar{0}_V) \text{ (using (c))} \\ &amp;= T(\bar{0}_V) + (1)T(\bar{0}_V) \text{ (using (c))} \\ &amp;= T(\bar{0}_V) + T(\bar{0}_V) \\ \bar{0}_W &amp;= T(\bar{0}_V) \end{align*} $$ Another way to do this is the following $$ \begin{align*} T(\bar{0}_V) &amp;= T(v_1 - v_1) \\ &amp;= T(v_1 + (-1)v_1) \\ &amp;= T(v_1) + (-1)T(v_1) \\ &amp;= \bar{0}_W \end{align*} $$ Remark 2: Suppose we have a linear transformation \(T: V \rightarrow W\) $$ \begin{align*} T(a_1u_1 + ... + a_ku_k) &amp;= T(a_1u_1 + ... + a_{k-1}u_{k-1}) + a_kT(u_k) \text{ Using property (3)}\\ &amp;= a_1T(u_1) + ... + a_kT(u_k) \end{align*} $$ This is crucial because this says that the image of a linear combination with coefficients \(a_1, ... a_k\) is again a linear combination in the new vector space with coefficients \(a_1,...a_k\) except that it’s a linear combination of the image of the original vectors. Example 1 For \(V, W\), the map \(T_0: V \rightarrow W, u \rightarrow \bar{0}_W\) is linear. We need to verify that it is linear by verifying \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). This is easy because for any vectors \(v_1, v_2\), $$ \begin{align*} T_0(v_1 + cv_2) &amp;= \bar{0}_W. \end{align*} $$ Moreover, we also have $$ \begin{align*} T_0(v_1) + T_0(cv_2) &amp;= \bar{0}_W + c\bar{0}_W \\ &amp;= \bar{0}_W \end{align*} $$ The two sides are equal and so \(T_0\) is linear. Example 2 The map \(I_V: V \rightarrow V, u \rightarrow u\) is linear as well. Example 3 The map \(T: \mathbf{R}^2 \rightarrow \mathbf{R}^2, (x,y) \rightarrow (-y,x)\) is linear as well. (a rotation by 90 degrees, todo: add pic). To see why it’s linear, notice that $$ \begin{align*} T((x_1,y_1) + c(x_2,y_2)) &amp;= T((x_1 + cx_2, y_1 + cy_2)) \text{ (add the two vectors to get one vector)}\\ &amp;= (-y_1-cy_2, x_1+cx_2). \end{align*} $$ Moreover, notice that $$ \begin{align*} T((x_1,y_1)) + T((cx_2, cy_2)) &amp;= (-y_1, x_1) + (-cy_2, cx_2) \\ &amp;= (-y_1-cy_2, x_1 + cx_2). \end{align*} $$ Both sides are equal and so the transformation is linear. Example 4 The map \(T: P \rightarrow P, f(x) \rightarrow f'(x)\) is linear. Note here that the map The map \(T (P_n \rightarrow P_{n-1}): f(x) \rightarrow f'(x)\) is different because the domain and codomain are different here! this is crucial. Example 6 Let \(A \in M_{m \times n}\). The map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\) is linear. Remember here \(A\bar{x}\) are the linear combinations of the column vectors of \(A\) with the coefficients being the entries of \(\bar{x}\). The crucial thing here is that if \(V\) and \(W\) are both finite dimensional can be represented with this kind of transformation (matrix). Example 6 For \(a &lt; b\), define the map \(T_a^b: C^0(\mathbf{R}) \rightarrow \mathbf{R}, f \rightarrow \int_a^b f(x)dx\) (\(C^0\) is the set of continuous functions on \(\mathbf{R})\)). Recall the dimension of \(\mathbf{R}\) is 1 and the dimension of \(C^0\) is infinte because the set of all polynomials (which has dimension infinity) is a subset of the set of continuous functions. Therefore, the set of continuous function has dimension infinity as well. This mapping goes from an infinite dimensional space to a finite dimensional space. To prove that this mapping in linear, we notice that $$ \begin{align*} T_a^b(f + cg) &amp;= \int_a^b (f(x) + cg(x))dx \\ &amp;= \int_a^b f(x)dx + c \int_a^b g(x)dx \ \text{ (By Calculus)} \\ &amp;= T_a^b(f) + cT_a^b(g). \end{align*} $$ References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 9: Basis Vectors and The Replacement Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem.html" rel="alternate" type="text/html" title="Lecture 9: Basis Vectors and The Replacement Theorem" /><published>2024-07-26T01:01:36-07:00</published><updated>2024-07-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem.html"><![CDATA[<!------------------------------------------------------------------------------------>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(B \subset V\) is a basis of \(V\) if
	<ol>
		<li>\(B\) is linearly independent.</li>
		<li>\(Span(B) = V\). (\(B\) generates \(V\))</li>
	</ol>
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Every vector space has a basis.
</div>
<p><br />
Proof in 1.7.
<br />
<br /></p>
<div class="purdiv">
Theorem 1.8
</div>
<div class="purbdiv">
If \(\beta \subset V\) is a basis then every \(u \in V\) can be expressed in a unique way as an element of \(Span(\beta)\).
</div>
<p><br />
Proof: Let \(u \in V\). Let \(\beta \subset V\) be a basis for \(V\). We can express \(u\) as</p>
<div>
	$$
	\begin{align*}
	 u = a_1u_1 + ... + a_ku_k.
	\end{align*}
	$$
</div>
<p>for \(u_1, ..., u_k \in \beta\) and \(a_1, ....,a_k \in \mathbf{R}\). We claim that this is the only way to express \(u\) in terms of the elements in \(\beta\). To see why, suppose for the sake of contradiction that it is not the only way. This means that we can also express \(u\) as</p>
<div>
 	$$
 	\begin{align*}
 	 u = b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}.
 	\end{align*}
 	$$
</div>
<p>But we know that \(u - u = \bar{0}\). Evaluating \(u-u\),</p>
<div>
	$$
	\begin{align*}
	 u - u &amp;= (a_1u_1 + ... + a_ku_k) - (b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\
	\bar{0} &amp;= (a_1-b_1)u_1 + ... + (a_k-b_k)u_k - (b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\
	\end{align*}
	$$
</div>
<p>We also know that \(\beta\) is linearly independent. So for the linear combination above, all the coefficients must be 0. Therefore, we must have,</p>
<div>
	$$
	a_1 = b_1, a_2 = b_2, ..., a_k=b_k, b_{k+1}=0, b_{l} = 0.
	$$
</div>
<p>This is exactly the first representation of \(u\) which is a contradiction and so \(u\) can only be uniquely expressed in terms of the elements of \(\beta\).
<br />
<br />
<b>Note here</b> that up to this point, this was all covered in lecture 8 but I moved it here.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>In \(\mathbf{R}^n\), let</p>
<div>
	$$
	\begin{align*}
	 e_1 = (1,0,...,0), e_2=(0,1,0,...,0),...,e_n=(0,...,1)
	\end{align*}
	$$
</div>
<p>\(\beta = \{e_1,e_2,...,e_n\}\) is the standard basis of \(\mathbf{R}^n\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>In the vector space of polynomials of degree at most \(n\) (\(P_n\)), the standard basis is \(\beta = \{1, x, x^2, ..., x^n\}\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Recall the space of all sequences, \(V = \{\{a_n\}\}\) where \(\{a_n\}\) is a sequence. Let \(e_j\) be the sequence</p>
<div>
	$$
	\begin{align*}
	 0,0,...,0,1,0,0....,0,...
	\end{align*}
	$$
</div>
<p>Where the \(j\)th term is the term 1 above. Then, the standard basis is \(\beta = \{e_1, e_2, ....\}\). This basis has infinitely many terms unlike the previous two examples.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>The vector space of all polynomials (\(P\)). The standard basis is \(\beta = \{1, x, x^2, x^3, ...\}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>\(\mathcal{F}(\mathbf{R})\) has a basis … hard to describe but it exists!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.9
</div>
<div class="purbdiv">
If \(V\) has a finite generating set, then \(V\) has a finite basis.
</div>
<p><br />
Proof: This follows from the Refinement Theorem. If \(\{u_1,...,u_k\}\) is a finite generating set, then we can find a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and has span \(Span(\{u_{i1},...,u_{ij}\}) = V\).
<br />
<br />
Study notes: <a href="https://strncat.github.io/jekyll/update/2024/07/30/1-6-theorem-1.9.html">Here</a> is another proof from the book.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.10 (Replacement Theorem)
</div>
<div class="purbdiv">
Suppose \(\mathcal{S} = \{s_1,...,s_n\}\) generates \(V\). If \(\ \mathcal{U} = \{u_1,...,u_k\}\) is a linearly independent subset of \(V\), then \(k \leq n\) and there is a subset \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) such that \(Span(\mathcal{U} \cup \mathcal{T}) = V\).
</div>
<p><br />
Notes: So here, \(\mathcal{U}\) is a linearly independent subset of \(V\). But this doesn’t mean that it’s a basis because it might need some additional vectors added to it. If we know another set \(S\) that generates \(V\), then there is a subset \(\mathcal{T} \subset \mathcal{S}\) such that the span of both \(\mathcal{T}\) and \(\mathcal{U}\) will generate \(V\).
<br />
<br />
<b>Proof</b>: By induction on \(k\).
<br />
Base case: \(k = 0\). This means that \(\mathcal{U} = \emptyset\). The empty set is linearly independent and \(k \leq n\). Also, \(n - k = n\) and We can take \(\mathcal{T} = \mathcal{S}\). We know that \(\mathcal{S}\) generates \(V\), so \(\mathcal{U} \cup \mathcal{T}\) generates \(V\) as required.
<br /><br />
Inductive Step: Assume that the theorem is true for \(j\). We need to show that it’s true for \(j+1\). So suppose \(\mathcal{U}_{j+1} = \{u_1, ..., u_{j+1}\}\) is linearly independent. Specifically, we need to show that:</p>
<ul>
	<li>\(j + 1 \leq n\)</li>
	<li>There exists a subset \(\mathcal{T}_{j+1} \in \mathcal{S}\) of size \(n - (j+1)\) such that \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\).</li>
</ul>
<p>Throw the \(j+1\)th element away. So now we have \(\mathcal{U}_j = \{u_1,...,u_j\}\) which is linearly independent by theorem (from the book: If \(S_1 \subseteq S_2\) and \(S_2\) is linearly independent then, \(S_1\) must be linearly independent). 
<br />
By the inductive hypothesis, we know,</p>
<ul>
	<li>\(j \leq n\)</li>
	<li>There exists a subset \(\mathcal{T}_{j} = \{s_1,...,s_{n-j}\}\) has \(n-j\) elements such that \(Span(\{u_1,...,u_j,s_1,...,s_{n-j}\}) = Span(\mathcal{U}_{j} \cup \mathcal{T}_{j}) = V\).</li>
</ul>
<p>But we know that \(u_{j+1}\) is in \(V\). Since \(Span(\mathcal{U}_{j} \cup \mathcal{T}_{j})\) generates \(V\), then we can write \(u_{j+1}\) in terms of the elements in the span.</p>
<div>
	$$
	\begin{align*}
	 u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}.
	\end{align*}
	$$
</div>
<p>Note here that \(b_1,...b_{n-j}\) can’t be all zeros (because if they are, then \(u_{j+1}\) can be written as a linear combination of the elements of \(\mathcal{U}_{j}\) alone. But that’s not possible since we said that the set \(\mathcal{U}_{j+1}\) is linearly independent.). Without the loss of generality, let \(b_{n-j}\) be non-zero. This means that,</p>
<div>
	$$
	\begin{align*}
	 n - j &amp;\geq 1 \\
	 n &amp;\geq j+1
	\end{align*}
	$$
</div>
<p>as desired. Now, we need to satisfy the second condition and find a subset of \(n-(j+1)\) elements such that the span of \(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}\) generates \(V\). We can’t choose the subset \(\mathcal{T}_{j}\) since it has \(n-j\) elements and we need \(n-j-1\) elements. So the strategy is to remove one element from \(\mathcal{T}_{j}\). 
<br />
<br />
Since we said earlier that \(b_{n-j}\) is not zero along with the inductive hypothesis, \(u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}\), we can re-write this as,</p>
<div>
	$$
	\begin{align*}
	 s_{n-j} = -\frac{1}{b_{n-j}} (a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}+u_{j+1}).
	\end{align*}
	$$
</div>
<p>basically as a linear combination of the other elements from the inductive hypothesis equation. This implies that \(s_{n-j} \in Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\). Let \(\mathcal{T}_{j+1} = s_1,...,s_{n-j-1}\). The last thing to prove is that (\(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1})) = V\). We will do this in two steps. First we will prove that,</p>
<div>
	$$
	\begin{align*}
	 Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}).
	\end{align*}
	$$
</div>
<p>These spans are equal which means that adding \(s_{n-j}\) to the span, didn’t increase the span. This is because we showed earlier that \(s_{n-j}\) is a linear combination of all the other elements \(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}\). Moreover,</p>
<div>
	$$
	\begin{align*}
	Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}).
	\end{align*}
	$$
</div>
<p>This is true because we also showed that \(u_{j+1}\) is a linear combinations of the elements \(\{u_1,...u_{j},s_1,...,s_{n-j}\}\). Therefore from the previous two equations, we can conclude that</p>
<div>
	$$
	\begin{align*}
	Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}).
	\end{align*}
	$$
</div>
<p>But we know that \(Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}) = V\) from the inductive hypothesis. Therefore, \(Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\) also generates \(V\) and so \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (Corollary 1 in the book)
</div>
<div class="purbdiv">
If \(V\) has a finite basis, then any basis of \(V\) has the same number of elements.
</div>
<p><br />
Proof: Let \(\beta\) be a finite basis with \(n\) elements. Let \(\bar{\beta}\) be another basis. We claim that \(\bar{\beta}\) is finite. Suppose for the sake of contradiction that it wasn’t, then \(\bar{\beta}\) contains a set \(\bar{U}\) that contains at least \(n+1\) linearly independent vectors. Apply the Replacement Theorem with \(\mathcal{S} = \beta, \mathcal{U} = \bar{U}\). This means that if the number of elements in \(\bar{U}\) is \(k\) then \(k\) must be less than the number of elements in \(S\). But \(S\) has \(n\) elements and so \(n+1 \leq n\) is a contradiction. Therefore, \(\bar{beta}\) must be finite.
<br />
<br />
To prove that it must have size \(n\), apply the Replacement Theorem again with \((\mathcal{S} = \beta, \mathcal{U} = \bar{\beta})\). We know the size of \(\bar{\beta}\) must be less than or equal to \(n\),</p>
<div>
	$$
	\begin{align*}
	|\bar{\beta}| \leq n.
	\end{align*}
	$$
</div>
<p>But if we apply the replacement theorem with \((\mathcal{S} = \bar{\beta}, \mathcal{U} = \beta)\), then the size of \(\beta\) must be less than or equal to \(\bar{\beta}\),</p>
<div>
	$$
	\begin{align*}
	n \leq |\bar{\beta}|.
	\end{align*}
	$$
</div>
<p>From these two inequalities, we must have \(\bar{\beta} = n\). \(\blacksquare\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(V\) is finite dimensional if it has a finite basis. The number of elements in any basis for \(V\) is the dimension of \(V\). Otherwise we say \(V\) is infinite dimensional.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
<li>\(\dim(\mathbf{R}^n) = n\) </li>
<li> \(\dim(M_{m \times n}) = mn\) </li>
One basis for this space is \(\{E^{ij} \in M_{m \times n} \ | \ a_{ij} = 1, \text{ all other elements are 0}\}\). So We'll have \(mn\) matrices where each matrix will have a 1 in the \((i,j)\) position.<br /><br />

<li> \(\dim(P_n) = n+1\) </li>
<li> \(\dim(P) = \infty\) </li>
</ul>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.11
</div>
<div class="purbdiv">
Let \(W\) be a subspace of \(V\). If \(V\) is finite dimensional, then \(\dim W \leq \dim V\), with \(\dim W = \dim V\) if and only if \(W = V\).
</div>
<p><br />
Proof:
Let \(W\) be a subspace of \(V\). We’re given that \(V\) is finite dimensional. Let \(\dim V = n\). Let \(\beta_V = \{u_1, ..., u_n\}\) be a basis for \(V\). The goal is to find a basis for \(W\) that has fewer elements than the basis of \(V\). (Note here that the strategy should not be modifying the basis for \(V\) since we don’t know if these vectors are even in \(W\). Instead we need to use another tool which is the replacement theorem.)
<br />
<br />
To find a basis for \(W\), we need a subset of \(W\) that is linearly independent and also generates \(W\). Let \(\mathcal{U} = \{w_1, ..., w_k\} \subseteq W\) be a linearly independent. Because \(\beta_V\) generates \(V\) and \(\mathcal{U}\) is a linearly independent set, we can use the Replacement Theorem by setting \(\mathcal{S} = \beta_V\) and \(U = \mathcal{U}\). This gives us the assertion that \(k \leq n\). (Note here that we don’t need to use the other result that the theorem asserts. We just need the assertion about the size).
<br />
<br />
With this observation (\(k \leq n\)), we will construct a basis for \(W\) recursively keeping the set linearly independent in the process,</p>
<ul>
	<li>If \(W = \{\bar{0}_V\}\) (\(W\) is a subspace and it must contain the zero vector), then \(\dim W = 0\) and we are done (\(\dim W \leq \dim V)\). </li>
	<li>Otherwise there is some non-zero vector so choose that vector \(w_1 \neq \bar{0}_v\)</li>
	<li>If \(W = Span(\{w_1\})\), then we stop.</li>
	<li> Otherwise, choose \(w_2\) not in \(Span(\{w_1\})\)). Note here that \(\{w_1, w_2\}\) is a linearly independent set by construction.</li>
</ul>
<p>We repeat this process, if the span is equal to \(W\), we stop. Otherwise we add a new vector if it’s not in the span of the current constructed set. This process will stop at some set \(\{w_1,...,w_k\}\) such that \(W = Span(\{w_1,...,w_k\})\). We know this set is linearly independent and that \(W = Span(\{w_1,...,w_k\})\) by construction. Therefore, it’s a basis for \(W\) and so \(\dim W = k\). By the Replacement Theorem we discussed previously we know that \(k \leq n\) and so \(\dim W \leq \dim V\). \(\blacksquare\) 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(B \subset V\) is a basis of \(V\) if \(B\) is linearly independent. \(Span(B) = V\). (\(B\) generates \(V\)) Theorem Every vector space has a basis. Proof in 1.7. Theorem 1.8 If \(\beta \subset V\) is a basis then every \(u \in V\) can be expressed in a unique way as an element of \(Span(\beta)\). Proof: Let \(u \in V\). Let \(\beta \subset V\) be a basis for \(V\). We can express \(u\) as $$ \begin{align*} u = a_1u_1 + ... + a_ku_k. \end{align*} $$ for \(u_1, ..., u_k \in \beta\) and \(a_1, ....,a_k \in \mathbf{R}\). We claim that this is the only way to express \(u\) in terms of the elements in \(\beta\). To see why, suppose for the sake of contradiction that it is not the only way. This means that we can also express \(u\) as $$ \begin{align*} u = b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}. \end{align*} $$ But we know that \(u - u = \bar{0}\). Evaluating \(u-u\), $$ \begin{align*} u - u &amp;= (a_1u_1 + ... + a_ku_k) - (b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\ \bar{0} &amp;= (a_1-b_1)u_1 + ... + (a_k-b_k)u_k - (b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\ \end{align*} $$ We also know that \(\beta\) is linearly independent. So for the linear combination above, all the coefficients must be 0. Therefore, we must have, $$ a_1 = b_1, a_2 = b_2, ..., a_k=b_k, b_{k+1}=0, b_{l} = 0. $$ This is exactly the first representation of \(u\) which is a contradiction and so \(u\) can only be uniquely expressed in terms of the elements of \(\beta\). Note here that up to this point, this was all covered in lecture 8 but I moved it here. Example 1 In \(\mathbf{R}^n\), let $$ \begin{align*} e_1 = (1,0,...,0), e_2=(0,1,0,...,0),...,e_n=(0,...,1) \end{align*} $$ \(\beta = \{e_1,e_2,...,e_n\}\) is the standard basis of \(\mathbf{R}^n\). Example 2 In the vector space of polynomials of degree at most \(n\) (\(P_n\)), the standard basis is \(\beta = \{1, x, x^2, ..., x^n\}\). Example 3 Recall the space of all sequences, \(V = \{\{a_n\}\}\) where \(\{a_n\}\) is a sequence. Let \(e_j\) be the sequence $$ \begin{align*} 0,0,...,0,1,0,0....,0,... \end{align*} $$ Where the \(j\)th term is the term 1 above. Then, the standard basis is \(\beta = \{e_1, e_2, ....\}\). This basis has infinitely many terms unlike the previous two examples. Example 4 The vector space of all polynomials (\(P\)). The standard basis is \(\beta = \{1, x, x^2, x^3, ...\}\). Example 5 \(\mathcal{F}(\mathbf{R})\) has a basis … hard to describe but it exists! Theorem 1.9 If \(V\) has a finite generating set, then \(V\) has a finite basis. Proof: This follows from the Refinement Theorem. If \(\{u_1,...,u_k\}\) is a finite generating set, then we can find a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and has span \(Span(\{u_{i1},...,u_{ij}\}) = V\). Study notes: Here is another proof from the book. Theorem 1.10 (Replacement Theorem) Suppose \(\mathcal{S} = \{s_1,...,s_n\}\) generates \(V\). If \(\ \mathcal{U} = \{u_1,...,u_k\}\) is a linearly independent subset of \(V\), then \(k \leq n\) and there is a subset \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) such that \(Span(\mathcal{U} \cup \mathcal{T}) = V\). Notes: So here, \(\mathcal{U}\) is a linearly independent subset of \(V\). But this doesn’t mean that it’s a basis because it might need some additional vectors added to it. If we know another set \(S\) that generates \(V\), then there is a subset \(\mathcal{T} \subset \mathcal{S}\) such that the span of both \(\mathcal{T}\) and \(\mathcal{U}\) will generate \(V\). Proof: By induction on \(k\). Base case: \(k = 0\). This means that \(\mathcal{U} = \emptyset\). The empty set is linearly independent and \(k \leq n\). Also, \(n - k = n\) and We can take \(\mathcal{T} = \mathcal{S}\). We know that \(\mathcal{S}\) generates \(V\), so \(\mathcal{U} \cup \mathcal{T}\) generates \(V\) as required. Inductive Step: Assume that the theorem is true for \(j\). We need to show that it’s true for \(j+1\). So suppose \(\mathcal{U}_{j+1} = \{u_1, ..., u_{j+1}\}\) is linearly independent. Specifically, we need to show that: \(j + 1 \leq n\) There exists a subset \(\mathcal{T}_{j+1} \in \mathcal{S}\) of size \(n - (j+1)\) such that \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). Throw the \(j+1\)th element away. So now we have \(\mathcal{U}_j = \{u_1,...,u_j\}\) which is linearly independent by theorem (from the book: If \(S_1 \subseteq S_2\) and \(S_2\) is linearly independent then, \(S_1\) must be linearly independent). By the inductive hypothesis, we know, \(j \leq n\) There exists a subset \(\mathcal{T}_{j} = \{s_1,...,s_{n-j}\}\) has \(n-j\) elements such that \(Span(\{u_1,...,u_j,s_1,...,s_{n-j}\}) = Span(\mathcal{U}_{j} \cup \mathcal{T}_{j}) = V\). But we know that \(u_{j+1}\) is in \(V\). Since \(Span(\mathcal{U}_{j} \cup \mathcal{T}_{j})\) generates \(V\), then we can write \(u_{j+1}\) in terms of the elements in the span. $$ \begin{align*} u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}. \end{align*} $$ Note here that \(b_1,...b_{n-j}\) can’t be all zeros (because if they are, then \(u_{j+1}\) can be written as a linear combination of the elements of \(\mathcal{U}_{j}\) alone. But that’s not possible since we said that the set \(\mathcal{U}_{j+1}\) is linearly independent.). Without the loss of generality, let \(b_{n-j}\) be non-zero. This means that, $$ \begin{align*} n - j &amp;\geq 1 \\ n &amp;\geq j+1 \end{align*} $$ as desired. Now, we need to satisfy the second condition and find a subset of \(n-(j+1)\) elements such that the span of \(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}\) generates \(V\). We can’t choose the subset \(\mathcal{T}_{j}\) since it has \(n-j\) elements and we need \(n-j-1\) elements. So the strategy is to remove one element from \(\mathcal{T}_{j}\). Since we said earlier that \(b_{n-j}\) is not zero along with the inductive hypothesis, \(u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}\), we can re-write this as, $$ \begin{align*} s_{n-j} = -\frac{1}{b_{n-j}} (a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}+u_{j+1}). \end{align*} $$ basically as a linear combination of the other elements from the inductive hypothesis equation. This implies that \(s_{n-j} \in Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\). Let \(\mathcal{T}_{j+1} = s_1,...,s_{n-j-1}\). The last thing to prove is that (\(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1})) = V\). We will do this in two steps. First we will prove that, $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}). \end{align*} $$ These spans are equal which means that adding \(s_{n-j}\) to the span, didn’t increase the span. This is because we showed earlier that \(s_{n-j}\) is a linear combination of all the other elements \(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}\). Moreover, $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}). \end{align*} $$ This is true because we also showed that \(u_{j+1}\) is a linear combinations of the elements \(\{u_1,...u_{j},s_1,...,s_{n-j}\}\). Therefore from the previous two equations, we can conclude that $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}). \end{align*} $$ But we know that \(Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}) = V\) from the inductive hypothesis. Therefore, \(Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\) also generates \(V\) and so \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). \(\blacksquare\) Theorem (Corollary 1 in the book) If \(V\) has a finite basis, then any basis of \(V\) has the same number of elements. Proof: Let \(\beta\) be a finite basis with \(n\) elements. Let \(\bar{\beta}\) be another basis. We claim that \(\bar{\beta}\) is finite. Suppose for the sake of contradiction that it wasn’t, then \(\bar{\beta}\) contains a set \(\bar{U}\) that contains at least \(n+1\) linearly independent vectors. Apply the Replacement Theorem with \(\mathcal{S} = \beta, \mathcal{U} = \bar{U}\). This means that if the number of elements in \(\bar{U}\) is \(k\) then \(k\) must be less than the number of elements in \(S\). But \(S\) has \(n\) elements and so \(n+1 \leq n\) is a contradiction. Therefore, \(\bar{beta}\) must be finite. To prove that it must have size \(n\), apply the Replacement Theorem again with \((\mathcal{S} = \beta, \mathcal{U} = \bar{\beta})\). We know the size of \(\bar{\beta}\) must be less than or equal to \(n\), $$ \begin{align*} |\bar{\beta}| \leq n. \end{align*} $$ But if we apply the replacement theorem with \((\mathcal{S} = \bar{\beta}, \mathcal{U} = \beta)\), then the size of \(\beta\) must be less than or equal to \(\bar{\beta}\), $$ \begin{align*} n \leq |\bar{\beta}|. \end{align*} $$ From these two inequalities, we must have \(\bar{\beta} = n\). \(\blacksquare\). Definition \(V\) is finite dimensional if it has a finite basis. The number of elements in any basis for \(V\) is the dimension of \(V\). Otherwise we say \(V\) is infinite dimensional. Examples \(\dim(\mathbf{R}^n) = n\) \(\dim(M_{m \times n}) = mn\) One basis for this space is \(\{E^{ij} \in M_{m \times n} \ | \ a_{ij} = 1, \text{ all other elements are 0}\}\). So We'll have \(mn\) matrices where each matrix will have a 1 in the \((i,j)\) position.]]></summary></entry><entry><title type="html">Lecture 8: More on Linear Dependance</title><link href="http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance.html" rel="alternate" type="text/html" title="Lecture 8: More on Linear Dependance" /><published>2024-07-25T01:01:36-07:00</published><updated>2024-07-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \( \{u_1,...,u_k\} \subset V\) is linearly dependent, then there exists one \(u_j\) can be expressed as a linear combination of the others.
</div>
<p><br />
Proof:
Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. This means that for some (not all zero) scalars \(a_1,...,a_k\), we must have</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + ... + a_ju_j + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p>Without the loss of generality that \(a_j \neq 0\) for some \(j\). Multiply the equation by \(1/a_j\)</p>
<div>
	$$
	\begin{align*}
	\frac{1}{a_j}(a_1u_1 + ... + a_ju_j + ... + a_ku_k) &amp;= \frac{1}{a_j}\bar{0} \\
	\frac{a_1}{a_j}u_1 + ... + u_j + ... + \frac{a_k}{a_j}u_k &amp;= \bar{0} \\
	-\frac{a_1}{a_j}u_1 - ... - \frac{a_k}{a_j}u_k &amp;= u_j
	\end{align*}
	$$
</div>
<p>Therefore, \(u_j\) is a linear combination of the other vectors as we wanted to show. \(\blacksquare\)
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (Refinement Theorem)
</div>
<div class="purbdiv">
Suppose \( \{u_1,...,u_k\} \subset V\) is linearly dependent. There is a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and satisfies
	$$
	\begin{align*}
	Span(\{u_{i1},...,u_{il}\}) = Span(\{u_1,...,u_k\}).
	\end{align*}
	$$
</div>
<p><br />
Proof:
Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. By the previous theorem, there exists some \(j\) where \(u_j\) is a linear combination of the other vectors in the set. So,</p>
<div>
	$$
	\begin{align*} 
	u_j = b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k.
	\end{align*}
	$$
</div>
<p>Now, delete \(u_j\) from this set. We claim that</p>
<div>
	$$
	\begin{align*} 
	Span(\{u_1,...,u_j,...,u_k\}) = Span(\{u_1,...\hat{u_j},...,u_k\}).
	\end{align*}
	$$
</div>
<p>(the hat symbol means that the variable has been deleted). To show this, we will show that the two sets are subsets of each others which will imply that the two sets are equal.
<br /><br />
Now, given a vector \(\bar{u} = a_1u_1 + ... + a_{j}u_{j} + ... + a_ku_k\) in \(Span(\{u_1,...,u_j,...,u_k\})\), we want to show that \(\bar{u} \in Span(\{u_1,...\hat{u_j},...,u_k\})\). But since we established that \(u_j\) is a linear combination of the other vectors so substitute \(u_j\) in \(\bar{u}\) as follows,</p>
<div>
	$$
	\begin{align*}
	&amp;= a_1u_1 + ... + a_{j}(b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k) + ... + a_ku_k \\
	&amp;= (a_1+ a_jb_1)u_1 + ... + (a_k+a_jb_k)u_k.
	\end{align*}
	$$
</div>
<p>From this we see that given a vector in \(Span(\{u_1,...,u_j,...,u_k\})\), it is also in \(Span(\{u_1,...\hat{u_j},...,u_k\})\). For the other direction, it’s trivial. If we have a vector \(\bar{u} = a_1u_1 + ... + 0u_{j} + ... + a_ku_k\), then it is in \(Span(\{u_1,...,u_j,...,u_k\})\). Therefore, the two spans are equal.
<br />
<br />
But now, if \(\{u_1,...,u_{j-1},u_{j+1},...,u_k\}\) is linearly independent, we then stop. Otherwise we find another \(u_j\) to throw out. This process will stop since we started with a finite number of vectors.
<br />
<br />
Remark: To find \(u_j\), we start with \(u_1\) and ask/settle the question (Q1) can you \(u_1\) as linear combinations of \(u_2, ... ,u_k\). If the answer is yes set \(u_j = u_1\). If the answer is No, then ask (Q2) can \(u_2\) be written as a linear combination of \(u_1, ... , u_k\)?
<br />
<br />
Conclusion: we can refine a finite subset \(\{u_1,...,u_k\}\) to obtain a linearly independent subset \(\{u_{i1},...,u_{il}\}\) with the same span.
<br />
<br />
The rest of this lecture covered the definition of what a basis is and some other small result. I decided to move these to lecture 9 since lecture 9 covered basis in depth.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem If \( \{u_1,...,u_k\} \subset V\) is linearly dependent, then there exists one \(u_j\) can be expressed as a linear combination of the others. Proof: Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. This means that for some (not all zero) scalars \(a_1,...,a_k\), we must have $$ \begin{align*} a_1u_1 + ... + a_ju_j + ... + a_ku_k = \bar{0}. \end{align*} $$ Without the loss of generality that \(a_j \neq 0\) for some \(j\). Multiply the equation by \(1/a_j\) $$ \begin{align*} \frac{1}{a_j}(a_1u_1 + ... + a_ju_j + ... + a_ku_k) &amp;= \frac{1}{a_j}\bar{0} \\ \frac{a_1}{a_j}u_1 + ... + u_j + ... + \frac{a_k}{a_j}u_k &amp;= \bar{0} \\ -\frac{a_1}{a_j}u_1 - ... - \frac{a_k}{a_j}u_k &amp;= u_j \end{align*} $$ Therefore, \(u_j\) is a linear combination of the other vectors as we wanted to show. \(\blacksquare\) Theorem (Refinement Theorem) Suppose \( \{u_1,...,u_k\} \subset V\) is linearly dependent. There is a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and satisfies $$ \begin{align*} Span(\{u_{i1},...,u_{il}\}) = Span(\{u_1,...,u_k\}). \end{align*} $$ Proof: Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. By the previous theorem, there exists some \(j\) where \(u_j\) is a linear combination of the other vectors in the set. So, $$ \begin{align*} u_j = b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k. \end{align*} $$ Now, delete \(u_j\) from this set. We claim that $$ \begin{align*} Span(\{u_1,...,u_j,...,u_k\}) = Span(\{u_1,...\hat{u_j},...,u_k\}). \end{align*} $$ (the hat symbol means that the variable has been deleted). To show this, we will show that the two sets are subsets of each others which will imply that the two sets are equal. Now, given a vector \(\bar{u} = a_1u_1 + ... + a_{j}u_{j} + ... + a_ku_k\) in \(Span(\{u_1,...,u_j,...,u_k\})\), we want to show that \(\bar{u} \in Span(\{u_1,...\hat{u_j},...,u_k\})\). But since we established that \(u_j\) is a linear combination of the other vectors so substitute \(u_j\) in \(\bar{u}\) as follows, $$ \begin{align*} &amp;= a_1u_1 + ... + a_{j}(b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k) + ... + a_ku_k \\ &amp;= (a_1+ a_jb_1)u_1 + ... + (a_k+a_jb_k)u_k. \end{align*} $$ From this we see that given a vector in \(Span(\{u_1,...,u_j,...,u_k\})\), it is also in \(Span(\{u_1,...\hat{u_j},...,u_k\})\). For the other direction, it’s trivial. If we have a vector \(\bar{u} = a_1u_1 + ... + 0u_{j} + ... + a_ku_k\), then it is in \(Span(\{u_1,...,u_j,...,u_k\})\). Therefore, the two spans are equal. But now, if \(\{u_1,...,u_{j-1},u_{j+1},...,u_k\}\) is linearly independent, we then stop. Otherwise we find another \(u_j\) to throw out. This process will stop since we started with a finite number of vectors. Remark: To find \(u_j\), we start with \(u_1\) and ask/settle the question (Q1) can you \(u_1\) as linear combinations of \(u_2, ... ,u_k\). If the answer is yes set \(u_j = u_1\). If the answer is No, then ask (Q2) can \(u_2\) be written as a linear combination of \(u_1, ... , u_k\)? Conclusion: we can refine a finite subset \(\{u_1,...,u_k\}\) to obtain a linearly independent subset \(\{u_{i1},...,u_{il}\}\) with the same span. The rest of this lecture covered the definition of what a basis is and some other small result. I decided to move these to lecture 9 since lecture 9 covered basis in depth. References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry></feed>