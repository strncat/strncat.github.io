<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-12-06T21:19:19-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Abstract Algebra</title><link href="http://localhost:4000/jekyll/update/2024/12/05/abstract-algebra.html" rel="alternate" type="text/html" title="Abstract Algebra" /><published>2024-12-05T08:01:36-08:00</published><updated>2024-12-05T08:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2024/12/05/abstract-algebra</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/12/05/abstract-algebra.html"><![CDATA[<!---------------------------- [1.6] Divisibility in the Integers ------------------------------->
<h3> [1.6] Divisibility in the Integers </h3>
<ol style="list-style-type:none;">
	   <li><a href="/jekyll/update/2024/11/01/1.6-z.html">
        [1.6] Z (1.6.1 - 1.6.2)
       </a></li>
	   <li><a href="/jekyll/update/2024/11/02/1.6-primes.html">
        [1.6] Prime Numbers (1.6.4 - 1.6.6)
       </a></li>
	   <li><a href="/jekyll/update/2024/11/03/1.6-remainders.html">
        [1.6] Remainders (1.6.7)
       </a></li>
	   <li><a href="/jekyll/update/2024/11/04/1.6-gcd.html">
        [1.6] Greatest Common Divisor (1.6.8 - 1.6.13)
       </a></li>
   </ol>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[[1.6] Divisibility in the Integers [1.6] Z (1.6.1 - 1.6.2) [1.6] Prime Numbers (1.6.4 - 1.6.6) [1.6] Remainders (1.6.7) [1.6] Greatest Common Divisor (1.6.8 - 1.6.13)]]></summary></entry><entry><title type="html">[1.6] Greatest Common Divisor (1.6.8 - 1.6.13)</title><link href="http://localhost:4000/jekyll/update/2024/11/04/1.6-gcd.html" rel="alternate" type="text/html" title="[1.6] Greatest Common Divisor (1.6.8 - 1.6.13)" /><published>2024-11-04T00:01:36-08:00</published><updated>2024-11-04T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2024/11/04/1.6-gcd</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/11/04/1.6-gcd.html"><![CDATA[<!------------------------------------------------------------------------------------>
<div class="mintheaderdiv">
Definition 1.6.8
</div>
<div class="mintbodydiv">
A natural number \(d\) is the greatest common divisor of nonzero integers \(m\) and \(n\) if
<ol type="a">
	<li>\(d\) divides \(m\) and \(n\) and</li>
	<li>whenever \(x \in \mathbf{N}\) divides \(m\) and \(n\), then \(x\) also divides \(d\)</li>
</ol>
</div>
<p><br />
The greatest common is unique if it exists by 1.6.2 (why?)
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.9
</div>
<div class="peachbodydiv">
For integers \(n\) and \(m\), let
$$
\begin{align*}
I(m,n) = \{am + bn \ | \ a, b \in \mathbf{Z}\}.
\end{align*}
$$
<ol type="a">
	<li>For \(x, y \in I(m,n)\), \(x + y \in I(m,n)\) and \(-x \in I(m,n)\).</li>
	<li>For all \(x \in \mathbf{Z}\), \(xI(m,n) \subseteq I(m,n)\).</li>
	<li>If \(b \in \mathbf{Z}\) divides \(m\) and \(n\), then \(b\) divides all elements of \(I(m,n)\).</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose \(x, y \in I(m,n)\), then \(x = am + bn\) for some \(a\) and \(b\) in \(\mathbf{Z}\) and \(y = cm + dn\) for some \(c\) and \(d\) in \(\mathbf{Z}\). Therefore</p>
<div>
$$
\begin{align*}
x + y &amp;= am + bn + cm + dn \\
      &amp;= (a+c)m + (b+d)n
\end{align*}
$$
</div>
<p>And so \(x+y \in I(m,n)\). \(\ \blacksquare\)
<br />
\((b)\) and \((c)\) have similar proofs.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma 1.6.10
</div>
<div class="yellowbodydiv">
Let \(m\) and \(n\) be nonzero integers. If a natural number \(d\) is a common divisor of \(m\) and \(n\) and an element of \(I(m,n)\), then \(d\) is the greatest common divisor of \(m\) and \(n\).
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
For any common divisor \(x\) of \(m\) and \(n\), \(x\) divides every element of \(I(m,n)\) by proposition 1.6.9(c) and in particular it divides \(d\). \(\ \blacksquare\)
<br />
<br />
Notes: \(d\) is an element of \(I(m,n)\). Therefore, \(d = am + bn\). For any other common divisor \(x\), \(x\) divides every element of \(I(m,n)\) so \(x\) divides \(d\) and we can write \(d = am + bn = qx\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Greatest Common Division</b></h4>
<p>Suppose \(m, n \in \mathbf{N}\) and suppose without the loss of generality that \(|m| \geq |n|\). Let \(q\) and \(r\) be the quotient and remainder when dividing \(m\) by \(n\)</p>
<div>
$$
\begin{align*}
m &amp;= nq_1 + r_1  \\
n &amp;= r_1q_2 + r_2  \\
r_1 &amp;= r_2q_3 + r_3 \\
\vdots \\
r_{n-3} &amp;= r_{n-2}q_{n-1} + r_{n-1} \\
r_{n-2} &amp;= r_{n-1}q_{n} + 0
\end{align*}
$$
</div>
<p>In the case where \(n_1 &gt; 0\), then define again \(q_2\) and \(n_2\) such that</p>
<div>
$$
\begin{align*}
n &amp;= q_2n_1 + n_2 \quad (\text{where } 0 \leq n_2 &lt; n_1) \\
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.11
</div>
<div class="peachbodydiv">
The natural number \(r_{n-1}\) is the greatest common divisor of \(m\) and \(n\) and furthermore \(r_{n-1} \in I(m,n)\)
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
We first need to show that this process terminates. Observe that the sequence \(\{r_1,r_2,...,r\} \in \mathbf{N}\) is decreasing so it truncates say at \(r_{n-1}\) (the last non-zero element of the sequence). So it must terminates by the well ordering principle.
<br />
<br />
Next, we need to show that \(r_{n-1}\) divides both \(m\) and \(n\). Observe that from the algorithm above that</p>
<div>
$$
\begin{align*}
r_{n-2} &amp;= r_{n-1}q_{n}
\end{align*}
$$
</div>
<p>This shows that \(r_{n-1}| r_{n-2}\). Looking at the equation above this one, we see</p>
<div>
$$
\begin{align*}
r_{n-3} &amp;= r_{n-2}q_{n} + r_{n-1}
\end{align*}
$$
</div>
<p>And combining this with the previous conclusion that \(r_{n-1} | r_{n-2}\), we see that \(r_{n-1}| r_{n-3}\). If we continue this process, we will see that \(r_{n-1}|a\) and \(r_{n-1}|b\). 
<br />
<br />
Finally, we need to show that \(r_{n-1}\) is the greatest common divisor. So let \(d\) be a common divisor of \(m\) and \(n\). So that \(d | m\) and \(d | n\). Using the first equation in the algorithm above,</p>
<div>
$$
\begin{align*}
m = nq + r_1
\end{align*}
$$
</div>
<p>we see that since \(d\) divides both \(m\) and \(n\). Then it must divide \(r_1\). Continuing with the second equation, we can see that \(d\) divides \(r_2\). Eventually, we can conclude that \(d|r_{n_1}\). Since \(d\) is arbitrary and \(r_{n-1}\) is a common divisor, then \(r_{n-1}\) must the greatest common divisor by definition. \(\ \blacksquare\)
<br />
<br />
TODO: Proof from the book
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Corollary 1.6.11
</div>
<div class="peachbodydiv">
Let \(m\) and \(n\) be nonzero integers, and write \(d = gcd(m,n)\). Then
<ol type="a">
	<li>\(d\) is the least element of \(\mathbf{N} \cap I(m,n)\).</li>
	<li>\(I(m,n) = \mathbf{Z}d\), the set of all integers multiples of \(d\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
<li><a href="https://www.youtube.com/watch?v=8cikffEcyPI&amp;t=5s">Michael Penn's Lectures</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 1.6.8 A natural number \(d\) is the greatest common divisor of nonzero integers \(m\) and \(n\) if \(d\) divides \(m\) and \(n\) and whenever \(x \in \mathbf{N}\) divides \(m\) and \(n\), then \(x\) also divides \(d\) The greatest common is unique if it exists by 1.6.2 (why?) Proposition 1.6.9 For integers \(n\) and \(m\), let $$ \begin{align*} I(m,n) = \{am + bn \ | \ a, b \in \mathbf{Z}\}. \end{align*} $$ For \(x, y \in I(m,n)\), \(x + y \in I(m,n)\) and \(-x \in I(m,n)\). For all \(x \in \mathbf{Z}\), \(xI(m,n) \subseteq I(m,n)\). If \(b \in \mathbf{Z}\) divides \(m\) and \(n\), then \(b\) divides all elements of \(I(m,n)\). Proof Suppose \(x, y \in I(m,n)\), then \(x = am + bn\) for some \(a\) and \(b\) in \(\mathbf{Z}\) and \(y = cm + dn\) for some \(c\) and \(d\) in \(\mathbf{Z}\). Therefore $$ \begin{align*} x + y &amp;= am + bn + cm + dn \\ &amp;= (a+c)m + (b+d)n \end{align*} $$ And so \(x+y \in I(m,n)\). \(\ \blacksquare\) \((b)\) and \((c)\) have similar proofs. Lemma 1.6.10 Let \(m\) and \(n\) be nonzero integers. If a natural number \(d\) is a common divisor of \(m\) and \(n\) and an element of \(I(m,n)\), then \(d\) is the greatest common divisor of \(m\) and \(n\). Proof For any common divisor \(x\) of \(m\) and \(n\), \(x\) divides every element of \(I(m,n)\) by proposition 1.6.9(c) and in particular it divides \(d\). \(\ \blacksquare\) Notes: \(d\) is an element of \(I(m,n)\). Therefore, \(d = am + bn\). For any other common divisor \(x\), \(x\) divides every element of \(I(m,n)\) so \(x\) divides \(d\) and we can write \(d = am + bn = qx\). Finding the Greatest Common Division Suppose \(m, n \in \mathbf{N}\) and suppose without the loss of generality that \(|m| \geq |n|\). Let \(q\) and \(r\) be the quotient and remainder when dividing \(m\) by \(n\) $$ \begin{align*} m &amp;= nq_1 + r_1 \\ n &amp;= r_1q_2 + r_2 \\ r_1 &amp;= r_2q_3 + r_3 \\ \vdots \\ r_{n-3} &amp;= r_{n-2}q_{n-1} + r_{n-1} \\ r_{n-2} &amp;= r_{n-1}q_{n} + 0 \end{align*} $$ In the case where \(n_1 &gt; 0\), then define again \(q_2\) and \(n_2\) such that $$ \begin{align*} n &amp;= q_2n_1 + n_2 \quad (\text{where } 0 \leq n_2 &lt; n_1) \\ \end{align*} $$ Proposition 1.6.11 The natural number \(r_{n-1}\) is the greatest common divisor of \(m\) and \(n\) and furthermore \(r_{n-1} \in I(m,n)\) Proof We first need to show that this process terminates. Observe that the sequence \(\{r_1,r_2,...,r\} \in \mathbf{N}\) is decreasing so it truncates say at \(r_{n-1}\) (the last non-zero element of the sequence). So it must terminates by the well ordering principle. Next, we need to show that \(r_{n-1}\) divides both \(m\) and \(n\). Observe that from the algorithm above that $$ \begin{align*} r_{n-2} &amp;= r_{n-1}q_{n} \end{align*} $$ This shows that \(r_{n-1}| r_{n-2}\). Looking at the equation above this one, we see $$ \begin{align*} r_{n-3} &amp;= r_{n-2}q_{n} + r_{n-1} \end{align*} $$ And combining this with the previous conclusion that \(r_{n-1} | r_{n-2}\), we see that \(r_{n-1}| r_{n-3}\). If we continue this process, we will see that \(r_{n-1}|a\) and \(r_{n-1}|b\). Finally, we need to show that \(r_{n-1}\) is the greatest common divisor. So let \(d\) be a common divisor of \(m\) and \(n\). So that \(d | m\) and \(d | n\). Using the first equation in the algorithm above, $$ \begin{align*} m = nq + r_1 \end{align*} $$ we see that since \(d\) divides both \(m\) and \(n\). Then it must divide \(r_1\). Continuing with the second equation, we can see that \(d\) divides \(r_2\). Eventually, we can conclude that \(d|r_{n_1}\). Since \(d\) is arbitrary and \(r_{n-1}\) is a common divisor, then \(r_{n-1}\) must the greatest common divisor by definition. \(\ \blacksquare\) TODO: Proof from the book Corollary 1.6.11 Let \(m\) and \(n\) be nonzero integers, and write \(d = gcd(m,n)\). Then \(d\) is the least element of \(\mathbf{N} \cap I(m,n)\). \(I(m,n) = \mathbf{Z}d\), the set of all integers multiples of \(d\) Proof TODO References Algebra: Abstract and Concrete by Frederick M. Goodman Michael Penn's Lectures]]></summary></entry><entry><title type="html">[1.6] Remainders (1.6.7)</title><link href="http://localhost:4000/jekyll/update/2024/11/03/1.6-remainders.html" rel="alternate" type="text/html" title="[1.6] Remainders (1.6.7)" /><published>2024-11-03T01:01:36-07:00</published><updated>2024-11-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/11/03/1.6-remainders</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/11/03/1.6-remainders.html"><![CDATA[<!------------------------------------------------------------------------------------>
<div class="peachheaderdiv">
Proposition 1.6.7
</div>
<div class="peachbodydiv">
Given integers \(a\) and \(d\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that 
$$
\begin{align*}
a = qd + r
\end{align*}
$$
where \(0 \leq r &lt; d\). 
</div>
<p><br />
<b>Proof</b>
<br />
We are given \(a\) and \(d\) such that \(d \geq 1\). <br />
We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\).
<br />
<br />
We have two cases:
<br />
\(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\).<br />
Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that</p>
<div> 
$$
\begin{align*}
(a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\
a &amp;= q'd + d + r \\
a &amp;= q'(d + 1) + r 
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done.
<br />
Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So</p>
<div> 
$$
\begin{align*}
-a &amp;= q'd + r' \\
a &amp;= -q'd - r' \\
a &amp;= -q'd - d + d - r' \\
a &amp;= (-q' - 1)d + (d - r') \\
  &amp;= (-q' - 1)d + (d - r').
\end{align*}
$$
</div>
<p>So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done.
<br />
<br />
To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that</p>
<div> 
$$
\begin{align*}
(q - q')d = r - r'
\end{align*}
$$
</div>
<p>But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Proposition 1.6.7 Given integers \(a\) and \(d\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that $$ \begin{align*} a = qd + r \end{align*} $$ where \(0 \leq r &lt; d\). Proof We are given \(a\) and \(d\) such that \(d \geq 1\). We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\). We have two cases: \(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\). Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that $$ \begin{align*} (a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\ a &amp;= q'd + d + r \\ a &amp;= q'(d + 1) + r \end{align*} $$ And we are done. Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done. Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So $$ \begin{align*} -a &amp;= q'd + r' \\ a &amp;= -q'd - r' \\ a &amp;= -q'd - d + d - r' \\ a &amp;= (-q' - 1)d + (d - r') \\ &amp;= (-q' - 1)d + (d - r'). \end{align*} $$ So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done. To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that $$ \begin{align*} (q - q')d = r - r' \end{align*} $$ But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\) References Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">[1.6] Prime Numbers (1.6.4 - 1.6.6)</title><link href="http://localhost:4000/jekyll/update/2024/11/02/1.6-primes.html" rel="alternate" type="text/html" title="[1.6] Prime Numbers (1.6.4 - 1.6.6)" /><published>2024-11-02T01:01:36-07:00</published><updated>2024-11-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/11/02/1.6-primes</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/11/02/1.6-primes.html"><![CDATA[<!------------------------------------------------------------------------------------>
<div class="mintheaderdiv">
Definition 1.6.3
</div>
<div class="mintbodydiv">
A natural number is prime if it is greater than 1 and not divisible by any natural number other than 1 and itself.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.4
</div>
<div class="peachbodydiv">
Any natural number other than 1 can be written as a product of prime numbers.
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b><br />
By Induction. <br />
Base Case: \(n = 2:\) 2 is a prime number and it is a product of prime numbers (only one factor).
<br />
<br />
Inductive Case: Suppose that for any number \(r\), we have \(2 \leq r &lt; n\) and \(r\) is a product of prime numbers. Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of prime numbers and we’re done. Otherwise, \(n\) is not a prime number and we can write \(n\) as a product of two integers \(a\) and \(b\) (\(n = ab\)) such that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\) (by ?). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of prime numbers. There \(n = ab\) is also a product of prime numbers. \(\ \blacksquare\) 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem 1.6.6
</div>
<div class="yellowbodydiv">
There are infinitely many prime numbers.
</div>
<p><br />
Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1\). We know that \(p\) is greater than any of the prime numbers \(p_1,p_2,...,p_k\). By Proposition 1.6.4, we can express \(p\) as a product of prime numbers. Observe now that \(p\) can not be divisible by any of the prime numbers \(p_1, p_2, ..., p_k\) (Why? for any \(p_i\),  \(\frac{(p_1p_2...p_k)+1}{p_i} = p_1..p_{i-1}p_{i+1}...p_k + \frac{1}{p_i}\) which is not an integer). Therefore, we must have \(p\) be a prime number. This is a contradiction and therefore, there are infinitely many primes. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.2
</div>
<div class="peachbodydiv">
Given integers \(a\) and \(d\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that 
$$
\begin{align*}
a = qd + r
\end{align*}
$$
where \(0 \leq r &lt; d\). 
</div>
<p><br />
<b>Proof</b>
<br />
We are given \(a\) and \(d\) such that \(d \geq 1\). <br />
We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\).
<br />
<br />
We have two cases:
<br />
\(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\).<br />
Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that</p>
<div> 
$$
\begin{align*}
(a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\
a &amp;= q'd + d + r \\
a &amp;= q'(d + 1) + r 
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done.
<br />
Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So</p>
<div> 
$$
\begin{align*}
-a &amp;= q'd + r' \\
a &amp;= -q'd - r' \\
a &amp;= -q'd - d + d - r' \\
a &amp;= (-q' - 1)d + (d - r') \\
  &amp;= (-q' - 1)d + (d - r').
\end{align*}
$$
</div>
<p>So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done.
<br />
<br />
To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that</p>
<div> 
$$
\begin{align*}
(q - q')d = r - r'
\end{align*}
$$
</div>
<p>But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 1.6.3 A natural number is prime if it is greater than 1 and not divisible by any natural number other than 1 and itself. Proposition 1.6.4 Any natural number other than 1 can be written as a product of prime numbers. Proof By Induction. Base Case: \(n = 2:\) 2 is a prime number and it is a product of prime numbers (only one factor). Inductive Case: Suppose that for any number \(r\), we have \(2 \leq r &lt; n\) and \(r\) is a product of prime numbers. Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of prime numbers and we’re done. Otherwise, \(n\) is not a prime number and we can write \(n\) as a product of two integers \(a\) and \(b\) (\(n = ab\)) such that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\) (by ?). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of prime numbers. There \(n = ab\) is also a product of prime numbers. \(\ \blacksquare\) Theorem 1.6.6 There are infinitely many prime numbers. Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1\). We know that \(p\) is greater than any of the prime numbers \(p_1,p_2,...,p_k\). By Proposition 1.6.4, we can express \(p\) as a product of prime numbers. Observe now that \(p\) can not be divisible by any of the prime numbers \(p_1, p_2, ..., p_k\) (Why? for any \(p_i\), \(\frac{(p_1p_2...p_k)+1}{p_i} = p_1..p_{i-1}p_{i+1}...p_k + \frac{1}{p_i}\) which is not an integer). Therefore, we must have \(p\) be a prime number. This is a contradiction and therefore, there are infinitely many primes. \(\ \blacksquare\) Proposition 1.6.2 Given integers \(a\) and \(d\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that $$ \begin{align*} a = qd + r \end{align*} $$ where \(0 \leq r &lt; d\). Proof We are given \(a\) and \(d\) such that \(d \geq 1\). We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\). We have two cases: \(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\). Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that $$ \begin{align*} (a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\ a &amp;= q'd + d + r \\ a &amp;= q'(d + 1) + r \end{align*} $$ And we are done. Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done. Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So $$ \begin{align*} -a &amp;= q'd + r' \\ a &amp;= -q'd - r' \\ a &amp;= -q'd - d + d - r' \\ a &amp;= (-q' - 1)d + (d - r') \\ &amp;= (-q' - 1)d + (d - r'). \end{align*} $$ So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done. To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that $$ \begin{align*} (q - q')d = r - r' \end{align*} $$ But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\) References Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">[1.6] Divisibility in the Integers (1.6.1 - 1.6.2)</title><link href="http://localhost:4000/jekyll/update/2024/11/01/1.6-z.html" rel="alternate" type="text/html" title="[1.6] Divisibility in the Integers (1.6.1 - 1.6.2)" /><published>2024-11-01T01:01:36-07:00</published><updated>2024-11-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/11/01/1.6-z</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/11/01/1.6-z.html"><![CDATA[<!------------------------------------------------------------------------------------>
<p>We have the following proposition which we will take as is.</p>
<div class="peachheaderdiv">
Proposition 1.6.1
</div>
<div class="peachbodydiv">
<ol type="a">
	<li>Addition on \(\mathbf{Z}\) is commutative and associative.</li>
	<li>0 is an identity element for addition; that is, for all \(a \in \mathbf{Z}\), \(a + 0 = a\).</li>
	<li>Every element \(a\) of \(\mathbf{Z}\) has an additive inverse \(-a\), satisfying \(a + (-a) = a\). We write \(a - b\) for \(a + (-b)\).</li>
	<li>Multiplication on \(\mathbf{Z}\) is commutative and associative.</li>
	<li>1 is an identity element for multiplication; that is, for all \(a \in \mathbf{Z}\), \(1a = a\).</li>
	<li>The distributive law holds; For all \(a, b, c \in \mathbf{Z}\), \(a(b+c) = ab + ac\).</li>	
	<li>\(\mathbf{N}\) is closed under addition and multiplication. That is, the sum and product of positive integers is positive.</li>
	<li>The product of non-zero integers is non-zero. That is \(|ab| \geq \max\{|a|,|b|\} \) for non-zero integers \(a\) and \(b\).</li>
</ol>
</div>
<p><br />
Based on the proposition above, we can now conclude the following
<br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.2
</div>
<div class="peachbodydiv">
<ol type="a">
	<li>If \(uv = 1\), then \(u = v = 1\) or \(u = v = =1\).</li>
	<li>If \(a|b\) and \(b|a\), then \(a = \pm b\).</li>
	<li>Divisibility is transitive: If \(a|b\) and \(b|c\), then \(a|c\).</li>
	<li>If \(a|b\) and \(a|c\), then \(a\) divides all integers that can be expressed in the form \(sb + tc\), where \(s\) and \(t\) are integers.</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof (a)</b><br />
We know \(a\) or \(b\) can’t be zero. By Proposition 1.6.1, we know that \(|ab| \geq \max\{|a|,|b|\}\). So</p>
<div> 
$$
\begin{align*}
|uv| &amp;\geq \max\{|u|,|v|\} \\
1 &amp;\geq \max\{|u|,|v|\} \\
\end{align*}
$$
</div>
<p>From this we see that \(u = v = 1\) or \(u = v = -1\). <br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof (b)</b><br />
If \(a|b\), then \(b = ca\) for some integer \(c\). Similarly, if \(b|a\), then \(a = db\) for some integer \(d\). Therefore,</p>
<div> 
$$
\begin{align*}
b &amp;= ca \\ 
b &amp;= cdb \\
0 &amp;= (cd - 1)b
\end{align*}
$$
</div>
<p>Since the product of non-zero integers is non-zero, then either \(cd = 1\) or \(b=0\). If \(b = 0\), then \(a\) must be zero. If \(cd = 1\), then by (a), \(c = d = \pm 1\) and so \(c = d\).
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof (c)</b><br />
If \(a|b\), then \(b = ua\) for some integer \(u\). Similarly, if \(b|c\), then \(c = vb\) for some integer \(v\). Therefore,</p>
<div> 
$$
\begin{align*}
c &amp;= vb \\ 
c &amp;= (uv)a
\end{align*}
$$
</div>
<p>From this we see that \(a|c\).
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof (d)</b><br />
If \(a|b\), then \(b = ua\) for some integer \(u\). Similarly, if \(a|c\), then \(c = va\) for some integer \(v\). Now let \(s\) and \(t\) be integers. Observe that</p>
<div> 
$$
\begin{align*}
sb + tc &amp;= ua + va \\ 
        &amp;= (u+v)a
\end{align*}
$$
</div>
<p>From this we see that \(a|(sb + tc) \ \blacksquare\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Prime Numbers</b></h4>
<!------------------------------------------------------------------------------------>
<div class="mintheaderdiv">
Definition 1.6.3
</div>
<div class="mintbodydiv">
A natural number is prime if it is greater than 1 and not divisible by any natural number other than 1 and itself.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.4
</div>
<div class="peachbodydiv">
Any natural number other than 1 can be written as a product of prime numbers.
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b><br />
By Induction. <br />
Base Case: \(n = 2:\) 2 is a prime number and it is a product of prime numbers (only one factor).
<br />
<br />
Inductive Case: Suppose that for any number \(r\), we have \(2 \leq r &lt; n\) and \(r\) is a product of prime numbers. Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of prime numbers and we’re done. Otherwise, \(n\) is not a prime number and we can write \(n\) as a product of two integers \(a\) and \(b\) (\(n = ab\)) such that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\) (by ?). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of prime numbers. There \(n = ab\) is also a product of prime numbers. \(\ \blacksquare\) 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem 1.6.6
</div>
<div class="yellowbodydiv">
There are infinitely many prime numbers.
</div>
<p><br />
Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1\). We know that \(p\) is greater than any of the prime numbers \(p_1,p_2,...,p_k\). By Proposition 1.6.4, we can express \(p\) as a product of prime numbers. Observe now that \(p\) can not be divisible by any of the prime numbers \(p_1, p_2, ..., p_k\) (Why? for any \(p_i\),  \(\frac{(p_1p_2...p_k)+1}{p_i} = p_1..p_{i-1}p_{i+1}...p_k + \frac{1}{p_i}\) which is not an integer). Therefore, we must have \(p\) be a prime number. This is a contradiction and therefore, there are infinitely many primes. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.2
</div>
<div class="peachbodydiv">
Given integers \(a\) and \(d\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that 
$$
\begin{align*}
a = qd + r
\end{align*}
$$
where \(0 \leq r &lt; d\). 
</div>
<p><br />
<b>Proof</b>
<br />
We are given \(a\) and \(d\) such that \(d \geq 1\). <br />
We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\).
<br />
<br />
We have two cases:
<br />
\(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\).<br />
Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that</p>
<div> 
$$
\begin{align*}
(a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\
a &amp;= q'd + d + r \\
a &amp;= q'(d + 1) + r 
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done.
<br />
Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So</p>
<div> 
$$
\begin{align*}
-a &amp;= q'd + r' \\
a &amp;= -q'd - r' \\
a &amp;= -q'd - d + d - r' \\
a &amp;= (-q' - 1)d + (d - r') \\
  &amp;= (-q' - 1)d + (d - r').
\end{align*}
$$
</div>
<p>So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done.
<br />
<br />
To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that</p>
<div> 
$$
\begin{align*}
(q - q')d = r - r'
\end{align*}
$$
</div>
<p>But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We have the following proposition which we will take as is. Proposition 1.6.1 Addition on \(\mathbf{Z}\) is commutative and associative. 0 is an identity element for addition; that is, for all \(a \in \mathbf{Z}\), \(a + 0 = a\). Every element \(a\) of \(\mathbf{Z}\) has an additive inverse \(-a\), satisfying \(a + (-a) = a\). We write \(a - b\) for \(a + (-b)\). Multiplication on \(\mathbf{Z}\) is commutative and associative. 1 is an identity element for multiplication; that is, for all \(a \in \mathbf{Z}\), \(1a = a\). The distributive law holds; For all \(a, b, c \in \mathbf{Z}\), \(a(b+c) = ab + ac\). \(\mathbf{N}\) is closed under addition and multiplication. That is, the sum and product of positive integers is positive. The product of non-zero integers is non-zero. That is \(|ab| \geq \max\{|a|,|b|\} \) for non-zero integers \(a\) and \(b\). Based on the proposition above, we can now conclude the following Proposition 1.6.2 If \(uv = 1\), then \(u = v = 1\) or \(u = v = =1\). If \(a|b\) and \(b|a\), then \(a = \pm b\). Divisibility is transitive: If \(a|b\) and \(b|c\), then \(a|c\). If \(a|b\) and \(a|c\), then \(a\) divides all integers that can be expressed in the form \(sb + tc\), where \(s\) and \(t\) are integers. Proof (a) We know \(a\) or \(b\) can’t be zero. By Proposition 1.6.1, we know that \(|ab| \geq \max\{|a|,|b|\}\). So $$ \begin{align*} |uv| &amp;\geq \max\{|u|,|v|\} \\ 1 &amp;\geq \max\{|u|,|v|\} \\ \end{align*} $$ From this we see that \(u = v = 1\) or \(u = v = -1\). Proof (b) If \(a|b\), then \(b = ca\) for some integer \(c\). Similarly, if \(b|a\), then \(a = db\) for some integer \(d\). Therefore, $$ \begin{align*} b &amp;= ca \\ b &amp;= cdb \\ 0 &amp;= (cd - 1)b \end{align*} $$ Since the product of non-zero integers is non-zero, then either \(cd = 1\) or \(b=0\). If \(b = 0\), then \(a\) must be zero. If \(cd = 1\), then by (a), \(c = d = \pm 1\) and so \(c = d\). Proof (c) If \(a|b\), then \(b = ua\) for some integer \(u\). Similarly, if \(b|c\), then \(c = vb\) for some integer \(v\). Therefore, $$ \begin{align*} c &amp;= vb \\ c &amp;= (uv)a \end{align*} $$ From this we see that \(a|c\). Proof (d) If \(a|b\), then \(b = ua\) for some integer \(u\). Similarly, if \(a|c\), then \(c = va\) for some integer \(v\). Now let \(s\) and \(t\) be integers. Observe that $$ \begin{align*} sb + tc &amp;= ua + va \\ &amp;= (u+v)a \end{align*} $$ From this we see that \(a|(sb + tc) \ \blacksquare\). Prime Numbers Definition 1.6.3 A natural number is prime if it is greater than 1 and not divisible by any natural number other than 1 and itself. Proposition 1.6.4 Any natural number other than 1 can be written as a product of prime numbers. Proof By Induction. Base Case: \(n = 2:\) 2 is a prime number and it is a product of prime numbers (only one factor). Inductive Case: Suppose that for any number \(r\), we have \(2 \leq r &lt; n\) and \(r\) is a product of prime numbers. Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of prime numbers and we’re done. Otherwise, \(n\) is not a prime number and we can write \(n\) as a product of two integers \(a\) and \(b\) (\(n = ab\)) such that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\) (by ?). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of prime numbers. There \(n = ab\) is also a product of prime numbers. \(\ \blacksquare\) Theorem 1.6.6 There are infinitely many prime numbers. Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1\). We know that \(p\) is greater than any of the prime numbers \(p_1,p_2,...,p_k\). By Proposition 1.6.4, we can express \(p\) as a product of prime numbers. Observe now that \(p\) can not be divisible by any of the prime numbers \(p_1, p_2, ..., p_k\) (Why? for any \(p_i\), \(\frac{(p_1p_2...p_k)+1}{p_i} = p_1..p_{i-1}p_{i+1}...p_k + \frac{1}{p_i}\) which is not an integer). Therefore, we must have \(p\) be a prime number. This is a contradiction and therefore, there are infinitely many primes. \(\ \blacksquare\) Proposition 1.6.2 Given integers \(a\) and \(d\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that $$ \begin{align*} a = qd + r \end{align*} $$ where \(0 \leq r &lt; d\). Proof We are given \(a\) and \(d\) such that \(d \geq 1\). We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\). We have two cases: \(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\). Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that $$ \begin{align*} (a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\ a &amp;= q'd + d + r \\ a &amp;= q'(d + 1) + r \end{align*} $$ And we are done. Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done. Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So $$ \begin{align*} -a &amp;= q'd + r' \\ a &amp;= -q'd - r' \\ a &amp;= -q'd - d + d - r' \\ a &amp;= (-q' - 1)d + (d - r') \\ &amp;= (-q' - 1)d + (d - r'). \end{align*} $$ So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done. To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that $$ \begin{align*} (q - q')d = r - r' \end{align*} $$ But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\) References Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">4.1-4.2: Study Notes</title><link href="http://localhost:4000/jekyll/update/2024/09/19/4.1-4.2.html" rel="alternate" type="text/html" title="4.1-4.2: Study Notes" /><published>2024-09-19T01:01:36-07:00</published><updated>2024-09-19T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/19/4.1-4.2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/19/4.1-4.2.html"><![CDATA[<!------------------------------------4.1------------------------------------------->
<p><br /></p>
<div class="purdiv">
Theorem 4.1
</div>
<div class="purbdiv">
The function \(\det: M_{2 \times 2}(\mathbf{F}) \rightarrow \mathbf{F}\) is a linear function of each of a \(2 \times 2\) matrix when the other row is held fixed. That is if \(u, v\) and \(w\) are in \(\mathbf{F}^2\) and \(k\) is a scalar, then
$$
\begin{align*}
\det \begin{pmatrix} u + kv \\ w \end{pmatrix}
= \det \begin{pmatrix} u \\ w \end{pmatrix} + k \det \begin{pmatrix} v \\ w \end{pmatrix}
\end{align*}
$$
and
$$
\begin{align*}
\det \begin{pmatrix} w \\ u + kv \end{pmatrix}
= \det \begin{pmatrix} w \\ u \end{pmatrix} + k \det \begin{pmatrix} w \\ v \end{pmatrix}
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
Let \(u = (a_1, a_2), v = (b_1, b_2)\) and \(w = (c_1, c_2)\) be in \(\mathbf{F}^2\) and let \(k\) be a scalar. Then</p>
<div>
$$
\begin{align*}
\det \begin{pmatrix} u \\ v \end{pmatrix} + \det k\begin{pmatrix} w \\ v \end{pmatrix}
&amp;= \det \begin{pmatrix} a_1 &amp; a_2 \\ c_1 &amp; c_2 \end{pmatrix} 
+ k \det \begin{pmatrix} b_1 &amp; b_2 \\ c_1 &amp; c_2 \end{pmatrix} 
\\
&amp;= (a_1c_2 - a_2c_1) + k(b_1c_2 - b_2c_1) \\
&amp;=  a_1c_2 - a_2c_1 + kb_1c_2 + kb_2c_1 \\
&amp;= c_2(a_1 + kb_1) - c_1(a_2 +kb_2) \\
&amp;= \det \begin{pmatrix} a_1+kb_1 &amp; a_2+kb_2 \\ c_1 &amp; c_2 \end{pmatrix} 
\end{align*}
$$
</div>
<p>The other direction is similarly calculated.
<br />
<!------------------------------------4.2------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.2
</div>
<div class="purbdiv">
Let \(A \in M_{2 \times 2}(\mathbf{F})\). Then the determinant of \(A\) is nonzero if and only if \(A\) is invertible. Moreover, if \(A\) is invertible, then
$$
\begin{align*}
A^{-1} = \frac{1}{\det(A)}
= \det \begin{pmatrix} A_{22} &amp; -A_{12} \\ -A_{21} &amp; A_{11} \end{pmatrix}
\end{align*}
$$
</div>
<!---------------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
<br />
We can construct the matrix such that we can show that \(AM = MA = I\) so this proves that \(A\) is invertible. Conversely, then \(A\) must have rank 2 and therefore we musteither have \(A_{21} \neq 0\) or \(A_{11} \neq 0\). If \(A_{11} \neq 0\), we can add \(-A_{21}/A_{11}\) times row 1 to row 2 to obtain</p>
<div>
$$
\begin{align*}
\begin{pmatrix} A_{11} &amp; -A_{12} \\ 0 &amp; A_{22} - \frac{A_{12}A_{21}}{A_{11}}\end{pmatrix}
\end{align*}
$$
</div>
<p>Because this is an elementary row operation, then we know the rank is preserved. Notice now that \(A_{21} = 0\). This means that \(A_{22} - \frac{A_{12}A_{21}}{A_{11}}\) can’t be zero. From this, we will see that the determinant is not zero. For the other case when \(A_{21} \neq 0\), we can instead add \(-A_{11}/A_{21}\) to achieve the same conclusion.å
<br />
<!---------------------------------------------------------------------------------->
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A \in M_{n \times n}(\mathbf{F})\). If \(n = 1\), so that \(A = (A_{11}\), we define \(\det(A) = A_{11}\). For \(n \geq 2\), we define \(\det(A)\) recursively as
$$
\begin{align*}
\det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) 
\end{align*}
$$
The scalar \(\det(A)\) is called the <b>determinant</b> of \(A\) and is also denoted by \(|A|\). The scalar
$$
\begin{align*}
(-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) 
\end{align*}
$$
is called the <b>cofactor</b> of the entry \(A\) in row \(i\), column \(j\).
</div>
<p><br />
<!------------------------------------4.3------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.3
</div>
<div class="purbdiv">
The determinant of \(n \times n\) matrix is a linear function of each row when the remaining rows are held fixed. That is for \(1 \leq r \leq n\). we have
$$
\begin{align*}
\det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u + kv \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix}
=
\det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix}
+
k
\det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ v \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix}
\end{align*}
$$
where \(k\) is a scalar and \(u,v\) are each \(a_i\) are row vectors in \(\mathbf{F}^n\)
</div>
<!---------------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
<br />
By induction on \(n\). If \(n = 1\), the result is immediate. <br />
Now suppose that \(n \geq 2\) and assume the hypothesis is true for \(n-1\), that is the determinant of any \((n - 1) \times (n-1)\) is a linear function of each row when the remaining rows are held fixed. 
<br />
<br />
Let \(A\) be an \(n \times n\) matrix with rows \(a_1, a_2,...,a_n\), respectively and suppose that for some \(r\) where \((1 \leq r \leq n)\), we have \(a_r = u + kv\) for some \(u, v \in \mathbf{F}^n\) and scalar \(k\). Let \(u = (b_1,...,b_n)\) and let \(v = (c_1, ...,c_n)\). Now, let \(B\) and \(C\) be the matrices obtained from \(A\) by replacing row \(r\) of \(A\) by \(u\) and \(v\) respectively. We want to show that \(\det(A) = \det(B) + k\det(C)\).
<br />
<br />
Case \(r = 1\): TODO (it’s in lecture 18)
<br />
<br />
Case \(r &gt; 1\): For the matrices \(\tilde{A_{1j}}\), \(\tilde{B_{1j}}\), \(\tilde{C_{1j}}\) which are constructed by removing the first row and the \(j\)th column, we know that they are the same except for row \(r-1\) (\(r-1\) since the first row is removed, so the \(r\)‘th row is now the \(r-1\)th row). The row \(r-1\) specifically looks like</p>
<div>
$$
\begin{align*}
(b_1 + kc_1,...,b_{j-1} + kc_{j-1},b_{j+1} + kc_{j+1},...,b_n + kc_n)
\end{align*}
$$
</div>
<p>Moreover, these matrices are now of size \((n-1) \times (n-1)\) so we can invoke the inductive hypothesis to conclude that</p>
<div>
$$
\begin{align*}
\tilde{A_{1j}} = \tilde{B_{1j}} + k\tilde{C_{1j}}
\end{align*}
$$
</div>
<p>We also know that the first row is equal among all three matrices so \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). Let’s now compute the determinant using the definition above</p>
<div>
$$
\begin{align*}
\det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\
        &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} [\tilde{B_{1j}} + k\tilde{C_{1j}}] \\
        &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \tilde{B_{1j}} + k \sum_{j=1}^{n} (-1)^{1+j}  \tilde{C_{1j}}] \\
		&amp;= \det(B) + k\det(C).
\end{align*}
$$
</div>
<p>Therefore the inductive hypothesis is true for any \(n \times n\) matrix. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------4.3(c)---------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.3 (Corollary)
</div>
<div class="purbdiv">
If \(A \in M_{n \times n}(\mathbf{F})\) has a row consisting entirely of zeros, then \(\det(A)=0\).
</div>
<p><br />
<b>Proof</b>
TODO (Exercise 4.2, 24)
<br />
<br />
<!------------------------------------Lemma---------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.3 (Corollary)
</div>
<div class="purbdiv">
Let \(B \in M_{n \times n}(\mathbf{F})\) where \(n \geq 2\). If row \(i\) of \(B\) equals \(e_k\) for some \(k \ (1 \leq k \leq n)\), then \(\det(B) = (-1)^{i+k}\det(\tilde{B_{ik}})\)
</div>
<p><br />
<!---------------------------------------------------------------------------------->
<b>Proof</b>
<br />
By Induction on \(n\). 
<br />
<br />
Base Case \(n = 2\): TODO
<br />
<br />
Inductive Case: Assume this is true for \((n - 1) \times (n - 1)\). We will show this is true for an \(n \times n\) matrix. Let \(B\) be an \(n \times\) matrix where the \(i\)th row is \(e_k\). If \(i = 1\), then we’re done by the definition of the determinant. Otherwise suppose that \(i \neq 1\). 
<br />
<br />
Now, for column \(j \neq k\), let \(C_{ij}\) be the \((n-2) \times (n-2)\) matrix obtained from \(B\) by deleting rows 1 and \(i\) and columns \(j\) and \(k\). Then, \(\tilde{B_{1j}}\) is the following vector in \(\mathbf{F}^{n-1}\)</p>
<div>
$$
\begin{align*}
\begin{cases} e_{k-1} \quad \text{ if }j &lt; k \\ 
              0 \quad \quad \ \text{ if } j = k \\
			  e_{k} \quad \quad \text{ if } j &gt; k
\end{cases}
\end{align*}
$$
</div>
<p>TODO ….
<br />
<!------------------------------------4.4------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.4
</div>
<div class="purbdiv">
The determinant of a square matrix can be evaluated by any cofactor expansion along any row. That is if \(A \in M_{n \times n}(\mathbf{F})\). Then for any integer \(i \ (1 \leq i \leq n)\)
	$$
	\begin{align*}
	\det(A) = \sum_{j=1}^{n} (-1)^{i+j} A_{ij} \det(\tilde{A_{ij}}) 
	\end{align*}
	$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
<br />
<br /></p>
<hr />

<p><br /></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 4.1 The function \(\det: M_{2 \times 2}(\mathbf{F}) \rightarrow \mathbf{F}\) is a linear function of each of a \(2 \times 2\) matrix when the other row is held fixed. That is if \(u, v\) and \(w\) are in \(\mathbf{F}^2\) and \(k\) is a scalar, then $$ \begin{align*} \det \begin{pmatrix} u + kv \\ w \end{pmatrix} = \det \begin{pmatrix} u \\ w \end{pmatrix} + k \det \begin{pmatrix} v \\ w \end{pmatrix} \end{align*} $$ and $$ \begin{align*} \det \begin{pmatrix} w \\ u + kv \end{pmatrix} = \det \begin{pmatrix} w \\ u \end{pmatrix} + k \det \begin{pmatrix} w \\ v \end{pmatrix} \end{align*} $$ Proof Let \(u = (a_1, a_2), v = (b_1, b_2)\) and \(w = (c_1, c_2)\) be in \(\mathbf{F}^2\) and let \(k\) be a scalar. Then $$ \begin{align*} \det \begin{pmatrix} u \\ v \end{pmatrix} + \det k\begin{pmatrix} w \\ v \end{pmatrix} &amp;= \det \begin{pmatrix} a_1 &amp; a_2 \\ c_1 &amp; c_2 \end{pmatrix} + k \det \begin{pmatrix} b_1 &amp; b_2 \\ c_1 &amp; c_2 \end{pmatrix} \\ &amp;= (a_1c_2 - a_2c_1) + k(b_1c_2 - b_2c_1) \\ &amp;= a_1c_2 - a_2c_1 + kb_1c_2 + kb_2c_1 \\ &amp;= c_2(a_1 + kb_1) - c_1(a_2 +kb_2) \\ &amp;= \det \begin{pmatrix} a_1+kb_1 &amp; a_2+kb_2 \\ c_1 &amp; c_2 \end{pmatrix} \end{align*} $$ The other direction is similarly calculated. Theorem 4.2 Let \(A \in M_{2 \times 2}(\mathbf{F})\). Then the determinant of \(A\) is nonzero if and only if \(A\) is invertible. Moreover, if \(A\) is invertible, then $$ \begin{align*} A^{-1} = \frac{1}{\det(A)} = \det \begin{pmatrix} A_{22} &amp; -A_{12} \\ -A_{21} &amp; A_{11} \end{pmatrix} \end{align*} $$ Proof We can construct the matrix such that we can show that \(AM = MA = I\) so this proves that \(A\) is invertible. Conversely, then \(A\) must have rank 2 and therefore we musteither have \(A_{21} \neq 0\) or \(A_{11} \neq 0\). If \(A_{11} \neq 0\), we can add \(-A_{21}/A_{11}\) times row 1 to row 2 to obtain $$ \begin{align*} \begin{pmatrix} A_{11} &amp; -A_{12} \\ 0 &amp; A_{22} - \frac{A_{12}A_{21}}{A_{11}}\end{pmatrix} \end{align*} $$ Because this is an elementary row operation, then we know the rank is preserved. Notice now that \(A_{21} = 0\). This means that \(A_{22} - \frac{A_{12}A_{21}}{A_{11}}\) can’t be zero. From this, we will see that the determinant is not zero. For the other case when \(A_{21} \neq 0\), we can instead add \(-A_{11}/A_{21}\) to achieve the same conclusion.å Definition Let \(A \in M_{n \times n}(\mathbf{F})\). If \(n = 1\), so that \(A = (A_{11}\), we define \(\det(A) = A_{11}\). For \(n \geq 2\), we define \(\det(A)\) recursively as $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ The scalar \(\det(A)\) is called the determinant of \(A\) and is also denoted by \(|A|\). The scalar $$ \begin{align*} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ is called the cofactor of the entry \(A\) in row \(i\), column \(j\). Theorem 4.3 The determinant of \(n \times n\) matrix is a linear function of each row when the remaining rows are held fixed. That is for \(1 \leq r \leq n\). we have $$ \begin{align*} \det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u + kv \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix} = \det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix} + k \det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ v \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix} \end{align*} $$ where \(k\) is a scalar and \(u,v\) are each \(a_i\) are row vectors in \(\mathbf{F}^n\) Proof By induction on \(n\). If \(n = 1\), the result is immediate. Now suppose that \(n \geq 2\) and assume the hypothesis is true for \(n-1\), that is the determinant of any \((n - 1) \times (n-1)\) is a linear function of each row when the remaining rows are held fixed. Let \(A\) be an \(n \times n\) matrix with rows \(a_1, a_2,...,a_n\), respectively and suppose that for some \(r\) where \((1 \leq r \leq n)\), we have \(a_r = u + kv\) for some \(u, v \in \mathbf{F}^n\) and scalar \(k\). Let \(u = (b_1,...,b_n)\) and let \(v = (c_1, ...,c_n)\). Now, let \(B\) and \(C\) be the matrices obtained from \(A\) by replacing row \(r\) of \(A\) by \(u\) and \(v\) respectively. We want to show that \(\det(A) = \det(B) + k\det(C)\). Case \(r = 1\): TODO (it’s in lecture 18) Case \(r &gt; 1\): For the matrices \(\tilde{A_{1j}}\), \(\tilde{B_{1j}}\), \(\tilde{C_{1j}}\) which are constructed by removing the first row and the \(j\)th column, we know that they are the same except for row \(r-1\) (\(r-1\) since the first row is removed, so the \(r\)‘th row is now the \(r-1\)th row). The row \(r-1\) specifically looks like $$ \begin{align*} (b_1 + kc_1,...,b_{j-1} + kc_{j-1},b_{j+1} + kc_{j+1},...,b_n + kc_n) \end{align*} $$ Moreover, these matrices are now of size \((n-1) \times (n-1)\) so we can invoke the inductive hypothesis to conclude that $$ \begin{align*} \tilde{A_{1j}} = \tilde{B_{1j}} + k\tilde{C_{1j}} \end{align*} $$ We also know that the first row is equal among all three matrices so \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). Let’s now compute the determinant using the definition above $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} [\tilde{B_{1j}} + k\tilde{C_{1j}}] \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \tilde{B_{1j}} + k \sum_{j=1}^{n} (-1)^{1+j} \tilde{C_{1j}}] \\ &amp;= \det(B) + k\det(C). \end{align*} $$ Therefore the inductive hypothesis is true for any \(n \times n\) matrix. \(\ \blacksquare\) Theorem 4.3 (Corollary) If \(A \in M_{n \times n}(\mathbf{F})\) has a row consisting entirely of zeros, then \(\det(A)=0\). Proof TODO (Exercise 4.2, 24) Theorem 4.3 (Corollary) Let \(B \in M_{n \times n}(\mathbf{F})\) where \(n \geq 2\). If row \(i\) of \(B\) equals \(e_k\) for some \(k \ (1 \leq k \leq n)\), then \(\det(B) = (-1)^{i+k}\det(\tilde{B_{ik}})\) Proof By Induction on \(n\). Base Case \(n = 2\): TODO Inductive Case: Assume this is true for \((n - 1) \times (n - 1)\). We will show this is true for an \(n \times n\) matrix. Let \(B\) be an \(n \times\) matrix where the \(i\)th row is \(e_k\). If \(i = 1\), then we’re done by the definition of the determinant. Otherwise suppose that \(i \neq 1\). Now, for column \(j \neq k\), let \(C_{ij}\) be the \((n-2) \times (n-2)\) matrix obtained from \(B\) by deleting rows 1 and \(i\) and columns \(j\) and \(k\). Then, \(\tilde{B_{1j}}\) is the following vector in \(\mathbf{F}^{n-1}\) $$ \begin{align*} \begin{cases} e_{k-1} \quad \text{ if }j &lt; k \\ 0 \quad \quad \ \text{ if } j = k \\ e_{k} \quad \quad \text{ if } j &gt; k \end{cases} \end{align*} $$ TODO …. Theorem 4.4 The determinant of a square matrix can be evaluated by any cofactor expansion along any row. That is if \(A \in M_{n \times n}(\mathbf{F})\). Then for any integer \(i \ (1 \leq i \leq n)\) $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{i+j} A_{ij} \det(\tilde{A_{ij}}) \end{align*} $$ Proof References Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">3.1-3.2: Study Notes</title><link href="http://localhost:4000/jekyll/update/2024/09/18/3.1-3.2.html" rel="alternate" type="text/html" title="3.1-3.2: Study Notes" /><published>2024-09-18T01:01:36-07:00</published><updated>2024-09-18T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/18/3.1-3.2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/18/3.1-3.2.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A\) be an \(m \times n\). Any one of the following three operations on the rows [columns] of \(A\) is called an <b>elementary row [column] operation</b>
<ol type="1">
	<li>interchanging any two rows [columns] of \(A\);</li>
	<li>multiplying any row [column] of \(A\) by a non-zero scalar;</li>
	<li>adding any scalar multiple of a row [column] of \(A\) to another row [column].</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of <b>type 1, 2 or 3</b> according to whether the elementary operation performed on \(I_n\) is of type 1, 2 or 3 operation, respectively.
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.1
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{F})\) and suppose that \(B\) is obtained from \(A\) by performing an elementary row [column] operation. Then, there exists an \(m \times m [n \times n]\) elementary matrix \(E\) such that \(B = EA [B = AE]\). In fact, \(E\) is obtained from \(I_m [I_n]\) by performing the same elementary row [column] operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m [n \times n]\) matrix, then \(EA [AE]\) is the matrix obtained from \(A\) by performing the same elementary row operation.
</div>
<p><br />
<!----------------------------------------3.2------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.2
</div>
<div class="purbdiv">
Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type.
</div>
<p><br />
<b>Proof</b>
<br />
<br />
Let \(E\) be an elementary \(n \times n\) matrix. Then \(E\) can be obtained by an elementary row operation on \(I_n\) by definition. Reverse the steps used to transform \(I_n\) into \(E\) to get \(I_n\) back. The result is that \(I_n\) can be obtained from \(E\) by an elementary row operation of the same type. By Theorem 3.1, there is an elementary matrix \(\overline{E}\) such that \(\overline{E}E = I_n\). But since \(\overline{E}E = I_n\), then by Exercise 10 from 2.4, \(\overline{E}\) and \(E\) are invertible and we have \(E^{-1} = \overline{E}\). \(\ \blacksquare\)
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
If \(A \in M_{n \times n}(\mathbf{F})\) we define the <b>rank</b> of \(A\), denoted \(\text{rank}(A)\), to be the rank of the linear transformation \(L_A: \mathbf{F}^n \rightarrow \mathbf{F}^m\)
</div>
<p><br />
One important implication here is that an \(n \times n\) matrix is invertible if and only if its rank is \(n\).
<br />
<br />
<!----------------------------------------3.3------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.3
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation between finite-dimensional vector spaces, and let \(\beta\) and \(\gamma\) be ordered bases for \(V\) and \(W\), respectively. Then \(rank(T) = rank([T]_{\beta}^{\gamma})\)
</div>
<p><br />
The rank of a linear transformation is the same as the rank of one its matrix representations.
<br />
<br />
<!--------------------------------------3.4-------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.4
</div>
<div class="purbdiv">
Let \(A\) be an \(m \times n\) matrix. If \(P\) and \(Q\) are invertible \(m \times m\) and \(n \times n\) matrices, respectively, then 
<ol type="a">
	<li>\(\text{rank}(AQ) = \text{rank}(A)\),</li>
	<li>\(\text{rank}(PA) = \text{rank}(A)\) and therefore,</li>
	<li>\(\text{rank}(PAQ) = \text{rank}(A)\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
<br />
(a) Observe that</p>
<div>
$$
\begin{align*}
R(L_{AQ}) &amp;= R(L_AL_Q) \\
          &amp;= L_AL_Q(\mathbf{F}) \quad \text{(to get the range, we apply the linear map)}\\
		  &amp;= L_A(L_Q(\mathbf{F})) \quad \text{(we apply $L_Q$ first)}\\
		  &amp;= L_A(\mathbf{F}) \quad \text{(because $L_Q$ is onto)}\\
		  &amp;= R(L_A)
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\text{rank}(AQ) &amp;= \dim(R(L_{AQ})) \quad \text{(the rank is the dimension of the range)} \\
                &amp;= \dim(R(L_A)) \quad \text{(by the previous result)}  \\
				&amp;= rank(A).
\end{align*}
$$
</div>
<p><br />
<!--------------------------------------3.4(c)------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.4 (Corollary)
</div>
<div class="purbdiv">
Elementary row and column operations on a matrix are rank-preserving.
</div>
<p><br />
<!--------------------------------------3.5-------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.5
</div>
<div class="purbdiv">
The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns.
</div>
<p><br />
<!----------------------->
<b>Proof</b>
<br />
<br />
For any \(A \in M_{m \times n}(\mathbf{F})\),</p>
<div>
$$
\begin{align*}
\text{rank}(A) = \text{rank}(L_A) = \dim(R(L_A))
\end{align*}
$$
</div>
<p>Let \(\beta\) be the standard ordered basis for \(\mathbf{F}^n\). Then \(\beta\) spans \(\mathbf{F}^n\). Theorem 2.2 assets that \(R(T) = \text(T(\beta))\) so</p>
<div>
$$
\begin{align*}
R(L_A) &amp;= \text{span}(L_A(\beta)) \quad \text{(by Theorem 2.2)} \\
       &amp;= \text{span}(\{L_A(e_1),...,L_A(e_n)\})
\end{align*}
$$
</div>
<p>By theorem 2.13(b) \(L_A(e_j) = Ae_j = a_j\) where \(a_j\) is the \(j\)th column of \(A\). So</p>
<div>
$$
\begin{align*}
R(L_A) &amp;= \text{span}(\{a_1,...,a_n\})
\end{align*}
$$
</div>
<p>Therefore</p>
<div>
$$
\begin{align*}
\text{rank}(A) &amp;= \dim(R(L_A)) = \dim(\text{span}(\{a_1,...,a_n\})) \ \blacksquare
\end{align*}
$$
</div>
<p>Note: If you’ve forgotten 2.13 which you just did. Remember that by definition, matrix-vector multiplication \(Av\) is defined as \(a_1v_1 + a_2v_2 +...+ a_nv_n\) where \(a_1,...,a_n\) are the columns of \(A\) and \(v_1,...,v_n\) are the entries of the vector \(v\). Therefore, if you multiply \(Ae_j\) where \(e_j\) is from the standard basis, then we know that vector is all zeros except for the \(j\)th entry. So this means that \(Ae_j = a_j\). 
<br />
<br />
<!--------------------------------------3.6-------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6
</div>
<div class="purbdiv">
Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then \(r \leq m, r \leq n\), and by means of a finite number of elementary row and column operations, \(A\) can be transformed into the matrix
$$
\begin{align*}
D = \begin{pmatrix}
I_r &amp; O_1 \\
O_2 &amp; O_3
\end{pmatrix}
\end{align*}
$$
where \(O_1\), \(O_2\) and \(O_3\) are zero matrices. Thus \(D_{ii} = 1\) for \(i \leq r\) and \(D_{ij} = 0\) otherwise.
</div>
<p><br />
<!------------------------------------3.6(c)--------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6 (Corollary 1)
</div>
<div class="purbdiv">
Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then there exists invertible matrices \(B\) and \(C\) of sizes \(m \times m \) and \(n \times n\) respectively, such that \(D = BAC\) where
$$
\begin{align*}
D = \begin{pmatrix}
I_r &amp; O_1 \\
O_2 &amp; O_3
\end{pmatrix}
\end{align*}
$$
is the \(m \times n\) in which \(O_1\), \(O_2\) and \(O_3\) are zero matrices.
</div>
<p><br />
<b>Proof</b>
TODO
<br />
<!------------------------------------3.6(c)--------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6 (Corollary 2)
</div>
<div class="purbdiv">
Let \(A\) be \(m \times n\) matrices. Then
<ol type="i">
</ol>
</div>
<p><br />
<b>Proof</b>
TODO
<br /></p>

<p><br />
<!------------------------------------3.6(c)--------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6 (Corollary 3)
</div>
<div class="purbdiv">
Every invertible matrix is a product of elementary matrices
</div>
<p><br />
<b>Proof</b>
TODO
<br /></p>

<p><br />
<br /></p>
<hr />

<p><br /></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition Let \(A\) be an \(m \times n\). Any one of the following three operations on the rows [columns] of \(A\) is called an elementary row [column] operation interchanging any two rows [columns] of \(A\); multiplying any row [column] of \(A\) by a non-zero scalar; adding any scalar multiple of a row [column] of \(A\) to another row [column]. Definition An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of type 1, 2 or 3 according to whether the elementary operation performed on \(I_n\) is of type 1, 2 or 3 operation, respectively. Theorem 3.1 Let \(A \in M_{n \times n}(\mathbf{F})\) and suppose that \(B\) is obtained from \(A\) by performing an elementary row [column] operation. Then, there exists an \(m \times m [n \times n]\) elementary matrix \(E\) such that \(B = EA [B = AE]\). In fact, \(E\) is obtained from \(I_m [I_n]\) by performing the same elementary row [column] operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m [n \times n]\) matrix, then \(EA [AE]\) is the matrix obtained from \(A\) by performing the same elementary row operation. Theorem 3.2 Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type. Proof Let \(E\) be an elementary \(n \times n\) matrix. Then \(E\) can be obtained by an elementary row operation on \(I_n\) by definition. Reverse the steps used to transform \(I_n\) into \(E\) to get \(I_n\) back. The result is that \(I_n\) can be obtained from \(E\) by an elementary row operation of the same type. By Theorem 3.1, there is an elementary matrix \(\overline{E}\) such that \(\overline{E}E = I_n\). But since \(\overline{E}E = I_n\), then by Exercise 10 from 2.4, \(\overline{E}\) and \(E\) are invertible and we have \(E^{-1} = \overline{E}\). \(\ \blacksquare\) Definition If \(A \in M_{n \times n}(\mathbf{F})\) we define the rank of \(A\), denoted \(\text{rank}(A)\), to be the rank of the linear transformation \(L_A: \mathbf{F}^n \rightarrow \mathbf{F}^m\) One important implication here is that an \(n \times n\) matrix is invertible if and only if its rank is \(n\). Theorem 3.3 Let \(T: V \rightarrow W\) be a linear transformation between finite-dimensional vector spaces, and let \(\beta\) and \(\gamma\) be ordered bases for \(V\) and \(W\), respectively. Then \(rank(T) = rank([T]_{\beta}^{\gamma})\) The rank of a linear transformation is the same as the rank of one its matrix representations. Theorem 3.4 Let \(A\) be an \(m \times n\) matrix. If \(P\) and \(Q\) are invertible \(m \times m\) and \(n \times n\) matrices, respectively, then \(\text{rank}(AQ) = \text{rank}(A)\), \(\text{rank}(PA) = \text{rank}(A)\) and therefore, \(\text{rank}(PAQ) = \text{rank}(A)\) Proof (a) Observe that $$ \begin{align*} R(L_{AQ}) &amp;= R(L_AL_Q) \\ &amp;= L_AL_Q(\mathbf{F}) \quad \text{(to get the range, we apply the linear map)}\\ &amp;= L_A(L_Q(\mathbf{F})) \quad \text{(we apply $L_Q$ first)}\\ &amp;= L_A(\mathbf{F}) \quad \text{(because $L_Q$ is onto)}\\ &amp;= R(L_A) \end{align*} $$ Therefore, $$ \begin{align*} \text{rank}(AQ) &amp;= \dim(R(L_{AQ})) \quad \text{(the rank is the dimension of the range)} \\ &amp;= \dim(R(L_A)) \quad \text{(by the previous result)} \\ &amp;= rank(A). \end{align*} $$ Theorem 3.4 (Corollary) Elementary row and column operations on a matrix are rank-preserving. Theorem 3.5 The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns. Proof For any \(A \in M_{m \times n}(\mathbf{F})\), $$ \begin{align*} \text{rank}(A) = \text{rank}(L_A) = \dim(R(L_A)) \end{align*} $$ Let \(\beta\) be the standard ordered basis for \(\mathbf{F}^n\). Then \(\beta\) spans \(\mathbf{F}^n\). Theorem 2.2 assets that \(R(T) = \text(T(\beta))\) so $$ \begin{align*} R(L_A) &amp;= \text{span}(L_A(\beta)) \quad \text{(by Theorem 2.2)} \\ &amp;= \text{span}(\{L_A(e_1),...,L_A(e_n)\}) \end{align*} $$ By theorem 2.13(b) \(L_A(e_j) = Ae_j = a_j\) where \(a_j\) is the \(j\)th column of \(A\). So $$ \begin{align*} R(L_A) &amp;= \text{span}(\{a_1,...,a_n\}) \end{align*} $$ Therefore $$ \begin{align*} \text{rank}(A) &amp;= \dim(R(L_A)) = \dim(\text{span}(\{a_1,...,a_n\})) \ \blacksquare \end{align*} $$ Note: If you’ve forgotten 2.13 which you just did. Remember that by definition, matrix-vector multiplication \(Av\) is defined as \(a_1v_1 + a_2v_2 +...+ a_nv_n\) where \(a_1,...,a_n\) are the columns of \(A\) and \(v_1,...,v_n\) are the entries of the vector \(v\). Therefore, if you multiply \(Ae_j\) where \(e_j\) is from the standard basis, then we know that vector is all zeros except for the \(j\)th entry. So this means that \(Ae_j = a_j\). Theorem 3.6 Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then \(r \leq m, r \leq n\), and by means of a finite number of elementary row and column operations, \(A\) can be transformed into the matrix $$ \begin{align*} D = \begin{pmatrix} I_r &amp; O_1 \\ O_2 &amp; O_3 \end{pmatrix} \end{align*} $$ where \(O_1\), \(O_2\) and \(O_3\) are zero matrices. Thus \(D_{ii} = 1\) for \(i \leq r\) and \(D_{ij} = 0\) otherwise. Theorem 3.6 (Corollary 1) Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then there exists invertible matrices \(B\) and \(C\) of sizes \(m \times m \) and \(n \times n\) respectively, such that \(D = BAC\) where $$ \begin{align*} D = \begin{pmatrix} I_r &amp; O_1 \\ O_2 &amp; O_3 \end{pmatrix} \end{align*} $$ is the \(m \times n\) in which \(O_1\), \(O_2\) and \(O_3\) are zero matrices. Proof TODO Theorem 3.6 (Corollary 2) Let \(A\) be \(m \times n\) matrices. Then Proof TODO]]></summary></entry><entry><title type="html">7.1: Study Notes</title><link href="http://localhost:4000/jekyll/update/2024/09/17/7.1.html" rel="alternate" type="text/html" title="7.1: Study Notes" /><published>2024-09-17T01:01:36-07:00</published><updated>2024-09-17T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/17/7.1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/17/7.1.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be a scalar. A non-zero vector \(x\) in \(V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \((T - \lambda I)^p(x) = 0\) for some positive integer \(p\).
</div>
<p><br />
Note here that if \(x\) is a generalized eigenvector of \(T\) corresponding to \(\lambda\) and \(p\) is the smallest positive integer for which \((T - \lambda I)^{p}(x) = 0\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I)^{p-1}(x) &amp;= y \neq 0 \\
(T - \lambda I)(T - \lambda I)^{p-1}(x) &amp;= (T - \lambda I) y \\
(T - \lambda I)^{p}(x) &amp;= T(y) - \lambda y \\
\bar{0} &amp;= T(y) - \lambda y \\
T(y) &amp;= \lambda y
\end{align*}
$$
</div>
<p>So \(y\) is an eigenvector of \(T\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). The generalized eigenspace of \(T\) corresponding to \(\lambda\) denoted \(K_{\lambda}\), is the subset of \(V\) defined by
$$
\begin{align*}
K_{\lambda} = \{ x \in V: (T - \lambda I)^p(x) = 0 \quad \text{for some positive integer $p$} \}
\end{align*}
$$
</div>
<p><br />
Note that \(K_{\lambda}\) consists of the zero vector and all generalized eigenvectors corresponding to \(\lambda\).
<br />
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.1
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then
<ol type="a">
	<li>\(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\) (the eigenspace of \(T\) corresponding to \(\lambda\))</li>
	<li>For any eigenvalue \(\mu\) of \(T\) such that \(\mu \neq \lambda\), \(K_{\lambda} \cap K_{\mu} = \{0\}\)</li>
	<li>For any scalar \(\mu \neq \lambda\), the restriction of \(T - \mu I\) to \(K_{\lambda}\) is one-to-one and onto.</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
(a) Showing that \(K_{\lambda}\) is a subspace is straightforward. We need to show that \(\bar{0} \in K_{\lambda}\) and that \(K_{\lambda}\) is closed under scalar multiplication and addition.
<br />
<br />
To show that \(K_{\lambda}\) is \(T\)-invariant. We need to show for any \(x \in K_{\lambda}\), that \(T(x) \in K_{\lambda}\). By definition, let be \(p\) be a positive integer, then \((T - \lambda I)^p(x) = \bar{0}\). We know to show that \((T - \lambda I)^p(T(x)) = \bar{0}\). Observe that</p>
<div>
$$
\begin{align*}
(T - \lambda I)^p(T(x)) = T(T - \lambda I)^p(x) = T(\bar{0}) = \bar{0}
\end{align*}
$$
</div>
<p>(b) TODO
<br />
(c) Let \(\mu\) be a scalar such that \(\mu \neq \lambda\). Let \(x \in K_{\lambda}\).  Let \(p\) be the smallest integer such that \((T - \lambda I)^p x = 0\) and</p>
<div>
	$$
	\begin{align*}
	W = \text{span}\{x, (T - \lambda I)(x),...,(T - \lambda I)^{p-1}(x)\}
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.2
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that the characteristic polynomial splits. Suppose that \(\lambda\) is an eigenvalue of \(T\) with multiplicity \(m\). Then
<ol type="a">
	<li>\(\dim(K_{\lambda}) \leq m\)</li>
	<li>\(K_{\lambda} = N((T - \lambda I)^m)\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
(a) Let \(W = K_{\lambda}\). Let the characteristic polynomial of \(W\) be \(h(t)\) We know by Theorem 7.1, that \(W\) is a \(T\)-invariant subspace of \(V\). Therefore, by Theorem 5.20, \(h(t)\) divides the characteristic polynomial of \(T\). From Theorem 7.1(b), we know that \(\lambda\) is the only eigenvalue of \(T_W\). Therefore \(h(t) = (-1)^d(t - \lambda)^d\) where \(d = \dim(W)\) and \(d \leq m\). 
<br />
<br />
(b) By definition, we know that \(N(T)=\{x \in V \ | \ T(x) = 0\}\). We also know that \(K_{\lambda} =\{ x \in V \ | \ (T - \lambda I)^p = 0 \ \text{ for }p &gt; 0 \}\). So we can see that for any \(x \in N((T - \lambda I)^m)\)</p>
<div>
	$$
	\begin{align*}
	(T - \lambda I)^m(x) = 0
	\end{align*}
	$$
</div>
<p>Since \(m\) is the multiplicity of an eigenvalue, then it’s positive and so \(x \in K_{\lambda}\) by definition. So \(N((T - \lambda I)^m) \in K_{\lambda}\).
<br />
<br />
Now, suppose that \(x \in K_{\lambda}\). We want to show that \(x \in N((T - \lambda I)^m)\) where \(m\) is the multiplicity of \(\lambda\). Since the characteristic polynomial of \(T\) splits, then let it be of the form \(f(t) = (t - \lambda)^m g(t)\) where \(g(t)\) is the product of powers of the form \(t - \mu\) for eigenvalues \(\mu \neq \lambda\). By Theorem 7.1, \(g(T)\) is one-to-one on \(K_{\lambda}\)[TODO: WHY?]. It is also onto since \(K_{\lambda}\) is finite dimensional. Since  \(x \in K_{\lambda}\), then there exists some \(y\) such that \(g(T)(x) = y\). [WHY?]. Hence</p>
<div>
	$$
	\begin{align*}
	(T - \lambda I)^m(x) = (T - \lambda I)^m g(T)(y) = f(T)(y) = \bar{0}
	\end{align*}
	$$
</div>
<p>Why? ….. [TODO]
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.3
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that characteristic polynomial of \(T\) splits, and let \(\lambda_1,\lambda_2,..., \lambda_k\) be the distinct eigenvalues of \(T\). Then, for every \(x \in V\), there are exist unique vectors \(v_i \in K_{\lambda}\), for \(i = 1,2,...,k\) such that
$$
\begin{align*}
x = v_1 + v_2 + ... + v_k
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.4
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial \((t - \lambda_1)^{m_1}(t - \lambda_2)^{m_2}...(t - \lambda_k)^{m_k}\) splits. For \(i = 1,2,...,k\), let \(\beta_i\) be an ordered basis for \(K_{\lambda_i}\). Then
<ol type="a">
	<li>\(\beta_i \cap \beta_j = \emptyset \text{ for } i \neq j\)</li>
	<li>\(\beta = \beta_1 \cup \beta_2 \cup ... \cup \beta_k \) is an ordered basis for \(V\)</li>
	<li>\(\dim(K_{\lambda_j}) = m_i\) for all \(i\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
(a) Consequence of Theorem 7.1(b)
<br />
<br />
(b) We need to show that \(\beta\) is linearly independent and that \(\beta\) spans \(V\). To see that \(\beta\) is linearly independent. 
<br />
<br />
(c) 
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.6
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Suppose that \(\gamma_1, \gamma_2,...,\gamma_q\) are cycles of generalized eigenvectors of \(T\) corresponding to \(\lambda\) such that the initial vectors of the \(\gamma_i\)'s are distinct and form a linearly independent set. Then, the \(\gamma_i\)'s are disjoint and their union \(\gamma = \bigcup\limits_{i=1}^q \gamma_i\) is linearly independent.
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary (7.6)
</div>
<div class="purbdiv">
Every cycle of generalized eigenvectors of a linear operator is linearly independent
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.7
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then \(K_{\lambda}\) has an ordered basis consisting of a union of disjoint cycles of generalized eigenvectors corresponding to \(\lambda\).
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary 1 (7.7)
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial splits. Then \(T\) has a Jordan canonical form.
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be a scalar. A non-zero vector \(x\) in \(V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \((T - \lambda I)^p(x) = 0\) for some positive integer \(p\). Note here that if \(x\) is a generalized eigenvector of \(T\) corresponding to \(\lambda\) and \(p\) is the smallest positive integer for which \((T - \lambda I)^{p}(x) = 0\), then $$ \begin{align*} (T - \lambda I)^{p-1}(x) &amp;= y \neq 0 \\ (T - \lambda I)(T - \lambda I)^{p-1}(x) &amp;= (T - \lambda I) y \\ (T - \lambda I)^{p}(x) &amp;= T(y) - \lambda y \\ \bar{0} &amp;= T(y) - \lambda y \\ T(y) &amp;= \lambda y \end{align*} $$ So \(y\) is an eigenvector of \(T\). Definition Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). The generalized eigenspace of \(T\) corresponding to \(\lambda\) denoted \(K_{\lambda}\), is the subset of \(V\) defined by $$ \begin{align*} K_{\lambda} = \{ x \in V: (T - \lambda I)^p(x) = 0 \quad \text{for some positive integer $p$} \} \end{align*} $$ Note that \(K_{\lambda}\) consists of the zero vector and all generalized eigenvectors corresponding to \(\lambda\). Theorem 7.1 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then \(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\) (the eigenspace of \(T\) corresponding to \(\lambda\)) For any eigenvalue \(\mu\) of \(T\) such that \(\mu \neq \lambda\), \(K_{\lambda} \cap K_{\mu} = \{0\}\) For any scalar \(\mu \neq \lambda\), the restriction of \(T - \mu I\) to \(K_{\lambda}\) is one-to-one and onto. Proof (a) Showing that \(K_{\lambda}\) is a subspace is straightforward. We need to show that \(\bar{0} \in K_{\lambda}\) and that \(K_{\lambda}\) is closed under scalar multiplication and addition. To show that \(K_{\lambda}\) is \(T\)-invariant. We need to show for any \(x \in K_{\lambda}\), that \(T(x) \in K_{\lambda}\). By definition, let be \(p\) be a positive integer, then \((T - \lambda I)^p(x) = \bar{0}\). We know to show that \((T - \lambda I)^p(T(x)) = \bar{0}\). Observe that $$ \begin{align*} (T - \lambda I)^p(T(x)) = T(T - \lambda I)^p(x) = T(\bar{0}) = \bar{0} \end{align*} $$ (b) TODO (c) Let \(\mu\) be a scalar such that \(\mu \neq \lambda\). Let \(x \in K_{\lambda}\). Let \(p\) be the smallest integer such that \((T - \lambda I)^p x = 0\) and $$ \begin{align*} W = \text{span}\{x, (T - \lambda I)(x),...,(T - \lambda I)^{p-1}(x)\} \end{align*} $$ Theorem 7.2 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that the characteristic polynomial splits. Suppose that \(\lambda\) is an eigenvalue of \(T\) with multiplicity \(m\). Then \(\dim(K_{\lambda}) \leq m\) \(K_{\lambda} = N((T - \lambda I)^m)\) Proof (a) Let \(W = K_{\lambda}\). Let the characteristic polynomial of \(W\) be \(h(t)\) We know by Theorem 7.1, that \(W\) is a \(T\)-invariant subspace of \(V\). Therefore, by Theorem 5.20, \(h(t)\) divides the characteristic polynomial of \(T\). From Theorem 7.1(b), we know that \(\lambda\) is the only eigenvalue of \(T_W\). Therefore \(h(t) = (-1)^d(t - \lambda)^d\) where \(d = \dim(W)\) and \(d \leq m\). (b) By definition, we know that \(N(T)=\{x \in V \ | \ T(x) = 0\}\). We also know that \(K_{\lambda} =\{ x \in V \ | \ (T - \lambda I)^p = 0 \ \text{ for }p &gt; 0 \}\). So we can see that for any \(x \in N((T - \lambda I)^m)\) $$ \begin{align*} (T - \lambda I)^m(x) = 0 \end{align*} $$ Since \(m\) is the multiplicity of an eigenvalue, then it’s positive and so \(x \in K_{\lambda}\) by definition. So \(N((T - \lambda I)^m) \in K_{\lambda}\). Now, suppose that \(x \in K_{\lambda}\). We want to show that \(x \in N((T - \lambda I)^m)\) where \(m\) is the multiplicity of \(\lambda\). Since the characteristic polynomial of \(T\) splits, then let it be of the form \(f(t) = (t - \lambda)^m g(t)\) where \(g(t)\) is the product of powers of the form \(t - \mu\) for eigenvalues \(\mu \neq \lambda\). By Theorem 7.1, \(g(T)\) is one-to-one on \(K_{\lambda}\)[TODO: WHY?]. It is also onto since \(K_{\lambda}\) is finite dimensional. Since \(x \in K_{\lambda}\), then there exists some \(y\) such that \(g(T)(x) = y\). [WHY?]. Hence $$ \begin{align*} (T - \lambda I)^m(x) = (T - \lambda I)^m g(T)(y) = f(T)(y) = \bar{0} \end{align*} $$ Why? ….. [TODO] Theorem 7.3 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that characteristic polynomial of \(T\) splits, and let \(\lambda_1,\lambda_2,..., \lambda_k\) be the distinct eigenvalues of \(T\). Then, for every \(x \in V\), there are exist unique vectors \(v_i \in K_{\lambda}\), for \(i = 1,2,...,k\) such that $$ \begin{align*} x = v_1 + v_2 + ... + v_k \end{align*} $$ Proof TODO Theorem 7.4 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial \((t - \lambda_1)^{m_1}(t - \lambda_2)^{m_2}...(t - \lambda_k)^{m_k}\) splits. For \(i = 1,2,...,k\), let \(\beta_i\) be an ordered basis for \(K_{\lambda_i}\). Then \(\beta_i \cap \beta_j = \emptyset \text{ for } i \neq j\) \(\beta = \beta_1 \cup \beta_2 \cup ... \cup \beta_k \) is an ordered basis for \(V\) \(\dim(K_{\lambda_j}) = m_i\) for all \(i\) Proof (a) Consequence of Theorem 7.1(b) (b) We need to show that \(\beta\) is linearly independent and that \(\beta\) spans \(V\). To see that \(\beta\) is linearly independent. (c) Theorem 7.6 Let \(T\) be a linear operator on a vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Suppose that \(\gamma_1, \gamma_2,...,\gamma_q\) are cycles of generalized eigenvectors of \(T\) corresponding to \(\lambda\) such that the initial vectors of the \(\gamma_i\)'s are distinct and form a linearly independent set. Then, the \(\gamma_i\)'s are disjoint and their union \(\gamma = \bigcup\limits_{i=1}^q \gamma_i\) is linearly independent. Proof TODO Corollary (7.6) Every cycle of generalized eigenvectors of a linear operator is linearly independent Theorem 7.7 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then \(K_{\lambda}\) has an ordered basis consisting of a union of disjoint cycles of generalized eigenvectors corresponding to \(\lambda\). Proof TODO Corollary 1 (7.7) Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial splits. Then \(T\) has a Jordan canonical form. Proof TODO References Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 39: Jordan Blocks and Generalized Eigenvectors</title><link href="http://localhost:4000/jekyll/update/2024/09/16/lec39-jordan-blocks-generalized-eigenvectors.html" rel="alternate" type="text/html" title="Lecture 39: Jordan Blocks and Generalized Eigenvectors" /><published>2024-09-16T01:01:36-07:00</published><updated>2024-09-16T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/16/lec39-jordan-blocks-generalized-eigenvectors</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/16/lec39-jordan-blocks-generalized-eigenvectors.html"><![CDATA[<p>Last lecture, we saw why matrices in Jordan Canonical form are useful and we spent the entire lecture  understanding the following major theorem
<br /></p>
<div class="purdiv">
Theorem (JCF)
</div>
<div class="purbdiv">
Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Specifically, we looked at the first Jordan block (\(A_1\)) in \([T]_{\beta}^{\beta}\) and analyzed what these basis elements need to be in order for \([T]_{\beta}^{\beta}\) to be in Jordan Canonical form. This led to FACT 1 which was that a Jordan Canonical Basis must consists of generalized eigenvectors. We saw observed that</p>
<div>
$$
\begin{align*}
(T - \lambda_1 I_V)(v_1) &amp;= \bar{0}_V \\
(T - \lambda_1 I_V)^2(v_2) &amp;= \bar{0}_V \\
(T - \lambda_1 I_V)^3(v_3) &amp;= \bar{0}_V \\
\vdots \\
(T - \lambda_1 I_V)^{n_1}(v_{n_1}) &amp;= \bar{0}_V
\end{align*}
$$
</div>
<p>From this we see that</p>
<div>
$$
\begin{align*}
(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}) &amp;= v_1 \\
(T - \lambda_1 I_V)^{n_1 - 2}(v_{n_1}) &amp;= v_2 \\
(T - \lambda_1 I_V)^{n_1 - 3}(v_{n_1}) &amp;= v_3 \\
\vdots \\
(T - \lambda I_V)(v_{n_1}) &amp;= v_{n_1-1} \\
v_{n_1} &amp;= v_{n_1}
\end{align*}
$$
</div>
<p>In other words,</p>
<div>
$$
\begin{align*}
\{v_1,...v_{n_1-1},v_{n_1}\} = \{(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}),...,(T - \lambda_1 I_V)^{2}(v_{n_1}), (T - \lambda_1 I_V)(v_{n_1}), v_{n_1}\}
\end{align*}
$$
</div>
<p>So we can see here that the first \(n_1\) basis vectors coming from the \(A_1\) block are all obtained from the last vector and applying the maps over and over again. This led to FACT 2 which was
<!------------------------------------------------------------------------------------>
that A Jordan Canonical Basis is made of “cyclic pieces”. The basis is not only made of generalized eigenvectors but they also appear in this cyclic pattern. This leads us to the following definition
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(x \in K_{\lambda}\) and \(p\) be the smallest integer such that \((T - \lambda_i I_V)^p(x) = \bar{0}_V\). Let 
$$
\begin{align*}
\gamma = \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\}
\end{align*}
$$
\(\gamma\) is the cycle of generalized eigenvectors generated by \(x\). The length of \(\gamma\) is \(p\). The initial vector of \(\gamma\) is \((T - \lambda_i I_V)^{p-1}(x)\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
What can we say about these objects?
<br />
<br /></p>
<div class="purdiv">
Theorem 2.1
</div>
<div class="purbdiv">
<ol type="a">
	<li>\(\gamma\) is linearly independent</li>
	<li>\(W = \text{span}(\gamma)\) is \(T\)-invariant</li>
	<li>The matrix representation of the restriction of \(T\) to \(W\), \([T_W]_{\gamma}^{\gamma}\) is a Jordan block</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
<br />
Re-write \(\gamma\) as</p>
<div>
$$
\begin{align*}
\gamma &amp;= \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\} \\
       &amp;= \{v_1, ..., v_n\}
\end{align*}
$$
</div>
<p>These satisfy the following relations</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)(v_1) &amp;= (T - \lambda I_V)^p(x) = \bar{0}_V\\
(T - \lambda I_V)(v_i) &amp;= (T - \lambda I_V)^p(v_{i-1})
\end{align*}
$$
</div>
<p>We can re-write these equations as</p>
<div>
$$
\begin{align*}
T(v_1) &amp;= \lambda v_1\\
T(v_i) &amp;= \lambda v_i + v_{i-1}
\end{align*}
$$
</div>
<p>This means that \(T\) maps \(v_i\) to a linear combination of \(v_i\)’s again (which are in \(\gamma\)). This implies that \(T(v_j) \in \text{span}(\gamma) \quad \forall j=1,...,p\). This means that \(T(W) \subseteq W\). Therefore, \(W\) is \(T\)-invariant and (b) holds.
<br />
<br />
We can also write the matrix representative of \(T\) with respect to \(W\)</p>
<div>
$$
\begin{align*}
[T_W]_{\gamma}^{\gamma} = 
\begin{pmatrix}
\lambda &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \lambda &amp; \ddots &amp; \vdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<p>From this we see that \((c)\) holds. 
<br />
<br />
So now we only need to prove \((a)\). We need to build a basis of each \(K_{\lambda}\) out of cycles. To show this we need the next two results
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2.2
</div>
<div class="purbdiv">
Let \(\gamma_1,...,\gamma_m\) be cycles for \(\lambda\) with linearly independent initial vectors. Then
$$
\begin{align*}
\gamma = \cup_{j=1}^m \gamma_j
\end{align*}
$$
is linearly independent.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and
<br /></p>
<div class="purdiv">
Theorem 2.3
</div>
<div class="purbdiv">
Each \(K_{\lambda}\) has a basis consisting of disjoint cycles.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The JCF Proof</b></h4>
<p>Theorems 1.1 to 2.3 combined prove the JCF Theorem! To see how. Let \(\lambda_1,...,\lambda_k\) be the disjoint eigenvalues of \(T\). For \(\lambda_j\) find a basis \(\beta_j\) of \(K_{\lambda_j}\) consisting of disjoint cycles (Theorem 2.3).
<br />
By Theorem 1.4, \(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis of \(V\). And by Theorem 2.1, \([T]_{\beta}^{\beta}\) is in JCF.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(A = \begin{pmatrix}
3 &amp; 1 &amp; -2 \\
-1 &amp; 0 &amp; 5 \\
-1 &amp; -1 &amp; 4
\end{pmatrix}\). Put \(A\) in JCF if possible. Here \(T\) is \(L_A: \mathbf{R}^3 \rightarrow \mathbf{R}^3\). 
<br />
<br />
We first need to check if the characteristic polynomial splits.</p>
<div>
$$
\begin{align*}
\det(A - tI_3) = (3 - t)(2 - t)^2
\end{align*}
$$
</div>
<p>So the characteristic polynomial splits. (In fact \(\lambda_1\) has algebraic multiplicity 1 and \(\lambda_2\) has algebraic multiplicity 2).
<br />
<br />
We know the general form of the Jordan Canonical Basis where \(\beta = \beta_1 \cup ... \cup \beta_m\) where \(\beta_j = \gamma_1^j \cup \gamma_2^j \cup ... \cup \gamma_1^{k_j}\), a collection of disjoint cycles. So let’s build these pieces starting with the first eigenvalue as follows
<br />
<br />
\(\lambda_1 = 3\): The algebraic multiplicity of \(\lambda_1\) is 1. This implies that the generalized eigenspace \(K_{\lambda_1} = E_{\lambda_1}\) why is that? The dimension of the generalized eigenspace is equal to the algebraic multiplicity so its dimension is 1. But we know that \(E_{\lambda_1}\) has a non-zero dimension and that it sits inside \(K_{\lambda_1}\). Therefore \(K_{\lambda_1} = E_{\lambda_1}\). What about the cycles of this generalized eigenspace? the length of the cycle is (\(p=1\)). So all we need to do is find the nullspace of this eigenspace.</p>
<div>
$$
\begin{align*}
A - 3I_3 = 
A = \begin{pmatrix}
0 &amp; 1 &amp; -2 \\
-1 &amp; -3 &amp; 5 \\
-1 &amp; -1 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>Putting this in echelon form, we see</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -2 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>The solution will this lead to</p>
<div>
$$
\begin{align*}
\beta_1 = \left\{
\begin{pmatrix}
-1 \\
2 \\
1
\end{pmatrix}
\right\}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>\(\lambda_2 = 2\): By Theorem 1.2, \(K_{\lambda_2} = N((A - 2I_3)^2)\) so</p>
<div>
$$
(A - 2I_3)^2 = 
\begin{align*}
A - 2I_3 = 
\begin{pmatrix}
1 &amp; 1 &amp; -2 \\
-1 &amp; -2 &amp; 5 \\
-1 &amp; -1 &amp; 2
\end{pmatrix}^2 
= 
\begin{pmatrix}
2 &amp; 1 &amp; -1 \\
-4 &amp; -2 &amp; 2 \\
-2 &amp; -1 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>Putting this in echelon form, we see</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; \frac{1}{2} &amp; -\frac{1}{2} \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>The solution set is then</p>
<div>
$$
\begin{align*}
K_{\lambda_2} = \text{span}\left\{
\begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}
\begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}
\right\}
\end{align*}
$$
</div>
<p>So we have a basis but we don’t have cycles yet. So we need to start generating cycles. Let’s choose the second vector above.</p>
<div>
$$
\begin{align*}
(A - 2I_3)(x) = 
\begin{pmatrix}
1 &amp; 1 &amp; -2 \\
-1 &amp; -2 &amp; 5 \\
-1 &amp; -1 &amp; 2
\end{pmatrix}
\begin{pmatrix}
1 \\
0 \\
2
\end{pmatrix}
=
\begin{pmatrix}
-3 \\
9 \\
3
\end{pmatrix}
\end{align*}
$$
</div>
<p>So given \(x \in K_{\lambda} \implies \gamma=\{(T - \lambda I_V)^{p-1}(x),...,x\}\). The term above is the second term from last. So the term remaining is just when \(p = 0\) which means it’s \(x\) itself. Therefore</p>
<div>
$$
\begin{align*}
\gamma = \left\{
\begin{pmatrix}
-3 \\
9 \\
3
\end{pmatrix},
\begin{pmatrix}
1 \\
0 \\
2
\end{pmatrix}
\right\} = \beta_2
\end{align*}
$$
</div>
<p>Therefore, the Jordan Canonical Basis is</p>
<div>
$$
\begin{align*}
\beta = \beta_1 \cup \beta_2 
= \left\{
\begin{pmatrix}
	-1 \\
	2 \\
	1
\end{pmatrix},
\begin{pmatrix}
-3 \\
9 \\
3
\end{pmatrix},
\begin{pmatrix}
1 \\
0 \\
2
\end{pmatrix}
\right\}
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
[L_A]_{\beta}^{\beta} =
\begin{pmatrix}
3 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>The diagonal elements are the eigenvalues. What about other non-zero elements? The second block is generated by a cycle of length 2 So there is a 1 in that block.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Determining the Jordan Block for an Eigenvalue</b></h4>
<p>But can we get to the end JCF form without having to compute all of these cycles?
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(\lambda\) be an eigenvalue of \(T\) with algebraic multiplicity \(m\). 
$$
\begin{align*}
r_1 &amp;= \dim(V) - rank(T - \lambda I_V) \\
r_i &amp;= rank((T - \lambda I_V)^{i-1}) - rank((T - \lambda I_V)^i)
\end{align*}
$$
Using \(r_1, r_2, ...\) form the diagram
$$
\begin{align*}
\begin{matrix}
\circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_1\text{ dots} \\
\circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_2\text{ dots} \\
\vdots &amp; \vdots \\
\circ &amp; \circ &amp;        &amp;       &amp; \quad &amp; r_j\text{ dots} \\
\end{matrix}
\end{align*}
$$
Then each column corresponds to Jordan block of \(\lambda\) of size the number of the dots in each column 
</div>
<p><br />
Observation:</p>
<ul>
	<li>\(r_1 \geq r_2 \geq r_3 \geq ...\)</li>
	<li>\(r_1 + r_2 + r_3 + ... = m\)</li>
	<li>\(r_1 =\) the number of Jordan Blocks for \(\lambda\)</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last lecture, we saw why matrices in Jordan Canonical form are useful and we spent the entire lecture understanding the following major theorem Theorem (JCF) Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form. Specifically, we looked at the first Jordan block (\(A_1\)) in \([T]_{\beta}^{\beta}\) and analyzed what these basis elements need to be in order for \([T]_{\beta}^{\beta}\) to be in Jordan Canonical form. This led to FACT 1 which was that a Jordan Canonical Basis must consists of generalized eigenvectors. We saw observed that $$ \begin{align*} (T - \lambda_1 I_V)(v_1) &amp;= \bar{0}_V \\ (T - \lambda_1 I_V)^2(v_2) &amp;= \bar{0}_V \\ (T - \lambda_1 I_V)^3(v_3) &amp;= \bar{0}_V \\ \vdots \\ (T - \lambda_1 I_V)^{n_1}(v_{n_1}) &amp;= \bar{0}_V \end{align*} $$ From this we see that $$ \begin{align*} (T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}) &amp;= v_1 \\ (T - \lambda_1 I_V)^{n_1 - 2}(v_{n_1}) &amp;= v_2 \\ (T - \lambda_1 I_V)^{n_1 - 3}(v_{n_1}) &amp;= v_3 \\ \vdots \\ (T - \lambda I_V)(v_{n_1}) &amp;= v_{n_1-1} \\ v_{n_1} &amp;= v_{n_1} \end{align*} $$ In other words, $$ \begin{align*} \{v_1,...v_{n_1-1},v_{n_1}\} = \{(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}),...,(T - \lambda_1 I_V)^{2}(v_{n_1}), (T - \lambda_1 I_V)(v_{n_1}), v_{n_1}\} \end{align*} $$ So we can see here that the first \(n_1\) basis vectors coming from the \(A_1\) block are all obtained from the last vector and applying the maps over and over again. This led to FACT 2 which was that A Jordan Canonical Basis is made of “cyclic pieces”. The basis is not only made of generalized eigenvectors but they also appear in this cyclic pattern. This leads us to the following definition Definition Let \(x \in K_{\lambda}\) and \(p\) be the smallest integer such that \((T - \lambda_i I_V)^p(x) = \bar{0}_V\). Let $$ \begin{align*} \gamma = \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\} \end{align*} $$ \(\gamma\) is the cycle of generalized eigenvectors generated by \(x\). The length of \(\gamma\) is \(p\). The initial vector of \(\gamma\) is \((T - \lambda_i I_V)^{p-1}(x)\) What can we say about these objects? Theorem 2.1 \(\gamma\) is linearly independent \(W = \text{span}(\gamma)\) is \(T\)-invariant The matrix representation of the restriction of \(T\) to \(W\), \([T_W]_{\gamma}^{\gamma}\) is a Jordan block Proof Re-write \(\gamma\) as $$ \begin{align*} \gamma &amp;= \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\} \\ &amp;= \{v_1, ..., v_n\} \end{align*} $$ These satisfy the following relations $$ \begin{align*} (T - \lambda I_V)(v_1) &amp;= (T - \lambda I_V)^p(x) = \bar{0}_V\\ (T - \lambda I_V)(v_i) &amp;= (T - \lambda I_V)^p(v_{i-1}) \end{align*} $$ We can re-write these equations as $$ \begin{align*} T(v_1) &amp;= \lambda v_1\\ T(v_i) &amp;= \lambda v_i + v_{i-1} \end{align*} $$ This means that \(T\) maps \(v_i\) to a linear combination of \(v_i\)’s again (which are in \(\gamma\)). This implies that \(T(v_j) \in \text{span}(\gamma) \quad \forall j=1,...,p\). This means that \(T(W) \subseteq W\). Therefore, \(W\) is \(T\)-invariant and (b) holds. We can also write the matrix representative of \(T\) with respect to \(W\) $$ \begin{align*} [T_W]_{\gamma}^{\gamma} = \begin{pmatrix} \lambda &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; \lambda &amp; \ddots &amp; \vdots \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \lambda \end{pmatrix} \end{align*} $$ From this we see that \((c)\) holds. So now we only need to prove \((a)\). We need to build a basis of each \(K_{\lambda}\) out of cycles. To show this we need the next two results Theorem 2.2 Let \(\gamma_1,...,\gamma_m\) be cycles for \(\lambda\) with linearly independent initial vectors. Then $$ \begin{align*} \gamma = \cup_{j=1}^m \gamma_j \end{align*} $$ is linearly independent. and Theorem 2.3 Each \(K_{\lambda}\) has a basis consisting of disjoint cycles. The JCF Proof Theorems 1.1 to 2.3 combined prove the JCF Theorem! To see how. Let \(\lambda_1,...,\lambda_k\) be the disjoint eigenvalues of \(T\). For \(\lambda_j\) find a basis \(\beta_j\) of \(K_{\lambda_j}\) consisting of disjoint cycles (Theorem 2.3). By Theorem 1.4, \(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis of \(V\). And by Theorem 2.1, \([T]_{\beta}^{\beta}\) is in JCF. Example Let \(A = \begin{pmatrix} 3 &amp; 1 &amp; -2 \\ -1 &amp; 0 &amp; 5 \\ -1 &amp; -1 &amp; 4 \end{pmatrix}\). Put \(A\) in JCF if possible. Here \(T\) is \(L_A: \mathbf{R}^3 \rightarrow \mathbf{R}^3\). We first need to check if the characteristic polynomial splits. $$ \begin{align*} \det(A - tI_3) = (3 - t)(2 - t)^2 \end{align*} $$ So the characteristic polynomial splits. (In fact \(\lambda_1\) has algebraic multiplicity 1 and \(\lambda_2\) has algebraic multiplicity 2). We know the general form of the Jordan Canonical Basis where \(\beta = \beta_1 \cup ... \cup \beta_m\) where \(\beta_j = \gamma_1^j \cup \gamma_2^j \cup ... \cup \gamma_1^{k_j}\), a collection of disjoint cycles. So let’s build these pieces starting with the first eigenvalue as follows \(\lambda_1 = 3\): The algebraic multiplicity of \(\lambda_1\) is 1. This implies that the generalized eigenspace \(K_{\lambda_1} = E_{\lambda_1}\) why is that? The dimension of the generalized eigenspace is equal to the algebraic multiplicity so its dimension is 1. But we know that \(E_{\lambda_1}\) has a non-zero dimension and that it sits inside \(K_{\lambda_1}\). Therefore \(K_{\lambda_1} = E_{\lambda_1}\). What about the cycles of this generalized eigenspace? the length of the cycle is (\(p=1\)). So all we need to do is find the nullspace of this eigenspace. $$ \begin{align*} A - 3I_3 = A = \begin{pmatrix} 0 &amp; 1 &amp; -2 \\ -1 &amp; -3 &amp; 5 \\ -1 &amp; -1 &amp; 1 \end{pmatrix} \end{align*} $$ Putting this in echelon form, we see $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The solution will this lead to $$ \begin{align*} \beta_1 = \left\{ \begin{pmatrix} -1 \\ 2 \\ 1 \end{pmatrix} \right\} \end{align*} $$ \(\lambda_2 = 2\): By Theorem 1.2, \(K_{\lambda_2} = N((A - 2I_3)^2)\) so $$ (A - 2I_3)^2 = \begin{align*} A - 2I_3 = \begin{pmatrix} 1 &amp; 1 &amp; -2 \\ -1 &amp; -2 &amp; 5 \\ -1 &amp; -1 &amp; 2 \end{pmatrix}^2 = \begin{pmatrix} 2 &amp; 1 &amp; -1 \\ -4 &amp; -2 &amp; 2 \\ -2 &amp; -1 &amp; 1 \end{pmatrix} \end{align*} $$ Putting this in echelon form, we see $$ \begin{align*} \begin{pmatrix} 1 &amp; \frac{1}{2} &amp; -\frac{1}{2} \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The solution set is then $$ \begin{align*} K_{\lambda_2} = \text{span}\left\{ \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \right\} \end{align*} $$ So we have a basis but we don’t have cycles yet. So we need to start generating cycles. Let’s choose the second vector above. $$ \begin{align*} (A - 2I_3)(x) = \begin{pmatrix} 1 &amp; 1 &amp; -2 \\ -1 &amp; -2 &amp; 5 \\ -1 &amp; -1 &amp; 2 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} = \begin{pmatrix} -3 \\ 9 \\ 3 \end{pmatrix} \end{align*} $$ So given \(x \in K_{\lambda} \implies \gamma=\{(T - \lambda I_V)^{p-1}(x),...,x\}\). The term above is the second term from last. So the term remaining is just when \(p = 0\) which means it’s \(x\) itself. Therefore $$ \begin{align*} \gamma = \left\{ \begin{pmatrix} -3 \\ 9 \\ 3 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \right\} = \beta_2 \end{align*} $$ Therefore, the Jordan Canonical Basis is $$ \begin{align*} \beta = \beta_1 \cup \beta_2 = \left\{ \begin{pmatrix} -1 \\ 2 \\ 1 \end{pmatrix}, \begin{pmatrix} -3 \\ 9 \\ 3 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \right\} \end{align*} $$ and $$ \begin{align*} [L_A]_{\beta}^{\beta} = \begin{pmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ The diagonal elements are the eigenvalues. What about other non-zero elements? The second block is generated by a cycle of length 2 So there is a 1 in that block. Determining the Jordan Block for an Eigenvalue But can we get to the end JCF form without having to compute all of these cycles? Theorem Let \(\lambda\) be an eigenvalue of \(T\) with algebraic multiplicity \(m\). $$ \begin{align*} r_1 &amp;= \dim(V) - rank(T - \lambda I_V) \\ r_i &amp;= rank((T - \lambda I_V)^{i-1}) - rank((T - \lambda I_V)^i) \end{align*} $$ Using \(r_1, r_2, ...\) form the diagram $$ \begin{align*} \begin{matrix} \circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_1\text{ dots} \\ \circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_2\text{ dots} \\ \vdots &amp; \vdots \\ \circ &amp; \circ &amp; &amp; &amp; \quad &amp; r_j\text{ dots} \\ \end{matrix} \end{align*} $$ Then each column corresponds to Jordan block of \(\lambda\) of size the number of the dots in each column Observation: \(r_1 \geq r_2 \geq r_3 \geq ...\) \(r_1 + r_2 + r_3 + ... = m\) \(r_1 =\) the number of Jordan Blocks for \(\lambda\) References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 37/38: The Jordan Canonical Form and Generalized Eigenvectors</title><link href="http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form.html" rel="alternate" type="text/html" title="Lecture 37/38: The Jordan Canonical Form and Generalized Eigenvectors" /><published>2024-09-15T01:01:36-07:00</published><updated>2024-09-15T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form.html"><![CDATA[<p>Last time we proved that If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\). This therefore lead to the conclusion that \(T\) is diagonalizable.
<!------------------------------------------------------------------------------------></p>
<h4><b>A Test For Diagonalizability</b></h4>
<p>We studied previously a few ways a linear operator can be tested for diagonalizability. From lecture 25,</p>
<div class="purdiv">
Theorem (5.8(a))
</div>
<div class="purbdiv">
\(T\) is diagonalizable if and only if
<ul style="list-style-type:lower-alpha">
	<li>The characteristic polynomial splits over its field \(\mathbf{R}\) or \(\mathbf{C}\)</li>
	<li>For each eigenvalue of \(T\), its geometric multiplicity (\(\dim(E_{\lambda})\)) = algebraic multiplicity</li>
</ul>
</div>
<p><br />
We mentioned last time too that if \(V\) is over \(\mathbf{C}\), the the characteristic polynomial always splits. Note also that you can have \((a)\) but not \((b)\). For example</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>splits but doesn’t satisfy \((b)\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Jordan Canonical Form</b></h4>
<p>Based on the previous observation. It turns out there is a nice form that we can put \(T\) into in order to achieve \((a)\) and \((b)\)
<br /></p>
<div class="purdiv">
Theorem (JCF)
</div>
<div class="purbdiv">
Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form.
</div>
<p><br />
But what is a Jordan Canonical form? We first define a Jordan Block as follows
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A Jordan Block is a square matrix of the form
$$
\begin{align*}
\begin{pmatrix}
\lambda
\end{pmatrix}
\quad \text{ or } \quad 
\begin{pmatrix}
\lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
So they’re almost diagonal but not really. For example, for a \(2 \times 2\) and \(3 \times 3\) matrices, a Jordan block looks like</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp; \lambda
\end{pmatrix}
,
\begin{pmatrix}
\lambda &amp; 1 &amp; 0 \\
0 &amp; \lambda &amp; 1 \\
0 &amp; 0 &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<p>Using these Jordan blocks, we can now define what the Jordan Canonical form is
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A square matrix \(A\) is in Jordan Canonical Form if
$$
\begin{align*}
A =
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_{l}
\end{pmatrix},
\ \text{ where $A_j$ is a Jordan block}
\end{align*}
$$
</div>
<p><br />
You can think of this matrix as more of a generalization of a diagonal matrix.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>The following are examples of matrix in Jordan Canonical Form</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix},
B = 
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that the characteristic polynomial of both \(A\) and \(B\) is \((1-t)^3(2-t)^2\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing Powers of Matrices in JFC</b></h4>
<p>It turns out that we can write a formula for powers of matrices in JFC. It is not as easy as taking the power of a diagonal matrix but at least have a formula.
<br />
<br />
Fact 1:</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
\lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda
\end{pmatrix}
\implies
\begin{pmatrix}
\lambda^k &amp; k\lambda^{k-1} &amp; \cdots &amp; \frac{k(k-1)...(k-n+2)}{n!}\lambda^{k-n+1} \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; k\lambda^{k-1} \\
0 &amp; \cdots &amp; 0 &amp; \lambda^k
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that the third entry in the first row for example is \(\frac{k(k-1)}{2!}\lambda^{k-1}\)
<br />
<br /> 
Fact 2: If \(A\) is in Jordan Canonical form, then the power of the matrix is as follows</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_l
\end{pmatrix}^k
\implies
\begin{pmatrix}
A_1^k &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2^k &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_l^k
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Proof of JCF Theorem</b></h4>
<p>So now we see that matrices in JCF are useful and we have enough motivation to prove the theorem above which again states that if the characteristic polynomial splits, then there is a basis such that \([T]^{\beta}_{\beta}\) is in JFC! To prove it, let \(\beta = \{v_1, ..., v_n\}\) of \(V\). We this basis to be such that</p>
<div>
$$
\begin{align*}
[T]^{\beta}_{\beta} = 
\begin{pmatrix}
[T(v)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta}
\end{pmatrix}
=
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; A_k
\end{pmatrix}
\end{align*}
$$
</div>
<p>These vectors \(v_1,...,v_j\) are not necessarily eigenvectors. What are they? Let’s focus on \(A_1\) above and let \(A_1\) be of size \(n_1 \times n_1\).</p>
<div>
$$
\begin{align*}
A_1 =
\begin{pmatrix}
\lambda_1 &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_1 &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda_1
\end{pmatrix}
\end{align*}
$$
</div>
<p>Since \(A_1\) has the form above (Jordan block), then we know at least for \(\lambda_1\), \(T(v_1) = \lambda_1 v_1\). What about \(T(v_2)\)? We see that column 2 has 1 in the first row and then \(\lambda_2\) in the second row. So \(T(v_2) = v_1 + \lambda_2 v_2\). Therefore, \(v_2\) is not an eigenvector but we can observe that</p>
<div>
$$
\begin{align*}
T(v_2) &amp;= v_1 + \lambda_1 v_2 \\
T(v_2) - \lambda_1 v_2 &amp;= v_1 \\
(T - \lambda_1 I_V)(v_2) &amp;= v_1
\end{align*}
$$
</div>
<p>So at this point, we see that \(v_2\) is not an eigenvector but if we apply the map \((T - \lambda_1 I_V)\) on it, it becomes an eigenvector. What if we apply this map twice?</p>
<div>
$$
\begin{align*}
(T - \lambda_1 I_V)^2(v_2) &amp;= \bar{0}_V \quad \text{ (because $v_1$ is an eigenvector)}
\end{align*}
$$
</div>
<p>What about the remaining vectors?</p>
<div>
$$
\begin{align*}
T(v_3) &amp;= v_2 + \lambda_1 v_3 \\
(T - \lambda_1 I_V)(v_3) &amp;= v_2 \\
(T - \lambda_1 I_V)^3(v_3) &amp;= (T - \lambda_1 I_V)^2(v_2) \\
(T - \lambda_1 I_V)^3(v_2) &amp;= \bar{0}_V
\end{align*}
$$
</div>
<p>So they’re not eigenvectors but they satisfy these equations. Based on this observation, we’re going to define the following
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(x \in V\) is a generalized eigenvector of \(T: V \rightarrow V\) corresponding to \(\lambda\) if 
$$
\begin{align*}
(T - \lambda I_V)^p(x) = \bar{0}_V
\end{align*}
$$
for some integer \(p &gt; 0\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Observation: When \(p\) is the smallest integer for which \((T - \lambda I_V)^p(x) = \bar{0}_V\), then \(y = (T - \lambda I_V)^{p-1}(x)\) is an eigenvector. 
<br />
<br />
So now we know that the basis we want to build will consists of generalized eigenvectors. These generalized eigenvectors belongs to subspaces we define next
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(\lambda\) be an eigenvalue of \(T: V \rightarrow V\). The generalized eigenspace of \(\lambda\) is
$$
\begin{align*}
K_{\lambda} = \{x \in V : (T - \lambda I_V)^p(x) = \bar{0}_V \text{ for some } p &gt; 0 \}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="purdiv">
Theorem 1.1
</div>
<div class="purbdiv">
<ol type="a">
	<li>\(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\).</li>
	<li>For \(\mu \neq \lambda\), the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one.</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
<br />
(a) We need to show three things. \(K_{\lambda}\) contains \(E_{\lambda}\). This is clearly true by definition and \(E_{\lambda} \subseteq K_{\lambda}\). 
<br />
<br />
Next we need to show that \(K_{\lambda}\) is a subspace. This means we need to show that it contains the zero vector and it is closed under scalar multiplication and addition. \((T - \lambda I_V)(\bar{0}_V) = \bar{0}_V\) so \(\bar{0}_V \in K_{\lambda}\). Now consider \(x, y \in K_{\lambda}\) and \(c \in \mathbf{F}\), we need to show that \(x + cy \in K_{\lambda}\). Since \(x\) and \(y\) are in \(K_{\lambda}\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)(x) &amp;= \bar{0}_V \text{ for } p &gt; 0 \\
(T - \lambda I_V)(y) &amp;= \bar{0}_V \text { for } q &gt; 0
\end{align*}
$$
</div>
<p>Therefore</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p+q}(x + cy) &amp;= (T - \lambda I_V)^{p+q}(x) + c(T - \lambda I_V)^{p+q}(y) \\
                 &amp;= (T - \lambda I_V)^{p}(x)(T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y)(T - \lambda I_V)^{q}(y) \\
				&amp;= (T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y) \\
				&amp; = \bar{0}_V
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
Next, we need to show that \(K_{\lambda}\) is \(T\)-invariant. This means that we want to show that \(T(K_{\lambda}) \subseteq K_{\lambda}\). Therefore, Let \(x \in K_{\lambda}\). We want to show that \(T(x) \in K_{\lambda}\). Since \(x \in K_{\lambda}\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p}(x) = \bar{0}_V
\end{align*}
$$
</div>
<p>Apply the linear map \(T\) to both sides</p>
<div>
$$
\begin{align*}
T(T - \lambda I_V)^{p}(x) &amp;= T(\bar{0}_V)
\end{align*}
$$
</div>
<p>\(T\) and \((T - \lambda I_V)^{p}\) comute. Why? Taking \((T - \lambda I_V)\) to a power expands to some form of \(\lambda^k T^l\). So</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p}T(x) &amp;= \bar{0} \quad \text{(because $T(\bar{0}_V) = \bar{0}$ above)}
\end{align*}
$$
</div>
<p>So we see above that \(T(x)\) belongs to \(K_{\lambda}\) as we wanted to show.</p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
(b) Next, we want to prove that the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one. So we want to think of this map \(T - \mu I_V\) as map on \(K_{\lambda}\) but what is the right target? We know it maps to \(V\) but should we consider another target like \(K_{\lambda}\)? Let’s look at the image of this map when it acts on a vector in \(K_{\lambda}\)</p>
<div>
$$
\begin{align*}
(T - \mu I_V)(x) &amp;= T(x) - \mu x \\                       
\end{align*}
$$
</div>
<p>We know that \(K_{\lambda}\) is \(T\)-invariant so \(T(x) \in K_{\lambda}\). What about \(\mu x\)? This is just a multiply of \(x\) and since \(K_{\lambda}\) is a subspace then we know that \(\mu x \in K_{\lambda}\). Therefore the addition of the two terms is also in \(K_{\lambda}\) since \(K_{\lambda}\) is a subspace. So this tells us that the target we want to consider is \(K_{\lambda}\).
<br />
<br />
So now we want to prove that this map \(T - \mu I_V: K_{\lambda} \rightarrow K_{\lambda}\) is one to one. One way to show this is to prove that the nullspace of this map is trivial. This means that the solution to</p>
<div>
$$
\begin{align*}
(T - \mu I_V)(x) &amp;= \bar{0}_V \\                       
\end{align*}
$$
</div>
<p>is the trivial solution where \(x = \bar{0}_V\). Suppose for the sake of contradiction that this isn’t true and \(x \neq \bar{0}_V\) but \((T - \mu I_V)(x) = \bar{0}_V\). 
<br />
<br />
However, we know that \(x \in K_{\lambda}\) so it must be killed by some power of the operator so let \(p\) be the smallest integer such that</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^p(x) &amp;= \bar{0}_V                     
\end{align*}
$$
</div>
<p>But if we take the power just below \(p\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p-1}(x) = y \neq \bar{0}_V                    
\end{align*}
$$
</div>
<p>This implies that \(y\) is an eigenvector and \(y \in E_{\lambda}\). Observe next what happens when we apply the map \((T - \mu I_V)\) on the eigenvector \(y\)</p>
<div>
$$
\begin{align*}
(T - \mu I_V)(y) &amp;= (T - \mu I_V)(T - \lambda I_V)^{p-1}(x) \\
                &amp;= (T - \lambda I_V)^{p-1}(T - \mu I_V)(x) \quad \text{($T$ and $(T-\mu I)$ and $(T - \lambda I)^{p-1}$ commute)}\\ 
				&amp;= \bar{0}_V             
\end{align*}
$$
</div>
<p>So we’ve shown that \(y\) is an eigenvector for an eigenvalue \(\mu\). So \(y \neq \bar{0}_V\) and \(y \in E_{\mu}\). But we also see that \(y \in E_{\lambda}\). However \(\lambda \neq \mu\). So \(y \in E_{\mu} \cap E_{\lambda}\). So this is a contradiction. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Generalized Eigenvectors</b></h4>
<p>So to remind ourselves, the goal of this whole process is to find a basis consisting of generalized eigenvectors. The next theorem makes it practically easier to find them.
<br /></p>
<div class="purdiv">
Theorem 1.2
</div>
<div class="purbdiv">
If \(\lambda\) has algebraic multiplicity \(m\), then the generalized eigenspace
$$
\begin{align*}
K_{\lambda} = N((T - \lambda I_V)^m)          
\end{align*}
$$
</div>
<p><br />
This makes finding a basis for \(K_{\lambda}\) simple because it’s just a matter of finding the nullspace like we did before by putting the matrix in echelon form. What’s next? We want these generalized eigenvectors to span \(V\) since we want a basis. The following theorem confirms it.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.3
</div>
<div class="purbdiv">
Let \(\lambda_1, ..., \lambda_k\) be the distinct eigenvalues of \(T:V \rightarrow V\). For any \(x \in V\), there are \(v_j \in K_{\lambda_j}\) such that 
$$
\begin{align*}
x = v_1 + ... + v_k        
\end{align*}
$$
In other words, \(\text{span}(K_{\lambda_1} \cup ... \cup K_{\lambda_k}) = V\)
</div>
<p><br />
<!------------------------------------------------------------------------------------>
The next thing that we need is obviously knowing that these generalized eigenvectors are linearly independent. Once we get that we can construct the basis that we want.
<br /></p>
<div class="purdiv">
Theorem 1.4
</div>
<div class="purbdiv">
Let \(\beta_j\) be a basis for \(K_{\lambda_j}\). Then
<ol type="a">
	<li>\(\beta_i \cap \beta_j = \emptyset\) for \(i \neq j\) </li>
	<li>\(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis for \(V\) </li>
	<li>\(\dim(K_{\lambda_j}) = \) algebraic multiplicity of \(\lambda_j \) </li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
<br />
(a): Assume for the sake of contradiction that \(\beta_i \cap \beta_j \neq \emptyset\). Then there exists \(x \in \beta_i \cap \beta_j\). We know that \(\beta_i \cap \beta_j \subseteq K_{\lambda_i} \cap K_{\lambda_j}\). Since \(i \neq j\), then \(\lambda_i \neq \lambda_j\). Therefore by Theorem 1.1(b), the restriction of \((T - \lambda_i I_V)\) to \(K_{\lambda_j}\) is 1-1.</p>
<div>
$$
\begin{align*}
&amp;\implies (T - \lambda_i I_V)\Big|_{K_{\lambda_j}} \quad \text{ is one-to-one}    \\
\end{align*}
$$
</div>
<p>Since it’s one-to-one, then its nullspace is just the zero vector. This also means no other non-zero vector will make \((T - \lambda_i I_V)(w)\) zero for any \(w \in  K_{\lambda_j}\). So pick \(x \in K_{\lambda_i} \cap K_{\lambda_j}\). Then</p>
<div>
$$
\begin{align*}
&amp;\implies  (T - \lambda_i I_V)(x) \neq \bar{0}_V \\ 
&amp;\implies  (T - \lambda_i I_V)^2(x) \neq \bar{0}_V \\ 
&amp;\implies  (T - \lambda_i I_V)^p(x) \neq \bar{0}_V \quad \text{ for any $p &gt; 0$}
\end{align*}
$$
</div>
<p>So we won’t get a zero no matter since the map is 1-1. But \(x\) is also in \(K_{\lambda_i}\). This implies that \(x \not\in K_{\lambda_i}\). This is a contradiction. So the intersection is empty as we wanted to show.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we proved that If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\). This therefore lead to the conclusion that \(T\) is diagonalizable. A Test For Diagonalizability We studied previously a few ways a linear operator can be tested for diagonalizability. From lecture 25, Theorem (5.8(a)) \(T\) is diagonalizable if and only if The characteristic polynomial splits over its field \(\mathbf{R}\) or \(\mathbf{C}\) For each eigenvalue of \(T\), its geometric multiplicity (\(\dim(E_{\lambda})\)) = algebraic multiplicity We mentioned last time too that if \(V\) is over \(\mathbf{C}\), the the characteristic polynomial always splits. Note also that you can have \((a)\) but not \((b)\). For example $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix} \end{align*} $$ splits but doesn’t satisfy \((b)\) Jordan Canonical Form Based on the previous observation. It turns out there is a nice form that we can put \(T\) into in order to achieve \((a)\) and \((b)\) Theorem (JCF) Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form. But what is a Jordan Canonical form? We first define a Jordan Block as follows Definition A Jordan Block is a square matrix of the form $$ \begin{align*} \begin{pmatrix} \lambda \end{pmatrix} \quad \text{ or } \quad \begin{pmatrix} \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda \end{pmatrix} \end{align*} $$ So they’re almost diagonal but not really. For example, for a \(2 \times 2\) and \(3 \times 3\) matrices, a Jordan block looks like $$ \begin{align*} \begin{pmatrix} \lambda &amp; 1 \\ 0 &amp; \lambda \end{pmatrix} , \begin{pmatrix} \lambda &amp; 1 &amp; 0 \\ 0 &amp; \lambda &amp; 1 \\ 0 &amp; 0 &amp; \lambda \end{pmatrix} \end{align*} $$ Using these Jordan blocks, we can now define what the Jordan Canonical form is Definition A square matrix \(A\) is in Jordan Canonical Form if $$ \begin{align*} A = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_{l} \end{pmatrix}, \ \text{ where $A_j$ is a Jordan block} \end{align*} $$ You can think of this matrix as more of a generalization of a diagonal matrix. Examples The following are examples of matrix in Jordan Canonical Form $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ Note here that the characteristic polynomial of both \(A\) and \(B\) is \((1-t)^3(2-t)^2\). Computing Powers of Matrices in JFC It turns out that we can write a formula for powers of matrices in JFC. It is not as easy as taking the power of a diagonal matrix but at least have a formula. Fact 1: $$ \begin{align*} A = \begin{pmatrix} \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda \end{pmatrix} \implies \begin{pmatrix} \lambda^k &amp; k\lambda^{k-1} &amp; \cdots &amp; \frac{k(k-1)...(k-n+2)}{n!}\lambda^{k-n+1} \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; k\lambda^{k-1} \\ 0 &amp; \cdots &amp; 0 &amp; \lambda^k \end{pmatrix} \end{align*} $$ Note here that the third entry in the first row for example is \(\frac{k(k-1)}{2!}\lambda^{k-1}\) Fact 2: If \(A\) is in Jordan Canonical form, then the power of the matrix is as follows $$ \begin{align*} A = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_l \end{pmatrix}^k \implies \begin{pmatrix} A_1^k &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2^k &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_l^k \end{pmatrix} \end{align*} $$ Proof of JCF Theorem So now we see that matrices in JCF are useful and we have enough motivation to prove the theorem above which again states that if the characteristic polynomial splits, then there is a basis such that \([T]^{\beta}_{\beta}\) is in JFC! To prove it, let \(\beta = \{v_1, ..., v_n\}\) of \(V\). We this basis to be such that $$ \begin{align*} [T]^{\beta}_{\beta} = \begin{pmatrix} [T(v)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta} \end{pmatrix} = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; A_k \end{pmatrix} \end{align*} $$ These vectors \(v_1,...,v_j\) are not necessarily eigenvectors. What are they? Let’s focus on \(A_1\) above and let \(A_1\) be of size \(n_1 \times n_1\). $$ \begin{align*} A_1 = \begin{pmatrix} \lambda_1 &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_1 &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_1 \end{pmatrix} \end{align*} $$ Since \(A_1\) has the form above (Jordan block), then we know at least for \(\lambda_1\), \(T(v_1) = \lambda_1 v_1\). What about \(T(v_2)\)? We see that column 2 has 1 in the first row and then \(\lambda_2\) in the second row. So \(T(v_2) = v_1 + \lambda_2 v_2\). Therefore, \(v_2\) is not an eigenvector but we can observe that $$ \begin{align*} T(v_2) &amp;= v_1 + \lambda_1 v_2 \\ T(v_2) - \lambda_1 v_2 &amp;= v_1 \\ (T - \lambda_1 I_V)(v_2) &amp;= v_1 \end{align*} $$ So at this point, we see that \(v_2\) is not an eigenvector but if we apply the map \((T - \lambda_1 I_V)\) on it, it becomes an eigenvector. What if we apply this map twice? $$ \begin{align*} (T - \lambda_1 I_V)^2(v_2) &amp;= \bar{0}_V \quad \text{ (because $v_1$ is an eigenvector)} \end{align*} $$ What about the remaining vectors? $$ \begin{align*} T(v_3) &amp;= v_2 + \lambda_1 v_3 \\ (T - \lambda_1 I_V)(v_3) &amp;= v_2 \\ (T - \lambda_1 I_V)^3(v_3) &amp;= (T - \lambda_1 I_V)^2(v_2) \\ (T - \lambda_1 I_V)^3(v_2) &amp;= \bar{0}_V \end{align*} $$ So they’re not eigenvectors but they satisfy these equations. Based on this observation, we’re going to define the following Definition \(x \in V\) is a generalized eigenvector of \(T: V \rightarrow V\) corresponding to \(\lambda\) if $$ \begin{align*} (T - \lambda I_V)^p(x) = \bar{0}_V \end{align*} $$ for some integer \(p &gt; 0\) Observation: When \(p\) is the smallest integer for which \((T - \lambda I_V)^p(x) = \bar{0}_V\), then \(y = (T - \lambda I_V)^{p-1}(x)\) is an eigenvector. So now we know that the basis we want to build will consists of generalized eigenvectors. These generalized eigenvectors belongs to subspaces we define next Definition Let \(\lambda\) be an eigenvalue of \(T: V \rightarrow V\). The generalized eigenspace of \(\lambda\) is $$ \begin{align*} K_{\lambda} = \{x \in V : (T - \lambda I_V)^p(x) = \bar{0}_V \text{ for some } p &gt; 0 \} \end{align*} $$ Theorem 1.1 \(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\). For \(\mu \neq \lambda\), the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one. Proof (a) We need to show three things. \(K_{\lambda}\) contains \(E_{\lambda}\). This is clearly true by definition and \(E_{\lambda} \subseteq K_{\lambda}\). Next we need to show that \(K_{\lambda}\) is a subspace. This means we need to show that it contains the zero vector and it is closed under scalar multiplication and addition. \((T - \lambda I_V)(\bar{0}_V) = \bar{0}_V\) so \(\bar{0}_V \in K_{\lambda}\). Now consider \(x, y \in K_{\lambda}\) and \(c \in \mathbf{F}\), we need to show that \(x + cy \in K_{\lambda}\). Since \(x\) and \(y\) are in \(K_{\lambda}\), then $$ \begin{align*} (T - \lambda I_V)(x) &amp;= \bar{0}_V \text{ for } p &gt; 0 \\ (T - \lambda I_V)(y) &amp;= \bar{0}_V \text { for } q &gt; 0 \end{align*} $$ Therefore $$ \begin{align*} (T - \lambda I_V)^{p+q}(x + cy) &amp;= (T - \lambda I_V)^{p+q}(x) + c(T - \lambda I_V)^{p+q}(y) \\ &amp;= (T - \lambda I_V)^{p}(x)(T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y)(T - \lambda I_V)^{q}(y) \\ &amp;= (T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y) \\ &amp; = \bar{0}_V \end{align*} $$ Next, we need to show that \(K_{\lambda}\) is \(T\)-invariant. This means that we want to show that \(T(K_{\lambda}) \subseteq K_{\lambda}\). Therefore, Let \(x \in K_{\lambda}\). We want to show that \(T(x) \in K_{\lambda}\). Since \(x \in K_{\lambda}\), then $$ \begin{align*} (T - \lambda I_V)^{p}(x) = \bar{0}_V \end{align*} $$ Apply the linear map \(T\) to both sides $$ \begin{align*} T(T - \lambda I_V)^{p}(x) &amp;= T(\bar{0}_V) \end{align*} $$ \(T\) and \((T - \lambda I_V)^{p}\) comute. Why? Taking \((T - \lambda I_V)\) to a power expands to some form of \(\lambda^k T^l\). So $$ \begin{align*} (T - \lambda I_V)^{p}T(x) &amp;= \bar{0} \quad \text{(because $T(\bar{0}_V) = \bar{0}$ above)} \end{align*} $$ So we see above that \(T(x)\) belongs to \(K_{\lambda}\) as we wanted to show. (b) Next, we want to prove that the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one. So we want to think of this map \(T - \mu I_V\) as map on \(K_{\lambda}\) but what is the right target? We know it maps to \(V\) but should we consider another target like \(K_{\lambda}\)? Let’s look at the image of this map when it acts on a vector in \(K_{\lambda}\) $$ \begin{align*} (T - \mu I_V)(x) &amp;= T(x) - \mu x \\ \end{align*} $$ We know that \(K_{\lambda}\) is \(T\)-invariant so \(T(x) \in K_{\lambda}\). What about \(\mu x\)? This is just a multiply of \(x\) and since \(K_{\lambda}\) is a subspace then we know that \(\mu x \in K_{\lambda}\). Therefore the addition of the two terms is also in \(K_{\lambda}\) since \(K_{\lambda}\) is a subspace. So this tells us that the target we want to consider is \(K_{\lambda}\). So now we want to prove that this map \(T - \mu I_V: K_{\lambda} \rightarrow K_{\lambda}\) is one to one. One way to show this is to prove that the nullspace of this map is trivial. This means that the solution to $$ \begin{align*} (T - \mu I_V)(x) &amp;= \bar{0}_V \\ \end{align*} $$ is the trivial solution where \(x = \bar{0}_V\). Suppose for the sake of contradiction that this isn’t true and \(x \neq \bar{0}_V\) but \((T - \mu I_V)(x) = \bar{0}_V\). However, we know that \(x \in K_{\lambda}\) so it must be killed by some power of the operator so let \(p\) be the smallest integer such that $$ \begin{align*} (T - \lambda I_V)^p(x) &amp;= \bar{0}_V \end{align*} $$ But if we take the power just below \(p\), then $$ \begin{align*} (T - \lambda I_V)^{p-1}(x) = y \neq \bar{0}_V \end{align*} $$ This implies that \(y\) is an eigenvector and \(y \in E_{\lambda}\). Observe next what happens when we apply the map \((T - \mu I_V)\) on the eigenvector \(y\) $$ \begin{align*} (T - \mu I_V)(y) &amp;= (T - \mu I_V)(T - \lambda I_V)^{p-1}(x) \\ &amp;= (T - \lambda I_V)^{p-1}(T - \mu I_V)(x) \quad \text{($T$ and $(T-\mu I)$ and $(T - \lambda I)^{p-1}$ commute)}\\ &amp;= \bar{0}_V \end{align*} $$ So we’ve shown that \(y\) is an eigenvector for an eigenvalue \(\mu\). So \(y \neq \bar{0}_V\) and \(y \in E_{\mu}\). But we also see that \(y \in E_{\lambda}\). However \(\lambda \neq \mu\). So \(y \in E_{\mu} \cap E_{\lambda}\). So this is a contradiction. Finding the Generalized Eigenvectors So to remind ourselves, the goal of this whole process is to find a basis consisting of generalized eigenvectors. The next theorem makes it practically easier to find them. Theorem 1.2 If \(\lambda\) has algebraic multiplicity \(m\), then the generalized eigenspace $$ \begin{align*} K_{\lambda} = N((T - \lambda I_V)^m) \end{align*} $$ This makes finding a basis for \(K_{\lambda}\) simple because it’s just a matter of finding the nullspace like we did before by putting the matrix in echelon form. What’s next? We want these generalized eigenvectors to span \(V\) since we want a basis. The following theorem confirms it. Theorem 1.3 Let \(\lambda_1, ..., \lambda_k\) be the distinct eigenvalues of \(T:V \rightarrow V\). For any \(x \in V\), there are \(v_j \in K_{\lambda_j}\) such that $$ \begin{align*} x = v_1 + ... + v_k \end{align*} $$ In other words, \(\text{span}(K_{\lambda_1} \cup ... \cup K_{\lambda_k}) = V\) The next thing that we need is obviously knowing that these generalized eigenvectors are linearly independent. Once we get that we can construct the basis that we want. Theorem 1.4 Let \(\beta_j\) be a basis for \(K_{\lambda_j}\). Then \(\beta_i \cap \beta_j = \emptyset\) for \(i \neq j\) \(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis for \(V\) \(\dim(K_{\lambda_j}) = \) algebraic multiplicity of \(\lambda_j \) Proof (a): Assume for the sake of contradiction that \(\beta_i \cap \beta_j \neq \emptyset\). Then there exists \(x \in \beta_i \cap \beta_j\). We know that \(\beta_i \cap \beta_j \subseteq K_{\lambda_i} \cap K_{\lambda_j}\). Since \(i \neq j\), then \(\lambda_i \neq \lambda_j\). Therefore by Theorem 1.1(b), the restriction of \((T - \lambda_i I_V)\) to \(K_{\lambda_j}\) is 1-1. $$ \begin{align*} &amp;\implies (T - \lambda_i I_V)\Big|_{K_{\lambda_j}} \quad \text{ is one-to-one} \\ \end{align*} $$ Since it’s one-to-one, then its nullspace is just the zero vector. This also means no other non-zero vector will make \((T - \lambda_i I_V)(w)\) zero for any \(w \in K_{\lambda_j}\). So pick \(x \in K_{\lambda_i} \cap K_{\lambda_j}\). Then $$ \begin{align*} &amp;\implies (T - \lambda_i I_V)(x) \neq \bar{0}_V \\ &amp;\implies (T - \lambda_i I_V)^2(x) \neq \bar{0}_V \\ &amp;\implies (T - \lambda_i I_V)^p(x) \neq \bar{0}_V \quad \text{ for any $p &gt; 0$} \end{align*} $$ So we won’t get a zero no matter since the map is 1-1. But \(x\) is also in \(K_{\lambda_i}\). This implies that \(x \not\in K_{\lambda_i}\). This is a contradiction. So the intersection is empty as we wanted to show. References Math416 by Ely Kerman]]></summary></entry></feed>