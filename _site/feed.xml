<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-03T15:50:24-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 13: More Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 13: More Linear Transformations" /><published>2024-08-05T01:01:36-07:00</published><updated>2024-08-05T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html"><![CDATA[<p>Suppose we have \(A \in M_{m \times n}\) where \(A\) represents a linear map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\). We claim that if \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p>Proof: Let \(\beta = \{e_1,...,e_n\}\). where 
\(e_1 = \begin{pmatrix}
1 \\
0 \\
. \\
. \\
. \\
0 \\
\end{pmatrix}.\)
<br />
We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them  with respect to the basis \(\gamma\),</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma})
\end{align*}
$$
</div>
<p>But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1\) is just the first column of \(A\) since we only have 1 in the first coordinate of \(e_1\). Finally, writing the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) but that’s just the first column of \(A\) since it’s the standard basis. We use can use the same argument on the remaining columns to get</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p>So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformation as a Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases.<br />
For all \(v \in V\),
$$
\begin{align*}
[T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta}
\end{align*}
$$
\(T\) acts like matrix multiplication.
</div>
<p><br />
Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear.</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\
                &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\
				&amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\
             &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\
			 &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>The expression above is exactly matrix multiplication so we can re-write this as</p>
<div>
$$
\begin{align*}
a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
&amp;=
[T(v_1)]_{\gamma} + ... + [T(v_n)]_{\gamma} 
\begin{pmatrix}
a_1 \\
. \\
. \\
. \\
a_n \\
\end{pmatrix} \\
&amp;= [T]_{\beta}^{\gamma} [v]_{\beta}.
\end{align*}
$$
</div>
<p>This is exactly what we wanted to show.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The vector space of linear transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given vector spaces \(V, W\) define
$$
\begin{align*}
\mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}.
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Suppose we have \(A \in M_{m \times n}\) where \(A\) represents a linear map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\). We claim that if \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ Proof: Let \(\beta = \{e_1,...,e_n\}\). where \(e_1 = \begin{pmatrix} 1 \\ 0 \\ . \\ . \\ . \\ 0 \\ \end{pmatrix}.\) We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them with respect to the basis \(\gamma\), $$ \begin{align*} [L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma}) \end{align*} $$ But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1\) is just the first column of \(A\) since we only have 1 in the first coordinate of \(e_1\). Finally, writing the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) but that’s just the first column of \(A\) since it’s the standard basis. We use can use the same argument on the remaining columns to get $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). Linear Transformation as a Matrix Multiplication Theorem Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases. For all \(v \in V\), $$ \begin{align*} [T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta} \end{align*} $$ \(T\) acts like matrix multiplication. Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear. $$ \begin{align*} [T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\ &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\ &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \end{align*} $$ But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in $$ \begin{align*} [T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\ &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\ &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} \end{align*} $$ The expression above is exactly matrix multiplication so we can re-write this as $$ \begin{align*} a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} &amp;= [T(v_1)]_{\gamma} + ... + [T(v_n)]_{\gamma} \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \\ \end{pmatrix} \\ &amp;= [T]_{\beta}^{\gamma} [v]_{\beta}. \end{align*} $$ This is exactly what we wanted to show. The vector space of linear transformations Definition Given vector spaces \(V, W\) define $$ \begin{align*} \mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}. \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Section 1.6: Exercise 11</title><link href="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 11" /><published>2024-08-04T01:01:36-07:00</published><updated>2024-08-04T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html"><![CDATA[<div class="ydiv">
1.6 Exercise 11
</div>
<div class="ybdiv">
Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\).
</div>
<p><br />
Proof: 
<br />
To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	a_1(u+v) + a_2(au) = \bar{0}
	\end{align*}
	$$
</div>
<p>is only satisfied by the trivial solution. We’ll re-arrange the terms as follows,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1u + a_1v + aa_2u  \\
	&amp;= (a_1 + aa_2)u + a_1v \\
	&amp;= (a_1 + aa_2)u + a_1v.
	\end{align*}
	$$
</div>
<p>From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis.
<br />
<br />
Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1(au) + a_2(bv) \\
	\bar{0} &amp;= (a_1a)u + (a_2b)v.
	\end{align*}
	$$
</div>
<p>This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.6 Exercise 11 Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\). Proof: To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation $$ \begin{align*} a_1(u+v) + a_2(au) = \bar{0} \end{align*} $$ is only satisfied by the trivial solution. We’ll re-arrange the terms as follows, $$ \begin{align*} \bar{0} &amp;= a_1u + a_1v + aa_2u \\ &amp;= (a_1 + aa_2)u + a_1v \\ &amp;= (a_1 + aa_2)u + a_1v. \end{align*} $$ From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis. Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms, $$ \begin{align*} \bar{0} &amp;= a_1(au) + a_2(bv) \\ \bar{0} &amp;= (a_1a)u + (a_2b)v. \end{align*} $$ This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 20" /><published>2024-08-03T01:01:36-07:00</published><updated>2024-08-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html"><![CDATA[<p>Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case?</p>
<div class="ydiv">
1.6 Exercise 20
</div>
<div class="ybdiv">
Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\).
<ol style="list-style-type:lower-alpha">
	<li>Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.)</li>
	<li>Prove \(S\) contains at least \(n\) vectors.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Similar to the proof of <a href="http://127.0.0.1:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html">theorem 1.9</a>, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\)</li>
	
	<li>If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains 
	at least \(n\) vectors by <a href="http://127.0.0.1:4000/jekyll/update/2024/08/02/1-6-corollary-2.html">Corollary 2</a>
	 from theorem 1.9. \(\blacksquare\)</li>
</ol>
<p><br />
The book provided the solution <a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_1_6.html">here</a>.
though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case? 1.6 Exercise 20 Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\). Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.) Prove \(S\) contains at least \(n\) vectors. Proof: Similar to the proof of theorem 1.9, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\) If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains at least \(n\) vectors by Corollary 2 from theorem 1.9. \(\blacksquare\) The book provided the solution here. though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Corollary 2</title><link href="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html" rel="alternate" type="text/html" title="Section 1.6: Corollary 2" /><published>2024-08-02T01:01:36-07:00</published><updated>2024-08-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/02/1-6-corollary-2.html"><![CDATA[<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
	Let \(V\) be a vector space with dimension \(n\).
<ol style="list-style-type:lower-alpha">
	<li>Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\)</li>
	<li>Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\).</li>
	<li>Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\).</li>
</ol>
</div>
<p><br />
Proof: 
<br />
<br />
\((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\)
<br />
<br />
\(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Corollary 2 Let \(V\) be a vector space with dimension \(n\). Any finite generating set for \(V\) contains at least \(n\) vectors, and a generating set for \(V\) that contains exactly \(n\) vectors is a basis for \(V\) Any linearly independent subset of \(V\) that contains exactly \(n\) vectors is a basis for \(V\). Every linearly independent subset of \(V\) can be extended to a basis for \(V\), that is, if \(L\) is a linearly independent subset of \(V\), then there is a basis \(\beta\) of \(V\) such that \(L \subseteq \beta\). Proof: \((a)\): Let \(G\) be a finite generating set for \(V\). We know by theorem 1.9 that there exists some subset \(\beta\) that is a basis for \(V\). Furthermore, corollary 1 implies that every basis for \(V\) contains the same number of vectors and is by definition the dimension of \(V\) so \(\beta\) has \(n\) elements. Since \(\beta \subseteq V\), then \(V\) must have at least \(n\) elements. For the second part where \(G\) contains \(n\) vectors. In this case we must have \(\beta = G\) and so \(G\) is a basis for \(V\). \(\blacksquare\) \(2\): We know \(\beta\) is a basis for \(V\) that generates it. Suppose \(L\) is a linearly independent set of \(V\) with \(n\) elements. By the replacement theorem, there exists a subset \(H \subseteq \beta\) containing exactly \(n-n = 0\) vectors such that \(\beta \cup H\) is a basis for \(V\). Since \(H = \emptyset\), then \(L\) generates \(V\). Moreover, we know \(L\) is linearly independent so \(L\) is a basis for \(V\). \(\blacksquare\) \(3\): Also by the replacement theorem, if \(L\) is a linearly independent subset of \(V\) containing \(m\) vectors, then there exists a subset \(H\) of \(\beta\) containing exactly \(n-m\) vectors such that \(L \cup H\) generates \(V\). Furthermore, \(L \cup H\) contains at most \(n\) vectors. By (a), \(L \cup H\) contains exactly \(n\) vectors and is a basis for \(V\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html" rel="alternate" type="text/html" title="Section 1.5: Exercise 21" /><published>2024-08-01T01:01:36-07:00</published><updated>2024-08-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/01/1-5-ex-21.html"><![CDATA[<div class="ydiv">
1.5 Exercise 21
</div>
<div class="ybdiv">
Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). 
</div>
<p><br />
Proof:
<br />
\(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}.
	\end{align*}
	$$
</div>
<p>Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\
	u &amp;= w \\
	\end{align*}
	$$
</div>
<p>We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\)
<br />
<br />
\(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\),</p>
<div>
	$$
	\begin{align*}
	v = a_1u_1 + a_2u_2 + ... + a_nu_n.
	\end{align*}
	$$
</div>
<p>And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows,</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Substituting for \(v\) in the previous equation,</p>
<div>
	$$
	\begin{align*}
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\
	&amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}.
	\end{align*}
	$$
</div>
<p>Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.5 Exercise 21 Let \(S_1\) and \(S_2\) be disjoint linearly independent subsets of \(V\). Prove that \(S_1 \cup S_2\) is linearly dependent if and only if \(span(S_1) \cap span(S_2) \neq \{0\}\). Proof: \(\Rightarrow\): Since \(S_1 \cup S_2\) is linearly dependent, then there exists vectors \(u_1,u_2,...,u_n\) in \(S_1\), the vectors \(w_1,w_2,...,w_m\) in \(S_2\) and not all zero scalars \(a_1,...a_n,b_1,...,b_m\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n + b_1w_1 + b_2w_2 + ... + b_nw_m &amp;= \bar{0}. \end{align*} $$ Let \(u = a_1u_1 + a_2u_2 + ... + a_nu_n\) and let \(w = - (b_1w_1 + b_2w_2 + ... + b_nw_m)\). Re-arrange the terms above so that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n &amp;= - (b_1w_1 + b_2w_2 + ... + b_nw_m) \\ u &amp;= w \\ \end{align*} $$ We know \(u\) is a linear combination of the elements in \(S_1\) and therefore, \(u \in span(S_1)\). Moreover, \(w\) is a linear combination of the elements in \(S_2\) and therefore, \(w \in span(S_2)\). We also know that \(v = -w \neq \bar{0}\). This is because at least one scalar was non-zero by the definition of linear dependence. Therefore, \(v\) is a non-zero vector in both \(span(S_1)\) and \(span(S_2)\) and so \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\) \(\Leftarrow\): Suppose now that \(span(S_1) \cap span(S_2) \neq \{\bar{0}\}\). Let \(v\) be a non-zero vector in both \(span(S_1)\) and \(span(S_2)\). This means we can write \(v\) a linear combination of the elements of \(S_1\), $$ \begin{align*} v = a_1u_1 + a_2u_2 + ... + a_nu_n. \end{align*} $$ And we can also write \(v\) as a linear combination of the elements in \(S_2\) as follows, $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Substituting for \(v\) in the previous equation, $$ \begin{align*} &amp;a_1u_1 + a_2u_2 + ... + a_nu_n = b_1w_1 + b_2w_2 + ... + b_nw_m. \\ &amp;a_1u_1 + a_2u_2 + ... + a_nu_n - (b_1w_1 + b_2w_2 + ... + b_nw_m) = \bar{0}. \end{align*} $$ Since \(v\) is non-zero and both \(S_1\) and \(S_2\) are linearly independent, then all of these scalars are non-zero and so this means that \(S_1 \ cup S_2\) is linearly dependent by the definition of linear dependence. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.5: Theorem 1.7</title><link href="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html" rel="alternate" type="text/html" title="Section 1.5: Theorem 1.7" /><published>2024-07-31T01:01:36-07:00</published><updated>2024-07-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/31/1-6-theorem-1.7.html"><![CDATA[<p>This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture.</p>
<div class="purdiv">
Theorem 1.7
</div>
<div class="purbdiv">
If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\).
</div>
<p><br />
Proof: 
<br />
<br />
\(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}.
	\end{align*}
	$$
</div>
<p>But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\
	v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\
	v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\
	\end{align*}
	$$
</div>
<p>This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\).
<br />
<br />
\(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that</p>
<div>
	$$
	\begin{align*}
	v = b_1w_1 + b_2w_2 + ... + b_nw_m.
	\end{align*}
	$$
</div>
<p>Moving \(v\) to the other side, we see that,</p>
<div>
	$$
	\begin{align*}
	 b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}.
	\end{align*}
	$$
</div>
<p>We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.7 from the book which I don’t think we covered in lecture. Theorem 1.7 If \(S\) is a linearly independent subset of \(V\) and \(v\) is a vector in \(V\) but not \(S\). Then \(S \cup \{v\}\) is linearly dependent only if \(v \in span(S)\). Proof: \(\Rightarrow\): Since \(S \cup \{v\}\) is linearly dependent, then there are vectors \(u_1,u_2,...,u_n\) in \(S \cup \{v\}\) and not all zero scalars \(a_1,...,a_n\) such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_nu_n = \bar{0}. \end{align*} $$ But we know that \(S\) is linearly independent so this means one of the vectors in \(\{u_1,u_2,...,u_n\}\) must be \(v\). without the loss of generality let \(u_1 = v\) and so $$ \begin{align*} \bar{0} &amp;= a_1v + a_2u_2 + ... + a_nu_n \\ v &amp;= \frac{1}{a_1}(-a_2u_2 - ... - a_nu_n) \\ v &amp;= -\frac{a_2}{a_1}u_2 - ... - \frac{a_n}{a_1}u_n. \\ \end{align*} $$ This tells us that \(v\) is a linear combination of the elements of \(u_2,...,u_n\) in \(S\). Therefore, by definition, \(v \in span(S)\). \(\Leftarrow\): Let \(v \in span(S)\), then there exists vectors \(w_1,w_2,...,w_m\) in \(S\) and scalars \(b_1,...,b_n\) such that $$ \begin{align*} v = b_1w_1 + b_2w_2 + ... + b_nw_m. \end{align*} $$ Moving \(v\) to the other side, we see that, $$ \begin{align*} b_1w_1 + b_2w_2 + ... + b_nw_m + (-1).v = \bar{0}. \end{align*} $$ We know here that \(w\) is not in \(S\) and so none of the vectors \(w_1,...,w_m\) are equal to \(v\). Moreover, the coefficient of \(v\) is none zero. Therefore, by the definition of linear dependence, \(S \cup \{v\}\) is linearly dependent. \(\blacksquare\). References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Theorem 1.9</title><link href="http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html" rel="alternate" type="text/html" title="Section 1.6: Theorem 1.9" /><published>2024-07-30T01:01:36-07:00</published><updated>2024-07-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html"><![CDATA[<p>This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet.</p>
<div class="purdiv">
Theorem 1.9
</div>
<div class="purbdiv">
If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis.
</div>
<p><br />
Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases</p>
<ol>
	<li>\(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done.</li>
	<li>\(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) </li>
</ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet. Theorem 1.9 If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis. Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases \(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done. \(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 12: Linear Transformations Continued</title><link href="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 12: Linear Transformations Continued" /><published>2024-07-29T01:01:36-07:00</published><updated>2024-07-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is onto if \(R(T) = W\) or
	$$
	\begin{align*}
	 \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is 1-1 if
	$$
	\begin{align*}
	 T(v_1) = T(v_2) \Rightarrow v_1 = v_2.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent 
<ul>
	<li> \(T\) is 1-1. </li>
	<li> \(T\) is onto. </li>
	<li> \(rank(T) = \dim(V)\). </li>
</ul>
</div>
<p><br />
Note here that \(rank(T) = \dim(R(T))\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Recall \(A \in M_{m \times n}\) defines a linear map</p>
<div>
	$$
	\begin{align*}
	L_A:\mathbf{R}^n \rightarrow \mathbf{R}^m, \ \bar{x} \rightarrow A\bar{x} 
	\end{align*}
	$$
</div>
<p>We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example \(T_d: P_3 \rightarrow P_2, \ f \rightarrow f'\) and \(T_i: P_2 \rightarrow P_3, \ f \rightarrow \int_0^x f(t)dt\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Coordinate Expression for a vector</b></h4>
<p>Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form,</p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n
	\end{align*}
	$$
</div>
<p>The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. 
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [v]_{\beta} = 
\begin{pmatrix}
a_1 \\
.  \\
. \\
. \\
a_n
\end{pmatrix}
\in \mathbf{R}^n
	\end{align*}
	$$
is the coordinate expression for \(v\) with respect to \(\beta\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\).
<br /></p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta} &amp;= 
	 \begin{pmatrix}
	 2 \\
	 1 \\
	 \end{pmatrix} \\
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis,</p>
<div>
	$$
	\begin{align*}
	 v = (2,1) = a_1(1,1) + a_2(1,-1).
\end{align*}
	 $$
</div>
<p>From this, we get</p>
<div>
	$$
	\begin{align*}
	 a_1 + a_2 &amp;= 2 \\
	 a_1 - a_2 &amp;= 1 \\
\end{align*}
	 $$
</div>
<p>Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are</p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix} \\
	 &amp;= 
	 \begin{pmatrix}
	 \frac{3}{2} \\
	 \frac{1}{2} \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>So we can think of \([\quad]_{\beta'}\) as a map:</p>
<div>
	$$
	\begin{align*}
	 [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance”
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional.
We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix.
<br />
<br />
Let \(\beta\) be the basis for \(V\) where</p>
<div>
	$$
	\begin{align*}
	\beta = \{v_1, ..., v_n\}
\end{align*}
$$
</div>
<p>And let \(\gamma\) be the basis for \(W\) where</p>
<div>
	$$
	\begin{align*}
	\gamma =  \{w_1, ..., w_n\}
\end{align*}
$$
</div>
<p>For \(v \in V\), we have</p>
<div>
	$$
	\begin{align*}
	T(v) = a_1w_1 + ... + a_mw_m.
\end{align*}
$$
</div>
<p>And for each \(v_j\) within \(v\) we have,</p>
<div>
 	$$
 	\begin{align*}
 	T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m.
 \end{align*}
 $$
 </div>
<p>From this, we now have this definition,
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [T]_{\beta}^{\gamma} = (a_{ij}) \text{ where } i=1,...,m, j=1,...,n
	\end{align*}
	$$
is the matrix representation of \(T\) with respect to the bases \(\beta,\gamma\).
</div>
<p><br />
Remark:</p>
<div>
	$$
\begin{align*}
	 [T]_{\beta}^{\gamma} = 
\begin{pmatrix}
| &amp; &amp; |\\ 
[T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\
| &amp; &amp; |
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For all \(v \in V\), \([T(v)]_{\gamma} = [T(v)]_{\beta}^{\gamma}[v]_{\beta}\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(T_d: P_3 \rightarrow P_2, f \rightarrow f'\). 
<br />
\(P_3\) has a basis</p>
<div>
	$$
\begin{align*}
\beta = \{1, x, x^2, x^3\}.
\end{align*}
$$
</div>
<p>and \(P_2\) has a basis,</p>
<div>
	$$
\begin{align*}
\gamma = \{1, x, x^2\}.
\end{align*}
$$
</div>
<p>We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so,</p>
<div>
	$$
\begin{align*}
T_d(1) &amp;= 0 \\
T_d(x) &amp;= x \\
T_d(x^2) &amp;= 2x \\
T_d(x^3) &amp;= 3x^2
\end{align*}
$$
</div>
<p>Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\).</p>
<div>
	$$
\begin{align*}
[T_d(1)]_{\gamma} &amp;=  [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x)]_{\gamma} &amp;=  [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
1 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x^2)]_{\gamma} &amp;=  [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
2 \\
0
\end{pmatrix} \\
[T_d(x^3)]_{\gamma} &amp;=  [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>So the final matrix is</p>
<div>
	$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>As an example, suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix</p>
<div>
	$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}
\begin{pmatrix}
5 \\ 
4 \\
0 \\
3
\end{pmatrix} = 
\begin{pmatrix}
4 \\
0 \\
9
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this.</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(T: V \rightarrow W\) is onto if \(R(T) = W\) or $$ \begin{align*} \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w. \end{align*} $$ Definition \(T: V \rightarrow W\) is 1-1 if $$ \begin{align*} T(v_1) = T(v_2) \Rightarrow v_1 = v_2. \end{align*} $$ Theorem If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\) Theorem Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent \(T\) is 1-1. \(T\) is onto. \(rank(T) = \dim(V)\). Note here that \(rank(T) = \dim(R(T))\). Matrix Representation of linear transformations Recall \(A \in M_{m \times n}\) defines a linear map $$ \begin{align*} L_A:\mathbf{R}^n \rightarrow \mathbf{R}^m, \ \bar{x} \rightarrow A\bar{x} \end{align*} $$ We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example \(T_d: P_3 \rightarrow P_2, \ f \rightarrow f'\) and \(T_i: P_2 \rightarrow P_3, \ f \rightarrow \int_0^x f(t)dt\). Coordinate Expression for a vector Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form, $$ \begin{align*} v = a_1v_1 + ... + a_nv_n \end{align*} $$ The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. Definition $$ \begin{align*} [v]_{\beta} = \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \end{pmatrix} \in \mathbf{R}^n \end{align*} $$ is the coordinate expression for \(v\) with respect to \(\beta\). Example Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\). $$ \begin{align*} [v]_{\beta} &amp;= \begin{pmatrix} 2 \\ 1 \\ \end{pmatrix} \\ [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \end{align*} $$ These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis, $$ \begin{align*} v = (2,1) = a_1(1,1) + a_2(1,-1). \end{align*} $$ From this, we get $$ \begin{align*} a_1 + a_2 &amp;= 2 \\ a_1 - a_2 &amp;= 1 \\ \end{align*} $$ Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are $$ \begin{align*} [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \\ &amp;= \begin{pmatrix} \frac{3}{2} \\ \frac{1}{2} \\ \end{pmatrix} \end{align*} $$ So we can think of \([\quad]_{\beta'}\) as a map: $$ \begin{align*} [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n \end{align*} $$ The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance” Matrix Representation of linear transformations Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional. We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta\) be the basis for \(V\) where $$ \begin{align*} \beta = \{v_1, ..., v_n\} \end{align*} $$ And let \(\gamma\) be the basis for \(W\) where $$ \begin{align*} \gamma = \{w_1, ..., w_n\} \end{align*} $$ For \(v \in V\), we have $$ \begin{align*} T(v) = a_1w_1 + ... + a_mw_m. \end{align*} $$ And for each \(v_j\) within \(v\) we have, $$ \begin{align*} T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m. \end{align*} $$ From this, we now have this definition, Definition $$ \begin{align*} [T]_{\beta}^{\gamma} = (a_{ij}) \text{ where } i=1,...,m, j=1,...,n \end{align*} $$ is the matrix representation of \(T\) with respect to the bases \(\beta,\gamma\). Remark: $$ \begin{align*} [T]_{\beta}^{\gamma} = \begin{pmatrix} | &amp; &amp; |\\ [T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\ | &amp; &amp; | \end{pmatrix} \end{align*} $$ Theorem For all \(v \in V\), \([T(v)]_{\gamma} = [T(v)]_{\beta}^{\gamma}[v]_{\beta}\) Example Let \(T_d: P_3 \rightarrow P_2, f \rightarrow f'\). \(P_3\) has a basis $$ \begin{align*} \beta = \{1, x, x^2, x^3\}. \end{align*} $$ and \(P_2\) has a basis, $$ \begin{align*} \gamma = \{1, x, x^2\}. \end{align*} $$ We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so, $$ \begin{align*} T_d(1) &amp;= 0 \\ T_d(x) &amp;= x \\ T_d(x^2) &amp;= 2x \\ T_d(x^3) &amp;= 3x^2 \end{align*} $$ Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\). $$ \begin{align*} [T_d(1)]_{\gamma} &amp;= [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix} \\ [T_d(x)]_{\gamma} &amp;= [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \\ [T_d(x^2)]_{\gamma} &amp;= [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = \begin{pmatrix} 0 \\ 2 \\ 0 \end{pmatrix} \\ [T_d(x^3)]_{\gamma} &amp;= [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = \begin{pmatrix} 0 \\ 0 \\ 3 \end{pmatrix}. \end{align*} $$ So the final matrix is $$ \begin{align*} [T_d]^{\gamma}_{\beta} &amp;= \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix}. \end{align*} $$ As an example, suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix} \begin{pmatrix} 5 \\ 4 \\ 0 \\ 3 \end{pmatrix} = \begin{pmatrix} 4 \\ 0 \\ 9 \end{pmatrix}. \end{align*} $$ This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this.]]></summary></entry><entry><title type="html">Lecture 11: Null Space, Range, and Dimension Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem.html" rel="alternate" type="text/html" title="Lecture 11: Null Space, Range, and Dimension Theorem" /><published>2024-07-28T01:01:36-07:00</published><updated>2024-07-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The null space (kernel) of \(T\) is 
	$$
	\begin{align*}
	 N(T) = \{v \in V \ | \ T(v) = \bar{0}_W\} \subset V
	\end{align*}
	$$
The range (image) of \(T\) is
	$$
	\begin{align*}
	 R(T) = \{T(v) \ | v \in V \} \subset W
	\end{align*}
	$$
</div>
<p><br />
Notes: So the null space is any vector that the transformation turns it into the zero vector. The range just like the definition is the set of the images of all the vectors in \(V\). If all the vectors have an image, then the range is equal to the codomain and that’s when the transformation is onto.
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>\(N(T^b_a) = \{\)functions in \(C^0(\mathbf{R})\) whos average over \([a,b]\) is \(0\}\). In other words, it’s the set of all functions whose definite integral is 0. 
<br />
<br />
\(R(T^b_a) = \mathbf{R}\). Here we want all functions whose definite integral is some constant/some real number. In other words, this will be the entire codomain (\(\mathbf{R}\)). We call this map \((T^b_a)\) onto.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \( \ T: V \rightarrow W\) is linear then \(N(T)\) is a subspace of \(V\) and \(R(T)\) is a subspace of \(W\).
</div>
<p><br />
Proof:
<br />
We need to verify the three subspaces properties. To prove that \(R(T)\) is a subspace:</p>
<ul>
<li>We need to prove that \(\bar{0}_W \in R(T)\). This means that we need a \(v \in V\) such that \(T(v) = \bar{0}_W\). Since \(T\) is linear then \(T(\bar{0}_V) = \bar{0}_W\). (We proved this in the previous lecture) </li>
<li>We need \(R(T)\) to be closed under addition. Suppose \(w_1, w_2 \in R(T)\), then \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1\) and \(v_2\). Therefore,
	<div>
		$$
		\begin{align*}
		 w_1 + w_2 &amp;= T(v_1) + T(v_2) \\
		           &amp;= T(v_1 + v_2) \ \text{because $$T$$ is linear}
		\end{align*}
		$$
	</div>
By definition of a vector space \(v_1 + v_2 \in V\), so the image of the sum \(w_1 + w_2 \in R(T)\) as required.
</li>
<li> We need \(R(T)\) to be closed under multiplication. Suppose \(w_1 \in R(T)\) and \(c\) is a scalar. We need \(cw_1 \in R(T)\). Since \(w_1 \in R(T)\), then there must be a vector \(v_1 \in V\) such that \(w_1 = T(v_1)\). Notice now,
	<div>
		$$
		\begin{align*}
		 w_1 &amp;= T(v_1) \\
		 cw_1 &amp;= cT(v_1) \\
         &amp;= T(cv_1) \ \text{because $$T$$ is linear}
		\end{align*}
		$$
	</div>
By definition of a vector space \(cv_1 \in V\), so the image of the product \(cv_1 \in R(T)\) as required.

</li>
</ul>
<p>From this, we conclude that the \(R(T)\) is a subspace of \(W\).
Proving that \(N(T)\) is a subspace is an exercise.
<br />
<br /></p>
<div class="purdiv">
Theorem (Dimension Theorem)
</div>
<div class="purbdiv">
If \( \ T: V \rightarrow W\) is linear and \(V\) is finite dimensional, then
		$$
		\begin{align*}
		 \dim(N(T)) + \dim(R(T)) = \dim(V).
		\end{align*}
		$$
</div>
<p><br />
We know that \(N(T)\) is a subspace of \(V\). This means that \(\dim(N(T)) \leq \dim(V)\). This theorem tells us that the difference \(\dim(V) - \dim(N(T))\) is the dimension of \(R(T)\). Even if \(W\) is an infinite dimensional space, we know from linearity, it is finite dimensional. Typically, \(\dim(R(T))\) is called the rank of \(T\) and \(\dim(N(T))\) is called the nullity of \(T\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Consider the map \(T: \mathbf{R}^n \rightarrow \mathbf{R}^n\) where</p>
<div>
	$$
	\begin{align*}
	(x_1,...,x_n) \rightarrow (x_1,...,x_m,0,...,0) \quad (m &lt; n)
	\end{align*}
	$$
</div>
<p>The null space is the set of vectors where their images are the zero vector. This means that given some vector \(v=(x_1,...,x_n)\), we want all of \((x_1,...x_m)\) to be zero since we already know anything after \(m\) is zero by the definition of the map. Therefore,</p>
<div>
	$$
	\begin{align*}
	N(T) = \{(x_1,...,x_n) \ | \ x_1=0,...,x_m=0\}
	\end{align*}
	$$
</div>
<p>Therefore, \(\dim(N(T)) = n - m\). For the range of \(T\),</p>
<div>
	$$
	\begin{align*}
	R(T) &amp;= \{(x_1,...,x_m,0,...,0)\} \\
	 \dim(R(T)) &amp;= m
	\end{align*}
	$$
</div>
<p>From this we see that,</p>
<div>
	$$
	\begin{align*}
	 \dim(R(T)) + \dim(N(T)) &amp;= m + n - m \\ 
	                        &amp;= n \\ 
	                        &amp;= \dim(\mathbf{R}^n)
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Proof:</b></h4>
<p>Let \(T: V \rightarrow W\) be a linear transformation and \(V\) be finite dimensional. Let \(\dim(V) = n\). Let \(\dim(N(T)) = k\). We know that \(N(T)\) is a subspace of \(V\). Therefore, \(k \leq n\) (Theorem 1.11). Since \(\dim(N(T)) = k\), this means that any basis of the null space will have \(k\) elements. So let \(\beta_N = \{u_1,...,u_k\}\) be a basis for \(N(T)\).
<br />
<br />
(Note: Now that we have a basis for the null space and know its dimension, we need to somehow find a basis for the range to figure out its dimension as well so the plan is to go from the basis we just created to creating a basis for the range. To do this, we’re going to use the refinement and the replacement theorems)
<br />
<br />
<b>Claim 1:</b> We can extend the basis \(\beta_N\) by \(n-k\) vectors to form a basis of \(V\). To see this, let \(\beta\) be a basis for \(V\). We will use the replacement theorem by setting \(\mathcal{S} = \beta\) and \(\mathcal{U} = \beta_N\) (\(\beta_N\) is a basis so its vectors are linearly independent). The replacement theorem implies that \(k \leq n\) which we already know. Additionally, it implies that there is a subset of \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) elements such that \(\mathcal{U} \cup \mathcal{T}\) generates \(V\). Label the elements in \(\mathcal{T}\) as \(v_1,v_2,...v_{n-k}\) and let \(\beta_V = \mathcal{U} \cup \mathcal{T}\). So now we have,</p>
<div>
	$$
	\begin{align*}
	Span(\beta_V) &amp;= Span(\{u_1,...,u_k,v_1,...v_{n-k}\}) \\
	              &amp;= V.
	\end{align*}
	$$
</div>
<p>We claim that \(\beta_V\) is a basis for \(V\). How do we know this? We know that it spans \(V\) by the replacement theorem above. Moreover, it has \(k+n-k = n\) elements which is the dimension of \(V\). To see that the vectors are linearly independent, suppose that they’re not. Since \(\beta_V\) generates \(V\), then by the refinement theorem, we can take away an element and still have a span that generates \(V\). But then if we do take an element out, this means that we’ll have \(n-1\) elements in \(\beta_V\). This is a contradiction since we need at least \(n\) elements to span \(V\). (TODO: What result did we use here exactly?)
<br />
<br />
(Note: so now we need to relate this to the dimension of the range)
<br />
<br />
<b>Claim 2:</b> we claim that \(\{T(v_1),...,T(v_{n-k})\}\) is a basis for \(R(T)\). If we prove this, then we’ll be done since,</p>
<div>
	$$
	\begin{align*}
	 \dim(N(T)) + \dim(R(T)) &amp;= k + (n-k) \\ 
	                        &amp;= n \\ 
	                        &amp;= \dim(V).
	\end{align*}
	$$
</div>
<p>So now we need to prove that \(\{T(v_1),...,T(v_{n-k})\}\) is a basis. To do so, we need to prove that it generates \(R(T)\) and so \(Span(\{T(v_1),...,T(v_{n-k})\}) = R(T)\). Additionally, we need to prove that the vectors are linearly independent. To see that it generates \((R)\), we know that \(R(T) = \{T(v) \ | \ v \in V\}\). Furthermore, we know that any vector \(v \in V\) an be written as a linear combinations in terms of the elements in \(\beta_V\) and we know that \(T\) is linear. So now we can re-write the definition as,</p>
<div>
	$$
	\begin{align*}
	R(T) &amp;= \{T(v) \ | \ v \in V\} \\
	     &amp;= \{T(a_1u_1 + ... + a_ku_k + b_1v_1 + ... + b_{n-k}v_{n-k}) \ | \ a_1,...,a_k \in \mathbf{R}, b_1,...b_{n-k}\in \mathbf{R} \}. \\
	     &amp;= \{a_1T(u_1) + ... + a_kT(u_k) + b_1T(v_1) + ... + b_{n-k}T(v_{n-k}) \} \\
		 &amp;= \{b_1T(v_1) + ... + b_{n-k}T(v_{n-k})\} \quad \text{because $u_1,...u_k$ are in the null space ($T(u_i)=0_W$)} \\
		 &amp;= Span(\{T(v_1),...,T(v_{n-k})\}).
	\end{align*}
	$$
</div>
<p>Next, to see that the set is linearly independent, we need to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	 a_1T(v_1) + ... + a_{n-k}T(v_{n-k}) = \bar{0}_W.
	\end{align*}
	$$
</div>
<p>implies that \(a_1 = 0, ..., a_{n-k} = 0\). To see this, we’ll use the linearity of \(T\) where the image of a linear combination is the same as the linear combination of the images to get,</p>
<div>
	$$
	\begin{align*}
	 a_1T(v_1) + ... + a_{n-k}T(v_{n-k}) &amp;= \bar{0}_W \\
	 aT(a_1v_1 + ... + a_{n-k}v_{n-k})  &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p>If this equation holds, this means that \(a_1v_1 + ... + a_{n-k}v_{n-k}\) is in the null space \(N(T)\) by definition since it’s image is the zero vector. We defined previously \(\beta_N\) to be the basis for the null space. So now this linear combination must also be in the span of \(\beta_N\),</p>
<div>
	$$
	\begin{align*}
	 \{a_1v_1 + ... + a_{n-k}v_{n-k} &amp;\in Span(\beta_N) \\
	  &amp;\in Span(\{u_1,...,u_k\})
	\end{align*}
	$$
</div>
<p>We claim that this must imply that \(a_1 = 0, ..., a_{n-k} = 0\). Why? Suppose not, then the linear combination \(a_1v_1 + ... + a_{n-k}v_{n-k}\) would be a linear combination of the elements \(u_1,...,u_k\) and so we can write</p>
<div>
	$$
	\begin{align*}
	 a_1v_1 + ... + a_{n-k}v_{n-k} &amp;= b_1u_1 + ... + b_ku_k \\
	  a_1v_1 + ... + a_{n-k}v_{n-k} - b_1u_1 + ... + b_ku_k &amp;= 0
	\end{align*}
	$$
</div>
<p>From the above we see that now we have a linear combination of all the \(v\) vectors and all the \(u\) vectors equal to the zero vector. But all the \(u\) and \(v\) vectors \(\{u_1,...,u_k,v_1,...,v_{n-k}\}\) are part of the basis \(\beta_V\) by construction. So any linear combination of these vectors equaling the zero vector must imply that the coefficients are zero because otherwise they are linearly dependent and this is a contradiction. \(\blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The null space (kernel) of \(T\) is $$ \begin{align*} N(T) = \{v \in V \ | \ T(v) = \bar{0}_W\} \subset V \end{align*} $$ The range (image) of \(T\) is $$ \begin{align*} R(T) = \{T(v) \ | v \in V \} \subset W \end{align*} $$ Notes: So the null space is any vector that the transformation turns it into the zero vector. The range just like the definition is the set of the images of all the vectors in \(V\). If all the vectors have an image, then the range is equal to the codomain and that’s when the transformation is onto. Example \(N(T^b_a) = \{\)functions in \(C^0(\mathbf{R})\) whos average over \([a,b]\) is \(0\}\). In other words, it’s the set of all functions whose definite integral is 0. \(R(T^b_a) = \mathbf{R}\). Here we want all functions whose definite integral is some constant/some real number. In other words, this will be the entire codomain (\(\mathbf{R}\)). We call this map \((T^b_a)\) onto. Theorem If \( \ T: V \rightarrow W\) is linear then \(N(T)\) is a subspace of \(V\) and \(R(T)\) is a subspace of \(W\). Proof: We need to verify the three subspaces properties. To prove that \(R(T)\) is a subspace: We need to prove that \(\bar{0}_W \in R(T)\). This means that we need a \(v \in V\) such that \(T(v) = \bar{0}_W\). Since \(T\) is linear then \(T(\bar{0}_V) = \bar{0}_W\). (We proved this in the previous lecture) We need \(R(T)\) to be closed under addition. Suppose \(w_1, w_2 \in R(T)\), then \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1\) and \(v_2\). Therefore, $$ \begin{align*} w_1 + w_2 &amp;= T(v_1) + T(v_2) \\ &amp;= T(v_1 + v_2) \ \text{because $$T$$ is linear} \end{align*} $$ By definition of a vector space \(v_1 + v_2 \in V\), so the image of the sum \(w_1 + w_2 \in R(T)\) as required. We need \(R(T)\) to be closed under multiplication. Suppose \(w_1 \in R(T)\) and \(c\) is a scalar. We need \(cw_1 \in R(T)\). Since \(w_1 \in R(T)\), then there must be a vector \(v_1 \in V\) such that \(w_1 = T(v_1)\). Notice now, $$ \begin{align*} w_1 &amp;= T(v_1) \\ cw_1 &amp;= cT(v_1) \\ &amp;= T(cv_1) \ \text{because $$T$$ is linear} \end{align*} $$ By definition of a vector space \(cv_1 \in V\), so the image of the product \(cv_1 \in R(T)\) as required.]]></summary></entry><entry><title type="html">Lecture 10: Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 10: Linear Transformations" /><published>2024-07-27T01:01:36-07:00</published><updated>2024-07-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations.html"><![CDATA[<div class="purdiv">
Definition
</div>
<div class="purbdiv">
A map \(T\) from a vector space \(V\) to a vector space \(W\), \(T: V \rightarrow W\) is linear for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\) if
<ol>
	<li>\(T(v_1+v_2) = T(v_1) + T(v_2))\)</li>
	<li>\(T(cv_1) = cT(v_1)\)</li>
</ol>
</div>
<p><br />
Remark: The two conditions can be combined together and so \(T: V \rightarrow W\) is linear above if and only if \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\) for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\).
<br />
<br />
Proof: 
<br />
\(\Rightarrow\): Assume \(T\) is linear. Then,</p>
<div>
	$$
	\begin{align*}
	 T(v_1 + cv_2) &amp;= T(v_1 + cv_2) \\ 
	              &amp;= T(v_1) + T(cv_2) \text{ (By property (1))} \\
				  &amp;= T(v_1) + cT(v_2) \text{ (By property (2))}
	\end{align*}
	$$
</div>
<p>\(\Leftarrow\): Assume \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). Then to show that property (1) is true, notice that</p>
<div>
	$$
	\begin{align*}
	 T(v_1 + v_2) &amp;= T(v_1 + (1)v_2) \\
	              &amp;= T(v_1) + (1)T(v_2) \\
				  &amp;= T(v_1) + T(v_2).
	\end{align*}
	$$
</div>
<p>And to see that property (2) is true, notice that</p>
<div>
	$$
	\begin{align*}
	 T(cv_1) &amp;= T(bar{0}_V + cv_1) \\
	         &amp;= T(\bar{0}_V) + cT(v_1)
	\end{align*}
	$$
</div>
<p>To finish the proof we want to additionally show that \(T(\bar{0}_V) = \bar{0}_W\). How do we do this? We can only use the assumption that \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). To do this notice that,</p>
<div>
	$$
	\begin{align*}
	 \bar{0}_W + T(\bar{0}_V) &amp;= T(\bar{0}_V) \text{ (We're just adding the zero vector)} \\
	         &amp;= T(\bar{0}_V + (1)\bar{0}_V) \text{ (using (c))} \\
			 &amp;= T(\bar{0}_V) + (1)T(\bar{0}_V) \text{ (using (c))} \\
			 &amp;= T(\bar{0}_V) + T(\bar{0}_V) \\
	\bar{0}_W &amp;= T(\bar{0}_V) 
			 
	\end{align*}
	$$
</div>
<p>Another way to do this is the following</p>
<div>
	$$
	\begin{align*}
	 T(\bar{0}_V) &amp;= T(v_1 - v_1) \\
	         &amp;= T(v_1 + (-1)v_1) \\
			 &amp;= T(v_1) + (-1)T(v_1) \\
			 &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p><br />
Remark 2: Suppose we have a linear transformation \(T: V \rightarrow W\)</p>
<div>
	$$
	\begin{align*}
	 T(a_1u_1 + ... + a_ku_k) &amp;= T(a_1u_1 + ... + a_{k-1}u_{k-1}) + a_kT(u_k) \text{ Using property (3)}\\
	 &amp;= a_1T(u_1) + ... + a_kT(u_k)
	\end{align*}
	$$
</div>
<p><br />
This is crucial because this says that the image of a linear combination with coefficients \(a_1, ... a_k\) is again a linear combination in the new vector space with coefficients \(a_1,...a_k\) except that it’s a linear combination of the image of the original vectors.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>For \(V, W\), the map \(T_0: V \rightarrow W, u \rightarrow \bar{0}_W\) is linear.
<br />
<br />
We need to verify that it is linear by verifying \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). This is easy because for any vectors \(v_1, v_2\),</p>
<div>
	$$
	\begin{align*}
	 T_0(v_1 + cv_2) &amp;= \bar{0}_W.
	 \end{align*}
	$$
</div>
<p>Moreover, we also have</p>
<div>
	$$
	\begin{align*}
	 T_0(v_1) + T_0(cv_2) &amp;= \bar{0}_W + c\bar{0}_W \\
	                      &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p>The two sides are equal and so \(T_0\) is linear.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The map \(I_V: V \rightarrow V, u \rightarrow u\) is linear as well.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>The map \(T: \mathbf{R}^2 \rightarrow \mathbf{R}^2, (x,y) \rightarrow (-y,x)\) is linear as well. (a rotation by 90 degrees, todo: add pic). To see why it’s linear, notice that</p>
<div>
	$$
	\begin{align*}
	 T((x_1,y_1) + c(x_2,y_2)) &amp;= T((x_1 + cx_2, y_1 + cy_2)) \text{ (add the two vectors to get one vector)}\\ 
	 &amp;= (-y_1-cy_2, x_1+cx_2).
	 \end{align*}
	$$
</div>
<p>Moreover, notice that</p>
<div>
	$$
	\begin{align*}
	 T((x_1,y_1)) + T((cx_2, cy_2)) &amp;= (-y_1, x_1) + (-cy_2, cx_2) \\
	                                &amp;= (-y_1-cy_2, x_1 + cx_2).
	\end{align*}
	$$
</div>
<p>Both sides are equal and so the transformation is linear.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>The map \(T: P \rightarrow P, f(x) \rightarrow f'(x)\) is linear. Note here that the map The map \(T (P_n \rightarrow P_{n-1}): f(x) \rightarrow f'(x)\) is different because the domain and codomain are different here! this is crucial. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>Let \(A \in M_{m \times n}\). The map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\) is linear. Remember here \(A\bar{x}\) are the linear combinations of the column vectors of \(A\) with the coefficients being the entries of \(\bar{x}\). The crucial thing here is that if \(V\) and \(W\) are both finite dimensional can be represented with this kind of transformation (matrix). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>For \(a &lt; b\), define the map \(T_a^b: C^0(\mathbf{R}) \rightarrow \mathbf{R}, f \rightarrow \int_a^b f(x)dx\) (\(C^0\) is the set of continuous functions on \(\mathbf{R})\)). 
<br />
<br />
Recall the dimension of \(\mathbf{R}\) is 1 and the dimension of \(C^0\) is infinte because the set of all polynomials (which has dimension infinity) is a subset of the set of continuous functions. Therefore, the set of continuous function has dimension infinity as well. This mapping goes from an infinite dimensional space to a finite dimensional space.
<br />
<br />
To prove that this mapping in linear, we notice that</p>
<div>
	$$
	\begin{align*}
	 T_a^b(f + cg) &amp;= \int_a^b (f(x) + cg(x))dx \\
	          &amp;= \int_a^b f(x)dx + c \int_a^b g(x)dx \ \text{ (By Calculus)} \\
	 &amp;= T_a^b(f) + cT_a^b(g).
	\end{align*}
	$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A map \(T\) from a vector space \(V\) to a vector space \(W\), \(T: V \rightarrow W\) is linear for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\) if \(T(v_1+v_2) = T(v_1) + T(v_2))\) \(T(cv_1) = cT(v_1)\) Remark: The two conditions can be combined together and so \(T: V \rightarrow W\) is linear above if and only if \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\) for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\). Proof: \(\Rightarrow\): Assume \(T\) is linear. Then, $$ \begin{align*} T(v_1 + cv_2) &amp;= T(v_1 + cv_2) \\ &amp;= T(v_1) + T(cv_2) \text{ (By property (1))} \\ &amp;= T(v_1) + cT(v_2) \text{ (By property (2))} \end{align*} $$ \(\Leftarrow\): Assume \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). Then to show that property (1) is true, notice that $$ \begin{align*} T(v_1 + v_2) &amp;= T(v_1 + (1)v_2) \\ &amp;= T(v_1) + (1)T(v_2) \\ &amp;= T(v_1) + T(v_2). \end{align*} $$ And to see that property (2) is true, notice that $$ \begin{align*} T(cv_1) &amp;= T(bar{0}_V + cv_1) \\ &amp;= T(\bar{0}_V) + cT(v_1) \end{align*} $$ To finish the proof we want to additionally show that \(T(\bar{0}_V) = \bar{0}_W\). How do we do this? We can only use the assumption that \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). To do this notice that, $$ \begin{align*} \bar{0}_W + T(\bar{0}_V) &amp;= T(\bar{0}_V) \text{ (We're just adding the zero vector)} \\ &amp;= T(\bar{0}_V + (1)\bar{0}_V) \text{ (using (c))} \\ &amp;= T(\bar{0}_V) + (1)T(\bar{0}_V) \text{ (using (c))} \\ &amp;= T(\bar{0}_V) + T(\bar{0}_V) \\ \bar{0}_W &amp;= T(\bar{0}_V) \end{align*} $$ Another way to do this is the following $$ \begin{align*} T(\bar{0}_V) &amp;= T(v_1 - v_1) \\ &amp;= T(v_1 + (-1)v_1) \\ &amp;= T(v_1) + (-1)T(v_1) \\ &amp;= \bar{0}_W \end{align*} $$ Remark 2: Suppose we have a linear transformation \(T: V \rightarrow W\) $$ \begin{align*} T(a_1u_1 + ... + a_ku_k) &amp;= T(a_1u_1 + ... + a_{k-1}u_{k-1}) + a_kT(u_k) \text{ Using property (3)}\\ &amp;= a_1T(u_1) + ... + a_kT(u_k) \end{align*} $$ This is crucial because this says that the image of a linear combination with coefficients \(a_1, ... a_k\) is again a linear combination in the new vector space with coefficients \(a_1,...a_k\) except that it’s a linear combination of the image of the original vectors. Example 1 For \(V, W\), the map \(T_0: V \rightarrow W, u \rightarrow \bar{0}_W\) is linear. We need to verify that it is linear by verifying \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). This is easy because for any vectors \(v_1, v_2\), $$ \begin{align*} T_0(v_1 + cv_2) &amp;= \bar{0}_W. \end{align*} $$ Moreover, we also have $$ \begin{align*} T_0(v_1) + T_0(cv_2) &amp;= \bar{0}_W + c\bar{0}_W \\ &amp;= \bar{0}_W \end{align*} $$ The two sides are equal and so \(T_0\) is linear. Example 2 The map \(I_V: V \rightarrow V, u \rightarrow u\) is linear as well. Example 3 The map \(T: \mathbf{R}^2 \rightarrow \mathbf{R}^2, (x,y) \rightarrow (-y,x)\) is linear as well. (a rotation by 90 degrees, todo: add pic). To see why it’s linear, notice that $$ \begin{align*} T((x_1,y_1) + c(x_2,y_2)) &amp;= T((x_1 + cx_2, y_1 + cy_2)) \text{ (add the two vectors to get one vector)}\\ &amp;= (-y_1-cy_2, x_1+cx_2). \end{align*} $$ Moreover, notice that $$ \begin{align*} T((x_1,y_1)) + T((cx_2, cy_2)) &amp;= (-y_1, x_1) + (-cy_2, cx_2) \\ &amp;= (-y_1-cy_2, x_1 + cx_2). \end{align*} $$ Both sides are equal and so the transformation is linear. Example 4 The map \(T: P \rightarrow P, f(x) \rightarrow f'(x)\) is linear. Note here that the map The map \(T (P_n \rightarrow P_{n-1}): f(x) \rightarrow f'(x)\) is different because the domain and codomain are different here! this is crucial. Example 6 Let \(A \in M_{m \times n}\). The map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\) is linear. Remember here \(A\bar{x}\) are the linear combinations of the column vectors of \(A\) with the coefficients being the entries of \(\bar{x}\). The crucial thing here is that if \(V\) and \(W\) are both finite dimensional can be represented with this kind of transformation (matrix). Example 6 For \(a &lt; b\), define the map \(T_a^b: C^0(\mathbf{R}) \rightarrow \mathbf{R}, f \rightarrow \int_a^b f(x)dx\) (\(C^0\) is the set of continuous functions on \(\mathbf{R})\)). Recall the dimension of \(\mathbf{R}\) is 1 and the dimension of \(C^0\) is infinte because the set of all polynomials (which has dimension infinity) is a subset of the set of continuous functions. Therefore, the set of continuous function has dimension infinity as well. This mapping goes from an infinite dimensional space to a finite dimensional space. To prove that this mapping in linear, we notice that $$ \begin{align*} T_a^b(f + cg) &amp;= \int_a^b (f(x) + cg(x))dx \\ &amp;= \int_a^b f(x)dx + c \int_a^b g(x)dx \ \text{ (By Calculus)} \\ &amp;= T_a^b(f) + cT_a^b(g). \end{align*} $$ References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry></feed>