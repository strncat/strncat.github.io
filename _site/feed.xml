<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-29T21:11:34-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 11: Exercise 0</title><link href="http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0.html" rel="alternate" type="text/html" title="Lecture 11: Exercise 0" /><published>2024-08-12T01:01:36-07:00</published><updated>2024-08-12T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0.html"><![CDATA[<div class="ydiv">
Problem 4
</div>
<div class="ybdiv">
Let \(V\) and \(W\) be finite dimensional vector spaces and \(T: V \rightarrow W\) be linear.
<ol style="list-style-type:lower-alpha">
	<li>Prove that if \(\dim(V) &lt; \dim (W)\), then \(T\) cannot be onto.</li>
	<li>Prove that if \(\dim(W) &lt; \dim(V)\), then \(T\) cannot be one-to-one.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Suppose for the sake of contradiction that \(T\) was onto. This means that \(Rank(T) = \dim(W)\). The dimension theorem states that
	<div>
		$$
		\begin{align*}
		\dim(V) &amp;= Nullity(T) + Rank(T) \\
		       &amp;= Nullity(T) + \dim(W)
		\end{align*}
		$$
	</div>
This implies that \(Nullity(T) = \dim(V) - \dim(W) &lt; 0\) since \(\dim(V) &lt; \dim(W)\). This is a contradiction and so \(T\) cannot be onto. \(\blacksquare\)  
	</li>
	<li>Similarly, suppose for the sake of contradiction that \(T\) is one-to-one. This means that \(Nullity(T) = 0\) and so by the dimension theorem, it must be that \(\dim(V) = \dim(W)\). But this is a contradiction since \(\dim(W) &lt; \dim(V)\).</li> </ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.geneseo.edu/~heap/courses/333/exam2_F2007_practice_sol.pdf">Practice Midterm</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Problem 4 Let \(V\) and \(W\) be finite dimensional vector spaces and \(T: V \rightarrow W\) be linear. Prove that if \(\dim(V) &lt; \dim (W)\), then \(T\) cannot be onto. Prove that if \(\dim(W) &lt; \dim(V)\), then \(T\) cannot be one-to-one. Proof: Suppose for the sake of contradiction that \(T\) was onto. This means that \(Rank(T) = \dim(W)\). The dimension theorem states that $$ \begin{align*} \dim(V) &amp;= Nullity(T) + Rank(T) \\ &amp;= Nullity(T) + \dim(W) \end{align*} $$ This implies that \(Nullity(T) = \dim(V) - \dim(W) &lt; 0\) since \(\dim(V) &lt; \dim(W)\). This is a contradiction and so \(T\) cannot be onto. \(\blacksquare\) Similarly, suppose for the sake of contradiction that \(T\) is one-to-one. This means that \(Nullity(T) = 0\) and so by the dimension theorem, it must be that \(\dim(V) = \dim(W)\). But this is a contradiction since \(\dim(W) &lt; \dim(V)\). References: Practice Midterm]]></summary></entry><entry><title type="html">Lecture 19: Determinants</title><link href="http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants.html" rel="alternate" type="text/html" title="Lecture 19: Determinants" /><published>2024-08-11T01:01:36-07:00</published><updated>2024-08-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants.html"><![CDATA[<p>The determinant is a map</p>
<div>
$$
\begin{align*}
\det: &amp;M_{n \times n} \rightarrow \mathbf{R} \\
     &amp;A \rightarrow \det(A)
\end{align*}
$$
</div>
<p>Properties of the determinant</p>
<ul>
	<li>\(A\) is invertible iff \(\det(A) \neq 0\)</li>
	<li>\(\det(A)\) has geometric meaning.</li>
	To see this, let \([0,1]^n = \{(x_1,...,x_n) \in \mathbf{R}^n \ | \ x_i \in [0,1]\)
	
	
	<li>det is not linear except for \(n = 1\). It is linear in the rows of \(A\).</li>
	<li>\(\det(AB) = \det(A)\det(B)\)</li>
</ul>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Elementary Matrices</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Applying the three types of elementary row operations results in the following matrices</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix} \rightarrow E_1 &amp;= 
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix} \\
E_2 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; \lambda
\end{pmatrix} \\
E_3 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
\lambda &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>This leads to the next theorem
<br />
<br />
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The determinant is a map $$ \begin{align*} \det: &amp;M_{n \times n} \rightarrow \mathbf{R} \\ &amp;A \rightarrow \det(A) \end{align*} $$ Properties of the determinant \(A\) is invertible iff \(\det(A) \neq 0\) \(\det(A)\) has geometric meaning. To see this, let \([0,1]^n = \{(x_1,...,x_n) \in \mathbf{R}^n \ | \ x_i \in [0,1]\) det is not linear except for \(n = 1\). It is linear in the rows of \(A\). \(\det(AB) = \det(A)\det(B)\) Elementary Matrices Definition An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III. Example Applying the three types of elementary row operations results in the following matrices $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} \rightarrow E_1 &amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \\ E_2 &amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} \\ E_3 &amp;= \begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix} \end{align*} $$ This leads to the next theorem References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 18: Elementary Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/10/lec18-elementary-matrices.html" rel="alternate" type="text/html" title="Lecture 18: Elementary Matrices" /><published>2024-08-10T01:01:36-07:00</published><updated>2024-08-10T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/10/lec18-elementary-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/10/lec18-elementary-matrices.html"><![CDATA[<p>Recall three types of elementary row operations</p>
<ul style="list-style-type:lower-alpha">
	<li>\(R_i \leftrightarrow R_j\)</li>
	<li>\(R_i \rightarrow \lambda R_i\) where \(\lambda \neq 0\)</li>
	<li>\(R_i \rightarrow R_i + \lambda R_j\)</li>
</ul>
<p>Now that we’ve studied matrix multiplication we can state the fact that performing an elementary row operation on \(A \in M_{2 \times 2}\) can actually be described using matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Elementary Matrices</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Applying the three types of elementary row operations results in the following matrices</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix} \rightarrow E_1 &amp;= 
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix} \\
E_2 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; \lambda
\end{pmatrix} \\
E_3 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
\lambda &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>This leads to the next theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	Let \(E\) be the elementary matrix obtained from \(I_n\) by performing row operation \(\mathcal{R}(E = E(\mathcal{R})))\) for any \(A \in M_{m \times n}\), the product
	$$
	\begin{align*}
	E(\mathcal{R}) \cdot A_{m \times n}
    \end{align*}
	$$
	is equal to the matrix obtained from \(A\) by performing \(\mathcal{R}\).
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let’s apply the elementary matrices on the following given matrix</p>
<div>
$$
\begin{align*}
E_1  \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
\\
E_2
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} 
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ \lambda c &amp; \lambda d \end{pmatrix} 
\\
E_3
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;=\begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix}
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ c + \lambda a &amp; d + \lambda b \end{pmatrix} 
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>RREF by Matrix Multiplication</b></h4>
<p>Since we can perform elementary row operations by matrix multiplication, then we can possibly see how we can put a matrix in reduced row echelon by multiplication. But first, there is an observation</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	Each elementary matrix is invertible.
	$$
	\begin{align*}
	&amp;\mathcal{R}: R_i \leftrightarrow R_j \quad \quad \quad \quad \ \ \mathcal{R}^{-1}: R_j \leftrightarrow R_i \\
	&amp;\mathcal{R}: R_i \rightarrow \lambda R_i \quad \quad \quad \quad \mathcal{R}^{-1}: R_i \rightarrow \frac{1}{\lambda} R_i \\
	&amp;\mathcal{R}: R_i \rightarrow R_i + \lambda R_j \quad \quad \mathcal{R}^{-1}: R_i \rightarrow R_i - \lambda R_j
    \end{align*}
	$$
</div>
<p><br />
Proof: apply the elementary row operation by multiplying by \(E(\mathcal{R})\) and then apply the inverse again by multiplying by \(E(\mathcal{R}^{-1})\). The result is the identity matrix. In other words, \(E(\mathcal{R})E(\mathcal{R}^{-1}) = I_n\). 
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For every \(A \in M_{m \times n}\), there is a finite set of elementary matrices \(E_1,...,E_k \in M_{m \times n}\) such that \(E_k ... E_2E_1\) is in RREF.  
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	\(A \in M_{m \times n}\) is invertible if and only iff there is a finite set of elementary matrices \(E_1,...,E_k \in M_{n \times n}\) such that \(E_k...E_2E_1A = I_n\).  \((A \ | \ I_n)\).
</div>
<p><br />
Note here that from the expression above we can see that \(A^{-1} = E_k...E_1\) and \(A = E_1^{-1}E_2^{-1}...E^{-1}_{k-1}E^{-1}_{k}\)
<br />
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	\(A\) is invertible if and only iff it can be written as a product of elementary matrices.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Rank of a Matrix</b></h4>
<p>Recall that the rank of \(T: V \rightarrow W\) is \(\dim(R(T))\).</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	The rank of \(A \in M_{n \times n}\), \(rank(A)\) is the rank of \(L_A: \mathbf{R}^m \rightarrow \mathbf{R}^n\).
</div>
<p><br />
This definition is kind of awkward and instead we want to find an expression for rank(\(A\)) in terms of \(A\) itself and not have to rely on \(L_A\). To figure this out, we need the following result</p>
<div class="purdiv">
Proposition
</div>
<div class="purbdiv">
If \(B \in M_{n \times n}\) is invertible, then rank(\(BA\)) = rank(\(A\)).
</div>
<p><br />
So multiplication by \(B\) doesn’t change the rank.
<br />
<br />
Proof: <br />
By definition \(rank(BA)\) is the rank of the linear map \(L_{BA}\). But the rank of a linear map is the dimension of its range and so</p>
<div>
$$
\begin{align*}
rank(BA) &amp;= rank(L_{BA}) \\
         &amp;= \dim(R(L_{BA})).
\end{align*}
$$
</div>
<p>B definition, to get the range, we just apply the linear map on \(v \in \mathbf{R}^n\). This range is a subset of \(\mathbf{R}^m\). Applying the linear map is just multiplying \(v\) by the matrices \(A\) and \(B\). So we can see this below:</p>
<div>
$$
\begin{align*}
\dim(R(L_{BA})) &amp;= \{L_{BA}(v) \ | \ v \in \mathbf{R}^n \} \subset \mathbf{R}^m \\
                &amp;= \{BA(v) \ | \ v \mathbf{R}^n\} \\
				&amp;= \{B(A(v)) \ | \ v \mathbf{R}^n\} \\
				&amp;= \{B(A(v)) \ | \ v \mathbf{R}^n\}
\end{align*}
$$
</div>
<p>We can also re-arrange this by applying the linear map \(L_B\) on the set that we get from applying \(A\) as follows</p>
<div>
$$
\begin{align*}
\dim(R(L_{BA})) &amp;= L_B(\{A(v) \ | \ v \mathbf{R}^n\}) \\
\end{align*}
$$
</div>
<p>But this internal set is the range of the linear map \(L_A\), \(R(L_A)\). So define the map</p>
<div>
$$
\begin{align*}
L_B&amp;: \mathbf{R}^m \rightarrow \mathbf{R}^m \\
\tilde{L_B}&amp;: R(L_A) \rightarrow L_B(R(L_A))
\end{align*}
$$
</div>
<p>We claim this new map is invertible. It’s onto because we restricted the target to the image \(R(L_A)\). It’s one-to-one because \(B\) is invertible. Therefore, the dimension of the domain and the target are the same. So</p>
<div>
$$
\begin{align*}
\dim(R(L_A)) = \dim(L_B(R(L_A)))
\end{align*}
$$
</div>
<p>But we know that \(\dim(R(L_A))\) is the rank of \(A\). and \(\dim(L_B(R(L_A))\) is the rank of \(L_{BA}\) (why?) so \(rank(A) = rank(BA)\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Rank of a Matrix</b></h4>
<p>So now we can go back to our original goal of finding an expression for finding the rank of a matrix \(A\) without going back to the rank of the linear map \(L_A\). We just proved that \(rank(BA) = rank(A)\). for any invertible matrix \(B\). We further studied earlier that elementary matrices are invertible. So multiplying \(A\) by a set of elementary matrices will not change its rank. So we can get \(A\) in RREF without its rank changing. Based on this we have the following corollaries:</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	Elementary row operations don't change rank.
</div>
<p><br />
and</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	$$
	\begin{align*}
	rank(A) = rank(RREF(A))
    \end{align*}
	$$
</div>
<p><br />
Why do we want RREF? because it’s easy to read off and we can easily figure out the dimension easily from seeing a matrix in its RREF.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>What is the range of the following matrix?</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>We know the rank of \(A\). It is the dimension of the range of \(L_A\) or \(\dim(R(L_A))\). Notice for the above matrix, if we multiply \(A\) by a vector \(v\), the result is a linear combination of the columns of \(A\). Recall</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\begin{pmatrix} x \\ y \\ z \\ w \end{pmatrix}
=
x
\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}
+
y
\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
+
z
\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}
+
w
\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>In other words, the columns span the range. Here, we see that we have 3 non-zero columns and they are linearly independent. so \(rank(A) = \dim(R(L_A)) = 3\). This works here but it’s not generally true!! we’re missing something … so let’s clarify with another example
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>What is the range of the following matrix?</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>It’s still true that the range of this is still spanned by the columns (always true). But the rank of \(A\) in general is the number of columns with leading entries in \(REF\) of \(A\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Nullity and The Dimension Theorem</b></h4>
<p>So we know that \(\text{nullity}(A) = dim(N(A))\). We found a basis for the null space of \(A\) by solving \(Ax = 0\) and finding all the solutions and then writing a set that spans that solution set. From the basis we knew the dimension of the null space. Specifically when we solved \(Ax = 0\), it was the number of columns without leading entries. So we can write \(\text{nullity}(A) = dim(N(A)) =\) # of columns without leading entries.
<br />
<br />
So now if we put together the number of columns without leading entries (nullity of \(A\)) and the number of columns with leading entries (rank of \(A\)), then we get \(n\). This is basically the dimension theorem.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recall three types of elementary row operations \(R_i \leftrightarrow R_j\) \(R_i \rightarrow \lambda R_i\) where \(\lambda \neq 0\) \(R_i \rightarrow R_i + \lambda R_j\) Now that we’ve studied matrix multiplication we can state the fact that performing an elementary row operation on \(A \in M_{2 \times 2}\) can actually be described using matrix multiplication. Elementary Matrices Definition An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III. Example Applying the three types of elementary row operations results in the following matrices $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} \rightarrow E_1 &amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \\ E_2 &amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} \\ E_3 &amp;= \begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix} \end{align*} $$ This leads to the next theorem Theorem Let \(E\) be the elementary matrix obtained from \(I_n\) by performing row operation \(\mathcal{R}(E = E(\mathcal{R})))\) for any \(A \in M_{m \times n}\), the product $$ \begin{align*} E(\mathcal{R}) \cdot A_{m \times n} \end{align*} $$ is equal to the matrix obtained from \(A\) by performing \(\mathcal{R}\). Example Let’s apply the elementary matrices on the following given matrix $$ \begin{align*} E_1 \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} &amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} \\ E_2 \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} &amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = \begin{pmatrix} a &amp; b \\ \lambda c &amp; \lambda d \end{pmatrix} \\ E_3 \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} &amp;=\begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = \begin{pmatrix} a &amp; b \\ c + \lambda a &amp; d + \lambda b \end{pmatrix} \end{align*} $$ RREF by Matrix Multiplication Since we can perform elementary row operations by matrix multiplication, then we can possibly see how we can put a matrix in reduced row echelon by multiplication. But first, there is an observation Corollary Each elementary matrix is invertible. $$ \begin{align*} &amp;\mathcal{R}: R_i \leftrightarrow R_j \quad \quad \quad \quad \ \ \mathcal{R}^{-1}: R_j \leftrightarrow R_i \\ &amp;\mathcal{R}: R_i \rightarrow \lambda R_i \quad \quad \quad \quad \mathcal{R}^{-1}: R_i \rightarrow \frac{1}{\lambda} R_i \\ &amp;\mathcal{R}: R_i \rightarrow R_i + \lambda R_j \quad \quad \mathcal{R}^{-1}: R_i \rightarrow R_i - \lambda R_j \end{align*} $$ Proof: apply the elementary row operation by multiplying by \(E(\mathcal{R})\) and then apply the inverse again by multiplying by \(E(\mathcal{R}^{-1})\). The result is the identity matrix. In other words, \(E(\mathcal{R})E(\mathcal{R}^{-1}) = I_n\). Theorem For every \(A \in M_{m \times n}\), there is a finite set of elementary matrices \(E_1,...,E_k \in M_{m \times n}\) such that \(E_k ... E_2E_1\) is in RREF. Theorem \(A \in M_{m \times n}\) is invertible if and only iff there is a finite set of elementary matrices \(E_1,...,E_k \in M_{n \times n}\) such that \(E_k...E_2E_1A = I_n\). \((A \ | \ I_n)\). Note here that from the expression above we can see that \(A^{-1} = E_k...E_1\) and \(A = E_1^{-1}E_2^{-1}...E^{-1}_{k-1}E^{-1}_{k}\) Corollary \(A\) is invertible if and only iff it can be written as a product of elementary matrices. The Rank of a Matrix Recall that the rank of \(T: V \rightarrow W\) is \(\dim(R(T))\).]]></summary></entry><entry><title type="html">Lecture 17: Change of Coordinates and Matrix Representations</title><link href="http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates.html" rel="alternate" type="text/html" title="Lecture 17: Change of Coordinates and Matrix Representations" /><published>2024-08-09T01:01:36-07:00</published><updated>2024-08-09T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates.html"><![CDATA[<p>Given a vector space \(V\) with two finite basis \(\beta\) and \(\beta'\). For \(v \in V\), what is the relationship between \([v]_{\beta}\) and \([v]_{\beta'}\)?
<br />
<br />
Starting with \([v]_{\beta'}\), we want to derive an expression for \([v]_{\beta}\). We can take the</p>
<div>
$$
\begin{align*}
[v]_{\beta} &amp;= [I_V(v)]_{\beta'} \\
           &amp;= [I_V]_{\beta}^{\beta'}[v]_{\beta} \\
\end{align*}
$$
</div>
<p>In the first line we just re-wrote the vector so now the identity map is applied on \(v\) which doesn’t change \(v\). Since \(I_V\) is a linear map, we can use Theorem 2.14 to re-write this as the second line. We compute the matrix \([I_V]_{\beta}^{\beta'}\) by applying \(I_V\) on the basis vectors of \(\beta\). But since it’s the identity transformation then the vectors in \(\beta\) will not change. We then find the coordinates of each vector when written as a linear combination of the vectors of basis \(\beta'\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = P_1\), \(\beta = \{1, x\}\) and \(\beta' = \{1+x, 1-x\}\). Then to compute the matrix \([I_V]_{\beta}^{\beta'}\), we want to apply the map on the basis \(\beta\) and then write these relative to the basis \(\beta'\) (meaning for each vector, find its coordinates when written as a linear combination of the vectors of the basis \(\beta'\)).</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'} &amp;= 
\begin{pmatrix}
[I_V(1)]_{\beta'} &amp; [I_V(x)]_{\beta'}
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
[1]_{\beta'} &amp; [x]_{\beta'}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Now we need to find the coordinates when 1 is written as a linear combination of the vectors of basis \(\beta'\) so</p>
<div>
$$
\begin{align*}
1 &amp;= a(1 + x) + b(1 - x) \\
1 &amp;= a + ax + b - bx \\
1 &amp;= a + b + x(a - b) \\
\end{align*}
$$
</div>
<p>from this we see that \(a = b = 1/2\). Similarly we need to find the coordinates of \(x\)</p>
<div>
$$
\begin{align*}
x &amp;= a(1 + x) + b(1 - x) \\
0 &amp;= a + ax + b - bx - x \\
0 &amp;= a + b + x(a - b - 1) \\
\end{align*}
$$
</div>
<p>A solution to this is \(a = 1/2\) and \(b = -1/2\). So now we have</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'} &amp;= 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \\
\frac{1}{2} &amp; -\frac{1}{2}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We can now use this matrix to transform any vector in \(V\) written with respect to \(\beta\) to a vector written with respect to \(\beta'\). Given some polynomial in \(V\),</p>
<div>
$$
\begin{align*}
[a_0 + a_1x]_{\beta'} = 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \\
\frac{1}{2} &amp; -\frac{1}{2}
\end{pmatrix}
\begin{pmatrix}
a_0 \\
a_1
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{2}(a_0 + a_1) \\
\frac{1}{2}(a_0 - a_1)
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Inverse of the Change of Coordinates Matrix</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	$$
	\begin{align*}
([I_V]_{\beta}^{\beta'})^{-1} = [I_V]^{\beta}_{\beta'}
    \end{align*}
	$$
</div>
<p><br />
Proof: To prove that a matrix is an inverse of another matrix, we need to show that their product is the identity matrix so</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'}[I_V]^{\beta}_{\beta'} &amp;= [I_V \circ I_V]^{\beta'}_{\beta'} \text{ (Theorem 2.11)}\\
 &amp;= [I_V]^{\beta'}_{\beta'} \\
 &amp;= I
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Change of Bases</b></h4>
<p>Question: Given \(T: V \rightarrow W\) and finite bases \(\beta, \beta'\) for \(V\) and \(\gamma, \gamma'\) for \(W\), how are \([T]^{\gamma}_{\beta}\) and \([T]^{\gamma'}_{\beta'}\) related? The only tool we have is composition and using Theorem 2.11</p>
<div>
$$
\begin{align*}
[T]^{\gamma'}_{\beta'} &amp;= [I_W \circ T \circ I_V]^{\gamma'}_{\beta'} \\
	&amp;= [I_W]^{\gamma'}_{\gamma}  [T \circ I_V]^{\gamma}_{\beta'} \\
	&amp;= [I_W]^{\gamma'}_{\gamma}  [T]^{\gamma}_{\beta} [I_V]^{\beta}_{\beta'} \\
\end{align*}
$$
</div>
<p>If \(W = V\) and \(\gamma, \gamma' = \beta, \beta'\), then</p>
<div>
$$
\begin{align*}
[T]^{\gamma'}_{\beta'} &amp;= [I_W]^{\beta'}_{\beta}  [T]^{\beta}_{\beta} [I_V]^{\beta}_{\beta'} \\
						&amp;= Q^{-1}[T]^{\beta}_{\beta}Q
\end{align*}
$$
</div>
<p>Where \(Q = [I_V]^{\beta}_{\beta'}\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Similar Matrices</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	\(A, B \in M_{2 \times 2}\) are similar if \(\exists Q \in M_{2 \times 2}\) such that \(B = Q^{-1}AQ\)
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(A =  \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix}\) and consider the map \(L_A: \mathbf{R}^2 \rightarrow \mathbf{R}^2\). Can we understand \(L_A^k = L_A \circ L_A \circ ... \circ L_A\) (\(k\) times)? If we try this</p>
<div>
$$
\begin{align*}
A^k = \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} ...
\end{align*}
$$
</div>
<p>This is really hard to multiply. So what is \(A^2\) really? It’s the matrix representative of the composition \([L_A \circ L_A]\). In fact if we go back to just \([L_A]\). This is the matrix representative of this map with respect to the standard basis so \([L_A]_{\beta}^{\beta} = A\). The idea then is to find another basis such that the matrix representation of the map with respect to that basis is easier to multiply. So the plan is to find \(\beta'\) such that \([L_A]_{\beta'}^{\beta'}\) is easier to multiply.
<br />
<br />
Note that for \(v_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, v_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\), we’ll find that</p>
<div>
$$
\begin{align*}
L_A(v_1) &amp;= Av_1 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 8 \\ 8 \end{pmatrix} = 8v_1 \\
L_A(v_2) &amp;= Av_2 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 4 \\ -4 \end{pmatrix} = 4v_2.
\end{align*}
$$
</div>
<p>Notice that \(\beta' = \{v_1, v_2\}\) is a basis for \(\mathbf{R}^2\).</p>
<div>
$$
\begin{align*}
[L_A]_{\beta'}^{\beta'} &amp;= ([L_A(v_1)]_{\beta'}, [L_A]_{\beta'}) \\
              &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix}
\end{align*}
$$
</div>
<p>This leads to</p>
<div>
$$
\begin{align*}
[L_A \circ L_A ... \circ L_A]_{\beta'}^{\beta'} &amp;= [L_A]_{\beta'}^{\beta'} ... [L_A]_{\beta'}^{\beta'} \\
              &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} ... \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} \\
			  &amp;= \begin{pmatrix} 8^k &amp; 0 \\0 &amp; 4^k \end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of Orthogonal Projection</b></h4>
<p>Let \(W = Span\{(1,-1,0), (0,1,-1)\}\). \(W\) is a subspace of \(\mathbf{R}^3\) (It is a plane in \(\mathbf{R}^3\)). 
<br />
<br />
Let \(T: \mathbf{R}^3 \rightarrow \mathbf{R}^3\) be the orthogonal projection onto \(W\). Because we’re projecting onto \(W\), then we know that \(T(w_1) = w_1\) and \(T(w_2) = w_2\). 
<br />
<br />
Find \([T]_{\beta}^{\beta}\) where \(\beta\) is the standard basis.
<br />
<br />
The plan is to find \(\beta'\) so that \([T]_{\beta'}^{\beta'}\) is easily understood and use the change of basis formula to convert between the two.
<br />
<br />
What is a candidate for \(\beta'\), something that makes the matrix expression simple? well when \(T\) acts on \(w_1\) and \(w_2\), it doesn’t change them. But we still need one more vector. What should we choose for \(w_3\)? One candidate is the orthogonal vector to the place \(W\) which will mean that its projection is going to be zero. How do we find it? We can take the cross product of \(w_1\) and \(w_2\). \(w_3 = w_1 \times w_2 = (1,1,1)\) and \(T(w_3) = (0,0,0)\).</p>
<div>
$$
\begin{align*}
[T]_{\beta'}^{\beta'} &amp;= \begin{pmatrix} [T(w_1)]_{\beta'} &amp; [T(w_2)]_{\beta'} &amp; [T(w_3)]_{\beta'} \end{pmatrix} \\
               &amp;= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}
\end{align*}
$$
</div>
<p>Again we derived these by writing \(w_1\) as a linear combination of the basis vectors \(\beta'=\{w_1,w_2,w_3\}\). So \(w_1 = 1(w_1) + 0(w_2) + 0(w_3)\). and so the coordinate vector is \((1,0,0)\).
<br />
<br />
But this is \(T\) with respect to \(\beta'\) and we want \(T\) with respect to \(\beta\). So now we need to use the change of basis formula to get what we want. So</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} [I_{\mathbf{R}^3}]_{\beta}^{\beta'}
\end{align*}
$$
</div>
<p>So now we need to compute \([I_{\mathbf{R}^3}]_{\beta'}^{\beta}\). This means for each vector in \(\beta'\), we want to write it in terms of the standard basis. For example, \(w_1 = (1, -1, 0) = 1e_1 -1e_2 + 0e_3\) and so on. This will get us</p>
<div>
$$
\begin{align*}
[I_{\mathbf{R}^3}]_{\beta'}^{\beta} = \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 1 \\ 0 &amp; -1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>But for \([I_{\mathbf{R}^3}]_{\beta}^{\beta'}\) is a little tricker since we want to write the vectors in \(\beta'\) as linear combinations of the standard basis which we could do but we can also do the following instead</p>
<div>
$$
\begin{align*}
[I_{\mathbf{R}^3}]_{\beta}^{\beta'} = ([I_{\mathbf{R}^3}]_{\beta'}^{\beta})^{-1} = \frac{1}{3}\begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} 
 [I_{\mathbf{R}^3}]_{\beta}^{\beta'} \\
  &amp;= 
  \frac{1}{3} \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 1 \\ 0 &amp; -1 &amp; 1 \end{pmatrix} 
  \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}
  \begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix} \\
  &amp;= 
  \frac{1}{3}\begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>To verify this is correct, we should check that</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta}[w_1]_{\beta} &amp;= [w_1]_{\beta}.
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Given a vector space \(V\) with two finite basis \(\beta\) and \(\beta'\). For \(v \in V\), what is the relationship between \([v]_{\beta}\) and \([v]_{\beta'}\)? Starting with \([v]_{\beta'}\), we want to derive an expression for \([v]_{\beta}\). We can take the $$ \begin{align*} [v]_{\beta} &amp;= [I_V(v)]_{\beta'} \\ &amp;= [I_V]_{\beta}^{\beta'}[v]_{\beta} \\ \end{align*} $$ In the first line we just re-wrote the vector so now the identity map is applied on \(v\) which doesn’t change \(v\). Since \(I_V\) is a linear map, we can use Theorem 2.14 to re-write this as the second line. We compute the matrix \([I_V]_{\beta}^{\beta'}\) by applying \(I_V\) on the basis vectors of \(\beta\). But since it’s the identity transformation then the vectors in \(\beta\) will not change. We then find the coordinates of each vector when written as a linear combination of the vectors of basis \(\beta'\). Example Let \(V = P_1\), \(\beta = \{1, x\}\) and \(\beta' = \{1+x, 1-x\}\). Then to compute the matrix \([I_V]_{\beta}^{\beta'}\), we want to apply the map on the basis \(\beta\) and then write these relative to the basis \(\beta'\) (meaning for each vector, find its coordinates when written as a linear combination of the vectors of the basis \(\beta'\)). $$ \begin{align*} [I_V]_{\beta}^{\beta'} &amp;= \begin{pmatrix} [I_V(1)]_{\beta'} &amp; [I_V(x)]_{\beta'} \end{pmatrix} \\ &amp;= \begin{pmatrix} [1]_{\beta'} &amp; [x]_{\beta'} \end{pmatrix}. \end{align*} $$ Now we need to find the coordinates when 1 is written as a linear combination of the vectors of basis \(\beta'\) so $$ \begin{align*} 1 &amp;= a(1 + x) + b(1 - x) \\ 1 &amp;= a + ax + b - bx \\ 1 &amp;= a + b + x(a - b) \\ \end{align*} $$ from this we see that \(a = b = 1/2\). Similarly we need to find the coordinates of \(x\) $$ \begin{align*} x &amp;= a(1 + x) + b(1 - x) \\ 0 &amp;= a + ax + b - bx - x \\ 0 &amp;= a + b + x(a - b - 1) \\ \end{align*} $$ A solution to this is \(a = 1/2\) and \(b = -1/2\). So now we have $$ \begin{align*} [I_V]_{\beta}^{\beta'} &amp;= \begin{pmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; -\frac{1}{2} \end{pmatrix}. \end{align*} $$ We can now use this matrix to transform any vector in \(V\) written with respect to \(\beta\) to a vector written with respect to \(\beta'\). Given some polynomial in \(V\), $$ \begin{align*} [a_0 + a_1x]_{\beta'} = \begin{pmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; -\frac{1}{2} \end{pmatrix} \begin{pmatrix} a_0 \\ a_1 \end{pmatrix} = \begin{pmatrix} \frac{1}{2}(a_0 + a_1) \\ \frac{1}{2}(a_0 - a_1) \end{pmatrix} \end{align*} $$ The Inverse of the Change of Coordinates Matrix Theorem $$ \begin{align*} ([I_V]_{\beta}^{\beta'})^{-1} = [I_V]^{\beta}_{\beta'} \end{align*} $$ Proof: To prove that a matrix is an inverse of another matrix, we need to show that their product is the identity matrix so $$ \begin{align*} [I_V]_{\beta}^{\beta'}[I_V]^{\beta}_{\beta'} &amp;= [I_V \circ I_V]^{\beta'}_{\beta'} \text{ (Theorem 2.11)}\\ &amp;= [I_V]^{\beta'}_{\beta'} \\ &amp;= I \end{align*} $$ Change of Bases Question: Given \(T: V \rightarrow W\) and finite bases \(\beta, \beta'\) for \(V\) and \(\gamma, \gamma'\) for \(W\), how are \([T]^{\gamma}_{\beta}\) and \([T]^{\gamma'}_{\beta'}\) related? The only tool we have is composition and using Theorem 2.11 $$ \begin{align*} [T]^{\gamma'}_{\beta'} &amp;= [I_W \circ T \circ I_V]^{\gamma'}_{\beta'} \\ &amp;= [I_W]^{\gamma'}_{\gamma} [T \circ I_V]^{\gamma}_{\beta'} \\ &amp;= [I_W]^{\gamma'}_{\gamma} [T]^{\gamma}_{\beta} [I_V]^{\beta}_{\beta'} \\ \end{align*} $$ If \(W = V\) and \(\gamma, \gamma' = \beta, \beta'\), then $$ \begin{align*} [T]^{\gamma'}_{\beta'} &amp;= [I_W]^{\beta'}_{\beta} [T]^{\beta}_{\beta} [I_V]^{\beta}_{\beta'} \\ &amp;= Q^{-1}[T]^{\beta}_{\beta}Q \end{align*} $$ Where \(Q = [I_V]^{\beta}_{\beta'}\) Similar Matrices Definition \(A, B \in M_{2 \times 2}\) are similar if \(\exists Q \in M_{2 \times 2}\) such that \(B = Q^{-1}AQ\) Example Let \(A = \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix}\) and consider the map \(L_A: \mathbf{R}^2 \rightarrow \mathbf{R}^2\). Can we understand \(L_A^k = L_A \circ L_A \circ ... \circ L_A\) (\(k\) times)? If we try this $$ \begin{align*} A^k = \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} ... \end{align*} $$ This is really hard to multiply. So what is \(A^2\) really? It’s the matrix representative of the composition \([L_A \circ L_A]\). In fact if we go back to just \([L_A]\). This is the matrix representative of this map with respect to the standard basis so \([L_A]_{\beta}^{\beta} = A\). The idea then is to find another basis such that the matrix representation of the map with respect to that basis is easier to multiply. So the plan is to find \(\beta'\) such that \([L_A]_{\beta'}^{\beta'}\) is easier to multiply. Note that for \(v_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, v_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\), we’ll find that $$ \begin{align*} L_A(v_1) &amp;= Av_1 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 8 \\ 8 \end{pmatrix} = 8v_1 \\ L_A(v_2) &amp;= Av_2 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 4 \\ -4 \end{pmatrix} = 4v_2. \end{align*} $$ Notice that \(\beta' = \{v_1, v_2\}\) is a basis for \(\mathbf{R}^2\). $$ \begin{align*} [L_A]_{\beta'}^{\beta'} &amp;= ([L_A(v_1)]_{\beta'}, [L_A]_{\beta'}) \\ &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} \end{align*} $$ This leads to $$ \begin{align*} [L_A \circ L_A ... \circ L_A]_{\beta'}^{\beta'} &amp;= [L_A]_{\beta'}^{\beta'} ... [L_A]_{\beta'}^{\beta'} \\ &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} ... \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} \\ &amp;= \begin{pmatrix} 8^k &amp; 0 \\0 &amp; 4^k \end{pmatrix} \end{align*} $$ Matrix Representation of Orthogonal Projection Let \(W = Span\{(1,-1,0), (0,1,-1)\}\). \(W\) is a subspace of \(\mathbf{R}^3\) (It is a plane in \(\mathbf{R}^3\)). Let \(T: \mathbf{R}^3 \rightarrow \mathbf{R}^3\) be the orthogonal projection onto \(W\). Because we’re projecting onto \(W\), then we know that \(T(w_1) = w_1\) and \(T(w_2) = w_2\). Find \([T]_{\beta}^{\beta}\) where \(\beta\) is the standard basis. The plan is to find \(\beta'\) so that \([T]_{\beta'}^{\beta'}\) is easily understood and use the change of basis formula to convert between the two. What is a candidate for \(\beta'\), something that makes the matrix expression simple? well when \(T\) acts on \(w_1\) and \(w_2\), it doesn’t change them. But we still need one more vector. What should we choose for \(w_3\)? One candidate is the orthogonal vector to the place \(W\) which will mean that its projection is going to be zero. How do we find it? We can take the cross product of \(w_1\) and \(w_2\). \(w_3 = w_1 \times w_2 = (1,1,1)\) and \(T(w_3) = (0,0,0)\). $$ \begin{align*} [T]_{\beta'}^{\beta'} &amp;= \begin{pmatrix} [T(w_1)]_{\beta'} &amp; [T(w_2)]_{\beta'} &amp; [T(w_3)]_{\beta'} \end{pmatrix} \\ &amp;= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ Again we derived these by writing \(w_1\) as a linear combination of the basis vectors \(\beta'=\{w_1,w_2,w_3\}\). So \(w_1 = 1(w_1) + 0(w_2) + 0(w_3)\). and so the coordinate vector is \((1,0,0)\). But this is \(T\) with respect to \(\beta'\) and we want \(T\) with respect to \(\beta\). So now we need to use the change of basis formula to get what we want. So $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} [I_{\mathbf{R}^3}]_{\beta}^{\beta'} \end{align*} $$ So now we need to compute \([I_{\mathbf{R}^3}]_{\beta'}^{\beta}\). This means for each vector in \(\beta'\), we want to write it in terms of the standard basis. For example, \(w_1 = (1, -1, 0) = 1e_1 -1e_2 + 0e_3\) and so on. This will get us $$ \begin{align*} [I_{\mathbf{R}^3}]_{\beta'}^{\beta} = \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 1 \\ 0 &amp; -1 &amp; 1 \end{pmatrix} \end{align*} $$ But for \([I_{\mathbf{R}^3}]_{\beta}^{\beta'}\) is a little tricker since we want to write the vectors in \(\beta'\) as linear combinations of the standard basis which we could do but we can also do the following instead $$ \begin{align*} [I_{\mathbf{R}^3}]_{\beta}^{\beta'} = ([I_{\mathbf{R}^3}]_{\beta'}^{\beta})^{-1} = \frac{1}{3}\begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix} \end{align*} $$ Therefore, $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} [I_{\mathbf{R}^3}]_{\beta}^{\beta'} \\ &amp;= \frac{1}{3} \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 1 \\ 0 &amp; -1 &amp; 1 \end{pmatrix} \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix} \\ &amp;= \frac{1}{3}\begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix} \end{align*} $$ To verify this is correct, we should check that $$ \begin{align*} [T]_{\beta}^{\beta}[w_1]_{\beta} &amp;= [w_1]_{\beta}. \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 16: Inverse and Invertible Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/08/lec16-inverse-and-invertible-matrices.html" rel="alternate" type="text/html" title="Lecture 16: Inverse and Invertible Matrices" /><published>2024-08-08T01:01:36-07:00</published><updated>2024-08-08T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/08/lec16-inverse-and-invertible-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/08/lec16-inverse-and-invertible-matrices.html"><![CDATA[<p>Recall that \(\mathcal{L}(\mathbf{R}^n, \mathbf{R}^m) = \{L_A: A \in M_{m \times n}\}\). (The vector space of linear transformations)
<br />
<br />
Question: For what \(A \in M_{m \times n}\) does \(L_A\) have an inverse?
<br />
<br />
By Corollary of Theorem 2, we require \(n = m\). 
<br />
<br />
Moreover, consider the following map</p>
<div>
$$
\begin{align*}
&amp;L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\
&amp;\bar{x} \rightarrow A\bar{x}
\end{align*}
$$
</div>
<p>For this map to be invertible there must be a map</p>
<div>
$$
\begin{align*}
(L_A)^{-1}: \mathbf{R}^n \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>such that</p>
<div>
$$
\begin{align*}
(L_A)^{-1} \circ L_A = I_{\mathbf{R}^n} = L_A \circ (L_A)^{-1}
\end{align*}
$$
</div>
<p>This map is linear since the inverse of a linear map is linear. Since it is linear then we can represent it with a matrix so let \((L_A)^{-1} = L_B\) for some \(B \in M_{n \times n}\) and so</p>
<div>
$$
\begin{align*}
L_B \circ L_A &amp;= I_{\mathbf{R}^n} = L_A \circ L_B \\
L_{BA} &amp;= L_{I_n} = L_{AB} \\
BA &amp;= I_n = AB.
\end{align*}
$$
</div>
<p>Based on this, we have this definition</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is invertible if \(\exists B \in M_{n \times n}\) such that \(BA = I_n = AB\)
</div>
<p><br />
Remark: The inverse of \(A\) is unique if it exists.
<br />
Proof: Suppose \(BA = I_n = AB\) and \(CA = I_n = AC\). We need to show that \(C = B\). To do this,</p>
<div>
$$
\begin{align*}
CA &amp;= I_n \\
(CA)B &amp;= (I_n)B \\
C(AB) &amp;= B \\
C(I_n) &amp;= B \\
C &amp; B.
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Conditions for An Invertible Matrix</b></h4>
<p>So when is \(A\) invertible? 
<br />
<br />
\(A\) is invertible if and only if</p>
<ul>
	<li>\(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^n\) is invertible.</li>
	<li>\(L_A\) is 1-1 and onto. (you only need one as consequence of the dimension theorem since the dimension of the domain and the codomain are the same)</li>
	<li>\(L_A\) is 1-1. (see above or theorem 2.5)</li>
	<li>\(N(L_A) = \{\bar{0}\}\). The above is equivalent (proved in homework I think) to saying that the null space is only the zero vector.</li>
	<li>\(\{\bar{x} \ | \ A\bar{x} = \bar{0} = \{\bar{0}\}\). This is just the null space. We can check/settle this by putting the matrix in a row echelon form!</li>
	<li>a REF of \(A\) has leading entries in each column.</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation for an Inverse Linear Transformation</b></h4>
<p>Suppose \(T: V \rightarrow W\) is linear. Let \(\beta\) be a finite basis for \(V\) and \(\gamma\) be a finite basis for \(W\). 
<br />
<br />
We know that \(T\) has a matrix representative from \(\beta\) to \(\gamma\), \([T]_{\beta}^{\gamma}\). Therefore, the inverse of \(T\) will have a matrix representative instead from \(\gamma\) to \(\beta\), \([T]_{\gamma}^{\beta}\). We want to know what the relationship is between these two matrices.
<br />
<br />
Claim: If \(T: V \rightarrow W\) is invertible, then</p>
<div>
$$
\begin{align*}
[T^{-1}]_{\gamma}^{\beta} &amp;= ([T]_{\beta}^{\gamma})^{-1}
\end{align*}
$$
</div>
<p>To see this, let’s multiply both matrices. By defintion multiplying these matrices is the composition of the matrices from \(\beta\) to \(\beta\) (proved in the last lecture?)</p>
<div>
$$
\begin{align*}
[T^{-1}]_{\gamma}^{\beta}[T]^{\gamma}_{\beta} &amp;= [T^{-1} \circ T]_{\beta}^{\beta} \\
                              &amp;= [I_V]_{\beta}^{\beta} \\
							  &amp;= I_n
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation for an Inverse Linear Transformation</b></h4>
<p>If \(A\) is invertible, how do you find an its inverse?
<br />
<br />
We need \(B\) such that \(AB = I_n\)
<br />
<br />
We can think of \(AB\) as multiplying the columns of \(B\) by \(A\) and so</p>
<div>
$$
\begin{align*}
AB &amp;= I_n \\
A(\bar{b_1} ... \bar{b_n}) &amp;= (e_1 ... e_n) \\
A\bar{b_1} ... A\bar{b_n} &amp;= (e_1 ... e_n).
\end{align*}
$$
</div>
<p>This is equivalent to solving the following \(n\) equations in \(n\) variables.</p>
<div>
$$
\begin{align*}
Ab_1 &amp;= e_1 \\
... \\
Ab_n &amp;= e_n.
\end{align*}
$$
</div>
<p>We can solve this all at once because if you notice here, \(A\) is the same in every equation. We do this by grouping this in the following way</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
A &amp; | &amp; e_1 &amp; ... &amp; e_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>We then put this matrix in row reduced echelon form to get \((RREF(A) | B)\). Because we have \(n\) entries, then if \(RREF(A) = I_n\), then \(B = A^{-1}\). If \(RREF(A) \neq I_n\), then \(A\) is not invertible.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(A = \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{pmatrix}\). Find if \(A^{-1}\) exists.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\
3 &amp; 4 &amp; | &amp; 0 &amp; 1
\end{pmatrix}
R_2 \rightarrow -3R_1 + R_2
\begin{pmatrix}
1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}\\
\begin{pmatrix}
1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}
R_1 \rightarrow R_2 + R_1
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}\\
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\
0 &amp; -2 &amp; | &amp; -3 &amp; 1
\end{pmatrix}
R_2 \rightarrow -\frac{1}{2}R_2
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\
0 &amp; 1 &amp; | &amp; \frac{3}{2} &amp; -\frac{1}{2}
\end{pmatrix}\\
\end{align*}
$$
</div>
<p>From this we see that \(A\) is invertible and its inverse is \(\begin{pmatrix}
-2 &amp; 1 \\
\frac{3}{2} &amp; -\frac{1}{2}
\end{pmatrix}\)
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Special Rule for 2 by 2 Matrices</b></h4>
<p>In fact, any 2 by 2 matrix \(\begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix}\) is invertible if \(ad - bc = 0\). In which case</p>
<div>
$$
\begin{align*}
A^{-1} = \frac{1}{ad - bc}
\begin{pmatrix}
d &amp; -b \\
-c &amp; a
\end{pmatrix}
\end{align*}
$$
</div>
<p>Proof: We will show that this is true by using the same procedure from the last example. We will also assume that \(a \neq 0\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
c &amp; d &amp; | &amp; 0 &amp; 1
\end{pmatrix}
R_2 \rightarrow -\frac{c}{a}R_1 + R_2
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1
\end{pmatrix}\\
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1
\end{pmatrix}
R_2 \rightarrow \frac{1}{a}R_2
\begin{pmatrix}
a &amp; b &amp; | &amp; 1 &amp; 0 \\
0 &amp; ad - bc &amp; | &amp; -c &amp; a
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that if \(ad - bc = 0\), then the system for the last column is inconsistent and \(A\) has no inverse. We can proceed until we get to</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; | &amp; \frac{d}{ad - bc} &amp; \frac{-b}{ad - bc} \\
0 &amp; 1 &amp; | &amp; \frac{-c}{ad - bc} &amp; \frac{a}{ad - bc}
\end{pmatrix}
\end{align*}
$$
</div>
<p>And so this will be the inverse. We still need to settle the case when \(a = 0\). (Exercise)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recall that \(\mathcal{L}(\mathbf{R}^n, \mathbf{R}^m) = \{L_A: A \in M_{m \times n}\}\). (The vector space of linear transformations) Question: For what \(A \in M_{m \times n}\) does \(L_A\) have an inverse? By Corollary of Theorem 2, we require \(n = m\). Moreover, consider the following map $$ \begin{align*} &amp;L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\ &amp;\bar{x} \rightarrow A\bar{x} \end{align*} $$ For this map to be invertible there must be a map $$ \begin{align*} (L_A)^{-1}: \mathbf{R}^n \rightarrow \mathbf{R}^n \end{align*} $$ such that $$ \begin{align*} (L_A)^{-1} \circ L_A = I_{\mathbf{R}^n} = L_A \circ (L_A)^{-1} \end{align*} $$ This map is linear since the inverse of a linear map is linear. Since it is linear then we can represent it with a matrix so let \((L_A)^{-1} = L_B\) for some \(B \in M_{n \times n}\) and so $$ \begin{align*} L_B \circ L_A &amp;= I_{\mathbf{R}^n} = L_A \circ L_B \\ L_{BA} &amp;= L_{I_n} = L_{AB} \\ BA &amp;= I_n = AB. \end{align*} $$ Based on this, we have this definition Definition \(A \in M_{n \times n}\) is invertible if \(\exists B \in M_{n \times n}\) such that \(BA = I_n = AB\) Remark: The inverse of \(A\) is unique if it exists. Proof: Suppose \(BA = I_n = AB\) and \(CA = I_n = AC\). We need to show that \(C = B\). To do this, $$ \begin{align*} CA &amp;= I_n \\ (CA)B &amp;= (I_n)B \\ C(AB) &amp;= B \\ C(I_n) &amp;= B \\ C &amp; B. \end{align*} $$ Conditions for An Invertible Matrix So when is \(A\) invertible? \(A\) is invertible if and only if \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^n\) is invertible. \(L_A\) is 1-1 and onto. (you only need one as consequence of the dimension theorem since the dimension of the domain and the codomain are the same) \(L_A\) is 1-1. (see above or theorem 2.5) \(N(L_A) = \{\bar{0}\}\). The above is equivalent (proved in homework I think) to saying that the null space is only the zero vector. \(\{\bar{x} \ | \ A\bar{x} = \bar{0} = \{\bar{0}\}\). This is just the null space. We can check/settle this by putting the matrix in a row echelon form! a REF of \(A\) has leading entries in each column. Matrix Representation for an Inverse Linear Transformation Suppose \(T: V \rightarrow W\) is linear. Let \(\beta\) be a finite basis for \(V\) and \(\gamma\) be a finite basis for \(W\). We know that \(T\) has a matrix representative from \(\beta\) to \(\gamma\), \([T]_{\beta}^{\gamma}\). Therefore, the inverse of \(T\) will have a matrix representative instead from \(\gamma\) to \(\beta\), \([T]_{\gamma}^{\beta}\). We want to know what the relationship is between these two matrices. Claim: If \(T: V \rightarrow W\) is invertible, then $$ \begin{align*} [T^{-1}]_{\gamma}^{\beta} &amp;= ([T]_{\beta}^{\gamma})^{-1} \end{align*} $$ To see this, let’s multiply both matrices. By defintion multiplying these matrices is the composition of the matrices from \(\beta\) to \(\beta\) (proved in the last lecture?) $$ \begin{align*} [T^{-1}]_{\gamma}^{\beta}[T]^{\gamma}_{\beta} &amp;= [T^{-1} \circ T]_{\beta}^{\beta} \\ &amp;= [I_V]_{\beta}^{\beta} \\ &amp;= I_n \end{align*} $$ Matrix Representation for an Inverse Linear Transformation If \(A\) is invertible, how do you find an its inverse? We need \(B\) such that \(AB = I_n\) We can think of \(AB\) as multiplying the columns of \(B\) by \(A\) and so $$ \begin{align*} AB &amp;= I_n \\ A(\bar{b_1} ... \bar{b_n}) &amp;= (e_1 ... e_n) \\ A\bar{b_1} ... A\bar{b_n} &amp;= (e_1 ... e_n). \end{align*} $$ This is equivalent to solving the following \(n\) equations in \(n\) variables. $$ \begin{align*} Ab_1 &amp;= e_1 \\ ... \\ Ab_n &amp;= e_n. \end{align*} $$ We can solve this all at once because if you notice here, \(A\) is the same in every equation. We do this by grouping this in the following way $$ \begin{align*} \begin{pmatrix} A &amp; | &amp; e_1 &amp; ... &amp; e_n \end{pmatrix} \end{align*} $$ We then put this matrix in row reduced echelon form to get \((RREF(A) | B)\). Because we have \(n\) entries, then if \(RREF(A) = I_n\), then \(B = A^{-1}\). If \(RREF(A) \neq I_n\), then \(A\) is not invertible. Example Let \(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\). Find if \(A^{-1}\) exists. $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\ 3 &amp; 4 &amp; | &amp; 0 &amp; 1 \end{pmatrix} R_2 \rightarrow -3R_1 + R_2 \begin{pmatrix} 1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix}\\ \begin{pmatrix} 1 &amp; 2 &amp; | &amp; 1 &amp; 0 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix} R_1 \rightarrow R_2 + R_1 \begin{pmatrix} 1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix}\\ \begin{pmatrix} 1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\ 0 &amp; -2 &amp; | &amp; -3 &amp; 1 \end{pmatrix} R_2 \rightarrow -\frac{1}{2}R_2 \begin{pmatrix} 1 &amp; 0 &amp; | &amp; -2 &amp; 1 \\ 0 &amp; 1 &amp; | &amp; \frac{3}{2} &amp; -\frac{1}{2} \end{pmatrix}\\ \end{align*} $$ From this we see that \(A\) is invertible and its inverse is \(\begin{pmatrix} -2 &amp; 1 \\ \frac{3}{2} &amp; -\frac{1}{2} \end{pmatrix}\) Special Rule for 2 by 2 Matrices In fact, any 2 by 2 matrix \(\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}\) is invertible if \(ad - bc = 0\). In which case $$ \begin{align*} A^{-1} = \frac{1}{ad - bc} \begin{pmatrix} d &amp; -b \\ -c &amp; a \end{pmatrix} \end{align*} $$ Proof: We will show that this is true by using the same procedure from the last example. We will also assume that \(a \neq 0\). $$ \begin{align*} \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ c &amp; d &amp; | &amp; 0 &amp; 1 \end{pmatrix} R_2 \rightarrow -\frac{c}{a}R_1 + R_2 \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ 0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1 \end{pmatrix}\\ \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ 0 &amp; d - \frac{bc}{a} &amp; | &amp; -\frac{c}{a} &amp; 1 \end{pmatrix} R_2 \rightarrow \frac{1}{a}R_2 \begin{pmatrix} a &amp; b &amp; | &amp; 1 &amp; 0 \\ 0 &amp; ad - bc &amp; | &amp; -c &amp; a \end{pmatrix} \end{align*} $$ Note here that if \(ad - bc = 0\), then the system for the last column is inconsistent and \(A\) has no inverse. We can proceed until we get to $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; | &amp; \frac{d}{ad - bc} &amp; \frac{-b}{ad - bc} \\ 0 &amp; 1 &amp; | &amp; \frac{-c}{ad - bc} &amp; \frac{a}{ad - bc} \end{pmatrix} \end{align*} $$ And so this will be the inverse. We still need to settle the case when \(a = 0\). (Exercise) References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 15: Inverse and Invertible Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps.html" rel="alternate" type="text/html" title="Lecture 15: Inverse and Invertible Linear Transformations" /><published>2024-08-07T01:01:36-07:00</published><updated>2024-08-07T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/07/lec15-inverse-and-invertible-linear-maps.html"><![CDATA[<h4><b>Identity matrix and Kronecker delta</b></h4>
<div>
$$
\begin{align*}
I_n &amp;= 
\begin{pmatrix}
1 &amp; 0 &amp; \dotsb &amp; 0 \\
0 &amp; 1 &amp; \dotsb &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} \in M_{n \times n}.
\end{align*}
$$
</div>
<p>\(I_n\) is the \(n \times n\) identity matrix. To construct the matrix, we have the following rule.</p>
<div>
$$
 \begin{equation*}
(I)_{ij} = \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j \end{cases}
 \end{equation*}
$$
</div>
<p>\(\delta_{ij}\) is the Kronecker delta. For any matrix \(A \in M_{m \times n}\), \(AI_n = A\) and \(I_mA = A\). 
<br />
Exercise: Suppose we have a finite basis \(\beta\) for \(V\), then the identity map is</p>
<div>
$$
\begin{align*}
I: \ &amp;V \rightarrow V \\
      &amp;x \rightarrow x
\end{align*}
$$
</div>
<p>If we compute its matrix representative, we will see that \([I_V]_{\beta}^{\beta} = I_n\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Inverse Linear Transformation</b></h4>
<p>Next we define the inverse of a linear transformation</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inverse of \(T: V \rightarrow W\) is a map \(S: W \rightarrow V\) such that
$$
\begin{align*}
S \circ T = I_V \text{ and } T \circ S = I_W
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div>
$$
\begin{align*}
T : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
    &amp;(x, y) \rightarrow (y, -x)         
\end{align*}
$$
</div>
<p>has an inverse</p>
<div>
$$
\begin{align*}
S : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
    &amp;(x, y) \rightarrow (-y, x)         
\end{align*}
$$
</div>
<p>We can check that this is true by composing these transformations and check that we get the identity map.</p>
<div>
$$
\begin{align*}
S \circ T (x,y) &amp;= S(T(x, y)) = S(y, -x) = (x, y) \\
T \circ S (x,y) &amp;= T(S(x, y)) = T(-y, x) = (x, y) 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose we have the following vector spaces:</p>
<div>
$$
\begin{align*}
V &amp;= P \\
W &amp;= \hat{P} = \{a_1x + a_2x^2 ... + a_kx^k\}
\end{align*}
$$
</div>
<p>\(\hat{P}\) is the set of all polynomials without the constant term \(a_0\). We claim that \(\hat{P}\) is a vector space. The easiest way to verify this to show that \(\hat{P}\) is a subspace of \(P\). Now consider the following linear map between \(P\) to \(\hat{P}\) where we multiply the polynomial by \(x\).</p>
<div>
$$
\begin{align*}
T : \ &amp;P \rightarrow \hat{P}  \\
&amp;a_0 + a_1x + ... + a_kx^k \rightarrow a_0x + a_1x^2 + ... + a_kx^{k+1} \\
&amp;f \rightarrow xf
\end{align*}
$$
</div>
<p>This map has the following inverse linear transformation:</p>
<div>
$$
\begin{align*}
S &amp;: \hat{P} \rightarrow P  \\
&amp;a_1x + a_2x^2 + ... + a_kx^{k}  \rightarrow a_1 + a_2x + ... + a_kx^{k-1}  \\
&amp;f \rightarrow \frac{1}{x}f
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>The following linear transformation</p>
<div>
$$
\begin{align*}
T &amp;: P \rightarrow P  \\
&amp;f \rightarrow f'
\end{align*}
$$
</div>
<p>has no inverse! (it is onto but not 1-1). Not one-to-one since the derivative of any constant function is 0.
<br />
<br />
<!------------------------------------------------------------------------------------>
This leads us to the following theorem:</p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be linear.
<ul>
	<li>\(T\) has an inverse if and only if \(T\) is 1-1 (\(N(T)=\{\bar{0}_V\}\)) and onto (\(R(T) = W\)).</li>
	<li>If \(T\) has an inverse, then it is unique \((T^{-1})\).</li>
	<li>\(T^{-1}\) is linear.</li>
</ul>
</div>
<p><br />
The third property is not obvious and requires a proof.
<br />
<br />
<b>Proof:</b> Let \(T: V \rightarrow W\) be a linear map with inverse \(T^{-1}: W \rightarrow V\). We want to show that \(T^{-1}\) is linear. To do this, we need to show that</p>
<div>
$$
\begin{align*}
T^{-1}(w_1 + cw_2) = T^{-1}(w_1) + cT^{-1}(w_2).
\end{align*}
$$
</div>
<p>Because \(T\) has an inverse then \(T\) is onto. This means that \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1, v_2 \in V\). Moreover, since \(T\) is 1-1, then \(v_1 \neq v_2\). Now,</p>
<div>
$$
\begin{align*}
T^{-1}(w_1 + cw_2) &amp;= T^{-1}(T(v_1) + cT(v_2)) \\
 &amp;= T^{-1}(T(v_1 + cv_2)) \text{ (because $T$ is linear!)} \\
  &amp;= I(v_1 + cv_2) \text{ (because $T$ has an inverse!)} \\
  &amp;= v_1 + cv_2 \\
  &amp; = T^{-1}(w_1) + c T^{-1}(w_2)
\end{align*}
$$
</div>
<p>Therefore, \(T^{-1}\) is linear. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow W\) be linear and invertible. If \(\beta\) is a basis for \(V\), then \(T(\beta)\) is a basis for \(W\).
</div>
<p><br />
Proof (in the case where \(V\) and \(W\) are finite dimensional spaces):<br />
Let \(T: V \rightarrow W\) be a linear and invertible map and suppose that \(\dim(V)=n\). Choose a basis \(\beta = \{v_1, ..., v_n\}\) for \(V\). Consider the set of images,</p>
<div>
$$
\begin{align*}
T(\beta) = \{T(v_1),...,T(v_n)\}.
\end{align*}
$$
</div>
<p>We need to show that this set is a basis which means that we need to show that it spans \(W\) and so \(Span(T(\beta)) = W\) and that it is a linearly independent set. To show that it spans \(W\), we need for any \(w \in W\), scalars \(a_1,...,a_n\) such that</p>
<div>
$$
\begin{align*}
w = a_1T(v_1) + ... + a_nT(v_n).
\end{align*}
$$
</div>
<p>But we know that \(w = T(v)\) for some \(v \in V\) since \(T\) is onto. We also know that \(\beta\) is a basis for \(V\) and so we can write \(v\) as a linear combination of the vectors in \(\beta\) for some scalars \(a_1,...,a_n\).</p>
<div>
$$
\begin{align*}
v = a_1v_1 + ... + a_nv_n.
\end{align*}
$$
</div>
<p>And now because \(T\) is linear, we can do the following</p>
<div>
$$
\begin{align*}
w &amp;= T(v) \\
  &amp;= T(a_1v_1 + ... + a_nv_n) \\
  &amp; = a_1T(v_1) + ... + a_nT(v_n).
\end{align*}
$$
</div>
<p>Which is what we wanted to show. To show that the vectors in \(T(\beta)\) are linearly independent, we need to show that the solution to</p>
<div>
$$
\begin{align*}
a_1T(v_1) + ... + a_nT(v_n) = \bar{0}_W.
\end{align*}
$$
</div>
<p>is the trivial solution which means that \(a_1=0,..,a_n=0\). We can use the linearity of \(T\) to show that</p>
<div>
$$
\begin{align*}
\bar{0}_W &amp;= a_1T(v_1) + ... + a_nT(v_n) \\
 &amp;= T(a_1v_1 + ... + a_nv_n).
\end{align*}
$$
</div>
<p>But \(T\) is 1-1 and so this implies that \(a_1v_1 + ... + a_nv_n\) must be \(\bar{0}_V\). Therefore, we must have \(a_1=0,...,a_n=0\) as required.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be linear. If \(\dim(V) = n\) and \(T\) is invertible, then 
$$
\begin{align*}
\dim W = \dim V = n
\end{align*}
$$
</div>
<p><br />
(Study Notes:) This is really important. \(T\) is invertible and \(V\) having dimension \(n\) means that \(W\) has dimension \(n\). Also \(T\) is one-to-one and onto. Later we’ll learn if both \(V\) and \(W\) have the same dimension and \(T\) is one-to-one, then this is sufficient to conclude that \(T\) is invertible. (next lecture)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Isomorphism</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(V\) and \(W\) are isomorphic if there is an invertible linear map \(T: V \rightarrow W\). Such a map \(T\) is called isomorphism.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>\(\mathbf{R}^3\) and \(P_2\) are isomorphic. To see this, we need an invertible map from one to the other. The maps</p>
<div>
$$
\begin{align*}
&amp;T : \mathbf{R}^3 \rightarrow P_2  \\
&amp;(a_1,a_2,a_3) \rightarrow a_1 + a_2x + a_3x^2
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
&amp;U : \mathbf{R}^3 \rightarrow P_2  \\
&amp;(a_1,a_2,a_3) \rightarrow a_3 + (a_1 + a_2)x + (a_1 + a_2)x^2
\end{align*}
$$
</div>
<p>are both isomorphisms.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<div>
$$
\begin{align*}
&amp;U : P \rightarrow \hat{P}  \\
&amp;f \rightarrow xf
\end{align*}
$$
</div>
<p>is an isomorphism and so \(P\) and \(\hat{P}\) are isomorphic.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Criterion for Isomorphic Finite Dimensional Vector Spaces</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) is finite dimensional, then \(W\) is isomorphic to \(V\) if and only if \(\dim W = \dim V\).
</div>
<p><br />
Proof: 
\(\Leftarrow\): Suppose that \(\dim V = \dim W\). We want to show that they are isomorphic. This means that there exists some invertible map from one to the other. So let \(\beta = \{v_1,...,v_n\}\) be a basis for \(V\) and \(\alpha = \{w_1, ...,w_n\}\) be a basis for \(W\).
<br />
<br />
Define the map \(T: W \rightarrow V\) by \([T]_{\alpha}^{\beta} = I_n\). This \(T\) works. Why?</p>
<div>
$$
\begin{align*}
v_i &amp;= T(w_i) \\
       &amp;= T(a_1w_1 + ... + a_nw_n) \\
	   &amp;= a_1v_1 + ... + a_nv_n.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Identity matrix and Kronecker delta $$ \begin{align*} I_n &amp;= \begin{pmatrix} 1 &amp; 0 &amp; \dotsb &amp; 0 \\ 0 &amp; 1 &amp; \dotsb &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} \in M_{n \times n}. \end{align*} $$ \(I_n\) is the \(n \times n\) identity matrix. To construct the matrix, we have the following rule. $$ \begin{equation*} (I)_{ij} = \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j \end{cases} \end{equation*} $$ \(\delta_{ij}\) is the Kronecker delta. For any matrix \(A \in M_{m \times n}\), \(AI_n = A\) and \(I_mA = A\). Exercise: Suppose we have a finite basis \(\beta\) for \(V\), then the identity map is $$ \begin{align*} I: \ &amp;V \rightarrow V \\ &amp;x \rightarrow x \end{align*} $$ If we compute its matrix representative, we will see that \([I_V]_{\beta}^{\beta} = I_n\). Inverse Linear Transformation Next we define the inverse of a linear transformation Definition An inverse of \(T: V \rightarrow W\) is a map \(S: W \rightarrow V\) such that $$ \begin{align*} S \circ T = I_V \text{ and } T \circ S = I_W \end{align*} $$ Example 1 $$ \begin{align*} T : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (y, -x) \end{align*} $$ has an inverse $$ \begin{align*} S : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (-y, x) \end{align*} $$ We can check that this is true by composing these transformations and check that we get the identity map. $$ \begin{align*} S \circ T (x,y) &amp;= S(T(x, y)) = S(y, -x) = (x, y) \\ T \circ S (x,y) &amp;= T(S(x, y)) = T(-y, x) = (x, y) \end{align*} $$ Example 2 Suppose we have the following vector spaces: $$ \begin{align*} V &amp;= P \\ W &amp;= \hat{P} = \{a_1x + a_2x^2 ... + a_kx^k\} \end{align*} $$ \(\hat{P}\) is the set of all polynomials without the constant term \(a_0\). We claim that \(\hat{P}\) is a vector space. The easiest way to verify this to show that \(\hat{P}\) is a subspace of \(P\). Now consider the following linear map between \(P\) to \(\hat{P}\) where we multiply the polynomial by \(x\). $$ \begin{align*} T : \ &amp;P \rightarrow \hat{P} \\ &amp;a_0 + a_1x + ... + a_kx^k \rightarrow a_0x + a_1x^2 + ... + a_kx^{k+1} \\ &amp;f \rightarrow xf \end{align*} $$ This map has the following inverse linear transformation: $$ \begin{align*} S &amp;: \hat{P} \rightarrow P \\ &amp;a_1x + a_2x^2 + ... + a_kx^{k} \rightarrow a_1 + a_2x + ... + a_kx^{k-1} \\ &amp;f \rightarrow \frac{1}{x}f \end{align*} $$ Example 3 The following linear transformation $$ \begin{align*} T &amp;: P \rightarrow P \\ &amp;f \rightarrow f' \end{align*} $$ has no inverse! (it is onto but not 1-1). Not one-to-one since the derivative of any constant function is 0. This leads us to the following theorem: Theorem Let \(T: V \rightarrow W\) be linear. \(T\) has an inverse if and only if \(T\) is 1-1 (\(N(T)=\{\bar{0}_V\}\)) and onto (\(R(T) = W\)). If \(T\) has an inverse, then it is unique \((T^{-1})\). \(T^{-1}\) is linear. The third property is not obvious and requires a proof. Proof: Let \(T: V \rightarrow W\) be a linear map with inverse \(T^{-1}: W \rightarrow V\). We want to show that \(T^{-1}\) is linear. To do this, we need to show that $$ \begin{align*} T^{-1}(w_1 + cw_2) = T^{-1}(w_1) + cT^{-1}(w_2). \end{align*} $$ Because \(T\) has an inverse then \(T\) is onto. This means that \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1, v_2 \in V\). Moreover, since \(T\) is 1-1, then \(v_1 \neq v_2\). Now, $$ \begin{align*} T^{-1}(w_1 + cw_2) &amp;= T^{-1}(T(v_1) + cT(v_2)) \\ &amp;= T^{-1}(T(v_1 + cv_2)) \text{ (because $T$ is linear!)} \\ &amp;= I(v_1 + cv_2) \text{ (because $T$ has an inverse!)} \\ &amp;= v_1 + cv_2 \\ &amp; = T^{-1}(w_1) + c T^{-1}(w_2) \end{align*} $$ Therefore, \(T^{-1}\) is linear. \(\blacksquare\) Theorem Suppose \(T: V \rightarrow W\) be linear and invertible. If \(\beta\) is a basis for \(V\), then \(T(\beta)\) is a basis for \(W\). Proof (in the case where \(V\) and \(W\) are finite dimensional spaces): Let \(T: V \rightarrow W\) be a linear and invertible map and suppose that \(\dim(V)=n\). Choose a basis \(\beta = \{v_1, ..., v_n\}\) for \(V\). Consider the set of images, $$ \begin{align*} T(\beta) = \{T(v_1),...,T(v_n)\}. \end{align*} $$ We need to show that this set is a basis which means that we need to show that it spans \(W\) and so \(Span(T(\beta)) = W\) and that it is a linearly independent set. To show that it spans \(W\), we need for any \(w \in W\), scalars \(a_1,...,a_n\) such that $$ \begin{align*} w = a_1T(v_1) + ... + a_nT(v_n). \end{align*} $$ But we know that \(w = T(v)\) for some \(v \in V\) since \(T\) is onto. We also know that \(\beta\) is a basis for \(V\) and so we can write \(v\) as a linear combination of the vectors in \(\beta\) for some scalars \(a_1,...,a_n\). $$ \begin{align*} v = a_1v_1 + ... + a_nv_n. \end{align*} $$ And now because \(T\) is linear, we can do the following $$ \begin{align*} w &amp;= T(v) \\ &amp;= T(a_1v_1 + ... + a_nv_n) \\ &amp; = a_1T(v_1) + ... + a_nT(v_n). \end{align*} $$ Which is what we wanted to show. To show that the vectors in \(T(\beta)\) are linearly independent, we need to show that the solution to $$ \begin{align*} a_1T(v_1) + ... + a_nT(v_n) = \bar{0}_W. \end{align*} $$ is the trivial solution which means that \(a_1=0,..,a_n=0\). We can use the linearity of \(T\) to show that $$ \begin{align*} \bar{0}_W &amp;= a_1T(v_1) + ... + a_nT(v_n) \\ &amp;= T(a_1v_1 + ... + a_nv_n). \end{align*} $$ But \(T\) is 1-1 and so this implies that \(a_1v_1 + ... + a_nv_n\) must be \(\bar{0}_V\). Therefore, we must have \(a_1=0,...,a_n=0\) as required. Corollary Let \(T: V \rightarrow W\) be linear. If \(\dim(V) = n\) and \(T\) is invertible, then $$ \begin{align*} \dim W = \dim V = n \end{align*} $$ (Study Notes:) This is really important. \(T\) is invertible and \(V\) having dimension \(n\) means that \(W\) has dimension \(n\). Also \(T\) is one-to-one and onto. Later we’ll learn if both \(V\) and \(W\) have the same dimension and \(T\) is one-to-one, then this is sufficient to conclude that \(T\) is invertible. (next lecture) Isomorphism Definition \(V\) and \(W\) are isomorphic if there is an invertible linear map \(T: V \rightarrow W\). Such a map \(T\) is called isomorphism. Example 4 \(\mathbf{R}^3\) and \(P_2\) are isomorphic. To see this, we need an invertible map from one to the other. The maps $$ \begin{align*} &amp;T : \mathbf{R}^3 \rightarrow P_2 \\ &amp;(a_1,a_2,a_3) \rightarrow a_1 + a_2x + a_3x^2 \end{align*} $$ and $$ \begin{align*} &amp;U : \mathbf{R}^3 \rightarrow P_2 \\ &amp;(a_1,a_2,a_3) \rightarrow a_3 + (a_1 + a_2)x + (a_1 + a_2)x^2 \end{align*} $$ are both isomorphisms. Example 5 $$ \begin{align*} &amp;U : P \rightarrow \hat{P} \\ &amp;f \rightarrow xf \end{align*} $$ is an isomorphism and so \(P\) and \(\hat{P}\) are isomorphic. Criterion for Isomorphic Finite Dimensional Vector Spaces Theorem If \(V\) is finite dimensional, then \(W\) is isomorphic to \(V\) if and only if \(\dim W = \dim V\). Proof: \(\Leftarrow\): Suppose that \(\dim V = \dim W\). We want to show that they are isomorphic. This means that there exists some invertible map from one to the other. So let \(\beta = \{v_1,...,v_n\}\) be a basis for \(V\) and \(\alpha = \{w_1, ...,w_n\}\) be a basis for \(W\). Define the map \(T: W \rightarrow V\) by \([T]_{\alpha}^{\beta} = I_n\). This \(T\) works. Why? $$ \begin{align*} v_i &amp;= T(w_i) \\ &amp;= T(a_1w_1 + ... + a_nw_n) \\ &amp;= a_1v_1 + ... + a_nv_n. \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 14: Matrix Representation of Composition and Matrix Multiplication</title><link href="http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication.html" rel="alternate" type="text/html" title="Lecture 14: Matrix Representation of Composition and Matrix Multiplication" /><published>2024-08-06T01:01:36-07:00</published><updated>2024-08-06T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/06/lec14-composition-matrix-multiplication.html"><![CDATA[<h4><b>The Vector Space of Linear Transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given vector spaces \(V, W\) define
$$
\begin{align*}
\mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}.
\end{align*}
$$
</div>
<p><br />
FACT: The set of linear transformations \(\mathcal{L}(V, W)\) is a vector space. We can think of this as a way to get a new vector space from two vector spaces \(W\) and \(V\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Composition of Linear Transformations</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition
$$
\begin{align*}
T \circ S: X &amp;\rightarrow Z \\
x &amp;\rightarrow T(S(x))
\end{align*}
$$
</div>
<p><br />
Next, we will show that the composition of two linear transformations is also linear!
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) 
</div>
<p><br />
Proof: We want to show that</p>
<div>
$$
\begin{align*}
(T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2).	 
\end{align*}
$$
</div>
<p>To see expand the left hand side as follows:</p>
<div>
$$
\begin{align*}
(T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\
                     &amp;= T(S(x_1) + cS(x_2)) \text{ (because $S$ is linear)} \\
                     &amp;= T(S(x_1)) + cT(S(x_2)) \text{ (because $T$ is linear)} \\
                     &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2). \ \blacksquare			 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Matrix of Linear Composition</b></h4>
<p>Suppose now that \(X\), \(Y\) and \(Z\) are finite dimensional with fixed bases \(\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}\) and \(\gamma = \{z_1,...,z_n\}\).
<br />
<br />
How are \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related?
<br />
<br />
To answer this question, we need to define matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Multiplication</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by
$$
\begin{align*}
(AB)_{ij} = \sum_k^n A_{ik}B_{kj}.
\end{align*}
$$
Alternatively,
$$
\begin{align*}
AB &amp;= A \begin{pmatrix} | &amp;  &amp; | \\ \bar{b}_1 &amp; ... &amp; \bar{b}_p \\ | &amp;  &amp; |  \end{pmatrix}
   = \begin{pmatrix} | &amp;  &amp; | \\ A\bar{b}_1 &amp; ... &amp; A\bar{b}_p \\ | &amp;  &amp; |  \end{pmatrix} .
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<div>
$$
\begin{align*}
A =
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix},
B =
\begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}.
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
A\bar{b}_1 &amp;= 
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}
*
\begin{pmatrix} 1 \\ 1 \\ 2 \\ \end{pmatrix} = 1
\begin{pmatrix} 1  \\ 4  \end{pmatrix}
+ 1
\begin{pmatrix} 2  \\ 5  \end{pmatrix}
+ 
\begin{pmatrix} 3 \\ 6 \end{pmatrix}
=
\begin{pmatrix} 9 \\ 21 \end{pmatrix}
\\
A\bar{b}_1 &amp;= 
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}
*
\begin{pmatrix} 2 \\ 2 \\ 1 \\ \end{pmatrix} = 
\begin{pmatrix} 9 \\ 24 \end{pmatrix} \\
AB &amp;= 
\begin{pmatrix} 9 &amp; 9 \\ 21 &amp; 24 \end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Composition Using Matrix Multiplication</b></h4>
<p>Now that we defined matrix multiplication, we are ready to answer the question of how \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related.</p>
<div class="purdiv">
Theorem 2.11
</div>
<div class="purbdiv">
Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} = [T]^{\gamma}_{\beta} [S]^{\beta}_{\alpha}
\end{align*}
$$
</div>
<p><br />
In other words, the composition of the linear transformations \(S\) and \(T\) is equal to the matrix multiplication of the two matrices representing these linear transformations.
<br /></p>
<h4><b>Proof</b></h4>
<p>Fix the basis \(\alpha\) such that \(\alpha = \{x_1,...,x_n\}\). Then,</p>
<div>
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\
&amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\
&amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by theorem 2.14 (lecture 13))}\\
&amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\
&amp;= [T]^{\gamma}_{\beta} \circ [S]^{\beta}_{\alpha}. \blacksquare					 
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<div>
$$
\begin{align*}
A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\
B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p
\end{align*}
$$
</div>
<p>If we choose the standard basis for all vector spaces, then the theorem tells us</p>
<div>
$$
\begin{align*}
[L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]^{\gamma}_{\beta} \circ [L_A]^{\beta}_{\alpha}.
\end{align*}
$$
</div>
<p>But since we chose the standard basis then we know that \([L_A]^{\beta}_{\alpha} = A\) and \([L_B]^{\gamma}_{\beta} = B\). So we can also write</p>
<div>
$$
\begin{align*}
[L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]^{\gamma}_{\beta} \circ [L_A]^{\beta}_{\alpha} = BA = [L_{BA}]^{\gamma}_{\alpha}.
\end{align*}
$$
</div>
<p>In particular, this shows that these maps are equal \(L_B \circ L_A = L_{BA}\).</p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let</p>
<div>
$$
\begin{align*}
T_d: P_3 &amp;\rightarrow P_2 \\
f &amp;\rightarrow f'			 
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
T_i: P_2 &amp;\rightarrow P_3 \\
f &amp;\rightarrow \int_0^x f(t)dt			 
\end{align*}
$$
</div>
<p>For standard bases \(\beta = \{1,x,x^2, x^3\}\) of \(P_3\) and \(\gamma = \{1, x, x^2\}\) of \(P_2\). 
<br />
<br />
Extra notes: As a reminder, to find the matrix representative of of \(T_d\), we first apply the transformation on the vectors of \(\beta\).</p>
<div>
$$
\begin{align*}
T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. 		 
\end{align*}
$$
</div>
<p>and then we find the coordinates of these images with respect to \(\gamma\) which will be the column vectors of the matrix,</p>
<div>
$$
\begin{align*}
T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\
T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\
T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\
T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2).
\end{align*}
$$
</div>
<p>Therefore, \(T_d\) and \(T_i\) (can be found using the same method) are:</p>
<div>
$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}, [T_i]_{\gamma}^{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1/2 &amp; 0 \\
0 &amp; 0 &amp; 1/3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>To compose these matrices we just multiply them,</p>
<div>
$$
\begin{align*}
[T_i]_{\gamma}^{\beta}[T_d]^{\gamma}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>To verify, want to compute \([T_i \circ T_d]^{\beta}_{\beta}\) but instead of applying the composition on each vector. Let’s just apply it on a general object. In this case we need an object from \(P_3\):</p>
<div>
$$
\begin{align*}
(T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(T_d(a_0 + a_1x + a_2x^2 + a_3x^3)) \\
                                              &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\
                        &amp;= a_1x + a_2x^2 + a_3x^3
\end{align*}
$$
</div>
<p>So now the first column of this matrix should be the image of the first vector in the basis \(\beta\) so the image of \(1\) and that’s simply all zeros. For \(x\), we see \(a_1x\) just got mapped to itself again in the final equation. Similarly we get the same for the last two basis vectors so</p>
<div>
$$
\begin{align*}
[T_i \circ T_d]^{\beta}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} = 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Multiplication Properties</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
<ol style="list-style-type:lower-alpha">
	<li>\(A(BC) = (AB)C\)</li>
	<li>\(A(B+C) = AB + AC\)</li>
	<li>Not commutative. In general \(AB \neq BA\)</li>
</ol>
</div>
<p><br />
<b>Proof (b):</b>
<br /> 
As a reminder we know that \((AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}\). We also know that \(B+C\) is just summing the matching coordinates from each matrix, \(B_{ij} + C_{ij}\). Now, expand \(A(B+C)\).</p>
<div>
$$
\begin{align*}
(A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\
             &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\
			 &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\
			 &amp;= (AB)_{ij} + (AC)_{ij} \\
			 &amp;= (AB + AC)_{ij} \\
			 &amp;= AB + AC
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The Vector Space of Linear Transformations Definition Given vector spaces \(V, W\) define $$ \begin{align*} \mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}. \end{align*} $$ FACT: The set of linear transformations \(\mathcal{L}(V, W)\) is a vector space. We can think of this as a way to get a new vector space from two vector spaces \(W\) and \(V\). The Composition of Linear Transformations Definition Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition $$ \begin{align*} T \circ S: X &amp;\rightarrow Z \\ x &amp;\rightarrow T(S(x)) \end{align*} $$ Next, we will show that the composition of two linear transformations is also linear! Theorem \(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) Proof: We want to show that $$ \begin{align*} (T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2). \end{align*} $$ To see expand the left hand side as follows: $$ \begin{align*} (T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\ &amp;= T(S(x_1) + cS(x_2)) \text{ (because $S$ is linear)} \\ &amp;= T(S(x_1)) + cT(S(x_2)) \text{ (because $T$ is linear)} \\ &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2). \ \blacksquare \end{align*} $$ The Matrix of Linear Composition Suppose now that \(X\), \(Y\) and \(Z\) are finite dimensional with fixed bases \(\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}\) and \(\gamma = \{z_1,...,z_n\}\). How are \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related? To answer this question, we need to define matrix multiplication. Matrix Multiplication Definition Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by $$ \begin{align*} (AB)_{ij} = \sum_k^n A_{ik}B_{kj}. \end{align*} $$ Alternatively, $$ \begin{align*} AB &amp;= A \begin{pmatrix} | &amp; &amp; | \\ \bar{b}_1 &amp; ... &amp; \bar{b}_p \\ | &amp; &amp; | \end{pmatrix} = \begin{pmatrix} | &amp; &amp; | \\ A\bar{b}_1 &amp; ... &amp; A\bar{b}_p \\ | &amp; &amp; | \end{pmatrix} . \end{align*} $$ Example $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}. \end{align*} $$ $$ \begin{align*} A\bar{b}_1 &amp;= \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} * \begin{pmatrix} 1 \\ 1 \\ 2 \\ \end{pmatrix} = 1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + \begin{pmatrix} 3 \\ 6 \end{pmatrix} = \begin{pmatrix} 9 \\ 21 \end{pmatrix} \\ A\bar{b}_1 &amp;= \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} * \begin{pmatrix} 2 \\ 2 \\ 1 \\ \end{pmatrix} = \begin{pmatrix} 9 \\ 24 \end{pmatrix} \\ AB &amp;= \begin{pmatrix} 9 &amp; 9 \\ 21 &amp; 24 \end{pmatrix} \end{align*} $$ Matrix Composition Using Matrix Multiplication Now that we defined matrix multiplication, we are ready to answer the question of how \([S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}\) and \([T \circ S]_{\alpha}^{\gamma}\) related. Theorem 2.11 Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then $$ \begin{align*} [T \circ S]_{\alpha}^{\gamma} = [T]^{\gamma}_{\beta} [S]^{\beta}_{\alpha} \end{align*} $$ In other words, the composition of the linear transformations \(S\) and \(T\) is equal to the matrix multiplication of the two matrices representing these linear transformations. Proof Fix the basis \(\alpha\) such that \(\alpha = \{x_1,...,x_n\}\). Then, $$ \begin{align*} [T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\ &amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\ &amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by theorem 2.14 (lecture 13))}\\ &amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\ &amp;= [T]^{\gamma}_{\beta} \circ [S]^{\beta}_{\alpha}. \blacksquare \end{align*} $$ Example $$ \begin{align*} A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\ B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p \end{align*} $$ If we choose the standard basis for all vector spaces, then the theorem tells us $$ \begin{align*} [L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]^{\gamma}_{\beta} \circ [L_A]^{\beta}_{\alpha}. \end{align*} $$ But since we chose the standard basis then we know that \([L_A]^{\beta}_{\alpha} = A\) and \([L_B]^{\gamma}_{\beta} = B\). So we can also write $$ \begin{align*} [L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]^{\gamma}_{\beta} \circ [L_A]^{\beta}_{\alpha} = BA = [L_{BA}]^{\gamma}_{\alpha}. \end{align*} $$ In particular, this shows that these maps are equal \(L_B \circ L_A = L_{BA}\). Example Let $$ \begin{align*} T_d: P_3 &amp;\rightarrow P_2 \\ f &amp;\rightarrow f' \end{align*} $$ and $$ \begin{align*} T_i: P_2 &amp;\rightarrow P_3 \\ f &amp;\rightarrow \int_0^x f(t)dt \end{align*} $$ For standard bases \(\beta = \{1,x,x^2, x^3\}\) of \(P_3\) and \(\gamma = \{1, x, x^2\}\) of \(P_2\). Extra notes: As a reminder, to find the matrix representative of of \(T_d\), we first apply the transformation on the vectors of \(\beta\). $$ \begin{align*} T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. \end{align*} $$ and then we find the coordinates of these images with respect to \(\gamma\) which will be the column vectors of the matrix, $$ \begin{align*} T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\ T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\ T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\ T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2). \end{align*} $$ Therefore, \(T_d\) and \(T_i\) (can be found using the same method) are: $$ \begin{align*} [T_d]^{\gamma}_{\beta} &amp;= \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix}, [T_i]_{\gamma}^{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 0 \\ 0 &amp; 0 &amp; 1/3 \end{pmatrix}. \end{align*} $$ To compose these matrices we just multiply them, $$ \begin{align*} [T_i]_{\gamma}^{\beta}[T_d]^{\gamma}_{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ To verify, want to compute \([T_i \circ T_d]^{\beta}_{\beta}\) but instead of applying the composition on each vector. Let’s just apply it on a general object. In this case we need an object from \(P_3\): $$ \begin{align*} (T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(T_d(a_0 + a_1x + a_2x^2 + a_3x^3)) \\ &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\ &amp;= a_1x + a_2x^2 + a_3x^3 \end{align*} $$ So now the first column of this matrix should be the image of the first vector in the basis \(\beta\) so the image of \(1\) and that’s simply all zeros. For \(x\), we see \(a_1x\) just got mapped to itself again in the final equation. Similarly we get the same for the last two basis vectors so $$ \begin{align*} [T_i \circ T_d]^{\beta}_{\beta} = \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix} = \end{align*} $$ Matrix Multiplication Properties Theorem \(A(BC) = (AB)C\) \(A(B+C) = AB + AC\) Not commutative. In general \(AB \neq BA\) Proof (b): As a reminder we know that \((AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}\). We also know that \(B+C\) is just summing the matching coordinates from each matrix, \(B_{ij} + C_{ij}\). Now, expand \(A(B+C)\). $$ \begin{align*} (A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\ &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\ &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\ &amp;= (AB)_{ij} + (AC)_{ij} \\ &amp;= (AB + AC)_{ij} \\ &amp;= AB + AC \end{align*} $$ References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 12/13: Matrix Representation of Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 12/13: Matrix Representation of Linear Transformations" /><published>2024-08-05T01:01:36-07:00</published><updated>2024-08-05T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/05/lec13-more-linear-transformations.html"><![CDATA[<p>Recall \(A \in M_{m \times n}\) defines a linear map</p>
<div>
	$$
	\begin{align*}
	L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\ 
	            &amp;\bar{x} \rightarrow A\bar{x} 
	\end{align*}
	$$
</div>
<p>We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces can be represented by a matrix. For example the following linear transformations</p>
<div>
	$$
	\begin{align*}
	T_d: \ &amp;P_3 \rightarrow P_2 \\ 
	&amp;f \rightarrow f'
	\end{align*}
	$$
</div>
<p>and</p>
<div>
 	$$
 	\begin{align*}
 	T_i: \ &amp;P_2 \rightarrow P_3 \\ 
 	&amp;\ f \rightarrow \int_0^x f(t)dt
 	\end{align*}
 	$$
</div>
<p>are examples of linear transformations with finite dimensional vector spaces that can be represented by a matrix. But in order to show this, we will start with expressing vectors uniquely relative a basis in a vector space.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Coordinate Expression for a vector</b></h4>
<p>Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form,</p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n
	\end{align*}
	$$
</div>
<p>The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. 
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [v]_{\beta} = 
\begin{pmatrix}
a_1 \\
.  \\
. \\
. \\
a_n
\end{pmatrix}
\in \mathbf{R}^n
	\end{align*}
	$$
is the coordinate expression for \(v\) with respect to \(\beta\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\).
<br /></p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta} &amp;= 
	 \begin{pmatrix}
	 2 \\
	 1 \\
	 \end{pmatrix} \\
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis,</p>
<div>
	$$
	\begin{align*}
	 v = (2,1) = a_1(1,1) + a_2(1,-1).
\end{align*}
	 $$
</div>
<p>From this, we get</p>
<div>
	$$
	\begin{align*}
	 a_1 + a_2 &amp;= 2 \\
	 a_1 - a_2 &amp;= 1 \\
\end{align*}
	 $$
</div>
<p>Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are</p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix} \\
	 &amp;= 
	 \begin{pmatrix}
	 \frac{3}{2} \\
	 \frac{1}{2} \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>So we can think of \([\quad]_{\beta'}\) as a map:</p>
<div>
	$$
	\begin{align*}
	 [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance”
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional.
We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be the basis for \(V\) and let \(\gamma = \{w_1, ..., w_n\}\) be the basis for \(W\). For \(v \in V\), we have</p>
<div>
	$$
	\begin{align*}
	T(v) = a_1w_1 + ... + a_mw_m.
\end{align*}
$$
</div>
<p>(This is the image of \(v\) which can be expressed uniquely in terms of the vectors of the basis \(\gamma\) because we’re in the vector space \(W\) now.) \(v\) itself is a linear combination of the basis vectors of \(\beta\). So for each \(v_j\) we have,</p>
<div>
 	$$
 	\begin{align*}
 	T(v_j) &amp;= a_{1j}w_1 + ... + a_{mj}w_m \\
	&amp;= \sum_i^m a_{ij}w_i \text{ for $j = 1,2,..,n.$}
 \end{align*}
 $$
 </div>
<p>From this, we now have this definition,
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	Using the above notation, let the matrix \(A\) defined as \(A_{ij} = a_ij\) be the <b>matrix representation of T</b> in the ordered \(\beta\) and \(\gamma\) and we write
	$$
	\begin{align*}
	 [T]_{\beta}^{\gamma} = A
	\end{align*}
	$$
</div>
<p><br />
Remark: The column vectors of \(T\) are the images of the basis vectors \(\beta\) written with respect to \(\gamma\).</p>
<div>
	$$
\begin{align*}
	 [T]_{\beta}^{\gamma} = 
\begin{pmatrix}
| &amp; &amp; |\\ 
[T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\
| &amp; &amp; |
\end{pmatrix}
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let</p>
<div>
	$$
\begin{align*}
T_d: \ &amp;P_3 \rightarrow P_2 \\
        &amp;f \rightarrow f'. 
\end{align*}
$$
</div>
<p>Let \(\beta = \{1, x, x^2, x^3\}\) be the standard basis of \(P_3\). and \(\gamma = \{1, x, x^2\}\) be the standard basis of \(P_2\). We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so,</p>
<div>
	$$
\begin{align*}
T_d(1) &amp;= 0 \\
T_d(x) &amp;= x \\
T_d(x^2) &amp;= 2x \\
T_d(x^3) &amp;= 3x^2
\end{align*}
$$
</div>
<p>Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\).</p>
<div>
	$$
\begin{align*}
[T_d(1)]_{\gamma} &amp;=  [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x)]_{\gamma} &amp;=  [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = 
\begin{pmatrix}
1 \\ 
0 \\
0
\end{pmatrix} \\
[T_d(x^2)]_{\gamma} &amp;=  [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = 
\begin{pmatrix}
0 \\ 
2 \\
0
\end{pmatrix} \\
[T_d(x^3)]_{\gamma} &amp;=  [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = 
\begin{pmatrix}
0 \\ 
0 \\
3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>So the final matrix is</p>
<div>
	$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2.15(a)
</div>
<div class="purbdiv">
Let \(A \in M_{m \times n}\) and let
$$
\begin{align*}
L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\
&amp;\bar{x} \rightarrow A\bar{x}.
\end{align*}
$$
If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A.
\end{align*}
$$
</div>
<p><br />
(Note: We earlier said that any linear transformation between finite dimensional vector spaces can be represented by a matrix. \(L_A\) is a linear transformation. So what is the matrix representation of it? The claim is that as long as you use the standard basis for \(\mathbf{R}^n\) and \(\mathbf{R}^m\), then the representation is just \(A\) itself!)
<br />
<br />
Proof: \(\beta\) is the standard basis of \(\mathbf{R}^n\) so \(\beta = \{e_1,...,e_n\}\) where 
\(e_1 = \begin{pmatrix}
1 &amp; 0 &amp; ... &amp; 0
\end{pmatrix}^t\)
<br />
We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them with respect to the basis \(\gamma\),</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma})
\end{align*}
$$
</div>
<p>But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1 = \bar{a}_1e_{11}+\bar{a}_2e_{12}+...+\bar{a}_ne_{1n}\) where \(\bar{a}_1,...,\bar{a}_n\) are the columns of \(A\). But all the coefficients of \(e_1\) are zero except for the first one. So \(Ae_1 = 1\bar{a}_1\) which is just the first column of \(A\). Finally, the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) is just the first column of \(A\) since it’s the standard basis. Similarly, we can use the same argument to find out the remaining columns and so</p>
<div>
$$
\begin{align*}
[L_A]^{\gamma}_{\beta} = A. \blacksquare
\end{align*}
$$
</div>
<p><br />
So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformation as a Matrix Multiplication</b></h4>
<div class="purdiv">
Theorem 2.14
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases.<br />
For all \(v \in V\),
$$
\begin{align*}
[T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta}
\end{align*}
$$
\(T\) acts like matrix multiplication.
</div>
<p><br />
Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear.</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\
                &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\
				&amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in</p>
<div>
$$
\begin{align*}
[T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\
             &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\
			 &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
\end{align*}
$$
</div>
<p>The expression above is exactly matrix multiplication so we can re-write this as</p>
<div>
$$
\begin{align*}
a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma}
&amp;=
\begin{pmatrix}
[T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} 
\end{pmatrix}
\begin{pmatrix}
a_1 \\
. \\
. \\
. \\
a_n \\
\end{pmatrix} \\
&amp;= [T]_{\beta}^{\gamma} [v]_{\beta}.
\end{align*}
$$
</div>
<p>This is exactly what we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let’s continue the same example from earlier where we found the matrix representative for the linear transformation</p>
<div>
	$$
\begin{align*}
T_d: \ &amp;P_3 \rightarrow P_2 \\
        &amp;f \rightarrow f'. 
\end{align*}
$$
</div>
<p>Suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix</p>
<div>
	$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}
\begin{pmatrix}
5 \\ 
4 \\
0 \\
3
\end{pmatrix} = 
\begin{pmatrix}
4 \\
0 \\
9
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recall \(A \in M_{m \times n}\) defines a linear map $$ \begin{align*} L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\ &amp;\bar{x} \rightarrow A\bar{x} \end{align*} $$ We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces can be represented by a matrix. For example the following linear transformations $$ \begin{align*} T_d: \ &amp;P_3 \rightarrow P_2 \\ &amp;f \rightarrow f' \end{align*} $$ and $$ \begin{align*} T_i: \ &amp;P_2 \rightarrow P_3 \\ &amp;\ f \rightarrow \int_0^x f(t)dt \end{align*} $$ are examples of linear transformations with finite dimensional vector spaces that can be represented by a matrix. But in order to show this, we will start with expressing vectors uniquely relative a basis in a vector space. Coordinate Expression for a vector Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every \(v \in V\), \(v\) can be expressed uniquely in the form, $$ \begin{align*} v = a_1v_1 + ... + a_nv_n \end{align*} $$ The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. Definition $$ \begin{align*} [v]_{\beta} = \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \end{pmatrix} \in \mathbf{R}^n \end{align*} $$ is the coordinate expression for \(v\) with respect to \(\beta\). Example Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\). $$ \begin{align*} [v]_{\beta} &amp;= \begin{pmatrix} 2 \\ 1 \\ \end{pmatrix} \\ [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \end{align*} $$ These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis, $$ \begin{align*} v = (2,1) = a_1(1,1) + a_2(1,-1). \end{align*} $$ From this, we get $$ \begin{align*} a_1 + a_2 &amp;= 2 \\ a_1 - a_2 &amp;= 1 \\ \end{align*} $$ Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are $$ \begin{align*} [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \\ &amp;= \begin{pmatrix} \frac{3}{2} \\ \frac{1}{2} \\ \end{pmatrix} \end{align*} $$ So we can think of \([\quad]_{\beta'}\) as a map: $$ \begin{align*} [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n \end{align*} $$ The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance” Matrix Representation of linear transformations Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional. We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be the basis for \(V\) and let \(\gamma = \{w_1, ..., w_n\}\) be the basis for \(W\). For \(v \in V\), we have $$ \begin{align*} T(v) = a_1w_1 + ... + a_mw_m. \end{align*} $$ (This is the image of \(v\) which can be expressed uniquely in terms of the vectors of the basis \(\gamma\) because we’re in the vector space \(W\) now.) \(v\) itself is a linear combination of the basis vectors of \(\beta\). So for each \(v_j\) we have, $$ \begin{align*} T(v_j) &amp;= a_{1j}w_1 + ... + a_{mj}w_m \\ &amp;= \sum_i^m a_{ij}w_i \text{ for $j = 1,2,..,n.$} \end{align*} $$ From this, we now have this definition, Definition Using the above notation, let the matrix \(A\) defined as \(A_{ij} = a_ij\) be the matrix representation of T in the ordered \(\beta\) and \(\gamma\) and we write $$ \begin{align*} [T]_{\beta}^{\gamma} = A \end{align*} $$ Remark: The column vectors of \(T\) are the images of the basis vectors \(\beta\) written with respect to \(\gamma\). $$ \begin{align*} [T]_{\beta}^{\gamma} = \begin{pmatrix} | &amp; &amp; |\\ [T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \\ | &amp; &amp; | \end{pmatrix} \end{align*} $$ Example Let $$ \begin{align*} T_d: \ &amp;P_3 \rightarrow P_2 \\ &amp;f \rightarrow f'. \end{align*} $$ Let \(\beta = \{1, x, x^2, x^3\}\) be the standard basis of \(P_3\). and \(\gamma = \{1, x, x^2\}\) be the standard basis of \(P_2\). We want to compute \([T_d]_{\beta}^{\gamma}\). The first thing that we want to do is to apply \(T\) on the vectors of the basis \(\beta\) so, $$ \begin{align*} T_d(1) &amp;= 0 \\ T_d(x) &amp;= x \\ T_d(x^2) &amp;= 2x \\ T_d(x^3) &amp;= 3x^2 \end{align*} $$ Next, we want to write these with respect to the basis \(\gamma = \{1,x,x^2\}\) meaning that we want to express each result as a linear combination of the vectors in the basis \(\gamma\). $$ \begin{align*} [T_d(1)]_{\gamma} &amp;= [0]_{\gamma} = 0(1) + 0(x) + 0(x^2) = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix} \\ [T_d(x)]_{\gamma} &amp;= [1]_{\gamma} = 1(1) + 0(x) + 0(x^2) = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \\ [T_d(x^2)]_{\gamma} &amp;= [2x]_{\gamma} = 0(1) + 2(x) + 0(x^2) = \begin{pmatrix} 0 \\ 2 \\ 0 \end{pmatrix} \\ [T_d(x^3)]_{\gamma} &amp;= [3x^2]_{\gamma} = 0(1) + 2(x) + 2(x^2) = \begin{pmatrix} 0 \\ 0 \\ 3 \end{pmatrix}. \end{align*} $$ So the final matrix is $$ \begin{align*} [T_d]^{\gamma}_{\beta} &amp;= \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix}. \end{align*} $$ Theorem 2.15(a) Let \(A \in M_{m \times n}\) and let $$ \begin{align*} L_A: \ &amp;\mathbf{R}^n \rightarrow \mathbf{R}^m \\ &amp;\bar{x} \rightarrow A\bar{x}. \end{align*} $$ If \(\beta\) and \(\gamma\) are the standard bases for \(\mathbf{R}^n\) and \(\mathbf{R}^m\) respectively, then $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \end{align*} $$ (Note: We earlier said that any linear transformation between finite dimensional vector spaces can be represented by a matrix. \(L_A\) is a linear transformation. So what is the matrix representation of it? The claim is that as long as you use the standard basis for \(\mathbf{R}^n\) and \(\mathbf{R}^m\), then the representation is just \(A\) itself!) Proof: \(\beta\) is the standard basis of \(\mathbf{R}^n\) so \(\beta = \{e_1,...,e_n\}\) where \(e_1 = \begin{pmatrix} 1 &amp; 0 &amp; ... &amp; 0 \end{pmatrix}^t\) We can then find \([L_A]_{\beta}^{\gamma}\) by applying \(L_A\) on the vectors of \(\beta\) and then re-writing them with respect to the basis \(\gamma\), $$ \begin{align*} [L_A]^{\gamma}_{\beta} = ([L_A(e_1)]_{\gamma},..., [L_A(e_n)]_{\gamma}) \end{align*} $$ But then consider \([L_A(e_1)]_{\gamma}\). We know \(L_A\) by definition takes a vector \(\bar{x}\) and produces \(A\bar{x}\). So \([L_A(e_1)]_{\gamma}\) = \([Ae_1]_{\gamma}\). Moreover, \(Ae_1 = \bar{a}_1e_{11}+\bar{a}_2e_{12}+...+\bar{a}_ne_{1n}\) where \(\bar{a}_1,...,\bar{a}_n\) are the columns of \(A\). But all the coefficients of \(e_1\) are zero except for the first one. So \(Ae_1 = 1\bar{a}_1\) which is just the first column of \(A\). Finally, the coordinate representatives of the first column of \(A\) with respect to basis \(\gamma\) is just the first column of \(A\) since it’s the standard basis. Similarly, we can use the same argument to find out the remaining columns and so $$ \begin{align*} [L_A]^{\gamma}_{\beta} = A. \blacksquare \end{align*} $$ So far, we’ve taken a linear transformation from a vector space \(V\) to another \(W\) and represented it with a matrix \([T]_{\beta}^{\gamma}\). Above, we can also take a matrix \(A\) and turn it into a linear map \(L_A\) and if we do so with the standard bases \(\mathbf{R}^n\) and \(\mathbf{R}^m\), we can recover the matrix \(A\). Linear Transformation as a Matrix Multiplication Theorem 2.14 Let \(T: V \rightarrow W\) be a linear transformation, Let \(\beta, \gamma\) be fixed bases. For all \(v \in V\), $$ \begin{align*} [T(v)]_{\gamma} = [T]_{\beta}^{\gamma} [v]_{\beta} \end{align*} $$ \(T\) acts like matrix multiplication. Proof: Let \(\beta=\{v_1,...,v_n\}\), \(\gamma=\{w_1,...,w_m\}\). We can write \(v\) below as a linear combination of the basis vectors in \(\beta\). We can then apply \(T\) which is linear. $$ \begin{align*} [T(v)]_{\gamma} &amp;= [T(a_1v_1 + ... + a_nv_n)]_{\gamma} \\ &amp;= [T(a_1v_1) + ... + T(a_nv_n)]_{\gamma} \\ &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \end{align*} $$ But this map \([ ]_{\gamma}\) is also linear. Therefore, we can distribute it in $$ \begin{align*} [T(v)]_{\gamma} &amp;= [a_1T(v_1) + ... + a_nT(v_n)]_{\gamma} \\ &amp;= [a_1T(v_1)]_{\gamma} + ... + [a_nT(v_n)]_{\gamma} \\ &amp;= a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} \end{align*} $$ The expression above is exactly matrix multiplication so we can re-write this as $$ \begin{align*} a_1[T(v_1)]_{\gamma} + ... + a_n[T(v_n)]_{\gamma} &amp;= \begin{pmatrix} [T(v_1)]_{\gamma} &amp; ... &amp; [T(v_n)]_{\gamma} \end{pmatrix} \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \\ \end{pmatrix} \\ &amp;= [T]_{\beta}^{\gamma} [v]_{\beta}. \end{align*} $$ This is exactly what we wanted to show. \(\blacksquare\) Example Let’s continue the same example from earlier where we found the matrix representative for the linear transformation $$ \begin{align*} T_d: \ &amp;P_3 \rightarrow P_2 \\ &amp;f \rightarrow f'. \end{align*} $$ Suppose we have the function \(f(x) = 3x^3 + 4x + 5\) in \(P_3\). We can express this function in terms of the basis vectors in \(\beta = \{1, x, x^2, x^3\}\) as the coefficients vector \((5, 4, 0, 3)\). So now, let’s multiply this vector by the linear transformation matrix $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3 \end{pmatrix} \begin{pmatrix} 5 \\ 4 \\ 0 \\ 3 \end{pmatrix} = \begin{pmatrix} 4 \\ 0 \\ 9 \end{pmatrix}. \end{align*} $$ This gives us the coefficients vector with respect to the basis \(\gamma\). This means that the function will be \(4 + 9x^2\) which is exactly what we would get if manually applied the transformation on \(f(x)\) to get \(f'(x)\) but now instead we have a matrix to do this. References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Section 1.6: Exercise 11</title><link href="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 11" /><published>2024-08-04T01:01:36-07:00</published><updated>2024-08-04T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/04/1-6-ex-11.html"><![CDATA[<div class="ydiv">
1.6 Exercise 11
</div>
<div class="ybdiv">
Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\).
</div>
<p><br />
Proof: 
<br />
To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	a_1(u+v) + a_2(au) = \bar{0}
	\end{align*}
	$$
</div>
<p>is only satisfied by the trivial solution. We’ll re-arrange the terms as follows,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1u + a_1v + aa_2u  \\
	&amp;= (a_1 + aa_2)u + a_1v \\
	&amp;= (a_1 + aa_2)u + a_1v.
	\end{align*}
	$$
</div>
<p>From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis.
<br />
<br />
Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms,</p>
<div>
	$$
	\begin{align*}
	\bar{0} &amp;= a_1(au) + a_2(bv) \\
	\bar{0} &amp;= (a_1a)u + (a_2b)v.
	\end{align*}
	$$
</div>
<p>This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.6 Exercise 11 Let \(u\) and \(v\) be distinct vectors of a vector space \(V\). Show that if \(\{u, v\}\) is basis for \(V\), and \(a\) and \(b\) are non-zero scalars, then both \(\{u+v, au\}\) and \(\{au, bv\}\) are also bases for \(V\). Proof: To prove that \(\{u+v, au\}\) is basis, we need to show that the set of vectors are linearly independent and that \(span(\{u+v, au\}) = V\). Let \(a_1, a_2\) be any two scalars. Then we want to prove that the following equation $$ \begin{align*} a_1(u+v) + a_2(au) = \bar{0} \end{align*} $$ is only satisfied by the trivial solution. We’ll re-arrange the terms as follows, $$ \begin{align*} \bar{0} &amp;= a_1u + a_1v + aa_2u \\ &amp;= (a_1 + aa_2)u + a_1v \\ &amp;= (a_1 + aa_2)u + a_1v. \end{align*} $$ From this, we can see that the only solution is the trivial solution since \(\{u,v\}\) are linearly independent. Moreover, since \(\{u,v\}\) is a basis, then its dimension is 2. Therefore, \(\{u+v, au\}\) is a basis. Similarly, to see that \(\{au, bv\}\), we’ll prove the following equation’s solution is the trivial solution only by re-arranging the terms, $$ \begin{align*} \bar{0} &amp;= a_1(au) + a_2(bv) \\ \bar{0} &amp;= (a_1a)u + (a_2b)v. \end{align*} $$ This is again a linear combination of two linearly independent vectors and so the trivial solution is the only possible solution to this equation. Since the set has 2 vectors, then it’s a basis as well. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 1.6: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html" rel="alternate" type="text/html" title="Section 1.6: Exercise 20" /><published>2024-08-03T01:01:36-07:00</published><updated>2024-08-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/03/1-6-ex-20.html"><![CDATA[<p>Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case?</p>
<div class="ydiv">
1.6 Exercise 20
</div>
<div class="ybdiv">
Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\).
<ol style="list-style-type:lower-alpha">
	<li>Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.)</li>
	<li>Prove \(S\) contains at least \(n\) vectors.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Similar to the proof of <a href="http://127.0.0.1:4000/jekyll/update/2024/07/30/1-6-theorem-1.9.html">theorem 1.9</a>, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\)</li>
	
	<li>If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains 
	at least \(n\) vectors by <a href="http://127.0.0.1:4000/jekyll/update/2024/08/02/1-6-corollary-2.html">Corollary 2</a>
	 from theorem 1.9. \(\blacksquare\)</li>
</ol>
<p><br />
The book provided the solution <a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_1_6.html">here</a>.
though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Note: I’m not convinced with the argument for (a) since \(S\) could be infinite. The book’s official solution doesn’t seem to handle this case? 1.6 Exercise 20 Let \(V\) be a vector space having dimension \(n\), and let \(S\) be a subset of \(V\) that generates \(V\). Prove that there is a subset of \(S\) that is a basis for \(V\). (Be careful not to assume that \(S\) is finite.) Prove \(S\) contains at least \(n\) vectors. Proof: Similar to the proof of theorem 1.9, we know that \(S\) generates \(V\). If n = 0, then \(V=\{0\}\) and \(\emptyset\) is a subset of \(S\) and a basis for \(V\). Otherwise, \(n \geq 0\) and there is at least one non-zero vector. Let this vector be \(u\). Let \(\beta = \{u\}\). We know that \(\beta\) is linearly independent. Continue if possible while \(\beta\) remains linearly independent. This process will stop at some point since we know that \(V\) is of dimension \(n\). (will it stop??? \(S\) can have infinitely many linearly dependent vectors ugh). After we stop then we know that \(\beta\) is a linearly independent set. We just need to prove that it generates \(V\) by showing that \(S \in span(\beta)\). Let \(v \in S\), if \(v \in \beta\), then we're done. Otherwise we know that \(v \not\in \beta\). This however means that \(\beta \cup v\) is linearly dependent by construction. This means that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. \(\blacksquare\) If \(S\) is infinite then it contains more than \(n\) vectors. If \(S\) is finite, then we know \(S\) contains at least \(n\) vectors by Corollary 2 from theorem 1.9. \(\blacksquare\) The book provided the solution here. though here is the thing, they’re using theorem 1.10 (replacement theorem). The replacement theorem assumes that \(S\) is finite? doesn’t it?? References: Linear Algebra 5th Edition]]></summary></entry></feed>