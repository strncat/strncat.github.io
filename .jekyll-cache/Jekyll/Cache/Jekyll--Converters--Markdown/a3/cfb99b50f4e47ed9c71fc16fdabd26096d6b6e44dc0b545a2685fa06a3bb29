I"Ý'<h3>The Vector Space of Linear Transformations&lt;/b&gt;&lt;/h4&gt;
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given vector spaces \(V, W\) define
$$
\begin{align*}
\mathcal{L}(V, W) = \{T: V \rightarrow W \ | \ T \text{ is linear}\}.
\end{align*}
$$
</div>
<br />
FACT: The set of linear transformations $$\mathcal{L}(V, W)$$ is a vector space. We can think of this as a way to get a new vector space from two vector spaces $$W$$ and $$V$$.
<br />
<br />
<!------------------------------------------------------------------------------------>
<h3>The Composition of Linear Transformations&lt;/b&gt;&lt;/h4&gt;
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Define the linear maps \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\). We define composition
$$
\begin{align*}
T \circ S: X &amp;\rightarrow Z \\
x &amp;\rightarrow T(S(x))
\end{align*}
$$
</div>
<br />
Next, we will show that the composition of two linear transformations is also linear!
<br />
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \circ S: X \rightarrow Z\) is linear (\(T \circ S \in \mathcal{L}(X, Z))\) 
</div>
<br />
Proof: We want to show that
<div>
$$
\begin{align*}
(T \circ S)(x_1 + cx_2) = (T \circ S)(x_1) + c(T \circ S)(x_2).	 
\end{align*}
$$
</div>
 To see expand the left hand side as follows:
<div>
$$
\begin{align*}
(T \circ S)(x_1 + cx_2) &amp;= T(S(x_1 + cx_2)) \\
                     &amp;= T(S(x_1) + cS(x_2)) \text{ (because $S$ is linear)} \\
                     &amp;= T(S(x_1)) + cT(S(x_2)) \text{ (because $T$ is linear)} \\
                     &amp;= (T \circ S)(x_1) + c(T \circ S)(x_2). \ \blacksquare			 
\end{align*}
$$
</div>
<br />
<hr />
<br />
<!------------------------------------------------------------------------------------>
<h3>The Matrix of Linear Composition&lt;/b&gt;&lt;/h4&gt;
Suppose now that $$X$$, $$Y$$ and $$Z$$ are finite dimensional with fixed bases $$\alpha = \{x_1,...,x_n\}, \beta = \{y_1,...,y_m\}$$ and $$\gamma = \{z_1,...,z_n\}$$.
<br />
<br />
How are $$[S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}$$ and $$[T \circ S]_{\alpha}^{\gamma}$$ related?
<br />
<br />
To answer this question, we need to define matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------>
<h3>Matrix Multiplication&lt;/b&gt;&lt;/h4&gt;
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given \(A \in M_{m \times n}\) and \(B \in M_{n \times p}\). We define their product \(AB \in M_{m \times p}\) by
$$
\begin{align*}
(AB)_{ij} = \sum_k^n A_{ik}B_{kj}.
\end{align*}
$$
Alternatively,
$$
\begin{align*}
AB &amp;= A \begin{pmatrix} | &amp;  &amp; | \\ \bar{b}_1 &amp; ... &amp; \bar{b}_p \\ | &amp;  &amp; |  \end{pmatrix}
   = \begin{pmatrix} | &amp;  &amp; | \\ A\bar{b}_1 &amp; ... &amp; A\bar{b}_p \\ | &amp;  &amp; |  \end{pmatrix} .
\end{align*}
$$
</div>
<br />
<!------------------------------------------------------------------------------------>
<h3>Example&lt;/b&gt;&lt;/h4&gt;
<div>
$$
\begin{align*}
A =
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix},
B =
\begin{pmatrix} 1 &amp; 2 \\ 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}.
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
A\bar{b}_1 &amp;= 
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}
*
\begin{pmatrix} 1 \\ 1 \\ 2 \\ \end{pmatrix} = 1
\begin{pmatrix} 1  \\ 4  \end{pmatrix}
+ 1
\begin{pmatrix} 2  \\ 5  \end{pmatrix}
+ 
\begin{pmatrix} 3 \\ 6 \end{pmatrix}
=
\begin{pmatrix} 9 \\ 21 \end{pmatrix}
\\
A\bar{b}_1 &amp;= 
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}
*
\begin{pmatrix} 2 \\ 2 \\ 1 \\ \end{pmatrix} = 
\begin{pmatrix} 9 \\ 24 \end{pmatrix} \\
AB &amp;= 
\begin{pmatrix} 9 &amp; 9 \\ 21 &amp; 24 \end{pmatrix}
\end{align*}
$$
</div>
<br />
<hr />
<br />
<!------------------------------------------------------------------------------------>
<h3>Matrix Composition Using Matrix Multiplication&lt;/b&gt;&lt;/h4&gt;
Now that we defined matrix multiplication, we are ready to answer the question of how $$[S]_{\alpha}^{\beta}, [T]_{\beta}^{\gamma}$$ and $$[T \circ S]_{\alpha}^{\gamma}$$ related.
<div class="purdiv">
Theorem 2.11
</div>
<div class="purbdiv">
Given \(S \in \mathcal{L}(X, Y)\) and \(T \in \mathcal{L}(Y, Z)\) and finite bases \(\alpha, \beta, \gamma\) for \(X, Y, Z\), then
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} = [T]^{\gamma}_{\beta} [S]^{\beta}_{\alpha}
\end{align*}
$$
</div>
<br />
In other words, the composition of the linear transformations $$S$$ and $$T$$ is equal to the matrix multiplication of the two matrices representing these linear transformations.
<br />
<h3>Proof&lt;/b&gt;&lt;/h4&gt;
Fix the basis $$\alpha$$ such that $$\alpha = \{x_1,...,x_n\}$$. Then,
<div>
$$
\begin{align*}
[T \circ S]_{\alpha}^{\gamma} &amp;= ([(T \circ S)(x_1)]_{\gamma} ... [(T \circ S)(x_n)]_{\gamma}) \\
&amp;= ([(T(S(x_1))]_{\gamma} ... [(T(S(x_n))]_{\gamma}) \text{( by definition)}\\
&amp;= ([T]_{\beta}^{\gamma}[(S(x_1)]_{\beta} ... [T]_{\beta}^{\gamma}[(S(x_n)]_{\beta}) \text{( by theorem 2.14 (lecture 13))}\\
&amp;= [T]_{\beta}^{\gamma}([(S(x_1)]_{\beta} ... [(S(x_n)]_{\beta}) \\
&amp;= [T]^{\gamma}_{\beta} \circ [S]^{\beta}_{\alpha}. \blacksquare					 
\end{align*}
$$
</div>
<hr />
<br />
<!------------------------------------------------------------------------------------>
<h3>Example&lt;/b&gt;&lt;/h4&gt;
<div>
$$
\begin{align*}
A &amp;\in M_{m \times n} \rightarrow L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m \\
B &amp;\in M_{p \times m} \rightarrow L_B: \mathbf{R}^m \rightarrow \mathbf{R}^p
\end{align*}
$$
</div>
If we choose the standard basis for all vector spaces, then the theorem tells us
<div>
$$
\begin{align*}
[L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]^{\gamma}_{\beta} \circ [L_A]^{\beta}_{\alpha}.
\end{align*}
$$
</div>
But since we chose the standard basis then we know that $$[L_A]^{\beta}_{\alpha} = A$$ and $$[L_B]^{\gamma}_{\beta} = B$$. So we can also write
<div>
$$
\begin{align*}
[L_B \circ L_A]_{\alpha}^{\gamma} &amp;= [L_B]^{\gamma}_{\beta} \circ [L_A]^{\beta}_{\alpha} = BA = [L_{BA}]^{\gamma}_{\alpha}.
\end{align*}
$$
</div>
In particular, this shows that these maps are equal $$L_B \circ L_A = L_{BA}$$.
<hr />
<br />
<!------------------------------------------------------------------------------------>
<h3>Example&lt;/b&gt;&lt;/h4&gt;
Let 
<div>
$$
\begin{align*}
T_d: P_3 &amp;\rightarrow P_2 \\
f &amp;\rightarrow f'			 
\end{align*}
$$
</div>
and
<div>
$$
\begin{align*}
T_i: P_2 &amp;\rightarrow P_3 \\
f &amp;\rightarrow \int_0^x f(t)dt			 
\end{align*}
$$
</div>
For standard bases $$\beta = \{1,x,x^2, x^3\}$$ of $$P_3$$ and $$\gamma = \{1, x, x^2\}$$ of $$P_2$$. 
<br />
<br />
Extra notes: As a reminder, to find the matrix representative of of $$T_d$$, we first apply the transformation on the vectors of $$\beta$$.
<div>
$$
\begin{align*}
T(1) = 0, T(x) = 1, T(x^2) = 2x, T(x^3) = 3x^2. 		 
\end{align*}
$$
</div>
and then we find the coordinates of these images with respect to $$\gamma$$ which will be the column vectors of the matrix,
<div>
$$
\begin{align*}
T(1) &amp;= 0 = 0(1) + 0(x) + 0(x^2) \\
T(x) &amp;= 1 = 1(1) + 0(x) + 0(x^2) \\
T(x^2) &amp;= 2x = 0(1) + 2(x) + 0(x^2) \\
T(x^3) &amp;= 3x^2 = 0(1) + 0(x) + 3(x^2).
\end{align*}
$$
</div>
Therefore, $$T_d$$ and $$T_i$$ (can be found using the same method) are:
<div>
$$
\begin{align*}
[T_d]^{\gamma}_{\beta} &amp;= 
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3
\end{pmatrix}, [T_i]_{\gamma}^{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1/2 &amp; 0 \\
0 &amp; 0 &amp; 1/3
\end{pmatrix}.
\end{align*}
$$
</div>
To compose these matrices we just multiply them,
<div>
$$
\begin{align*}
[T_i]_{\gamma}^{\beta}[T_d]^{\gamma}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
To verify, want to compute $$[T_i \circ T_d]^{\beta}_{\beta}$$ but instead of applying the composition on each vector. Let's just apply it on a general object. In this case we need an object from $$P_3$$:
<div>
$$
\begin{align*}
(T_i \circ T_d)(a_0 + a_1x + a_2x^2 + a_3x^3) &amp;= T_i(T_d(a_0 + a_1x + a_2x^2 + a_3x^3)) \\
                                              &amp;= T_i(a_1 + 2a_2x + 3a_3x^2) \\
                        &amp;= a_1x + a_2x^2 + a_3x^3
\end{align*}
$$
</div>
So now the first column of this matrix should be the image of the first vector in the basis $$\beta$$ so the image of $$1$$ and that's simply all zeros. For $$x$$, we see $$a_1x$$ just got mapped to itself again in the final equation. Similarly we get the same for the last two basis vectors so
<div>
$$
\begin{align*}
[T_i \circ T_d]^{\beta}_{\beta} = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix} = 
\end{align*}
$$
</div>
<br />
<hr />
<br />
<!------------------------------------------------------------------------------------>
<h3>Matrix Multiplication Properties&lt;/b&gt;&lt;/h4&gt;
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
<ol style="list-style-type:lower-alpha">
	<li>\(A(BC) = (AB)C\)</li>
	<li>\(A(B+C) = AB + AC\)</li>
	<li>Not commutative. In general \(AB \neq BA\)</li>
</ol>
</div>
<br />
<b>Proof (b):</b>
<br /> 
As a reminder we know that $$(AB)_{ij} = \sum_{k=1}^n A_{ik}B_{kj}$$. We also know that $$B+C$$ is just summing the matching coordinates from each matrix, $$B_{ij} + C_{ij}$$. Now, expand $$A(B+C)$$.
<div>
$$
\begin{align*}
(A(B+C))_{ij} &amp;= \sum_k A_{ik}(B_{kj} + C_{kj}) \\
             &amp;= \sum_k A_{ik}B_{kj} + A_{ik}C_{kj} \\
			 &amp;= \sum_k A_{ik}B_{kj} + \sum_k A_{ik}C_{kj} \\
			 &amp;= (AB)_{ij} + (AC)_{ij} \\
			 &amp;= (AB + AC)_{ij} \\
			 &amp;= AB + AC
\end{align*}
$$
</div>
<br />
<hr />
<br />
<!------------------------------------------------------------------------------------>
<h3>References&lt;/b&gt;&lt;/h4&gt;
<ul>
<li>Math416 by Ely Kerman</li>
</ul>






















</h3></h3></h3></h3></h3></h3></h3></h3></h3></h3></h3>
:ET