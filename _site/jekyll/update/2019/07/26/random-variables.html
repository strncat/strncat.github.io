<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.0 -->
<title>Random Variables | strncat’s notebook</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Random Variables" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="0 References My study notes from CS109 http://web.stanford.edu/class/archive/cs/cs109/cs109.1188// Specifically: http://web.stanford.edu/class/archive/cs/cs109/cs109.1188/lectures/06_random_variables.pdf What is a Random Variable? A random variable is a real-valued function defined on a sample space. Why define a random variable? sometimes instead of being interested in the individual outcomes of an experiment, we are interested in some groups of the outcomes or more formally some function of the outcome. Example 1: Suppose we’re interested in counting the number of heads in trials of flipping a coin. We can define a random variable to represent the number of heads in 5 trials. Using , we can now refer to the probability of seeing two heads in 5 trials as . This is much simpler that listing the exact outcomes we’re interested in which are getting heads in trial 1 and 2 or getting heads in 1 or 3 or getting heads in trials 1 and 4 only and so on. Example 2: Suppose we roll two dice and we’re interested in the sum of the two dice. We define a random variable to be the sum of two dice (function of outcomes). We can now refer to the probability that the sum of the dice is 7 as . This is much simpler that saying that we want the probability of seeing any of these outcomes: . Discrete Random Variables If our random variable takes on countable values , we call it a discrete random variable. Probability Mass Function Suppose we have a random variable that takes on a discrete values in . Define the probability mass function to be the probability that takes on a particular value . In other words, the PMF is defined as $$ \begin{align*} p_X(k) = P(X = k) \end{align*} $$ Furthermore, the PMF of satisfies: $$ \begin{align*} \sum_{i=1}^{\infty} p_X(k_i) = 1 \end{align*} $$ This also means that for any value that is not in , we have , in other words, $$ \begin{align*} P(X=k) = \Big\{ \begin{array}{@{}lr@{}} p_X(k) \quad \text{ for } k \in R_X \\ 0 \quad \quad \quad \text{ otherwise} \\ \end{array} \end{align*} $$ We can also refer to as just if the random variable is clear from the context. Example 2: Suppose we roll the two dice again from example 2. Define a random variable to be to the sum of the two dice. We know . Below is a graph of the of , for all values in . Cumulative Distribution Function Now instead of defining the probability that a random variable takes on a particular value, we define a new function, the cumulative distribution function (CDF) that gives the probability that is less than or equal to a particular value. $$ \begin{align*} F_X(k) = F(k) = P(X \leq k), \quad \text{ where } -\infty &lt; k &lt; \infty \end{align*} $$ For a discrete random variable, this will be just the sum of all variables $$ \begin{align*} F_X(k) = F(k) = \sum_{\text{ all } i \leq k} p(i) \end{align*} $$ Expectation The expectation or expected value of a random variable is defined as: $$ \begin{align*} E[X] = \sum_{k:p(k)&gt;0} p(k)*k \end{align*} $$ In other words, the expected value is a weighted average of the value of the random variable (values weighted by their probabilities). Example 4: Suppose we roll two dice again from example 2 and 3. Define a random variable to be to the sum of the two dice. We can use our PMF from the previous section to compute the expected value as $$ \begin{align*} E[X] &amp;= 2*P(X=2) + 3*P(X=3) + 4*P(X=4) + 5 * P(X=5) * 6*P(X=6) + 7*P(X=7) \\ \\ &amp;+ 8*P(X=8) + 9*P(X=9) + 10*P(X=10) + 11*P(X=11) * 12*P(X=12) \\ E[X] &amp;= 2*\frac{1}{36} + 3*\frac{2}{36} + 4*\frac{3}{36} + 5*\frac{4}{36} + 6*\frac{5}{36} + 7*\frac{6}{36} + 8*\frac{5}{36} + 9*\frac{4}{36} + 10*\frac{3}{36} \\ &amp;+ 11*\frac{2}{36} + 12*\frac{1}{36} = 7 \end{align*} $$ Expectation of a function of a random variable Suppose we have a random variable and we have a function where is real-valued function. Suppose we want to calculate the expected value of . Define $$ \begin{align*} E[g(X)] = E[Y] &amp;= \sum_j y_jp(y_j) \\ &amp;= \sum_i g(x_i) p(x_i) \\ \end{align*} $$ PROOF? Example 5: Suppose we roll a die and define to be the value on the die. Define a new random variable to be . What is ? Using the above, Properties of Expectation Two important properties of expectation are the following: (1) Linearity of expectation: $$ \begin{align*} E[aX + b] &amp;= aE[X] + b \end{align*} $$" />
<meta property="og:description" content="0 References My study notes from CS109 http://web.stanford.edu/class/archive/cs/cs109/cs109.1188// Specifically: http://web.stanford.edu/class/archive/cs/cs109/cs109.1188/lectures/06_random_variables.pdf What is a Random Variable? A random variable is a real-valued function defined on a sample space. Why define a random variable? sometimes instead of being interested in the individual outcomes of an experiment, we are interested in some groups of the outcomes or more formally some function of the outcome. Example 1: Suppose we’re interested in counting the number of heads in trials of flipping a coin. We can define a random variable to represent the number of heads in 5 trials. Using , we can now refer to the probability of seeing two heads in 5 trials as . This is much simpler that listing the exact outcomes we’re interested in which are getting heads in trial 1 and 2 or getting heads in 1 or 3 or getting heads in trials 1 and 4 only and so on. Example 2: Suppose we roll two dice and we’re interested in the sum of the two dice. We define a random variable to be the sum of two dice (function of outcomes). We can now refer to the probability that the sum of the dice is 7 as . This is much simpler that saying that we want the probability of seeing any of these outcomes: . Discrete Random Variables If our random variable takes on countable values , we call it a discrete random variable. Probability Mass Function Suppose we have a random variable that takes on a discrete values in . Define the probability mass function to be the probability that takes on a particular value . In other words, the PMF is defined as $$ \begin{align*} p_X(k) = P(X = k) \end{align*} $$ Furthermore, the PMF of satisfies: $$ \begin{align*} \sum_{i=1}^{\infty} p_X(k_i) = 1 \end{align*} $$ This also means that for any value that is not in , we have , in other words, $$ \begin{align*} P(X=k) = \Big\{ \begin{array}{@{}lr@{}} p_X(k) \quad \text{ for } k \in R_X \\ 0 \quad \quad \quad \text{ otherwise} \\ \end{array} \end{align*} $$ We can also refer to as just if the random variable is clear from the context. Example 2: Suppose we roll the two dice again from example 2. Define a random variable to be to the sum of the two dice. We know . Below is a graph of the of , for all values in . Cumulative Distribution Function Now instead of defining the probability that a random variable takes on a particular value, we define a new function, the cumulative distribution function (CDF) that gives the probability that is less than or equal to a particular value. $$ \begin{align*} F_X(k) = F(k) = P(X \leq k), \quad \text{ where } -\infty &lt; k &lt; \infty \end{align*} $$ For a discrete random variable, this will be just the sum of all variables $$ \begin{align*} F_X(k) = F(k) = \sum_{\text{ all } i \leq k} p(i) \end{align*} $$ Expectation The expectation or expected value of a random variable is defined as: $$ \begin{align*} E[X] = \sum_{k:p(k)&gt;0} p(k)*k \end{align*} $$ In other words, the expected value is a weighted average of the value of the random variable (values weighted by their probabilities). Example 4: Suppose we roll two dice again from example 2 and 3. Define a random variable to be to the sum of the two dice. We can use our PMF from the previous section to compute the expected value as $$ \begin{align*} E[X] &amp;= 2*P(X=2) + 3*P(X=3) + 4*P(X=4) + 5 * P(X=5) * 6*P(X=6) + 7*P(X=7) \\ \\ &amp;+ 8*P(X=8) + 9*P(X=9) + 10*P(X=10) + 11*P(X=11) * 12*P(X=12) \\ E[X] &amp;= 2*\frac{1}{36} + 3*\frac{2}{36} + 4*\frac{3}{36} + 5*\frac{4}{36} + 6*\frac{5}{36} + 7*\frac{6}{36} + 8*\frac{5}{36} + 9*\frac{4}{36} + 10*\frac{3}{36} \\ &amp;+ 11*\frac{2}{36} + 12*\frac{1}{36} = 7 \end{align*} $$ Expectation of a function of a random variable Suppose we have a random variable and we have a function where is real-valued function. Suppose we want to calculate the expected value of . Define $$ \begin{align*} E[g(X)] = E[Y] &amp;= \sum_j y_jp(y_j) \\ &amp;= \sum_i g(x_i) p(x_i) \\ \end{align*} $$ PROOF? Example 5: Suppose we roll a die and define to be the value on the die. Define a new random variable to be . What is ? Using the above, Properties of Expectation Two important properties of expectation are the following: (1) Linearity of expectation: $$ \begin{align*} E[aX + b] &amp;= aE[X] + b \end{align*} $$" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2019/07/26/random-variables.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2019/07/26/random-variables.html" />
<meta property="og:site_name" content="strncat’s notebook" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-26T07:01:36-07:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/jekyll/update/2019/07/26/random-variables.html","headline":"Random Variables","dateModified":"2019-07-26T07:01:36-07:00","datePublished":"2019-07-26T07:01:36-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2019/07/26/random-variables.html"},"description":"0 References My study notes from CS109 http://web.stanford.edu/class/archive/cs/cs109/cs109.1188// Specifically: http://web.stanford.edu/class/archive/cs/cs109/cs109.1188/lectures/06_random_variables.pdf What is a Random Variable? A random variable is a real-valued function defined on a sample space. Why define a random variable? sometimes instead of being interested in the individual outcomes of an experiment, we are interested in some groups of the outcomes or more formally some function of the outcome. Example 1: Suppose we’re interested in counting the number of heads in trials of flipping a coin. We can define a random variable to represent the number of heads in 5 trials. Using , we can now refer to the probability of seeing two heads in 5 trials as . This is much simpler that listing the exact outcomes we’re interested in which are getting heads in trial 1 and 2 or getting heads in 1 or 3 or getting heads in trials 1 and 4 only and so on. Example 2: Suppose we roll two dice and we’re interested in the sum of the two dice. We define a random variable to be the sum of two dice (function of outcomes). We can now refer to the probability that the sum of the dice is 7 as . This is much simpler that saying that we want the probability of seeing any of these outcomes: . Discrete Random Variables If our random variable takes on countable values , we call it a discrete random variable. Probability Mass Function Suppose we have a random variable that takes on a discrete values in . Define the probability mass function to be the probability that takes on a particular value . In other words, the PMF is defined as $$ \\begin{align*} p_X(k) = P(X = k) \\end{align*} $$ Furthermore, the PMF of satisfies: $$ \\begin{align*} \\sum_{i=1}^{\\infty} p_X(k_i) = 1 \\end{align*} $$ This also means that for any value that is not in , we have , in other words, $$ \\begin{align*} P(X=k) = \\Big\\{ \\begin{array}{@{}lr@{}} p_X(k) \\quad \\text{ for } k \\in R_X \\\\ 0 \\quad \\quad \\quad \\text{ otherwise} \\\\ \\end{array} \\end{align*} $$ We can also refer to as just if the random variable is clear from the context. Example 2: Suppose we roll the two dice again from example 2. Define a random variable to be to the sum of the two dice. We know . Below is a graph of the of , for all values in . Cumulative Distribution Function Now instead of defining the probability that a random variable takes on a particular value, we define a new function, the cumulative distribution function (CDF) that gives the probability that is less than or equal to a particular value. $$ \\begin{align*} F_X(k) = F(k) = P(X \\leq k), \\quad \\text{ where } -\\infty &lt; k &lt; \\infty \\end{align*} $$ For a discrete random variable, this will be just the sum of all variables $$ \\begin{align*} F_X(k) = F(k) = \\sum_{\\text{ all } i \\leq k} p(i) \\end{align*} $$ Expectation The expectation or expected value of a random variable is defined as: $$ \\begin{align*} E[X] = \\sum_{k:p(k)&gt;0} p(k)*k \\end{align*} $$ In other words, the expected value is a weighted average of the value of the random variable (values weighted by their probabilities). Example 4: Suppose we roll two dice again from example 2 and 3. Define a random variable to be to the sum of the two dice. We can use our PMF from the previous section to compute the expected value as $$ \\begin{align*} E[X] &amp;= 2*P(X=2) + 3*P(X=3) + 4*P(X=4) + 5 * P(X=5) * 6*P(X=6) + 7*P(X=7) \\\\ \\\\ &amp;+ 8*P(X=8) + 9*P(X=9) + 10*P(X=10) + 11*P(X=11) * 12*P(X=12) \\\\ E[X] &amp;= 2*\\frac{1}{36} + 3*\\frac{2}{36} + 4*\\frac{3}{36} + 5*\\frac{4}{36} + 6*\\frac{5}{36} + 7*\\frac{6}{36} + 8*\\frac{5}{36} + 9*\\frac{4}{36} + 10*\\frac{3}{36} \\\\ &amp;+ 11*\\frac{2}{36} + 12*\\frac{1}{36} = 7 \\end{align*} $$ Expectation of a function of a random variable Suppose we have a random variable and we have a function where is real-valued function. Suppose we want to calculate the expected value of . Define $$ \\begin{align*} E[g(X)] = E[Y] &amp;= \\sum_j y_jp(y_j) \\\\ &amp;= \\sum_i g(x_i) p(x_i) \\\\ \\end{align*} $$ PROOF? Example 5: Suppose we roll a die and define to be the value on the die. Define a new random variable to be . What is ? Using the above, Properties of Expectation Two important properties of expectation are the following: (1) Linearity of expectation: $$ \\begin{align*} E[aX + b] &amp;= aE[X] + b \\end{align*} $$","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="strncat's notebook" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">strncat&#39;s notebook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">about</a></div>
      </nav></div>
  
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Random Variables</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-07-26T07:01:36-07:00" itemprop="datePublished">Jul 26, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <table border="0"><tr><td bgcolor="#FFFDD0">
<b>0 References</b>
</td></tr></table>
<p>My study notes from CS109 http://web.stanford.edu/class/archive/cs/cs109/cs109.1188// <br />
Specifically: http://web.stanford.edu/class/archive/cs/cs109/cs109.1188/lectures/06_random_variables.pdf
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>What is a Random Variable?</b>
</td></tr></table>
<p>A random variable is a real-valued function defined on a sample space. Why define a random variable? sometimes instead of being interested in the individual outcomes of an experiment, we are interested in some groups of the outcomes or more formally some <b>function of the outcome</b>. 
<br />
<br />
<b>Example 1:</b>
<br /> 
Suppose we’re interested in <i>counting</i> the number of heads in <script type="math/tex">5</script> trials of flipping a coin. We can define a random variable <script type="math/tex">Y</script> to represent the number of heads in 5 trials. Using <script type="math/tex">Y</script>, we can now refer to the probability of seeing two heads in 5 trials as <script type="math/tex">P(Y=2)</script>. This is much simpler that listing the exact outcomes we’re interested in which are getting heads in trial 1 and 2 or getting heads in 1 or 3 or getting heads in trials 1 and 4 only and so on. 
<br />
<br />
<b>Example 2:</b>
<br /> 
Suppose we roll two dice and we’re interested in the sum of the two dice. We define a random variable <script type="math/tex">X</script> to be the sum of two dice (function of outcomes). We can now refer to the probability that the sum of the dice is 7 as <script type="math/tex">P(X=7)</script>. This is much simpler that saying that we want the probability of seeing any of these outcomes: <script type="math/tex">(3,4),(4,3),(2,5),(5,2),(1,6),(6,1)</script>.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Discrete Random Variables</b>
</td></tr></table>
<p>If our random variable takes on countable values <script type="math/tex">x_1, x_2, x_3,...,x_n</script>, we call it a discrete random variable. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Probability Mass Function</b>
</td></tr></table>
<p>Suppose we have a random variable <script type="math/tex">X</script> that takes on a discrete values in <script type="math/tex">R_X = \{k_1, k_2,...,k_n\}</script>. Define the probability mass function <script type="math/tex">p_X(k)</script> to be the probability that <script type="math/tex">X</script> takes on a particular value <script type="math/tex">k</script>. In other words, the PMF is defined as</p>
<div center="">
$$
\begin{align*}
p_X(k) = P(X = k)
\end{align*}
$$
</div>
<p>Furthermore, the PMF of <script type="math/tex">X</script> satisfies:</p>
<div center="">
$$
\begin{align*}
\sum_{i=1}^{\infty} p_X(k_i) = 1
\end{align*}
$$
</div>
<p>This also means that for any value <script type="math/tex">k</script> that is not in <script type="math/tex">R_X</script>, we have <script type="math/tex">p_X(k) = 0</script>, in other words,</p>
<div center="">
$$
\begin{align*}
 P(X=k) = \Big\{ \begin{array}{@{}lr@{}}
        p_X(k) \quad \text{ for } k \in R_X \\
        0 \quad \quad \quad \text{ otherwise} \\
        \end{array}
\end{align*}
$$
</div>
<p>We can also refer to <script type="math/tex">p_X(k)</script> as just <script type="math/tex">p(k)</script> if the random variable is clear from the context. 
<br />
<br />
<b>Example 2:</b>
<br /> 
Suppose we roll the two dice again from example 2. Define a random variable <script type="math/tex">X</script> to be to the sum of the two dice. We know <script type="math/tex">R_X = \{2,3,4,5,6,7,8,9,10,11,12\}</script>. Below is a graph of the <script type="math/tex">PMF</script> of <script type="math/tex">X</script>, <script type="math/tex">p_X(k)</script> for all values in <script type="math/tex">R_X</script>.
<img src="http://localhost:4000/assets/random/pmf.png" width="100%" />
<br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Cumulative Distribution Function</b>
</td></tr></table>
<p>Now instead of defining the probability that a random variable <script type="math/tex">X</script> takes on a particular value, we define a new function, the cumulative distribution function (CDF) that gives the probability that <script type="math/tex">X</script> is less than or equal to a particular value.</p>
<div center="">
$$
\begin{align*}
F_X(k) = F(k) = P(X \leq k), \quad \text{ where } -\infty &lt; k &lt; \infty
\end{align*}
$$
</div>
<p>For a discrete random variable, this will be just the sum of all variables</p>
<div center="">
$$
\begin{align*}
F_X(k) = F(k) = \sum_{\text{ all } i \leq k} p(i)
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Expectation</b>
</td></tr></table>
<p>The expectation or expected value of a random variable <script type="math/tex">X</script> is defined as:</p>
<div center="">
$$
\begin{align*}
E[X] = \sum_{k:p(k)&gt;0} p(k)*k
\end{align*}
$$
</div>
<p>In other words, the expected value is a weighted average of the value of the random variable (values weighted by their probabilities).
<br />
<br />
<b>Example 4:</b>
<br /> 
Suppose we roll two dice again from example 2 and 3. Define a random variable <script type="math/tex">X</script> to be to the sum of the two dice. We can use our PMF from the previous section to compute the expected value as</p>
<div center="">
$$
\begin{align*}
E[X] &amp;= 2*P(X=2) + 3*P(X=3) + 4*P(X=4) + 5 * P(X=5) * 6*P(X=6) + 7*P(X=7) \\
\\ &amp;+ 8*P(X=8) + 9*P(X=9) + 10*P(X=10) + 11*P(X=11) * 12*P(X=12) \\
E[X] &amp;= 2*\frac{1}{36} + 3*\frac{2}{36} + 4*\frac{3}{36} + 5*\frac{4}{36} + 6*\frac{5}{36} + 7*\frac{6}{36} + 8*\frac{5}{36} + 9*\frac{4}{36} + 10*\frac{3}{36} \\
&amp;+ 11*\frac{2}{36} + 12*\frac{1}{36} = 7
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Expectation of a function of a random variable</b>
</td></tr></table>
<p>Suppose we have a random variable <script type="math/tex">X</script> and we have a function <script type="math/tex">g</script> where <script type="math/tex">g</script> is real-valued function. Suppose we want to calculate the expected value of <script type="math/tex">g(X)</script>. Define</p>
<div center="">
$$
\begin{align*}
E[g(X)] = E[Y] &amp;= \sum_j y_jp(y_j) \\
&amp;= \sum_i g(x_i) p(x_i) \\
\end{align*}
$$
</div>
<p>PROOF?
<br />
<br />
<b>Example 5:</b>
<br /> 
Suppose we roll a die and define <script type="math/tex">X</script> to be the value on the die. Define a new random variable <script type="math/tex">Y</script> to be <script type="math/tex">X^2</script>. What is <script type="math/tex">E[Y]</script>?<br />
<br />
Using the above, <script type="math/tex">E[Y] = E[X^2] = \sum_i (k_i^2)p(k_i) = 1/6*(1+4+9+16+25+36) \approx 15.167</script>
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Properties of Expectation</b>
</td></tr></table>
<p>Two important properties of expectation are the following: <br />
(1) Linearity of expectation:</p>
<div center="">
$$
\begin{align*}
 E[aX + b] &amp;= aE[X] + b 
\end{align*}
$$
</div>

<p>(2) Expectation of the sum of two random variables is the sum of expectation of the two random variables:</p>

<div center="">
$$
\begin{align*}
 E[X + Y] &amp;= E[X] + E[Y]
\end{align*}
$$
</div>

<p><b>Example 6:</b>
<br /> 
Suppose we roll a die and let <script type="math/tex">X</script> be a random variable representing the outcome of the roll. Suppose also that you will a number of dollars equals to <script type="math/tex">3X+5</script>. What is the expected value of your winnings? We can let <script type="math/tex">Y</script> be a random variable representing our winnings. Now we have</p>

<div center="">
$$
\begin{align*}
E[Y] = E[6X^2+5] &amp;= \sum_i (3x_i + 5)p(x_i) \\
&amp;= \frac{1}{6} \sum_{i=1}^6 3x_i+5 \\
&amp;= \frac{1}{6} (8+11+14+17+20+23) = 15.5
\end{align*}
$$
</div>

<p>However using the linearity of expectation, we know that <script type="math/tex">E[X]=3.5</script>. Therefore we could do the following:</p>
<div center="">
$$
\begin{align*}
E[Y] = E[3X+5] &amp;= 3E[X]+5  \\
&amp;= 3(3.5) + 5 = 15.5
\end{align*}
$$
</div>

<p><b>Example 7:</b>
<br /> 
Suppose we roll two dice again and we’re interested in the expectation of the sum of two dice. We calculated this value previously in example 4 using the PMF. Let’s use the second property of expectation. Let <script type="math/tex">X_1</script> be a random variable representing the value of the first die and <script type="math/tex">X_2</script> be a random variable representing the sum value of the second die. Let the sum of the two dice be <script type="math/tex">X_1 + X_2</script>.</p>
<div center="">
$$
\begin{align*}
E[X_1 + X_2] = E[X_1] + E[X_2] = 7
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Example 8: St. Petersburg Paradox</b>
</td></tr></table>
<p>A fair coin comes up heads with <script type="math/tex">p = 0.5</script>. We flip the coin until we see the first tails. We will then win <script type="math/tex">2^n</script> dollars where <script type="math/tex">n</script> is the number of heads seen before the first tail. How much would you pay to play?
<br />
<br />
Let’s define the following random variables: <br />
Let <script type="math/tex">Y</script> be the number of “heads” before the the first “tails”.<br />
Let <script type="math/tex">W</script> be a random variable representing our winnings. <script type="math/tex">W = 2^Y</script>. 
<br />
<br />
What is the probability of seeing <script type="math/tex">i</script> heads before seeing the first tail on the <script type="math/tex">i+1</script>th trial? <script type="math/tex">P(Y = i) = (1/2) * (1/2) * ... = (1/2)^{i+1}</script>. This is because  we stop at the <script type="math/tex">i+1</script> flip which is a tail. Each outcome has a probability equals to <script type="math/tex">1/2</script>.
<br />
<br />
What is the expected value of our winnings?<br /></p>
<div center="">
$$
\begin{align*}
E[W] = E[2^Y] &amp;= \sum_i 2^i P(Y=i) =  \sum_i 2^i p(i) \\
&amp;= (\frac{1}{2})^1 2^0 + (\frac{1}{2})^2 2^1 +  (\frac{1}{2})^3 2^2 + ... \\
&amp;= \sum_i^{\infty} (\frac{1}{2})^{i+1} 2^i = \infty
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br /></p>
<table border="0"><tr><td bgcolor="#FFFDD0">
<b>Example 9: Roulette</b>
</td></tr></table>
<p>Consider an even money bet (betting “Red” in Roulette). <script type="math/tex">p=18/38</script> you win <script type="math/tex">Y</script> dollars, otherwise <script type="math/tex">1-p</script> you lose <script type="math/tex">Y</script> dollars. Consider the following strategy:
(1) Let <script type="math/tex">Y=1</script>. <br />
(2) Bet <script type="math/tex">Y</script>.<br />
(3) If win then stop.<br />
(4) else let <script type="math/tex">Y=2Y</script> go to step 2.<br />
<br />
What is the expected value of our winning? <br />
Define <script type="math/tex">Z</script> to be the winnings until we stop.</p>
<div center="">
$$
\begin{align*}
E[Z] &amp;= p*1 + (1-p)p*(2-1) + (1-p)^2p*(4-2-1) + ... \\
&amp;= \sum_{i=0}^{\infty} p(1-p)^i(2^i - \sum_{j=0}^{i-1}2^j) \\
&amp;= p\sum_{i=0}^{\infty} (1-p)^i = p\frac{1}{1-(1-p)} = 1
\end{align*}
$$
</div>


  </div><div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /jekyll/update/2019/07/26/random-variables.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier =  /jekyll/update/2019/07/26/random-variables; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://strncat-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/jekyll/update/2019/07/26/random-variables.html" hidden></a>
</article>
		 
      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">strncat&#39;s notebook</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">strncat&#39;s notebook</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>hello</p>
      </div>
    </div>

  </div>

</footer>
-->

	
  </body>

</html>
