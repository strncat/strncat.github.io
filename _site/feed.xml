<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-08T09:09:06-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">The Jordan Canonical Form</title><link href="http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form.html" rel="alternate" type="text/html" title="The Jordan Canonical Form" /><published>2024-09-15T01:01:36-07:00</published><updated>2024-09-15T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form.html"><![CDATA[<p>Last time we proved that If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\). This therefore lead to the conclusion that \(T\) is diagonalizable.
<!------------------------------------------------------------------------------------></p>
<h4><b>A Test For Diagonalizability</b></h4>
<p>We studied previously a few ways a linear operator can be tested for diagonalizability. From lecture 25,</p>
<div class="purdiv">
Theorem (5.8(a))
</div>
<div class="purbdiv">
\(T\) is diagonalizable if and only if
<ul style="list-style-type:lower-alpha">
	<li>The characteristic polynomial splits over its field \(\mathbf{R}\) or \(\mathbf{C}\)</li>
	<li>For each eigenvalue of \(T\), its geometric multiplicity (\(\dim(E_{\lambda})\)) = algebraic multiplicity</li>
</ul>
</div>
<p><br />
We mentioned last time too that if \(V\) is over \(\mathbf{C}\), the the characteristic polynomial always splits. Note also that you can have \((a)\) but not \((b)\). For example</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>splits but doesn’t satisfy $(b)$
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Jordan Canonical Form</b></h4>
<p>Based on the previous observation. It turns out there is a nice form that we can put \(T\) into in order to achieve \((a)\) and \((b)\)
<br /></p>
<div class="purdiv">
Theorem (JCF)
</div>
<div class="purbdiv">
If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A Jordan Block is a square matrix of the form
$$
\begin{align*}
\begin{pmatrix}
\lambda
\end{pmatrix}
\quad \text{ or } \quad 
\begin{pmatrix}
\lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
For example, a \(2 \times 2\) and a \(3 \times 3\) matrices, Jordan Canonical Form look like</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp; \lambda
\end{pmatrix}
,
\begin{pmatrix}
\lambda &amp; 1 &amp; 0 \\
0 &amp; \lambda &amp; 1 \\
0 &amp; 0 &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A square matrix \(A\) is in Jordan Canonical Form if
$$
\begin{align*}
A =
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_{l}
\end{pmatrix},
\ \text{ where $A_j$ is a Jordan block}
\end{align*}
$$
</div>
<p><br />
You can think of this matrix as more of a generalization of a diagonal matrix.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>The following are examples of matrix in Jordan Canonical Form</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix},
B = 
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that the characteristic polynomial of both \(A\) and \(B\) is \((1-t)^3(2-t)^2\).</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we proved that If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\). This therefore lead to the conclusion that \(T\) is diagonalizable. A Test For Diagonalizability We studied previously a few ways a linear operator can be tested for diagonalizability. From lecture 25, Theorem (5.8(a)) \(T\) is diagonalizable if and only if The characteristic polynomial splits over its field \(\mathbf{R}\) or \(\mathbf{C}\) For each eigenvalue of \(T\), its geometric multiplicity (\(\dim(E_{\lambda})\)) = algebraic multiplicity We mentioned last time too that if \(V\) is over \(\mathbf{C}\), the the characteristic polynomial always splits. Note also that you can have \((a)\) but not \((b)\). For example $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix} \end{align*} $$ splits but doesn’t satisfy $(b)$ Jordan Canonical Form Based on the previous observation. It turns out there is a nice form that we can put \(T\) into in order to achieve \((a)\) and \((b)\) Theorem (JCF) If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form. Definition A Jordan Block is a square matrix of the form $$ \begin{align*} \begin{pmatrix} \lambda \end{pmatrix} \quad \text{ or } \quad \begin{pmatrix} \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda \end{pmatrix} \end{align*} $$ For example, a \(2 \times 2\) and a \(3 \times 3\) matrices, Jordan Canonical Form look like $$ \begin{align*} \begin{pmatrix} \lambda &amp; 1 \\ 0 &amp; \lambda \end{pmatrix} , \begin{pmatrix} \lambda &amp; 1 &amp; 0 \\ 0 &amp; \lambda &amp; 1 \\ 0 &amp; 0 &amp; \lambda \end{pmatrix} \end{align*} $$ Definition A square matrix \(A\) is in Jordan Canonical Form if $$ \begin{align*} A = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_{l} \end{pmatrix}, \ \text{ where $A_j$ is a Jordan block} \end{align*} $$ You can think of this matrix as more of a generalization of a diagonal matrix. Examples The following are examples of matrix in Jordan Canonical Form $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ Note here that the characteristic polynomial of both \(A\) and \(B\) is \((1-t)^3(2-t)^2\).]]></summary></entry><entry><title type="html">Lecture 36: Isometries</title><link href="http://localhost:4000/jekyll/update/2024/09/14/lec36-isometries.html" rel="alternate" type="text/html" title="Lecture 36: Isometries" /><published>2024-09-14T01:01:36-07:00</published><updated>2024-09-14T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/14/lec36-isometries</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/14/lec36-isometries.html"><![CDATA[<p>We’ve studied special classes of linear maps over an inner product space to itself like normal maps and self-adjoint maps. Today we will study a new special class
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T:V \rightarrow V\) is an isometry if
$$
\begin{align*}
\langle T(x), T(y) \rangle = \langle x, y \rangle
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
For example, in \(\mathbf{R}^3\), this means that isometries preserve lengths and angles. <br />
When \(V\) is over \(\mathbf{C}\), an isometry is sometimes called unitary while if \(V\) is over \(\mathbf{R}\), \(T\) is called orthogonal.
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let \(T = L_A\) where</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
\cos\theta &amp; - \sin\theta \\
\sin\theta &amp; \cos\theta 
\end{pmatrix}
\end{align*}
$$
</div>
<p>Remember from the previous lecture, \(A\) was normal but not self-adjoint. We claim that \(A\) is an isometry. We can verify this by evaluating</p>
<div>
$$
\begin{align*}
L_A 
\begin{pmatrix}
x \\
y
\end{pmatrix}
&amp;=
\begin{pmatrix}
x\cos\theta - y\sin\theta \\
x\sin\theta + y\cos\theta 
\end{pmatrix}
\end{align*}
$$
</div>
<p>while,</p>
<div>
$$
\begin{align*}
\left\langle
L_A 
\begin{pmatrix}
x_1 \\
y_1
\end{pmatrix},
L_A 
\begin{pmatrix}
x_2 \\
y_2
\end{pmatrix}
\right\rangle
&amp;=
(x_1\cos\theta - y_1\sin\theta)(x_2\sin\theta + y_2\cos\theta) \\
&amp; =  x_1y_1 + x_2y_2
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Let</p>
<div>
$$
\begin{align*}
V &amp;= C^0([0,1]), \mathbf{C}) \\
  &amp;= \{ F(t) = f(t) + ig(t) \ | \ f, g \in C^0([0,1)]\} \\
\langle F, G \rangle &amp;= \int_0^1 F(t)\overline{G(t)} dt
\end{align*}
$$
</div>
<p>Suppose that \(H \in V\) such that \(|H(t)| = 1\). For real numbers, we have only two functions 1 and -1 but for complex numbers, we have many of these functions. For example \(H(t) = e^{ih(t)}\).
<br />
<br />
Now we’re going to use this function to define an isometry. Specifically,</p>
<div>
$$
\begin{align*}
T \ : \ &amp;V \rightarrow V 
   &amp;F \rightarrow FH
\end{align*}
$$
</div>
<p>This is an isometry. \(H\) is kind of rotating by some angle depending on \(t\). To verify this,</p>
<div>
$$
\begin{align*}
\langle T(F), T(G) \rangle &amp;= \int_0^1 (F(t)H(t))(\overline{G}(t)\overline{H}(t)) \ dt \\
                           &amp;= \int_0^1 F(t)\overline{G}(t)  \ dt \\
						   &amp;= \langle F, G \rangle
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Equivalent Conditions for Isometry</b></h4>
<p>What can we say about Isometries? What does being an Isometry imply?</p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
The following are equivalent
<ol type="a">
	<li>\(T: V \rightarrow V\) is an isometry</li>
	<li>\(TT^* = T^*T = I_V \) (an isometry is a normal map) </li>
	<li>If \(\beta\) is an orthonormal basis, then so is \(T(\beta)\). [In other words, \(T\) is an Isometry if it takes one orthonormal basis to another orthonormal basis].</li>
	<li>\(T(\beta)\) is orthonormal for some orthonormal \(\beta\). [This is a weaker statement. \(T\) needs to take one orthonormal basis to another at a minimum once (not every)].</li>
	<li>\(\Vert T(x) \Vert = \Vert x \Vert \ \forall x \in V \). So \(T\) preserves lengths.</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Why is \((e)\) true? to see why, we need the following lemma
<br /></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
Suppose 
\(S:V \rightarrow V\) is self-adjoint. If \(\langle S(x), x \rangle = 0\) for all \(x \in V\), then \(S = T_{0_V}\) (the zero map)
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis consisting of eigenvectors of \(S\) (this is possible because \(S\) is self adjoint). So \(S(v_j) = \lambda_j v_j\) for \(j = 1,...,n\). Then</p>
<div>
$$
\begin{align*}
0 &amp;= \langle S(v_j), v_j \rangle \quad \text{(by the given assumption)} \\
0 &amp;= \langle \lambda_j v_j, v_j \rangle \\
0 &amp;= \lambda_j \langle v_j, v_j \rangle \\
0 &amp;= \lambda_j \Vert v_j \Vert^2 \\
\end{align*}
$$
</div>
<p>\(v_j\) is a basis vector so it’s not zero. Therefore, we must have \(\lambda_j = 0\) for \(j = 1,...,n\). This implies that \(S\) is the zero map because all the eigenvalues are zero but we have a basis of eigenvectors. So every vector \(x\) can be written as a linear combination of the basis vectors and when we apply \(S\), we get a linear combination of \(\lambda_j v_j\) where all the \(\lambda_j\)’s are zero so \(S\) must be the zero vector.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Proof of Theorem</b></h4>
<p>We’re ready to prove the theorem. 
<br />
<br />
\((a) \implies (b):\)<br />
We’re given that \(T\) is an isometry so we know by definition that \(T\) preserves the inner product so \(\langle T(x), T(y) \rangle = \langle x, y \rangle\). We want to show that \(TT^* = T^*T\). We first observe that \(T^*T\) is also self-adjoint (which means it’s diagonalizable). To see this, see that the adjoint of \((T^*T)\) is \((T^*T)^* = T^*(T^*)^* = T^*T\). Since \(T^*T\) is self-adjoint, then we can add to it any multiple of the identity map and the new map is still self adjoint (proof is in last lecture?). So \(T^*T - I_V\) is self adjoint. Now observe</p>
<div>
$$
\begin{align*}
\langle (T^*T - I_V)(x), x \rangle &amp;= \langle T^*T(x) - x, x \rangle \\
                                 &amp;= \langle T^*T(x), x \rangle - \langle x, x \rangle \\
								 &amp;= \langle T(x), T(x) \rangle - \langle x, x \rangle \quad \text{(by definition of adjoint)}\\
								 &amp;= 0 \text{(Since $T$ is an Isometry)}\\
                                     
\end{align*}
$$
</div>
<p>So we showed that \(T^*T - I_V\) is a self adjoint map where for any vector \(x\), we have \(\langle (T^*T - I_V)(x), x \rangle - 0\). So this satisfies the lemma we proved previously. This implies that \(T^*T - I_V\) is the zero map. So</p>
<div>
$$
\begin{align*}
T^*T - I_V = T_0 \\
T^*T = I_V \\     
T^* = T^{-1}          
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
<!------------------------------------------------------------------------------------>
\((b) \implies (c):\)<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij}\). We know \(T(\beta) = \{T(v_1),...,T(v_n)\}\). We need to show that \(T(\beta)\) is an orthonormal basis which means that \(\langle T(v_i), T(v_j) \rangle = \delta_{ij}\). Now observe</p>
<div>
$$
\begin{align*}
\langle (T(v_i), T(v_j) \rangle &amp;= \langle v_i, T^*(T(v_j)) \rangle \\
                                 &amp;=  \langle v_i, v_j \rangle \quad \text{(by (b), $T^*T = I_V$)} \\
								 &amp;= \delta_{ij}
                                     
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>\((c) \implies (d):\) This is trivial since \(c\) is a stronger statement
<br />
<br />
<!------------------------------------------------------------------------------------>
\((d) \implies (e):\) 
<br />
We need to show that if there is some orthonormal basis that can be taken to another orthonormal basis, then this means that \(T\) preserves all lengths.
<br />
<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis with \(T(\beta) = \{T(v_1),...,T(v_n)\} = \{w_1, ..., w_n\}\) also an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij} = \langle w_i, w_j \rangle\). All these assumptions are by (d). Now we know to prove that \(\Vert T(x) \Vert = \Vert x \Vert\) for any \(x \in V\). Observe that</p>
<div>
$$
\begin{align*}
x &amp;= \sum_j a_jv_j \\
T(x) &amp;= \sum_j a_j T(v_j) = \sum_j  a_j w_j                               
\end{align*}
$$
</div>
<p>So now we use this to evaluate</p>
<div>
$$
\begin{align*}
\Vert T(x) \Vert^2 &amp;= \langle T(x), T(x) \rangle \\
                  &amp;= \langle \sum_i  a_i w_i, \sum_j  a_j w_j \rangle \\ 
				  &amp;= \sum_{i,j} a_i \bar{a_j} \langle w_i, w_j \rangle  \\
				  &amp;= \sum_{i} |a_i|^2 = \langle x,x \rangle                    
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>\((e) \implies (a):\) <br />
We want to show that if we preserve lengths, then we actually preserve all inner products so \(\Vert T(x) \Vert = \Vert x \Vert \implies \langle T(x), T(x) \rangle = \langle x, x \rangle\). To show this, we can use the following trick</p>
<div>
$$
\begin{align*}
\Vert x + y \Vert^2 &amp;= \langle x+y, x+y \rangle \\
                    &amp;= \Vert x\Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2   \\
					&amp;= \Vert x\Vert^2 + \langle x, y \rangle + \overline{\langle x, y \rangle} + \Vert y \Vert^2
\end{align*}
$$
</div>
<p>What is \(\langle x, y \rangle + \overline{\langle x, y \rangle}\)? It’s 2 times the real part of \(\langle x, y \rangle + \overline{\langle x, y \rangle}\). I don’t understand why? [TODO]</p>
<div>
$$
\begin{align*}
2Re\langle x, y \rangle &amp;= \frac{1}{2}(\Vert x + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2) \\
Im\langle x, y \rangle &amp;= -\frac{1}{2}(\Vert ix + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2)
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Isometries Are Rare</b></h4>
<p>From research, we know that Isometries are actually rare. One fact is that \(T\) is an isometry of \(\mathbf{R}^2\) if and only if</p>
<div>
$$
\begin{align*}
T = L_A \text{ for } 
A = 
\begin{pmatrix}
\cos\theta &amp; - \sin\theta \\
\sin\theta &amp; \cos\theta 
\end{pmatrix}
\text{ or }
A = 
\begin{pmatrix}
\cos\theta &amp;  \sin\theta \\
\sin\theta &amp; -\cos\theta 
\end{pmatrix}
\end{align*}
$$
</div>
<p>So these are the only possibilities for isometries for \(\mathbf{R}^2\).</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’ve studied special classes of linear maps over an inner product space to itself like normal maps and self-adjoint maps. Today we will study a new special class Definition \(T:V \rightarrow V\) is an isometry if $$ \begin{align*} \langle T(x), T(y) \rangle = \langle x, y \rangle \end{align*} $$ For example, in \(\mathbf{R}^3\), this means that isometries preserve lengths and angles. When \(V\) is over \(\mathbf{C}\), an isometry is sometimes called unitary while if \(V\) is over \(\mathbf{R}\), \(T\) is called orthogonal. Example 1 Let \(T = L_A\) where $$ \begin{align*} A = \begin{pmatrix} \cos\theta &amp; - \sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix} \end{align*} $$ Remember from the previous lecture, \(A\) was normal but not self-adjoint. We claim that \(A\) is an isometry. We can verify this by evaluating $$ \begin{align*} L_A \begin{pmatrix} x \\ y \end{pmatrix} &amp;= \begin{pmatrix} x\cos\theta - y\sin\theta \\ x\sin\theta + y\cos\theta \end{pmatrix} \end{align*} $$ while, $$ \begin{align*} \left\langle L_A \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, L_A \begin{pmatrix} x_2 \\ y_2 \end{pmatrix} \right\rangle &amp;= (x_1\cos\theta - y_1\sin\theta)(x_2\sin\theta + y_2\cos\theta) \\ &amp; = x_1y_1 + x_2y_2 \end{align*} $$ Example 2 Let $$ \begin{align*} V &amp;= C^0([0,1]), \mathbf{C}) \\ &amp;= \{ F(t) = f(t) + ig(t) \ | \ f, g \in C^0([0,1)]\} \\ \langle F, G \rangle &amp;= \int_0^1 F(t)\overline{G(t)} dt \end{align*} $$ Suppose that \(H \in V\) such that \(|H(t)| = 1\). For real numbers, we have only two functions 1 and -1 but for complex numbers, we have many of these functions. For example \(H(t) = e^{ih(t)}\). Now we’re going to use this function to define an isometry. Specifically, $$ \begin{align*} T \ : \ &amp;V \rightarrow V &amp;F \rightarrow FH \end{align*} $$ This is an isometry. \(H\) is kind of rotating by some angle depending on \(t\). To verify this, $$ \begin{align*} \langle T(F), T(G) \rangle &amp;= \int_0^1 (F(t)H(t))(\overline{G}(t)\overline{H}(t)) \ dt \\ &amp;= \int_0^1 F(t)\overline{G}(t) \ dt \\ &amp;= \langle F, G \rangle \end{align*} $$ Equivalent Conditions for Isometry What can we say about Isometries? What does being an Isometry imply? Theorem The following are equivalent \(T: V \rightarrow V\) is an isometry \(TT^* = T^*T = I_V \) (an isometry is a normal map) If \(\beta\) is an orthonormal basis, then so is \(T(\beta)\). [In other words, \(T\) is an Isometry if it takes one orthonormal basis to another orthonormal basis]. \(T(\beta)\) is orthonormal for some orthonormal \(\beta\). [This is a weaker statement. \(T\) needs to take one orthonormal basis to another at a minimum once (not every)]. \(\Vert T(x) \Vert = \Vert x \Vert \ \forall x \in V \). So \(T\) preserves lengths. Why is \((e)\) true? to see why, we need the following lemma Lemma Suppose \(S:V \rightarrow V\) is self-adjoint. If \(\langle S(x), x \rangle = 0\) for all \(x \in V\), then \(S = T_{0_V}\) (the zero map) Proof Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis consisting of eigenvectors of \(S\) (this is possible because \(S\) is self adjoint). So \(S(v_j) = \lambda_j v_j\) for \(j = 1,...,n\). Then $$ \begin{align*} 0 &amp;= \langle S(v_j), v_j \rangle \quad \text{(by the given assumption)} \\ 0 &amp;= \langle \lambda_j v_j, v_j \rangle \\ 0 &amp;= \lambda_j \langle v_j, v_j \rangle \\ 0 &amp;= \lambda_j \Vert v_j \Vert^2 \\ \end{align*} $$ \(v_j\) is a basis vector so it’s not zero. Therefore, we must have \(\lambda_j = 0\) for \(j = 1,...,n\). This implies that \(S\) is the zero map because all the eigenvalues are zero but we have a basis of eigenvectors. So every vector \(x\) can be written as a linear combination of the basis vectors and when we apply \(S\), we get a linear combination of \(\lambda_j v_j\) where all the \(\lambda_j\)’s are zero so \(S\) must be the zero vector. Proof of Theorem We’re ready to prove the theorem. \((a) \implies (b):\) We’re given that \(T\) is an isometry so we know by definition that \(T\) preserves the inner product so \(\langle T(x), T(y) \rangle = \langle x, y \rangle\). We want to show that \(TT^* = T^*T\). We first observe that \(T^*T\) is also self-adjoint (which means it’s diagonalizable). To see this, see that the adjoint of \((T^*T)\) is \((T^*T)^* = T^*(T^*)^* = T^*T\). Since \(T^*T\) is self-adjoint, then we can add to it any multiple of the identity map and the new map is still self adjoint (proof is in last lecture?). So \(T^*T - I_V\) is self adjoint. Now observe $$ \begin{align*} \langle (T^*T - I_V)(x), x \rangle &amp;= \langle T^*T(x) - x, x \rangle \\ &amp;= \langle T^*T(x), x \rangle - \langle x, x \rangle \\ &amp;= \langle T(x), T(x) \rangle - \langle x, x \rangle \quad \text{(by definition of adjoint)}\\ &amp;= 0 \text{(Since $T$ is an Isometry)}\\ \end{align*} $$ So we showed that \(T^*T - I_V\) is a self adjoint map where for any vector \(x\), we have \(\langle (T^*T - I_V)(x), x \rangle - 0\). So this satisfies the lemma we proved previously. This implies that \(T^*T - I_V\) is the zero map. So $$ \begin{align*} T^*T - I_V = T_0 \\ T^*T = I_V \\ T^* = T^{-1} \end{align*} $$ And we are done. \((b) \implies (c):\) Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij}\). We know \(T(\beta) = \{T(v_1),...,T(v_n)\}\). We need to show that \(T(\beta)\) is an orthonormal basis which means that \(\langle T(v_i), T(v_j) \rangle = \delta_{ij}\). Now observe $$ \begin{align*} \langle (T(v_i), T(v_j) \rangle &amp;= \langle v_i, T^*(T(v_j)) \rangle \\ &amp;= \langle v_i, v_j \rangle \quad \text{(by (b), $T^*T = I_V$)} \\ &amp;= \delta_{ij} \end{align*} $$ \((c) \implies (d):\) This is trivial since \(c\) is a stronger statement \((d) \implies (e):\) We need to show that if there is some orthonormal basis that can be taken to another orthonormal basis, then this means that \(T\) preserves all lengths. Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis with \(T(\beta) = \{T(v_1),...,T(v_n)\} = \{w_1, ..., w_n\}\) also an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij} = \langle w_i, w_j \rangle\). All these assumptions are by (d). Now we know to prove that \(\Vert T(x) \Vert = \Vert x \Vert\) for any \(x \in V\). Observe that $$ \begin{align*} x &amp;= \sum_j a_jv_j \\ T(x) &amp;= \sum_j a_j T(v_j) = \sum_j a_j w_j \end{align*} $$ So now we use this to evaluate $$ \begin{align*} \Vert T(x) \Vert^2 &amp;= \langle T(x), T(x) \rangle \\ &amp;= \langle \sum_i a_i w_i, \sum_j a_j w_j \rangle \\ &amp;= \sum_{i,j} a_i \bar{a_j} \langle w_i, w_j \rangle \\ &amp;= \sum_{i} |a_i|^2 = \langle x,x \rangle \end{align*} $$ \((e) \implies (a):\) We want to show that if we preserve lengths, then we actually preserve all inner products so \(\Vert T(x) \Vert = \Vert x \Vert \implies \langle T(x), T(x) \rangle = \langle x, x \rangle\). To show this, we can use the following trick $$ \begin{align*} \Vert x + y \Vert^2 &amp;= \langle x+y, x+y \rangle \\ &amp;= \Vert x\Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2 \\ &amp;= \Vert x\Vert^2 + \langle x, y \rangle + \overline{\langle x, y \rangle} + \Vert y \Vert^2 \end{align*} $$ What is \(\langle x, y \rangle + \overline{\langle x, y \rangle}\)? It’s 2 times the real part of \(\langle x, y \rangle + \overline{\langle x, y \rangle}\). I don’t understand why? [TODO] $$ \begin{align*} 2Re\langle x, y \rangle &amp;= \frac{1}{2}(\Vert x + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2) \\ Im\langle x, y \rangle &amp;= -\frac{1}{2}(\Vert ix + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2) \end{align*} $$ Isometries Are Rare From research, we know that Isometries are actually rare. One fact is that \(T\) is an isometry of \(\mathbf{R}^2\) if and only if $$ \begin{align*} T = L_A \text{ for } A = \begin{pmatrix} \cos\theta &amp; - \sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix} \text{ or } A = \begin{pmatrix} \cos\theta &amp; \sin\theta \\ \sin\theta &amp; -\cos\theta \end{pmatrix} \end{align*} $$ So these are the only possibilities for isometries for \(\mathbf{R}^2\).]]></summary></entry><entry><title type="html">Section 6.3: Theorem 6.11</title><link href="http://localhost:4000/jekyll/update/2024/09/13/6.3-theorem-6.11.html" rel="alternate" type="text/html" title="Section 6.3: Theorem 6.11" /><published>2024-09-13T01:01:36-07:00</published><updated>2024-09-13T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/13/6.3-theorem-6.11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/13/6.3-theorem-6.11.html"><![CDATA[<div class="purdiv">
Theorem 6.11
</div>
<div class="purbdiv">
Let \(V\) be an inner product space, and let \(T\) and \(U\) be linear operators on \(V\) whose adjoint exist. Then
<ol type="a">
	<li>\(T + U\) has an adjoint, and \((T+U)^{*} = T^* + U^*\).</li>
	<li>\(cT\) has an adjoint, and \((cT)^* = \bar{c}T^*\) for any \(c \in \mathbf{F}\).</li>
	<li>\(TU\) has an adjoint, and \((TU)^* = U^* T^*\).</li>
	<li>\(T^*\) has an adjoint, and \(T^{**} = T\).</li>
	<li>\(I\) has an adjoint, and \(I^* = I\).</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
For (a)</p>
<div>
$$
\begin{align*}
\langle (T + U)x, y \rangle &amp;= \langle T(x) + U(x), y \rangle \\
                        &amp;= \langle T(x), y \rangle + \langle U(x), y \rangle \\
                        &amp;= \langle x, T^*(y) \rangle + \langle x, U^*(y) \rangle \\
                        &amp;= \langle x, T^*(y) + U^*(y) \rangle \\                        
                        &amp;= \langle x, (T^* + U^*)(y) \rangle \\                        				
\end{align*}
$$
</div>
<p>Therefore, \((T+U)^*\) exists and it equals to \(T^*+U^*\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 6.11 (Corollary)
</div>
<div class="purbdiv">
Let \(A\) and \(B\) be \(n \times n\) matrices. Then
<ol type="a">
	<li>\((A + B)^* = A^* + B^*\).</li>
	<li>\((cA)^* = \bar{c}A^*\) for all \(c \in \mathbf{F}\).</li>
	<li>\((AB)^* = B^*A^*\).</li>
	<li>\(A^{**} = A\).</li>
	<li>\(I^* = I\).</li>
</ol>
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
[TODO]
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.11 Let \(V\) be an inner product space, and let \(T\) and \(U\) be linear operators on \(V\) whose adjoint exist. Then \(T + U\) has an adjoint, and \((T+U)^{*} = T^* + U^*\). \(cT\) has an adjoint, and \((cT)^* = \bar{c}T^*\) for any \(c \in \mathbf{F}\). \(TU\) has an adjoint, and \((TU)^* = U^* T^*\). \(T^*\) has an adjoint, and \(T^{**} = T\). \(I\) has an adjoint, and \(I^* = I\). Proof: For (a) $$ \begin{align*} \langle (T + U)x, y \rangle &amp;= \langle T(x) + U(x), y \rangle \\ &amp;= \langle T(x), y \rangle + \langle U(x), y \rangle \\ &amp;= \langle x, T^*(y) \rangle + \langle x, U^*(y) \rangle \\ &amp;= \langle x, T^*(y) + U^*(y) \rangle \\ &amp;= \langle x, (T^* + U^*)(y) \rangle \\ \end{align*} $$ Therefore, \((T+U)^*\) exists and it equals to \(T^*+U^*\). Theorem 6.11 (Corollary) Let \(A\) and \(B\) be \(n \times n\) matrices. Then \((A + B)^* = A^* + B^*\). \((cA)^* = \bar{c}A^*\) for all \(c \in \mathbf{F}\). \((AB)^* = B^*A^*\). \(A^{**} = A\). \(I^* = I\). Proof: [TODO] References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.3: Exercise 18</title><link href="http://localhost:4000/jekyll/update/2024/09/11/6.3-exercise-18.html" rel="alternate" type="text/html" title="Section 6.3: Exercise 18" /><published>2024-09-11T01:01:36-07:00</published><updated>2024-09-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/11/6.3-exercise-18</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/11/6.3-exercise-18.html"><![CDATA[<div class="ydiv">
Exercise 18
</div>
<div class="ybdiv">
Let \(A\) be an \(n \times n\). Prove that \(\det(A^*) = \overline{A}\).
</div>
<p><br />
Proof:
<br />
<br />
Let \(A\) be a matrix of size \(n \times n\). We first will prove that \(\det(\overline{A}) = \overline{\det{A}}\) by induction on \(n\).
<br />
<br />
Base Case: \(n = 1\). \(\det(\overline{A}) = \overline{a_{11}} = \overline{\det{(A)}}\).
<br />
<br />
Inductive Case: Assume \(\det(\overline{A}) = \overline{\det{A}}\) for \(n-1\). We will prove this for \(n\). Computing the determinant of \(\overline{A}\) by cofactor expansion along the first row and applying the inductive hypothesis</p>
<div>
	$$
	\begin{align*}
	\det(\overline{A}) &amp;= (-1)^{1+1}\overline{a_{11}}\det{\widetilde{\overline{A_{11}}}} 
	                    + (-1)^{1+2}\overline{a_{12}}\det{\widetilde{\overline{A_{12}}}} + ... + \\
						&amp;+ (-1)^{1+n}\overline{a_{1n}}\det{\widetilde{\overline{a_{1n}}}} \\
	&amp;= (-1)^{1+1}\overline{a_{11}}\overline{\det{\widetilde{A_{11}}}} +          (-1)^{1+2}\overline{a_{12}}\overline{\det{\widetilde{A_{11}}}} + ... + \\
		&amp;+ (-1)^{1+n}\overline{a_{1n}}\overline{\det{\widetilde{A_{11}}}} \\
	&amp;= \overline{\det{A}}
	\end{align*}
	$$
</div>
<p>So now we can apply this result to show that</p>
<div>
	$$
	\begin{align*}
	\det(A^*) &amp;= \det(\overline{A^t}) \\
	          &amp;= \overline{\det(A^t)} \\
			  &amp;= \overline{\det{A}} \quad \text{(we prove previously that $\det{A}=\det{A^t}$)}
	\end{align*}
	$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_6_2.html">Reference Solution</a></li>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 18 Let \(A\) be an \(n \times n\). Prove that \(\det(A^*) = \overline{A}\). Proof: Let \(A\) be a matrix of size \(n \times n\). We first will prove that \(\det(\overline{A}) = \overline{\det{A}}\) by induction on \(n\). Base Case: \(n = 1\). \(\det(\overline{A}) = \overline{a_{11}} = \overline{\det{(A)}}\). Inductive Case: Assume \(\det(\overline{A}) = \overline{\det{A}}\) for \(n-1\). We will prove this for \(n\). Computing the determinant of \(\overline{A}\) by cofactor expansion along the first row and applying the inductive hypothesis $$ \begin{align*} \det(\overline{A}) &amp;= (-1)^{1+1}\overline{a_{11}}\det{\widetilde{\overline{A_{11}}}} + (-1)^{1+2}\overline{a_{12}}\det{\widetilde{\overline{A_{12}}}} + ... + \\ &amp;+ (-1)^{1+n}\overline{a_{1n}}\det{\widetilde{\overline{a_{1n}}}} \\ &amp;= (-1)^{1+1}\overline{a_{11}}\overline{\det{\widetilde{A_{11}}}} + (-1)^{1+2}\overline{a_{12}}\overline{\det{\widetilde{A_{11}}}} + ... + \\ &amp;+ (-1)^{1+n}\overline{a_{1n}}\overline{\det{\widetilde{A_{11}}}} \\ &amp;= \overline{\det{A}} \end{align*} $$ So now we can apply this result to show that $$ \begin{align*} \det(A^*) &amp;= \det(\overline{A^t}) \\ &amp;= \overline{\det(A^t)} \\ &amp;= \overline{\det{A}} \quad \text{(we prove previously that $\det{A}=\det{A^t}$)} \end{align*} $$ References: Reference Solution Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.1: Theorem 6.1</title><link href="http://localhost:4000/jekyll/update/2024/09/11/6.1-theorem-6.1.html" rel="alternate" type="text/html" title="Section 6.1: Theorem 6.1" /><published>2024-09-11T01:01:36-07:00</published><updated>2024-09-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/11/6.1-theorem-6.1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/11/6.1-theorem-6.1.html"><![CDATA[<!------------------------------------------------------------------------------------>
<div class="purdiv">
Theorem 6.1
</div>
<div class="purbdiv">
Let \(V\) be an inner product space. Then for \(x, y, z \in V\) and \(c \in \mathbf{F}\), the following statements are true.
<ol type="a">
	<li>\(\langle x, y+z \rangle = \langle x, y \rangle + \langle x, z \rangle\)</li>
	<li>\(\langle x, cy \rangle = \bar{c} \langle x , y \rangle\)</li>
	<li>\(\langle x, 0 \rangle = \langle 0,x \rangle = 0 \) </li>
	<li>\(\langle x, x \rangle = 0 \text{ if and only if } x = 0\)</li>
	<li>If \(\langle x, y \rangle = \langle x, z \rangle\) for all \(x \in V\), then \(y = z\)</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof:</b>
<br />
<br />
For (a)</p>
<div>
$$
\begin{align*}
\langle x, y+z \rangle &amp;= \overline{\langle y+z, x \rangle} \\
                       &amp;=  \overline{\langle y, x \rangle + \langle z, x \rangle} \\
					   &amp;=  \overline{\langle y, x \rangle} + \overline{\langle z, x \rangle} \\
					   &amp;=  \langle x, y \rangle + \langle x, z \rangle \\
\end{align*}
$$
</div>
<p>For (b)</p>
<div>
$$
\begin{align*}
\langle x, cy \rangle &amp;= \overline{\langle cy, x \rangle} \\
                       &amp;= \overline{c\langle y, x \rangle} \\
					   &amp;= \overline{c} \overline{\langle y, x \rangle} \\
					   &amp;= \overline{c} \langle x, y \rangle \\
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.1 Let \(V\) be an inner product space. Then for \(x, y, z \in V\) and \(c \in \mathbf{F}\), the following statements are true. \(\langle x, y+z \rangle = \langle x, y \rangle + \langle x, z \rangle\) \(\langle x, cy \rangle = \bar{c} \langle x , y \rangle\) \(\langle x, 0 \rangle = \langle 0,x \rangle = 0 \) \(\langle x, x \rangle = 0 \text{ if and only if } x = 0\) If \(\langle x, y \rangle = \langle x, z \rangle\) for all \(x \in V\), then \(y = z\) Proof: For (a) $$ \begin{align*} \langle x, y+z \rangle &amp;= \overline{\langle y+z, x \rangle} \\ &amp;= \overline{\langle y, x \rangle + \langle z, x \rangle} \\ &amp;= \overline{\langle y, x \rangle} + \overline{\langle z, x \rangle} \\ &amp;= \langle x, y \rangle + \langle x, z \rangle \\ \end{align*} $$ For (b) $$ \begin{align*} \langle x, cy \rangle &amp;= \overline{\langle cy, x \rangle} \\ &amp;= \overline{c\langle y, x \rangle} \\ &amp;= \overline{c} \overline{\langle y, x \rangle} \\ &amp;= \overline{c} \langle x, y \rangle \\ \end{align*} $$ References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.2: Exercise 11</title><link href="http://localhost:4000/jekyll/update/2024/09/10/6.2-exercise-11.html" rel="alternate" type="text/html" title="Section 6.2: Exercise 11" /><published>2024-09-10T01:01:36-07:00</published><updated>2024-09-10T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/10/6.2-exercise-11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/10/6.2-exercise-11.html"><![CDATA[<div class="ydiv">
Exercise 11
</div>
<div class="ybdiv">
Let \(A\) be an \(n \times n\) matrix with complex entries. Prove that \(AA^* = I\) if and only if the rows of \(A\) form an orthonormal basis for \(\mathbf{C}^n\).
</div>
<p><br />
Proof:
<br />
<br />
By definition we know that \(A^*\) is defined as \((A^*)_{ij} = \overline{A_{ji}}\) for all \(i\) and \(j\). Therefore the row \(i\), column \(j\) entry of \(AA^*\) is</p>
<div>
	$$
	\begin{align*}
	(AA^*)_{ij} &amp;= \sum_{k=1}^n A_{ik}(A^*)_{kj} \\
	            &amp;= \sum_{k=1}^n A_{ik}\overline{A_{jk}} \\
				&amp;= A_{i1}\overline{A_{j1}} + A_{i2}\overline{A_{j2}} + ... + A_{in}\overline{A_{jn}} \\
				&amp;= \langle A_i, A_j \rangle
	\end{align*}
	$$
</div>
<p>where \(A_i\) and \(A_j\) are the \(i\)th and \(j\)th rows of \(A\). What does having \(AA^{*} = I\) then mean? The \(i\)th row and \(j\)th column entry of \(I\) is \(I_{ij} = \delta_{ij}\) and this corresponds to the inner product of the rows \(A_i\) and \(A_j\).</p>
<div> 
$$
\begin{align*}
\begin{pmatrix}
\cdots &amp; \cdots &amp; \cdots \\
\cdots &amp; A_{i} &amp; \cdots \\
\cdots &amp; \cdots &amp; \cdots \\
\end{pmatrix}
\begin{pmatrix}
\cdots &amp; \cdots &amp; \cdots \\
\cdots &amp; A_{j} &amp; \cdots \\
\cdots &amp; \cdots &amp; \cdots \\
\end{pmatrix}
=
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 = \delta_{22} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>So \(AA^{*} = I\) is equivalent to having \(\langle A_i, A_j \rangle = \delta_{ij}\). But this implies that the rows of \(A\) form an orthonormal set of \(n\) vectors in \(C^{n}\) by definition of an orthonormal set. Since there are \(n\) of them and they are linearly independent, then they form a basis for \(C^{n}\). \(\ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_6_2.html">Reference Solution </a>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
&lt;/ul&gt;





















</li></ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 11 Let \(A\) be an \(n \times n\) matrix with complex entries. Prove that \(AA^* = I\) if and only if the rows of \(A\) form an orthonormal basis for \(\mathbf{C}^n\). Proof: By definition we know that \(A^*\) is defined as \((A^*)_{ij} = \overline{A_{ji}}\) for all \(i\) and \(j\). Therefore the row \(i\), column \(j\) entry of \(AA^*\) is $$ \begin{align*} (AA^*)_{ij} &amp;= \sum_{k=1}^n A_{ik}(A^*)_{kj} \\ &amp;= \sum_{k=1}^n A_{ik}\overline{A_{jk}} \\ &amp;= A_{i1}\overline{A_{j1}} + A_{i2}\overline{A_{j2}} + ... + A_{in}\overline{A_{jn}} \\ &amp;= \langle A_i, A_j \rangle \end{align*} $$ where \(A_i\) and \(A_j\) are the \(i\)th and \(j\)th rows of \(A\). What does having \(AA^{*} = I\) then mean? The \(i\)th row and \(j\)th column entry of \(I\) is \(I_{ij} = \delta_{ij}\) and this corresponds to the inner product of the rows \(A_i\) and \(A_j\). $$ \begin{align*} \begin{pmatrix} \cdots &amp; \cdots &amp; \cdots \\ \cdots &amp; A_{i} &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots \\ \end{pmatrix} \begin{pmatrix} \cdots &amp; \cdots &amp; \cdots \\ \cdots &amp; A_{j} &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots \\ \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 = \delta_{22} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 \end{pmatrix} \end{align*} $$ So \(AA^{*} = I\) is equivalent to having \(\langle A_i, A_j \rangle = \delta_{ij}\). But this implies that the rows of \(A\) form an orthonormal set of \(n\) vectors in \(C^{n}\) by definition of an orthonormal set. Since there are \(n\) of them and they are linearly independent, then they form a basis for \(C^{n}\). \(\ \blacksquare\). References: Reference Solution Linear Algebra 5th Edition &lt;/ul&gt;]]></summary></entry><entry><title type="html">Section 6.2: Theorem 6.5</title><link href="http://localhost:4000/jekyll/update/2024/09/09/6.2-theorem-6.5.html" rel="alternate" type="text/html" title="Section 6.2: Theorem 6.5" /><published>2024-09-09T01:01:36-07:00</published><updated>2024-09-09T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/09/6.2-theorem-6.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/09/6.2-theorem-6.5.html"><![CDATA[<div class="purdiv">
Theorem 6.5
</div>
<div class="purbdiv">
Let \(V\) be a non-zero finite-dimensional inner product space. Then \(V\) has an orthonormal basis \(\beta\). Furthermore, if \(\beta = \{v_1, v_2, ..., v_n\}\) and \(x \in V\), then
$$
\begin{align*}
x = \sum_{i=1}^n \langle x, v_i \rangle v_i
\end{align*}
$$
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose we have a basis \(\beta_0\) for \(V\). We can apply Gram Schmidt to obtain an orthogonal set \(\beta'\) of nonzero vectors with \(\text{span}(\beta')=\text{span}(\beta_0) = V\). We can then normalize each vector to obtain an orthonormal set \(\beta\) that generates \(V\). By Corollary 2 of Theorem 6.3 (If \(S\) is an orthogonal set with non-zero vectors, then \(S\) is linearly independent), \(\beta\) is linearly independent. Therefore \(\beta\) is an orthonormal basis for \(V\). The rest follows from Corollary 1 of Theorem 6.3.\(\ \blacksquare\)
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<div class="purdiv">
Theorem 6.5 (Corollary)
</div>
<div class="purbdiv">
Let \(V\) be a finite-dimensional inner product space with an orthonormal basis \(\beta = \{v_1,v_2,...,v_n\}\). Let \(T\) be a linear operator on \(V\) and let \(A = [T]_{\beta}\). Then for any \(i\) and \(j\), \(A_{ij}= \langle T(v_j), v_i \rangle\).
</div>
<p><br />
<b>Proof</b>
From Theorem 6.5, we have</p>
<div>
$$
\begin{align*}
T(v_j) = \sum_{i=1}^n \langle T(v_j), v_i \rangle v_i
\end{align*}
$$
</div>
<p>Therefore, \(A_{ij} = \langle T(v_j), v_i \rangle\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.5 Let \(V\) be a non-zero finite-dimensional inner product space. Then \(V\) has an orthonormal basis \(\beta\). Furthermore, if \(\beta = \{v_1, v_2, ..., v_n\}\) and \(x \in V\), then $$ \begin{align*} x = \sum_{i=1}^n \langle x, v_i \rangle v_i \end{align*} $$ Proof: Suppose we have a basis \(\beta_0\) for \(V\). We can apply Gram Schmidt to obtain an orthogonal set \(\beta'\) of nonzero vectors with \(\text{span}(\beta')=\text{span}(\beta_0) = V\). We can then normalize each vector to obtain an orthonormal set \(\beta\) that generates \(V\). By Corollary 2 of Theorem 6.3 (If \(S\) is an orthogonal set with non-zero vectors, then \(S\) is linearly independent), \(\beta\) is linearly independent. Therefore \(\beta\) is an orthonormal basis for \(V\). The rest follows from Corollary 1 of Theorem 6.3.\(\ \blacksquare\) Theorem 6.5 (Corollary) Let \(V\) be a finite-dimensional inner product space with an orthonormal basis \(\beta = \{v_1,v_2,...,v_n\}\). Let \(T\) be a linear operator on \(V\) and let \(A = [T]_{\beta}\). Then for any \(i\) and \(j\), \(A_{ij}= \langle T(v_j), v_i \rangle\). Proof From Theorem 6.5, we have $$ \begin{align*} T(v_j) = \sum_{i=1}^n \langle T(v_j), v_i \rangle v_i \end{align*} $$ Therefore, \(A_{ij} = \langle T(v_j), v_i \rangle\). \(\ \blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.3: Theorem 6.10</title><link href="http://localhost:4000/jekyll/update/2024/09/08/6.3-theorem-6.10.html" rel="alternate" type="text/html" title="Section 6.3: Theorem 6.10" /><published>2024-09-08T01:01:36-07:00</published><updated>2024-09-08T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/08/6.3-theorem-6.10</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/08/6.3-theorem-6.10.html"><![CDATA[<div class="purdiv">
Theorem 6.10
</div>
<div class="purbdiv">
Let \(V\) be a finite-dimensional inner product space over \(\mathbf{F}\), and let \(\beta\) be a an orthonormal basis for \(V\). If \(T\) is a linear operation on \(V\), then
$$
\begin{align*}
[T^*]_{\beta} = [T]^*_{\beta}
\end{align*}
$$
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Let \(A = [T^*]_{\beta}\). By the <a href="https://strncat.github.io/jekyll/update/2024/09/09/6.2-theorem-6.5.html">Corollary from Theorem 6.5</a> we know that \(A_{ij} = \langle T^*(v_j), v_i \rangle\). Therefore,</p>
<div>
$$
\begin{align*}
\langle x, T(y) \rangle &amp;= \overline{\langle T(y), x \rangle} \\
                        &amp;= \overline{\langle y, T^*(x) \rangle} \\
                       &amp;= \langle T^*(x), y \rangle \\
					   &amp;= \overline{A_{ij}} \\
					   &amp;= (A^*)_{ij}. \ \blacksquare
				
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 6.10 (Corollary)
</div>
<div class="purbdiv">
Let \(A\) be \(n \times n\) matrix. Then \(L_{A^*} = (L_A)^*\)
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Let \(\beta\) be the standard basis for \(\mathbf{F}^n\). Then, we know that \([L_A]_{\beta} = A\) by Theorem 2.16. We can apply Theorem 6.10 to see that</p>
<div>
$$
\begin{align*}
[(L_A)^*]_{\beta} &amp;= [L_A]^*_{\beta} \quad \text{(By Theorem 6.10)} \\
                  &amp;= A^* \\
                  &amp;= [L_{A^*}]_{\beta}
				
\end{align*}
$$
</div>
<p>Therefore \(L_{A^*} = (L_A)^*\) as we wanted to show. \(\ \blacksquare\)
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.10 Let \(V\) be a finite-dimensional inner product space over \(\mathbf{F}\), and let \(\beta\) be a an orthonormal basis for \(V\). If \(T\) is a linear operation on \(V\), then $$ \begin{align*} [T^*]_{\beta} = [T]^*_{\beta} \end{align*} $$ Proof: Let \(A = [T^*]_{\beta}\). By the Corollary from Theorem 6.5 we know that \(A_{ij} = \langle T^*(v_j), v_i \rangle\). Therefore, $$ \begin{align*} \langle x, T(y) \rangle &amp;= \overline{\langle T(y), x \rangle} \\ &amp;= \overline{\langle y, T^*(x) \rangle} \\ &amp;= \langle T^*(x), y \rangle \\ &amp;= \overline{A_{ij}} \\ &amp;= (A^*)_{ij}. \ \blacksquare \end{align*} $$ Theorem 6.10 (Corollary) Let \(A\) be \(n \times n\) matrix. Then \(L_{A^*} = (L_A)^*\) Proof: Let \(\beta\) be the standard basis for \(\mathbf{F}^n\). Then, we know that \([L_A]_{\beta} = A\) by Theorem 2.16. We can apply Theorem 6.10 to see that $$ \begin{align*} [(L_A)^*]_{\beta} &amp;= [L_A]^*_{\beta} \quad \text{(By Theorem 6.10)} \\ &amp;= A^* \\ &amp;= [L_{A^*}]_{\beta} \end{align*} $$ Therefore \(L_{A^*} = (L_A)^*\) as we wanted to show. \(\ \blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.3: Theorem 6.9</title><link href="http://localhost:4000/jekyll/update/2024/09/07/6.3-theorem-6.9.html" rel="alternate" type="text/html" title="Section 6.3: Theorem 6.9" /><published>2024-09-07T01:01:36-07:00</published><updated>2024-09-07T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/07/6.3-theorem-6.9</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/07/6.3-theorem-6.9.html"><![CDATA[<div class="purdiv">
Theorem 6.9
</div>
<div class="purbdiv">
Let \(V\) be a finite-dimensional inner product space over \(\mathbf{F}\), and let \(T\) be a linear operator on \(V\). Then there exists a unique function \(T^*:V \rightarrow V\) such that \( \langle T(x), y \rangle = \langle x, T^*(y) \rangle\) for all \(x,y \in V\). Furthermore \(T^*\) is linear.
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Let \(y \in V\). Define \(g(x): V \rightarrow \mathbf{F}\) by \(g(x) =  \langle T(x), y \rangle\) for all \(x \in V\). \(g\) is linear. To see this, consider \(x_1, x_2 \in V\), then</p>
<div>
$$
\begin{align*}
g(cx_1 + x_2) &amp;= \langle T(cx_1 + x_2), y \rangle \\
              &amp;= \langle T(cx_1) + T(x_2), y \rangle  \\
              &amp;= \langle cT(x_1), y \rangle + \langle T(x_2), y \rangle \\
			  &amp;= cg(x_1) + g(x_2)
\end{align*}
$$
</div>
<p>So \(g\) is linear. By Theorem 6.8, there exists a unique vector \(y' \in V\) such that \(g(x) = \langle x, y' \rangle\). That is \(\langle T(x), y \rangle = \langle x, y' \rangle\) for all \(x \in V\). Now, define \(T^*: V \rightarrow\) by \(T^*(y) = y'\) so now we have \(\langle T(x), y \rangle = \langle x, T^*(y) \rangle\). \(T^*\) is also linear, to see this, let \(y_1, y_2 \in V\) and \(c \in \mathbf{F}\). Then for any \(x \in V\),</p>
<div>
$$
\begin{align*}
\langle x, T^*(cy_1 + y_2) \rangle &amp;= \langle  T(x), cy_1 + y_2 \rangle \\
                       &amp;= \langle  T(x), cy_1 \rangle + \langle  T(x), y_2 \rangle\\
                       &amp;= \bar{c}\langle  T(x), y_1 \rangle + \langle  T(x), y_2 \rangle\\
                    &amp;= \bar{c}\langle  x, T^*(y_1) \rangle + \langle  x, T^*(y_2) \rangle \\
                    &amp;= \langle  x, cT^*(y_1) + T^*(y_2) \rangle
				
\end{align*}
$$
</div>
<p>Since \(x\) is arbitrary, then \(T^*(cy_1 + y_2) = cT^*(y_1) + T^*(y_2)\) by Theorem 6.1(e). Finally we need to show that \(T^*\) is unique. Suppose it wasn’t. Then, let \(U: V \rightarrow V\) be linear such that it satisfies \(\langle T(x), y \rangle = \langle x, U(y) \rangle\) for all \(x, y \in V\). But this means that \(\langle x, U(y) \rangle = \langle x, T^*(y) \rangle\) for all \(x, y \in V\). Therefore, \(T^* = U\). \(\ \blacksquare\)
<br />
<br />
Note here that</p>
<div>
$$
\begin{align*}
\langle x, T(y) \rangle &amp;= \overline{\langle T(y), x \rangle} \\
                        &amp;= \overline{\langle y, T^*(x) \rangle} \\
                       &amp;= \langle T^*(x), y \rangle
				
\end{align*}
$$
</div>
<p>So we can shift back and forth between the two.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.9 Let \(V\) be a finite-dimensional inner product space over \(\mathbf{F}\), and let \(T\) be a linear operator on \(V\). Then there exists a unique function \(T^*:V \rightarrow V\) such that \( \langle T(x), y \rangle = \langle x, T^*(y) \rangle\) for all \(x,y \in V\). Furthermore \(T^*\) is linear. Proof: Let \(y \in V\). Define \(g(x): V \rightarrow \mathbf{F}\) by \(g(x) = \langle T(x), y \rangle\) for all \(x \in V\). \(g\) is linear. To see this, consider \(x_1, x_2 \in V\), then $$ \begin{align*} g(cx_1 + x_2) &amp;= \langle T(cx_1 + x_2), y \rangle \\ &amp;= \langle T(cx_1) + T(x_2), y \rangle \\ &amp;= \langle cT(x_1), y \rangle + \langle T(x_2), y \rangle \\ &amp;= cg(x_1) + g(x_2) \end{align*} $$ So \(g\) is linear. By Theorem 6.8, there exists a unique vector \(y' \in V\) such that \(g(x) = \langle x, y' \rangle\). That is \(\langle T(x), y \rangle = \langle x, y' \rangle\) for all \(x \in V\). Now, define \(T^*: V \rightarrow\) by \(T^*(y) = y'\) so now we have \(\langle T(x), y \rangle = \langle x, T^*(y) \rangle\). \(T^*\) is also linear, to see this, let \(y_1, y_2 \in V\) and \(c \in \mathbf{F}\). Then for any \(x \in V\), $$ \begin{align*} \langle x, T^*(cy_1 + y_2) \rangle &amp;= \langle T(x), cy_1 + y_2 \rangle \\ &amp;= \langle T(x), cy_1 \rangle + \langle T(x), y_2 \rangle\\ &amp;= \bar{c}\langle T(x), y_1 \rangle + \langle T(x), y_2 \rangle\\ &amp;= \bar{c}\langle x, T^*(y_1) \rangle + \langle x, T^*(y_2) \rangle \\ &amp;= \langle x, cT^*(y_1) + T^*(y_2) \rangle \end{align*} $$ Since \(x\) is arbitrary, then \(T^*(cy_1 + y_2) = cT^*(y_1) + T^*(y_2)\) by Theorem 6.1(e). Finally we need to show that \(T^*\) is unique. Suppose it wasn’t. Then, let \(U: V \rightarrow V\) be linear such that it satisfies \(\langle T(x), y \rangle = \langle x, U(y) \rangle\) for all \(x, y \in V\). But this means that \(\langle x, U(y) \rangle = \langle x, T^*(y) \rangle\) for all \(x, y \in V\). Therefore, \(T^* = U\). \(\ \blacksquare\) Note here that $$ \begin{align*} \langle x, T(y) \rangle &amp;= \overline{\langle T(y), x \rangle} \\ &amp;= \overline{\langle y, T^*(x) \rangle} \\ &amp;= \langle T^*(x), y \rangle \end{align*} $$ So we can shift back and forth between the two. References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.3: Theorem 6.8</title><link href="http://localhost:4000/jekyll/update/2024/09/06/6.3-theorem-6.8.html" rel="alternate" type="text/html" title="Section 6.3: Theorem 6.8" /><published>2024-09-06T01:01:36-07:00</published><updated>2024-09-06T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/06/6.3-theorem-6.8</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/06/6.3-theorem-6.8.html"><![CDATA[<div class="purdiv">
Theorem 6.8
</div>
<div class="purbdiv">
Let \(V\) be a finite-dimensional inner product space over \(\mathbf{F}\), and let \(g: V \rightarrow \mathbf{F}\) be a linear transformation. Then, there exists a unique vector \(y \in V\) such that \(g(x) = \langle x, y \rangle\) such that \(g(x) = \langle x, y \rangle\) for all \(x \in V\)
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis for \(V\), and let</p>
<div>
$$
\begin{align*}
y = \sum_{i=1}^{n} \overline{g(v_i)}v_i
\end{align*}
$$
</div>
<p>Define \(h: V \rightarrow \mathbf{F}\) by \(h(x) = \langle x, y \rangle\) which is linear. Furthermore, for \(i \leq j \leq n\), we have</p>
<div>
$$
\begin{align*}
h(v_j) = \langle v_j \rangle &amp;= \left\langle v_j, \sum_{i=1}^{n} \overline{g(v_i)}v_i \right\rangle \\
                             &amp;=  \sum_{i=1}^{n} g(v_i) \langle v_j, v_i \rangle \\
                             &amp;=  \sum_{i=1}^{n} g(v_i) \delta_{ij} \\
							 &amp;= g(v_j)
\end{align*}
$$
</div>
<p>But \(h\) and \(g\) agree on a basis so by the corollary from 2.6, \(g = h\).
<br />
<br />
To show that \(y\) is unique. Suppose it is not and let \(g(x) = \langle x, y' \rangle\). for all \(x\). Then, \(\langle x, y' \rangle = \langle x, y \rangle\) but by Theorem 6.1(e), this means that \(y = y'\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
Notes: So my understanding is that we can turn any linear transformation from \(V\) to \(\mathbf{F}\) into an inner product with some unique vector \(y \in V\). For example if \(V = \mathbf{R}^2\) and we have some \(g(x): V \rightarrow V\). where \(g(a,b) = 3a + 2b\). Then,</p>
<div>
$$
\begin{align*}
y = (3, 2)
\end{align*}
$$
</div>
<p>is a unique vector in \(V\) such that</p>
<div>
$$
\begin{align*}
g(x) &amp;= \langle x, y \rangle \\
     &amp;= \langle x, (3, 2) \rangle \\
	 &amp;= 2a + 3b
\end{align*}
$$
</div>
<p>At least this is what I understood!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.8 Let \(V\) be a finite-dimensional inner product space over \(\mathbf{F}\), and let \(g: V \rightarrow \mathbf{F}\) be a linear transformation. Then, there exists a unique vector \(y \in V\) such that \(g(x) = \langle x, y \rangle\) such that \(g(x) = \langle x, y \rangle\) for all \(x \in V\) Proof: Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis for \(V\), and let $$ \begin{align*} y = \sum_{i=1}^{n} \overline{g(v_i)}v_i \end{align*} $$ Define \(h: V \rightarrow \mathbf{F}\) by \(h(x) = \langle x, y \rangle\) which is linear. Furthermore, for \(i \leq j \leq n\), we have $$ \begin{align*} h(v_j) = \langle v_j \rangle &amp;= \left\langle v_j, \sum_{i=1}^{n} \overline{g(v_i)}v_i \right\rangle \\ &amp;= \sum_{i=1}^{n} g(v_i) \langle v_j, v_i \rangle \\ &amp;= \sum_{i=1}^{n} g(v_i) \delta_{ij} \\ &amp;= g(v_j) \end{align*} $$ But \(h\) and \(g\) agree on a basis so by the corollary from 2.6, \(g = h\). To show that \(y\) is unique. Suppose it is not and let \(g(x) = \langle x, y' \rangle\). for all \(x\). Then, \(\langle x, y' \rangle = \langle x, y \rangle\) but by Theorem 6.1(e), this means that \(y = y'\) as we wanted to show. \(\ \blacksquare\) Notes: So my understanding is that we can turn any linear transformation from \(V\) to \(\mathbf{F}\) into an inner product with some unique vector \(y \in V\). For example if \(V = \mathbf{R}^2\) and we have some \(g(x): V \rightarrow V\). where \(g(a,b) = 3a + 2b\). Then, $$ \begin{align*} y = (3, 2) \end{align*} $$ is a unique vector in \(V\) such that $$ \begin{align*} g(x) &amp;= \langle x, y \rangle \\ &amp;= \langle x, (3, 2) \rangle \\ &amp;= 2a + 3b \end{align*} $$ At least this is what I understood! References: Linear Algebra 5th Edition]]></summary></entry></feed>