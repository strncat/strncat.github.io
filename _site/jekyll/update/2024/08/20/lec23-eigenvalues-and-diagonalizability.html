<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="nemo&apos;s notebook" />
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">nemo&#39;s notebook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
  
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 23: Eigenvalues and Diagonalizability</h1>
    <!--
    <p class="post-meta">
      <time class="dt-published" datetime="2024-08-20T01:01:36-07:00" itemprop="datePublished">Aug 20, 2024
      </time></p>
     -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonal if all entries off diagonal are 0. \(A_{ij} = 0\) if \(i \neq j\).
</div>
<p><br />
All computations involving diagonal matrices are simple. If \(A, B\) are diagonal, then</p>
<div>
$$
\begin{align*}
\det(A) = A_{11}...A_{nn}
\end{align*}
$$
</div>
<p>Similarly, matrix multiplication of diagonal matrices is simple. The \(AB_{ij}\)th entry is</p>
<div>
$$
\begin{align*}
AB_{ij} &amp;= \begin{cases} A_{ij}B_{ij} \quad \text{if } i = j \\ 0\phantom{A_{ij}B} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This can be generalized to computing \((A)^k\) where the \(ij\) entry is</p>
<div>
$$
\begin{align*}
(A^k)_{ij} &amp;= \begin{cases} (A_{ij})^k \quad \text{if } i = j \\ 0\phantom{(A_{ij})} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This leads to the question of whether we can transform any matrix to a diagonal matrix so we can perform these computations easily. In the next definition we formalize this,
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T \in V \rightarrow V\) is diagonalizable if there is a basis \(\beta = \{v_1,...,v_n\}\) of \(V\) such that 
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
Two questions:
<br />
Questions 1: Does such a basis exist?
<br />
Question 2: If it exists, how can we compute it?
<br />
<br />
A basis \(\beta = \{v_1,...,v_n\}\) such that
\([T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}.\)
This is equivalent to</p>
<ul style="list-style: none;">
	<li> \(\leftrightarrow [T(v_j)]_{\beta} =   \begin{pmatrix} 0 \\\vdots \\\lambda_j \\ \vdots \\ 0 \end{pmatrix}\). for \(j = 1,..,n\).</li>
    <li>\(\leftrightarrow [T(v_j)]_{\beta} =  \lambda_j \begin{pmatrix} 0 \\\vdots \\1 \\ \vdots \\ 0 \end{pmatrix} = \lambda_j [v_j]_{\beta} = [\lambda_jv_j]_{\beta} \) for \(j = 1,...,n\). This is true because that coordinate expression is the coordinate expression of the \(j\)th vector in the basis \(\beta\). </li>
	<li>\(\leftrightarrow T[v_j] = \lambda_jv_j\) for \(j = 1,...,n\) and \(\lambda_1,...,\lambda_n \in \mathbf{R}\). We can just write the transformation since they are both with respect to basis \(\beta\).  </li>
	<li>\(\leftrightarrow T[v] = \lambda v\).  </li>
	<li>\(\leftrightarrow T[v] = \lambda I_V(v)\). Since the identity matrix does nothing. </li>
	<li>\(\leftrightarrow T[v] - \lambda I_V(v) = \bar{0}_V\). </li>
	<li>\(\leftrightarrow (T - \lambda I_V)(v) = \bar{0}_V\). </li>
</ul>
<p>The left hand side is a family of linear maps parameterized by \(\lambda\). So we want all the non-zero elements \(v\) of the null space (we don’t care about the zero solution since we want to build a basis).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of a Linear Transformation</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(T \in V \rightarrow V\) is a \(v \neq \bar{0}_V\) such that
$$
\begin{align*}
T(v) = \lambda v
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(T \in V \rightarrow V\) if \(\exists v \neq \bar{0}_V\) such that \(T(v) = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \in V \rightarrow V\) is diagonalizable if and only if there is a basis \(\beta = \{v_1,...,v_n\}\) consisting of eigenvectors.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
This is exactly the same definition we stated earlier. This is the same as finding</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}, \lambda_1,...,\lambda_n \text{ eigenvalues}
\end{align*}
$$
</div>
<p>Instead of focusing on these general linear maps. Let’s turn now to focus on matrices.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of Matrices</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonalizable if \(L_A\) is diagonalizable.
</div>
<p><br />
This is equivalent to “There is a \(Q \in M_{n \times n}\) such that \(Q^{-1}AQ\) is diagonal”.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(A \in M_{n \times n}\) is a \(v \neq \bar{0} \in \mathbf{R}^n\) such that \(Av = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and finally,
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(A \in M_{n \times n}\) if \(\exists v \neq \bar{0}_V\) such that \(Av = \lambda v\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding eigenvectors</b></h4>
<p>Okay now that we’ve narrowed down the discussion to matrices, how do we actually find these eigenvectors of \(A\)?
<br />
<br />
Again, let set the null space of \(A\) to \(N(A) = N(L_A)\). Next we will need the following lemma
<br /></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
\(v \in \mathbf{R}^n\) is an eigenvector of \(A\) with eigenvalue \(\lambda\) if and only if \(v \in N(A - \lambda I_n)\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(v\) is an eigenvector of (A). By definition this means that \(Av = \lambda v\). We can re-write this as,</p>
<div>
$$
\begin{align*}
Av &amp;= \lambda I_n v \\
Av - \lambda I_n v &amp;= \bar{0}
\end{align*}
$$
</div>
<p>But this precisely means that \(v \in N(A - \lambda I_n) \ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find all the eigen values of</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix} 
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0 \\
-2 &amp; 0 &amp; 3
\end{pmatrix}
\end{align*}
$$
</div>
<p>with eigenvalue 1.
<br />
<br />
By the lemma, we want all vectors in the null space of \(A - \lambda I_n\).</p>
<div>
$$
\begin{align*}
A - (1)I_n = 
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>We’ll put this matrix in row echelon form.</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
R_3 \rightarrow -2R_1 + R_3, R_1 \leftrightarrow -1R_1
\begin{pmatrix} 
1 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>From this we see that the null space consists of vectors of the form</p>
<div>
$$
\begin{align*}
&amp;= \{(x_1, x_2, x_3) \ | \ x_3 = t, x_1 = t, x_2 = 0\}. \\
&amp;= \{(t, 0, t) \ | \ t \in \mathbf{R} \} \\
&amp;= span\{ (1,0,1) \}
\end{align*}
$$
</div>
<p>This is easy because we are given the eigenvalue. But typically, we also need to find the eigenvalues too!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenspace</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
If \(\lambda\) is an eigenvalue of \(A\), then the eigenspace of \(A\) corresponding to \(\lambda\) is
$$
\begin{align*}
E_{\lambda} &amp;= N(A - \lambda I_n) \\
&amp;= \{ \text{eigenvectors for } \lambda \} \cup \{\bar{0}\}
\end{align*}
$$
</div>
<p><br />
So this is kind of like another name for the null space except that here we have the zero vector.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding Eigenvalues</b></h4>
<p>Again, if we’re given an eigenvalue, then finding the eigenspace or eigenvectors is easy and simple. We’re just solving a system of linear equations like we did for finding the nullspace. The question is how can we find the eigenvalues? for this we need the following theorem</p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(\lambda\) is an eigenvalue of \(A\) if and only if \(\det(A - \lambda I_n) = 0\).
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
\(\lambda\) is an eigenvalue is equivalent to</p>
<div>
$$
\begin{align*}
&amp;\leftrightarrow \exists v \neq \bar{0} \text{ such that } Av = \lambda v \\
&amp;\leftrightarrow (A - \lambda I_n)(v) = \bar{0} \text{ for } v \neq 0\\
&amp;\leftrightarrow N(A - \lambda I_n)(v) \neq \bar{0} \\
&amp;\leftrightarrow A - \lambda I_n \text{ is not 1-1} \\
&amp;\leftrightarrow A - \lambda I_n \text{ is not invertible} \\
&amp;\leftrightarrow \det(A - \lambda I_n) = 0. \\
\end{align*}
$$
</div>
<p><br />
So we see now that we have the necessary and sufficient conditions for \(\lambda\) to be an eigenvalue of \(A\). So what’s next? \(A\) is given to us in this equation but we need a \(\lambda\) that would make the equation \(\det(A - \lambda I_n)\) equal to zero. Let’s look at the following definition
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(f(t) = \det(A - tI_n)\) is the characteristic polynomial of \(A\).
</div>
<p><br />
What is this saying? we can interpret the right hand side as a function. We’re given \(A\). We know the identity matrix. So the unknown is \(t\). So inside the determinant, we’ll have a matrix with entries that depend on \(A\) and \(t\). We know the determinant is a map / inductive formula. So this expression when expanded as a whole is some number that depends on it. In fact it shouldn’t be surprising that \(f(t)\) is a polynomial of degree \(n\) (FACT). 
<br />
<br />
Based on this, we can rephrase the previous theorem as the following corollary,
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
\(\lambda\) is an eigenvalue of \(A\) if and only if \(f(t) = \det(A - \lambda I_n)\).
</div>
<p><br />
So eigenvalues are the roots of this polynomial so it’s not always easy to do. How many roots? We know the degree of \(f(t)\) is at most \(n\). Therefore,
<br /></p>
<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
\(A\) has at most \(n\) eigenvalues.
</div>
<p><br />
So we know at least that there can only be \(n\) roots/eigenvalues at most.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find the eigenvalues of
\(\begin{align*}
A =
\begin{pmatrix} 
0 &amp; -1 \\
1 &amp; 0
\end{pmatrix}.
\end{align*}\)
<br />
<br />
Let’s write the characteristic polynomial and find its roots so</p>
<div>
$$
\begin{align*}
f(t) = \det(A - tI_n) &amp;= 0 \\
\det
\begin{pmatrix} 
-t &amp; -1 \\
1 &amp; -t
\end{pmatrix} &amp;= 0 \\
t^2 + 1 &amp;= 0 \\
t^2 &amp;= -1 \\
\end{align*}
$$
</div>
<p>This polynomial has no real roots! and so the matrix \(A\) has no eigenvalues.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>

<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>


  </div><div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier =  /jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://strncat-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html" hidden></a>
</article>
		 
      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">nemo&#39;s notebook</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">nemo&#39;s notebook</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>personal study notes</p>
      </div>
    </div>

  </div>

</footer>
-->

	
  </body>

</html>
