<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-31T21:16:20-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 35: Self Adjoint Maps</title><link href="http://localhost:4000/jekyll/update/2024/09/05/lec35-self-adjoint-maps.html" rel="alternate" type="text/html" title="Lecture 35: Self Adjoint Maps" /><published>2024-09-05T01:01:36-07:00</published><updated>2024-09-05T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/05/lec35-self-adjoint-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/05/lec35-self-adjoint-maps.html"><![CDATA[<p>In the last two lectures we studied adjoint maps where an adjoint map is defined as follows
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that
$$
\begin{align*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and we also studied a special class of adjoint maps called normal adjoint maps where \(T\) is called normal if \(T \circ T^* = T^* \circ T\). We also studied properties of these normals maps. For example if \(T\) is normal then,</p>
<div>
$$
\begin{align*}
\Vert T(x) \Vert &amp;= \Vert T^*(x) \Vert \\
T(x) = \lambda x \ &amp;\implies \ T^*(x) = \bar{\lambda}x
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>Next we will study another special class of adjoint maps called self adjoint maps defined below
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow V\) is self-adjoint if
$$
\begin{align*}
T^* = T
\end{align*}
$$
</div>
<p><br />
Note here that self adjoint implies that \(T\) is normal. The converse is not true (rotation matrix is an example)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let \(A \in M_{n \times n}(\mathbf{R})\) and \(A_{ij} = A_{ji} \ \forall i,j\), then A (L_A) is self-adjoint.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose that \(V\) is a finite dimensional inner product space where \(W \subseteq V\) is a subspace.</p>
<div>
$$
\begin{align*}
T = \text{proj}_W \ : \ V &amp;\rightarrow V \\
                    x = w + z &amp;\rightarrow w
\end{align*}
$$
</div>
<p>So if you take a vector \(x \in V\), we know we can decompose it into two vectors \(w \in W\) and \(z \in W^{\perp}\). This map just produces the part that is in \(W\). We claim that \(\text{proj}_W\) is self-adjoint.
<br />
<br />
<b>Proof</b>
<br />
Take \(x_1, x_2 \in V\). We need to show that</p>
<div>
$$
\begin{align*}
\langle T(x_1), x_2 \rangle = \langle x_1, T(x_2) \rangle
\end{align*}
$$
</div>
<p>We know that \(x_1 = w_1 + z_1\) and \(x_2 = w_2 + z_2\) for some unique vectors \(w_1, w_2 \in W\) and \(z_1, z_2 \in W^{\perp}\). Then,</p>
<div>
$$
\begin{align*}
\langle T(x_1), x_2 \rangle &amp;= \langle T(w_1 + z_1), x_2 \rangle \\
                            &amp;= \langle w_1, w_2 + z_2 \rangle \\
							&amp;= \langle w_1, w_2 \rangle + \langle w_1, z_2 \rangle \\
							&amp;= \langle w_1, w_2 \rangle \quad \text{(because $w_1 \in W$ and $z_2 \in W^{\perp}$)}\\
\end{align*}
$$
</div>
<p>At this point, we recognize that \(w_2 = T(x_2)\) but we still have \(w_1\) and want to reach \(x_1\). Notice that \(x_1 = w_1 + z_1\). Moreover, \(\langle z_1, w_2 \rangle = 0\) So</p>
<div>
$$
\begin{align*}
\langle T(x_1), x_2\rangle &amp;= \langle w_1, w_2 \rangle \phantom{ \quad \text{(because $w_1 \in W$ and $z_2 \in W^{\perp}$)}}\\ 
                            &amp;=  \langle w_1, w_2 \rangle +  \langle z_1, w_2 \rangle \\
							&amp;= \langle x_1, w_2 \rangle \\
							&amp;= \langle x_1, T(x_2) \rangle 
\end{align*}
$$
</div>
<p>as we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Self-adjoint Maps are Diagonalizable</b></h4>
<p>Today’s goal is to prove that self adjoint maps are diagonalizable. Note here that when a matrix \(A\) is symmetric where \(A_{ij} = A_{ji}\), then \(A\) is self-adjoint. This implies \(A\) is diagonalizable and so  \(\det(A - tI_n)\) splits which is really useful to know!
<br />
<br />
Question: What is the diagonal form of the project map \(\text{proj}_W\)? because eigenvectors get mapped to a multiple of themselves, the projection of the vector is either all in \(W\) or all in \(Z\) and you get zero from the projection. Therefore, we notice here that the eigenvalues are 0s and 1s.
<br />
<br />
Proving that self adjoint maps are diagonalizable, requires a few things along the way so we will next prove the results that we need in order to prove that they’re diagonalizable.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvalues of Self-adjoint Maps</b></h4>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
If \(T: V \rightarrow V\) where \(V\) is over \(\mathbf{C}\) is self-adjoint, then all eigenvalues are real.
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Since \(T\) is self-adjoint, then \(T\) is normal. Then for any \(\lambda\),</p>
<div>
$$
\begin{align*}
T(x) &amp;= \lambda x \\
T^*(x) &amp;= \bar{\lambda} x \quad \text{because $T$ is normal} 
\end{align*}
$$
</div>
<p>But \(T = T^*\) since \(T\) is self-adjoint. Therefore,</p>
<div>
$$
\begin{align*}
\lambda x = \bar{\lambda} x \\
\implies \lambda = \bar{\lambda} 
\end{align*}
$$
</div>
<p>as we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Do Self-adjoint Maps have Eigenvalues?</b></h4>
<p>So eigenvalues are real but are there eigenvalues?
<br />
<br />
If \(V\) is a vector space over \(\mathbf{C}\), then \(T: V \rightarrow V\) always has eigenvalues. It doesn’t matter if \(T\) is self-adjoint or not. The characteristic polynomial \(\det([T]_{\beta}^{\beta} - tI_n)\) with complex entries always splits! (fact from algebra). 
<br />
What if \(V\) was over \(\mathbf{R}?\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
If \(T: V \rightarrow V\) where \(V\) is over \(\mathbf{R}\) is self-adjoint, then \(T\) has at least one eigenvalue.
</div>
<p><br />
An example is the rotation matrix. It is normal but not self-adjoint and it doesn’t have real eigenvalues.
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
Let \(\beta\) be an orthonormal basis of \(V\). We need to show that \(\det([T]_{\beta}^{\beta} - tI_n) = 0\) has a real root. Let \(A = [T]^{\beta}_{\beta}\). Then</p>
<div>
$$
\begin{align*}
A = [T]^{\beta}_{\beta} &amp;= [T^*]^{\beta}_{\beta} \quad \text{(Because $$T$$ is self-adjoint)} \\
                    &amp;= ([T]^{\beta}_{\beta})^* \quad \text{(Proof in last lecture)} \\
					&amp;= A^* \\
					&amp;= A^t \quad \text{(because $$V$$ is over $$\mathbf{R}$$)}
\end{align*}
$$
</div>
<p>So \(A\) is symmetric. The idea is to apply theorem 1 which states that if \(V\) is over \(\mathbf{C}\), then we have real eigenvalues. So consider the following map</p>
<div>
$$
\begin{align*}
L_A \ : \ &amp;\mathbf{C}^n \rightarrow \mathbf{C}^n \\
          &amp; z \rightarrow Az
\end{align*}
$$
</div>
<p>We know that \(L_A\) is self adjoint so \((L_A)^* = L_{A^*} = L_{A^t} = L_A\). So the fact from algebra, \(L_A\) has an eigenvalue. By theorem 1, this eigenvalue must be real. Therefore, \(\det([T]_{\beta}^{\beta} - tI_n) = 0\) has a real root. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors of a Self-adjoint Map</b></h4>
<p>What can we say about the eigenvectors of a linear self-adjoint map? 
<br /></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\)
</div>
<p><br />
<b>Proof</b>
<br />
[TODO]</p>

<!------------------------------------------------------------------------------------>
<p><br />
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
\(T\) is diagonalizable.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In the last two lectures we studied adjoint maps where an adjoint map is defined as follows Definition Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that $$ \begin{align*} \langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W \end{align*} $$ and we also studied a special class of adjoint maps called normal adjoint maps where \(T\) is called normal if \(T \circ T^* = T^* \circ T\). We also studied properties of these normals maps. For example if \(T\) is normal then, $$ \begin{align*} \Vert T(x) \Vert &amp;= \Vert T^*(x) \Vert \\ T(x) = \lambda x \ &amp;\implies \ T^*(x) = \bar{\lambda}x \end{align*} $$ Next we will study another special class of adjoint maps called self adjoint maps defined below Definition \(T: V \rightarrow V\) is self-adjoint if $$ \begin{align*} T^* = T \end{align*} $$ Note here that self adjoint implies that \(T\) is normal. The converse is not true (rotation matrix is an example) Example 1 Let \(A \in M_{n \times n}(\mathbf{R})\) and \(A_{ij} = A_{ji} \ \forall i,j\), then A (L_A) is self-adjoint. Example 2 Suppose that \(V\) is a finite dimensional inner product space where \(W \subseteq V\) is a subspace. $$ \begin{align*} T = \text{proj}_W \ : \ V &amp;\rightarrow V \\ x = w + z &amp;\rightarrow w \end{align*} $$ So if you take a vector \(x \in V\), we know we can decompose it into two vectors \(w \in W\) and \(z \in W^{\perp}\). This map just produces the part that is in \(W\). We claim that \(\text{proj}_W\) is self-adjoint. Proof Take \(x_1, x_2 \in V\). We need to show that $$ \begin{align*} \langle T(x_1), x_2 \rangle = \langle x_1, T(x_2) \rangle \end{align*} $$ We know that \(x_1 = w_1 + z_1\) and \(x_2 = w_2 + z_2\) for some unique vectors \(w_1, w_2 \in W\) and \(z_1, z_2 \in W^{\perp}\). Then, $$ \begin{align*} \langle T(x_1), x_2 \rangle &amp;= \langle T(w_1 + z_1), x_2 \rangle \\ &amp;= \langle w_1, w_2 + z_2 \rangle \\ &amp;= \langle w_1, w_2 \rangle + \langle w_1, z_2 \rangle \\ &amp;= \langle w_1, w_2 \rangle \quad \text{(because $w_1 \in W$ and $z_2 \in W^{\perp}$)}\\ \end{align*} $$ At this point, we recognize that \(w_2 = T(x_2)\) but we still have \(w_1\) and want to reach \(x_1\). Notice that \(x_1 = w_1 + z_1\). Moreover, \(\langle z_1, w_2 \rangle = 0\) So $$ \begin{align*} \langle T(x_1), x_2\rangle &amp;= \langle w_1, w_2 \rangle \phantom{ \quad \text{(because $w_1 \in W$ and $z_2 \in W^{\perp}$)}}\\ &amp;= \langle w_1, w_2 \rangle + \langle z_1, w_2 \rangle \\ &amp;= \langle x_1, w_2 \rangle \\ &amp;= \langle x_1, T(x_2) \rangle \end{align*} $$ as we wanted to show. \(\blacksquare\) Self-adjoint Maps are Diagonalizable Today’s goal is to prove that self adjoint maps are diagonalizable. Note here that when a matrix \(A\) is symmetric where \(A_{ij} = A_{ji}\), then \(A\) is self-adjoint. This implies \(A\) is diagonalizable and so \(\det(A - tI_n)\) splits which is really useful to know! Question: What is the diagonal form of the project map \(\text{proj}_W\)? because eigenvectors get mapped to a multiple of themselves, the projection of the vector is either all in \(W\) or all in \(Z\) and you get zero from the projection. Therefore, we notice here that the eigenvalues are 0s and 1s. Proving that self adjoint maps are diagonalizable, requires a few things along the way so we will next prove the results that we need in order to prove that they’re diagonalizable. Eigenvalues of Self-adjoint Maps Theorem 1 If \(T: V \rightarrow V\) where \(V\) is over \(\mathbf{C}\) is self-adjoint, then all eigenvalues are real. Proof Since \(T\) is self-adjoint, then \(T\) is normal. Then for any \(\lambda\), $$ \begin{align*} T(x) &amp;= \lambda x \\ T^*(x) &amp;= \bar{\lambda} x \quad \text{because $T$ is normal} \end{align*} $$ But \(T = T^*\) since \(T\) is self-adjoint. Therefore, $$ \begin{align*} \lambda x = \bar{\lambda} x \\ \implies \lambda = \bar{\lambda} \end{align*} $$ as we wanted to show. \(\blacksquare\) Do Self-adjoint Maps have Eigenvalues? So eigenvalues are real but are there eigenvalues? If \(V\) is a vector space over \(\mathbf{C}\), then \(T: V \rightarrow V\) always has eigenvalues. It doesn’t matter if \(T\) is self-adjoint or not. The characteristic polynomial \(\det([T]_{\beta}^{\beta} - tI_n)\) with complex entries always splits! (fact from algebra). What if \(V\) was over \(\mathbf{R}?\) Theorem 2 If \(T: V \rightarrow V\) where \(V\) is over \(\mathbf{R}\) is self-adjoint, then \(T\) has at least one eigenvalue. An example is the rotation matrix. It is normal but not self-adjoint and it doesn’t have real eigenvalues. Proof Let \(\beta\) be an orthonormal basis of \(V\). We need to show that \(\det([T]_{\beta}^{\beta} - tI_n) = 0\) has a real root. Let \(A = [T]^{\beta}_{\beta}\). Then $$ \begin{align*} A = [T]^{\beta}_{\beta} &amp;= [T^*]^{\beta}_{\beta} \quad \text{(Because $$T$$ is self-adjoint)} \\ &amp;= ([T]^{\beta}_{\beta})^* \quad \text{(Proof in last lecture)} \\ &amp;= A^* \\ &amp;= A^t \quad \text{(because $$V$$ is over $$\mathbf{R}$$)} \end{align*} $$ So \(A\) is symmetric. The idea is to apply theorem 1 which states that if \(V\) is over \(\mathbf{C}\), then we have real eigenvalues. So consider the following map $$ \begin{align*} L_A \ : \ &amp;\mathbf{C}^n \rightarrow \mathbf{C}^n \\ &amp; z \rightarrow Az \end{align*} $$ We know that \(L_A\) is self adjoint so \((L_A)^* = L_{A^*} = L_{A^t} = L_A\). So the fact from algebra, \(L_A\) has an eigenvalue. By theorem 1, this eigenvalue must be real. Therefore, \(\det([T]_{\beta}^{\beta} - tI_n) = 0\) has a real root. \(\ \blacksquare\) Eigenvectors of a Self-adjoint Map What can we say about the eigenvectors of a linear self-adjoint map? Theorem 3 If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\) Proof [TODO]]]></summary></entry><entry><title type="html">Lecture 34: Normal Adjoint Maps</title><link href="http://localhost:4000/jekyll/update/2024/09/04/lec34-normal-adjoint-maps.html" rel="alternate" type="text/html" title="Lecture 34: Normal Adjoint Maps" /><published>2024-09-04T01:01:36-07:00</published><updated>2024-09-04T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/04/lec34-normal-adjoint-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/04/lec34-normal-adjoint-maps.html"><![CDATA[<p>In the last lecture, we studied adjoint linear maps defined as
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that
$$
\begin{align*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Here are some additional facts
<br />
<br />
Fact 1: \(T^*\) is unique if it exists
<br />
<br />
Fact 2: In infinite dimensions \(T^*\) need not exist.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>\(A \in M_{m \times n}(\mathbf{F})\) with \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\)</p>
<div>
$$
\begin{align*}
(L_A)^* = L_{A^*} \quad \quad A^* = (\bar{A})^t
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Normal Linear Maps</b></h4>
<p>The goal of today is use this notion of adjoint maps to define more classes of linear maps which will be useful. Though we’re going to restrict the maps to maps from a finite dimensional inner product space \(V\) to \(V\) with \(\beta\) being an orthonormal basis. Therefore, in this setting, we will always have an adjoint map.</p>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow V\) and \(T^*: V \rightarrow V\) with \(\beta\) an orthonormal basis.
$$
\begin{align*}
[T^*]_{\beta}^{\beta} = \left( [T]_{\beta}^{\beta}  \right)^*
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow V\) and \(T^*: V \rightarrow V\). \(T\) is normal if
$$
\begin{align*}
T \circ T^* = T^* \circ T
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Let \(A \in M_{n \times n}(\mathbf{F})\) is normal if \(AA^* = A^*A\)</p>
<div>
$$
\begin{align*}
A = \begin{pmatrix}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that this is a rotation matrix. It has no eigenvalues or eigenvectors. This transformation doesn’t take any vector to a multiple of itself. All we’re doing is rotating a vector. To see if it’s a normal matrix (defines a normal operator), we need to compute \(A^*\),</p>
<div>
$$
\begin{align*}
A^* = \begin{pmatrix}
\cos \theta &amp; \sin \theta \\
-\sin \theta &amp; \cos \theta
\end{pmatrix}
\end{align*}
$$
</div>
<p>and then compute the products</p>
<div>
$$
\begin{align*}
AA^* = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}
=
 A^*A
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Suppose that \(A^* = - A\), then</p>
<div>
$$
\begin{align*}
AA^* = -A^2 = A^*A
\end{align*}
$$
</div>
<p>So \(A\) and \(A^t\) are negative of each other. For example</p>
<div>
$$
\begin{align*}
A = \begin{pmatrix}
0 &amp; c \\
-c &amp; 0
\end{pmatrix}
\in 
A \in M_{n \times n}(\mathbf{R})
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>A Sufficient Condition for Normal Linear Maps</b></h4>
<p>Next, we will describe a sufficient condition for an operator to be normal which will be useful later!
<br /></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow V\) and \(T^*: V \rightarrow V\) with \(\beta\) an orthonormal basis. If \(V\) admits an orthonormal basis consisting of eigenvectors of \(T\), then \(T\) is normal.
</div>
<p><br />
Remark: The converse isn’t too. Example 2 shows this.
<br />
<br />
<b>Proof</b>
<br />
Let \(\beta = \{v_1, ..., v_n\}\) be an orthonormal basis consisting of eigenvectors. Therefore</p>

<div>
$$
\begin{align*}
[T \circ T^*]_{\beta}^{\beta} &amp;= [T]_{\beta}^{\beta}[T^*]_{\beta}^{\beta} = [T]_{\beta}^{\beta} \left( [T]_{\beta}^{\beta}  \right)^*
\end{align*}
$$
</div>
<p>Computing \([T]_{\beta}^{\beta}\)</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;=
\begin{pmatrix}
[T(v_1)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta} 
\end{pmatrix}\\
&amp;= 
\begin{pmatrix}
[\lambda_1 v_1]_{\beta} &amp; \cdots &amp; [\lambda_n v_1]_{\beta} 
\end{pmatrix}    \quad       \text{ ($v_1,...,v_n$ are eigenvectors) }\\
&amp;= 
\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix} \quad \text{ (the coordinate representation of $v$ is just $\lambda$)}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} \left( [T]_{\beta}^{\beta}  \right)^* &amp;=
\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
<!---->
\begin{pmatrix}
\bar{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \bar{\lambda_2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \bar{\lambda_n}
\end{pmatrix}\\
&amp;= [T^* \circ T]_{\beta}^{\beta} \quad \text{ (matrix multiplication commutes for diagonal matrices)}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Properties of Normal Linear Maps</b></h4>
<p>So now that we have a sufficient condition to identify normal maps. Let’s study their properties.
<br /></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow V\) is normal. Then
<ol type="a">
	<li>\( \Vert T(v) \Vert\) = \( \Vert T^*(v) \Vert \quad \forall v \in V\)</li>
	<li>\(T+ cI_v\) is normal</li>
	<li>\(T(v) = \lambda v \ \implies \ T^*(v) = \bar{\lambda}v\)</li>
	<li>Suppose \(T(v_1) = \lambda_1v_1\) and \(T(v_2) = \lambda_2v_2\) where \(\lambda_1 \neq \lambda_2\) Then \(\langle v_1, v_2 \rangle = 0\)</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof</b>
<br />
For (a), notice that</p>
<div>
$$
\begin{align*}
\Vert T(v) \Vert^2 &amp;= \langle T(v), T(v) \rangle \\
                   &amp;= \langle v, T^*(T(v)) \rangle \quad \text{(definition of an adjoint map)} \\
                   &amp;= \langle v, T(T^*((v)) \rangle \quad \text{(because $T$ is normal)} \\
                   &amp;= \langle T^*(v), T^*(v) \rangle = \Vert T^*(v) \Vert^2 \quad \blacksquare
\end{align*}
$$
</div>
<p>For (b), we want to show that \((T + cI_v)(T + cI_v)^* = (T + cI_v)^*(T + cI_v)\) to prove that it is normal so,</p>
<div>
$$
\begin{align*}
(T + cI_v)(T + cI_v)^* &amp;= (T + cI_v)(T^* + \bar{c}I_v) \\
                       &amp;= TT^* + T\bar{c}I_V + cI_VT^* + cI_V\bar{c}I_V \\
                       &amp;= T^*T + \bar{c}I_VT + T^*cI_V + \bar{c}I_VcI_V \\
					   &amp;= (T + cI_v)^*(T + cI_v) \quad \blacksquare
\end{align*}
$$
</div>
<p>For (c),</p>
<div>
$$
\begin{align*}
v \in E_{\lambda}(T) \ &amp;\implies \ v \in N(T - \lambda I_V) \\
                       &amp;\implies \ \Vert (T - \lambda I_V) \Vert = 0
\end{align*}
$$
</div>
<p>By property (b), we know that \((T - \lambda I_V)\) is normal and by property (a), we know that the adjoint will have the same norm therefore,</p>
<div>
$$
\begin{align*}
\phantom{v \in E_{\lambda}(T)} \ &amp;\implies \ \Vert (T* - \bar{\lambda} I_V) \Vert = 0 \\
                                 &amp;\implies v \in E_{\bar{\lambda}}(T^*)
\end{align*}
$$
</div>
<p>For (d),</p>
<div>
$$
\begin{align*}
\lambda_1 \langle v_1, v_2 \rangle &amp;=  \langle \lambda_1 v_1, v_2 \rangle \\
                                   &amp;=  \langle T(v_1), v_2 \rangle \\
								   &amp;=  \langle v_1, T^*(v_2) \rangle \\
								   &amp;=  \langle v_1, \bar{\lambda_2} v_2 \rangle \quad \text{(by (c))} \\
								   &amp;=  \lambda_2 \langle v_1, v_2 \rangle							   
\end{align*}
$$
</div>
<p>But we know that \(\lambda_1 \neq \lambda_2\). Therefore, we must have that \(\langle v_1, v_2 \rangle = 0\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In the last lecture, we studied adjoint linear maps defined as Definition Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that $$ \begin{align*} \langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W \end{align*} $$ Here are some additional facts Fact 1: \(T^*\) is unique if it exists Fact 2: In infinite dimensions \(T^*\) need not exist. Example 1 \(A \in M_{m \times n}(\mathbf{F})\) with \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) $$ \begin{align*} (L_A)^* = L_{A^*} \quad \quad A^* = (\bar{A})^t \end{align*} $$ Normal Linear Maps The goal of today is use this notion of adjoint maps to define more classes of linear maps which will be useful. Though we’re going to restrict the maps to maps from a finite dimensional inner product space \(V\) to \(V\) with \(\beta\) being an orthonormal basis. Therefore, in this setting, we will always have an adjoint map. Theorem 1 Suppose \(T: V \rightarrow V\) and \(T^*: V \rightarrow V\) with \(\beta\) an orthonormal basis. $$ \begin{align*} [T^*]_{\beta}^{\beta} = \left( [T]_{\beta}^{\beta} \right)^* \end{align*} $$ Definition Suppose \(T: V \rightarrow V\) and \(T^*: V \rightarrow V\). \(T\) is normal if $$ \begin{align*} T \circ T^* = T^* \circ T \end{align*} $$ Example 2 Let \(A \in M_{n \times n}(\mathbf{F})\) is normal if \(AA^* = A^*A\) $$ \begin{align*} A = \begin{pmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{pmatrix} \end{align*} $$ Note here that this is a rotation matrix. It has no eigenvalues or eigenvectors. This transformation doesn’t take any vector to a multiple of itself. All we’re doing is rotating a vector. To see if it’s a normal matrix (defines a normal operator), we need to compute \(A^*\), $$ \begin{align*} A^* = \begin{pmatrix} \cos \theta &amp; \sin \theta \\ -\sin \theta &amp; \cos \theta \end{pmatrix} \end{align*} $$ and then compute the products $$ \begin{align*} AA^* = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = A^*A \end{align*} $$ Example 3 Suppose that \(A^* = - A\), then $$ \begin{align*} AA^* = -A^2 = A^*A \end{align*} $$ So \(A\) and \(A^t\) are negative of each other. For example $$ \begin{align*} A = \begin{pmatrix} 0 &amp; c \\ -c &amp; 0 \end{pmatrix} \in A \in M_{n \times n}(\mathbf{R}) \end{align*} $$ A Sufficient Condition for Normal Linear Maps Next, we will describe a sufficient condition for an operator to be normal which will be useful later! Theorem 2 Suppose \(T: V \rightarrow V\) and \(T^*: V \rightarrow V\) with \(\beta\) an orthonormal basis. If \(V\) admits an orthonormal basis consisting of eigenvectors of \(T\), then \(T\) is normal. Remark: The converse isn’t too. Example 2 shows this. Proof Let \(\beta = \{v_1, ..., v_n\}\) be an orthonormal basis consisting of eigenvectors. Therefore]]></summary></entry><entry><title type="html">Lecture 33: Least Squares, Adjoint Maps</title><link href="http://localhost:4000/jekyll/update/2024/09/03/lec33-adjoint-maps.html" rel="alternate" type="text/html" title="Lecture 33: Least Squares, Adjoint Maps" /><published>2024-09-03T01:01:36-07:00</published><updated>2024-09-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/03/lec33-adjoint-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/03/lec33-adjoint-maps.html"><![CDATA[<p>We’re going to use the projection we studied in the previous lecture in studying systems of linear equations that are inconsistent. So consider the system of equations</p>
<div>
$$
\begin{align*}
Ax = b	 
\end{align*}
$$
</div>
<p>and suppose that this system is inconsistent. This means that \(b\) can’t be written as \(Ax\) and so \(b \not\in \{Ax \ | \ x \in \mathbf{R}^n\}\). What is \(Ax\)? This is the range of the linear operator \(L_A\), the set of images when \(L_A\) is applied on \(x\) so \(R(L_A)\). But this is also the column space of \(A\) and so</p>
<div>
$$
\begin{align*}
b \not\in \{Ax \ | \ x \in \mathbf{R}^n\} = R(L_A) = Col(A)
\end{align*}
$$
</div>
<p>From theorem 2 however, we know that there is a vector in the column space of \(A\) that is closest to \(b\) (with respect to the standard inner product). The closest vector has the form \(Ax_0\) and satisfies</p>
<div>
$$
\begin{align*}
Ax_0 = proj_{Col(A)}(b)
\end{align*}
$$
</div>
<p>Note here that \(Ax_0\) is unique but \(x_0\) is not. Such an \(x_0\) is called a least squares approximate solution to \(Ax = b\). So how do we find \(x_0\)? We can just compute \(proj_{Col(A)}(b)\) directly by finding an orthonormal basis for \(Col(A)\) using Gram Schmidt but it is an intensive process. It turns out there is a alternative way to find this vector. Formally we have the following theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
If \(Ax = b\) is inconsistent and rank\((A)=n\), then there is a unique least squares approximate solution
$$
\begin{align*}
x_0 = (A^tA)^{-1}A^tb
\end{align*}
$$
</div>
<p><br />
But why is this true? to be able to prove this theorem we need a few other definitions and lemmas first. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Adjoint Linear Maps</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that
$$
\begin{align*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W
\end{align*}
$$
</div>
<p><br />
Note here that the inner product on the left is the inner product of \(W\) but the one on the right is the inner product of \(V\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
For \(A \in M_{m \times n}(\mathbf(F)), \ \) \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\)
Set \(A^* = (\bar{A})^t\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>The matrix</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; i \\
i+1 &amp; 3 &amp; 3
\end{pmatrix}^*
=
\begin{pmatrix}
1 &amp; i-1 \\
2 &amp; 3  \\
-i &amp; 3
\end{pmatrix}
\end{align*}
$$
</div>
<p>Now we are ready to prove the first result that we need to prove the theorem we introduced earlier (theorem 3). 
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Lemma 1
</div>
<div class="purbdiv">
\(A \in M_{m \times n}(\mathbf{F})\), \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) 
$$
\begin{align*}
L_A: \ \mathbf{F}^n \rightarrow  \mathbf{F}^m
\end{align*}
$$
has adjoint
$$
\begin{align*}
L_{A^*}: \ \mathbf{F}^m \rightarrow  \mathbf{F}^n
\end{align*}
$$
In other words, \((L_A)^* = L_{A^*}\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof</b>
<br />
In \(\mathbf{F}^n\), we’re going to re-write the standard inner product as</p>
<div>
$$
\begin{align*}
\langle x, y \rangle &amp;= x_1 \bar{y_1} + ... + x_n \bar{y_n} \\
&amp;= \begin{pmatrix}
\bar{y_1} &amp; \cdots &amp; \bar{y_n}
\end{pmatrix}
\begin{pmatrix}
x_1 \\ 
\vdots \\ 
x_n
\end{pmatrix} \\
&amp;= (\bar{y})^t x \\
&amp;= y^*x
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>With this observation, we are now ready to prove \((L_A)^* = L_{A^*}\). Specifically, we want to show that</p>
<div>
$$
\begin{align*}
\langle A(x), y \rangle = \langle x, A(y) \rangle \quad \forall x \in \mathbf{F}^n, y \in \mathbf{F}^m
\end{align*}
$$
</div>
<p>Expanding the right hand side:</p>
<div>
$$
\begin{align*}
\langle A(x), y \rangle &amp;= y^*(Ax) \quad \text{(by the previous observation)}\\ 
                       &amp;= (y^*A)x \\
					   &amp;= (A^* (y^*)^*)^* x \quad \text{because $(AB)^* = B^*A^*$}\\
					   &amp;= (A^*y)^* x \quad \text{because $(A^*)^* = A$}\\
					   &amp;= \langle x, A^*y \rangle \quad \blacksquare
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
This is great but we still need another result before we are ready to prove theorem 3.
<br />
<br /></p>
<div class="purdiv">
Lemma 2
</div>
<div class="purbdiv">
$$
\begin{align*}
rank(A^*A) = rank(A)
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
The dimension theorem implies that</p>
<div>
$$
\begin{align*}
\dim(N(A)) + \text{rank}(A) = n
\end{align*}
$$
</div>
<p>Applying the dimension theorem on \(A^*A\), we see that</p>
<div>
$$
\begin{align*}
\dim(N(A^*A)) + \text{rank}(A^*A) = n \quad \text{(becasuse $A^*A$ is an $n \times n$ matrix)}
\end{align*}
$$
</div>
<p>From these two equations, \(n=n\), we want to prove that \(\text{rank}(A^*A) = \text{rank}(A)\). Therefore, it suffices to show that \(N(A) = N(A^*A)\).
<br />
<br />
To show that \(N(A) = N(A^*A)\), we want to show that \(N(A) \subseteq N(A^*A)\) and \(N(A^*A) \subseteq N(A)\). But it should be clear that \(N(A) \subseteq N(A^*A)\). Why? because \(A^*A = 0\) implies that \(A = 0\). Next, we will show that \(N(A^*A) \subseteq N(A)\). So suppose that \(x \in N(A^*A)\), then this implies that</p>
<div>
$$
\begin{align*}
&amp;\implies A^*Ax = \bar{0}_{\mathbf{F}^n} \\
&amp;\implies \langle A^*Ax, x \rangle = \langle \bar{0}, x \rangle \quad \text{(take the inner product of both sides with $x$)}\\
&amp;\implies \langle A^*Ax, x \rangle = 0 \in \mathbf{F}\\
&amp;\implies \langle Ax, (A^*)^*x \rangle = 0 \quad \text{(by the definition of adjoint above)} \\
&amp;\implies \langle Ax, Ax \rangle = 0 \quad ((A^*)^* =A) \\
&amp;\implies Ax = \bar{0} \in \mathbf{F}^m \quad \text{(property (b) of the norms theorem)} \\
&amp;\implies x \in N(A) \quad \blacksquare
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Theorem 3 Proof</b></h4>
<p>We’re going to set \(\mathbf{F} = \mathbf{R}\). Therefore, \(A^* = A^t\). We are given that rank\((A)=n\). We want to show that</p>
<div>
$$
\begin{align*}
Ax_0 = \text{proj}_{Col(A)}b \ \Longleftrightarrow \ x_0 = (A^tA)^{-1}A^tb
\end{align*}
$$
</div>
<p>Which is what theorem 3 is asserting. The solution is unique and given by the above formula. One thing we immediately see is that \(\text{rank}(A) = n\) implies \(\text{rank}(A^tA) = n\) by lemma 2. This implies means that \(A^tA\) is invertible.
<br />
<br />
We also know that \(\text{proj}_{Col(A)}b\) is the unique vector \(w\) from theorem 1. In theorem 1, we asserted that every vector \(x\) (here it is \(b\)) can be decomposed into two components \(w \in W\) and \(z \in W^{\perp}\) such that \(b = w + z\). Here we have \(w = \text{proj}_{Col(A)}b = Ax_0\) but \(b - w = z\) so \(z = b -  Ax_0\) and we want \(z\) to be orthogonal to the column space of \(A\) or \((Col(A))\).</p>
<div>
$$
\begin{align*}
Ax_0 = \text{proj}_{Col(A)}b \ &amp;\Longleftrightarrow \ \langle b - Ax_0, Ax \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{(By Theorem 1)} \\
&amp; \Longleftrightarrow \ \langle A^t (b - Ax_0), x \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{move the adjoint to the other side} \\
&amp; \Longleftrightarrow \  A^t (b - Ax_0) = \bar{0} \\
&amp; \Longleftrightarrow \  A^tb - A^tAx_0 = \bar{0} \\
&amp; \Longleftrightarrow \  x_0 = (A^tA)^{-1}A^tb \quad \blacksquare \\
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’re going to use the projection we studied in the previous lecture in studying systems of linear equations that are inconsistent. So consider the system of equations $$ \begin{align*} Ax = b \end{align*} $$ and suppose that this system is inconsistent. This means that \(b\) can’t be written as \(Ax\) and so \(b \not\in \{Ax \ | \ x \in \mathbf{R}^n\}\). What is \(Ax\)? This is the range of the linear operator \(L_A\), the set of images when \(L_A\) is applied on \(x\) so \(R(L_A)\). But this is also the column space of \(A\) and so $$ \begin{align*} b \not\in \{Ax \ | \ x \in \mathbf{R}^n\} = R(L_A) = Col(A) \end{align*} $$ From theorem 2 however, we know that there is a vector in the column space of \(A\) that is closest to \(b\) (with respect to the standard inner product). The closest vector has the form \(Ax_0\) and satisfies $$ \begin{align*} Ax_0 = proj_{Col(A)}(b) \end{align*} $$ Note here that \(Ax_0\) is unique but \(x_0\) is not. Such an \(x_0\) is called a least squares approximate solution to \(Ax = b\). So how do we find \(x_0\)? We can just compute \(proj_{Col(A)}(b)\) directly by finding an orthonormal basis for \(Col(A)\) using Gram Schmidt but it is an intensive process. It turns out there is a alternative way to find this vector. Formally we have the following theorem Theorem 3 If \(Ax = b\) is inconsistent and rank\((A)=n\), then there is a unique least squares approximate solution $$ \begin{align*} x_0 = (A^tA)^{-1}A^tb \end{align*} $$ But why is this true? to be able to prove this theorem we need a few other definitions and lemmas first. Adjoint Linear Maps Definition Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that $$ \begin{align*} \langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W \end{align*} $$ Note here that the inner product on the left is the inner product of \(W\) but the one on the right is the inner product of \(V\). Definition For \(A \in M_{m \times n}(\mathbf(F)), \ \) \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) Set \(A^* = (\bar{A})^t\) Example The matrix $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; i \\ i+1 &amp; 3 &amp; 3 \end{pmatrix}^* = \begin{pmatrix} 1 &amp; i-1 \\ 2 &amp; 3 \\ -i &amp; 3 \end{pmatrix} \end{align*} $$ Now we are ready to prove the first result that we need to prove the theorem we introduced earlier (theorem 3). Lemma 1 \(A \in M_{m \times n}(\mathbf{F})\), \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) $$ \begin{align*} L_A: \ \mathbf{F}^n \rightarrow \mathbf{F}^m \end{align*} $$ has adjoint $$ \begin{align*} L_{A^*}: \ \mathbf{F}^m \rightarrow \mathbf{F}^n \end{align*} $$ In other words, \((L_A)^* = L_{A^*}\) Proof In \(\mathbf{F}^n\), we’re going to re-write the standard inner product as $$ \begin{align*} \langle x, y \rangle &amp;= x_1 \bar{y_1} + ... + x_n \bar{y_n} \\ &amp;= \begin{pmatrix} \bar{y_1} &amp; \cdots &amp; \bar{y_n} \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \\ &amp;= (\bar{y})^t x \\ &amp;= y^*x \end{align*} $$ With this observation, we are now ready to prove \((L_A)^* = L_{A^*}\). Specifically, we want to show that $$ \begin{align*} \langle A(x), y \rangle = \langle x, A(y) \rangle \quad \forall x \in \mathbf{F}^n, y \in \mathbf{F}^m \end{align*} $$ Expanding the right hand side: $$ \begin{align*} \langle A(x), y \rangle &amp;= y^*(Ax) \quad \text{(by the previous observation)}\\ &amp;= (y^*A)x \\ &amp;= (A^* (y^*)^*)^* x \quad \text{because $(AB)^* = B^*A^*$}\\ &amp;= (A^*y)^* x \quad \text{because $(A^*)^* = A$}\\ &amp;= \langle x, A^*y \rangle \quad \blacksquare \end{align*} $$ This is great but we still need another result before we are ready to prove theorem 3. Lemma 2 $$ \begin{align*} rank(A^*A) = rank(A) \end{align*} $$ Proof The dimension theorem implies that $$ \begin{align*} \dim(N(A)) + \text{rank}(A) = n \end{align*} $$ Applying the dimension theorem on \(A^*A\), we see that $$ \begin{align*} \dim(N(A^*A)) + \text{rank}(A^*A) = n \quad \text{(becasuse $A^*A$ is an $n \times n$ matrix)} \end{align*} $$ From these two equations, \(n=n\), we want to prove that \(\text{rank}(A^*A) = \text{rank}(A)\). Therefore, it suffices to show that \(N(A) = N(A^*A)\). To show that \(N(A) = N(A^*A)\), we want to show that \(N(A) \subseteq N(A^*A)\) and \(N(A^*A) \subseteq N(A)\). But it should be clear that \(N(A) \subseteq N(A^*A)\). Why? because \(A^*A = 0\) implies that \(A = 0\). Next, we will show that \(N(A^*A) \subseteq N(A)\). So suppose that \(x \in N(A^*A)\), then this implies that $$ \begin{align*} &amp;\implies A^*Ax = \bar{0}_{\mathbf{F}^n} \\ &amp;\implies \langle A^*Ax, x \rangle = \langle \bar{0}, x \rangle \quad \text{(take the inner product of both sides with $x$)}\\ &amp;\implies \langle A^*Ax, x \rangle = 0 \in \mathbf{F}\\ &amp;\implies \langle Ax, (A^*)^*x \rangle = 0 \quad \text{(by the definition of adjoint above)} \\ &amp;\implies \langle Ax, Ax \rangle = 0 \quad ((A^*)^* =A) \\ &amp;\implies Ax = \bar{0} \in \mathbf{F}^m \quad \text{(property (b) of the norms theorem)} \\ &amp;\implies x \in N(A) \quad \blacksquare \end{align*} $$ Theorem 3 Proof We’re going to set \(\mathbf{F} = \mathbf{R}\). Therefore, \(A^* = A^t\). We are given that rank\((A)=n\). We want to show that $$ \begin{align*} Ax_0 = \text{proj}_{Col(A)}b \ \Longleftrightarrow \ x_0 = (A^tA)^{-1}A^tb \end{align*} $$ Which is what theorem 3 is asserting. The solution is unique and given by the above formula. One thing we immediately see is that \(\text{rank}(A) = n\) implies \(\text{rank}(A^tA) = n\) by lemma 2. This implies means that \(A^tA\) is invertible. We also know that \(\text{proj}_{Col(A)}b\) is the unique vector \(w\) from theorem 1. In theorem 1, we asserted that every vector \(x\) (here it is \(b\)) can be decomposed into two components \(w \in W\) and \(z \in W^{\perp}\) such that \(b = w + z\). Here we have \(w = \text{proj}_{Col(A)}b = Ax_0\) but \(b - w = z\) so \(z = b - Ax_0\) and we want \(z\) to be orthogonal to the column space of \(A\) or \((Col(A))\). $$ \begin{align*} Ax_0 = \text{proj}_{Col(A)}b \ &amp;\Longleftrightarrow \ \langle b - Ax_0, Ax \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{(By Theorem 1)} \\ &amp; \Longleftrightarrow \ \langle A^t (b - Ax_0), x \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{move the adjoint to the other side} \\ &amp; \Longleftrightarrow \ A^t (b - Ax_0) = \bar{0} \\ &amp; \Longleftrightarrow \ A^tb - A^tAx_0 = \bar{0} \\ &amp; \Longleftrightarrow \ x_0 = (A^tA)^{-1}A^tb \quad \blacksquare \\ \end{align*} $$ References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 32: Decomposition of a Vector, Orthogonal Complement</title><link href="http://localhost:4000/jekyll/update/2024/09/02/lec32-decomposition-complement.html" rel="alternate" type="text/html" title="Lecture 32: Decomposition of a Vector, Orthogonal Complement" /><published>2024-09-02T01:01:36-07:00</published><updated>2024-09-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/02/lec32-decomposition-complement</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/02/lec32-decomposition-complement.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The orthogonal complement to \(S\) is 
$$
\begin{align*}
S^{\perp} = \{x \in V \ | \ \langle x, y \rangle = 0 \quad \forall y \in S\}
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose \(S = \{(0,0,1)\} \subset \mathbf{R}^3\). The orthogonal complement to \(S\) is any element \(x\) in \(\mathbf{R}^3\) such that for any element \(y \in S\), the inner product \(\langle x, y \rangle\) must be zero.</p>
<div>
$$
\begin{align*}
S^{\perp} &amp;= \{(x,y,z) \in V \ | \ \langle (x,y,z), (0,0,1) \rangle = 0\} \\
          &amp;= \{(x,y,z) \in V \ | \ z = 0\} \\
          &amp;= \{(x,y,0)\} \\
\end{align*}
$$
</div>
<p>Note here if we chose \(S' = span(S)\), then the orthogonal complement will be the same! 
<br />
Exercise 1: \(S^{\perp}\) is a subspace of \(V\).
<br />
Exercise 2: If \(S\) is a subspace, then \(S \cap S^{\perp} = \{0\}\).
<br />
<br />
How to use these orthogonal complements? We have the following theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
Suppose \(W\) is finite dimensional subspace of inner produce space \(V\). For each \(x \in V\) there is a unique \(w \in W\) and \(z \in W^{\perp}\) such that 
$$
\begin{align*}
x = w + z
\end{align*}
$$
<br />
If \(\{u_1,...,u_k\}\) is an orthonormal basis for \(W\), then \(w = \langle x, u_1 \rangle + ... + \langle x, u_k \rangle u_k\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
(TODO: Add pic)
<br />
<b>Proof</b>
<br />
We can always find \(\{u_1, ..., u_k\}\) using Gram-Schmidt. So we know \(w\) and we’re given \(x\). All we need is to verify that \(z = x - w\) and show that \(w\) and \(z\) are unique. In other words, it suffices to show for</p>
<div>
$$
\begin{align*}
w = \sum_{j=1}^{k} \langle x, u_j \rangle u_j
\end{align*}
$$
</div>
<p>that \(z = x - w\) is in \(W^{\perp}\) plus the uniqueness of this decomposition.</p>
<div>
$$
\begin{align*}
&amp;z = x - w \in W^{\perp} \\
\Leftrightarrow \ &amp;\langle z, y \rangle = 0 \ \forall y \in W
\end{align*}
$$
</div>
<p>It isn’t easy to verify this for every vector \(y \in W\). But we do have a basis for \(W\) and any \(y\) can be written as a linear combination of the basis elements. So we can instead check this for every basis element</p>
<div>
$$
\begin{align*}
\Leftrightarrow \ &amp;\langle z, u_j \rangle = 0 \ \forall j =1,...,k
\end{align*}
$$
</div>
<p>So let’s check for every basis element that it’s orthogonal to \(x - w\).</p>
<div>
$$
\begin{align*}
\Leftrightarrow \ \langle z, u_j \rangle &amp;= \langle x-w, u_j \rangle \\ 
 &amp;= \langle x - \sum_{j=1}^k \langle x, u_j \rangle u_j  , u_j \rangle \\
 &amp;= \langle x, u_j \rangle - \langle \sum_{i=1}^k \langle x, u_i \rangle u_i  , u_j \rangle  \\
 &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \langle u_i  , u_j \rangle  \\
 &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \delta_{ij} \\
 &amp;= \langle x, u_j \rangle - \langle x, u_j \rangle \\
 &amp;= 0
\end{align*}
$$
</div>
<p>So now we see that \(x = w + z\) where \(w \in W\) and \(z \in W^{\perp}\). We still need to show that this decomposition is unique. Suppose it’s not and that \(x = w + z = \tilde{w} + \tilde{z}\). This implies</p>
<div>
$$
\begin{align*}
w - \tilde{w} = z - \tilde{z} = \bar{0}_V
\end{align*}
$$
</div>
<p>But \(w,\tilde{w} \in W\) and \(z, \tilde{z} \in Z\) and since \(W \cap W^{\perp} = \bar{0}_V\), then \(w = \tilde{w}\) and \(z = \tilde{z}\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Projection of \(x\) onto \(W\)</b></h4>
<p>The \(w\) vector we found in the last theorem has a special name. It is the projection of \(x\) onto the subspace \(W\).
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(w\) above is called the orthogonal projection of \(x\) onto \(W\) and is denoted as
$$
\begin{align*}
proj_W(x) = w
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
It also has a special geometric interpretation. It is the closest vector to \(x\) in \(W\).
<br />
<br /></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
\(w = proj_W(x)\) is the vector in \(W\) closest to \(x\) in the following sense
$$
\begin{align*}
\Vert x - w \Vert \leq \Vert x - y \Vert \ \forall y \in W
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
It’s easier to square things since we don’t have deal with squareroots so</p>
<div>
$$
\begin{align*}
\Vert x - y \Vert^2 &amp;= \Vert (w + z) - y \Vert^2 \text{ (by theorem 1)} \\
                 &amp;= \Vert (w - y) + z \Vert^2 \text{ (w and y are both in $W$)} \\
                 &amp;= \langle (w - y) + z, (w - y) + z \rangle \\
                 &amp;= \langle (w - y), (w - y) \rangle 
				 + \langle (w - y), z \rangle 
				 + \langle z, z \rangle 
				 + \langle z, (w - y) \rangle \\
				 &amp;= \Vert w - y \Vert^2 + \langle (w - y), z \rangle  + \langle z, (w - y) \rangle + \Vert z \Vert^2 \\
				 &amp;= \Vert w - y \Vert^2 + \Vert z \Vert^2 \text{ ($z \in W^{\perp}$ and $w, y \in W$)} \\
				 &amp;\geq \Vert z \Vert^2 \\
				 &amp;=  \Vert x - w \Vert^2. \ \blacksquare
				
				 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Projection as a Map</b></h4>
<p>In general we can think of the projection onto \(W\) as a map.</p>
<div>
$$
\begin{align*}
proj_W: \ &amp; V \rightarrow W \\
		&amp;x \rightarrow proj_W(x)		 
\end{align*}
$$
</div>
<p>where the formula is</p>
<div>
$$
\begin{align*}
w = \sum_{j = 1}^{k} \langle x, u_j \rangle u_j		 
\end{align*}
$$
</div>
<p>This formula tells us that the projection is linear in \(x\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The orthogonal complement to \(S\) is $$ \begin{align*} S^{\perp} = \{x \in V \ | \ \langle x, y \rangle = 0 \quad \forall y \in S\} \end{align*} $$ Example 2 Suppose \(S = \{(0,0,1)\} \subset \mathbf{R}^3\). The orthogonal complement to \(S\) is any element \(x\) in \(\mathbf{R}^3\) such that for any element \(y \in S\), the inner product \(\langle x, y \rangle\) must be zero. $$ \begin{align*} S^{\perp} &amp;= \{(x,y,z) \in V \ | \ \langle (x,y,z), (0,0,1) \rangle = 0\} \\ &amp;= \{(x,y,z) \in V \ | \ z = 0\} \\ &amp;= \{(x,y,0)\} \\ \end{align*} $$ Note here if we chose \(S' = span(S)\), then the orthogonal complement will be the same! Exercise 1: \(S^{\perp}\) is a subspace of \(V\). Exercise 2: If \(S\) is a subspace, then \(S \cap S^{\perp} = \{0\}\). How to use these orthogonal complements? We have the following theorem Theorem 1 Suppose \(W\) is finite dimensional subspace of inner produce space \(V\). For each \(x \in V\) there is a unique \(w \in W\) and \(z \in W^{\perp}\) such that $$ \begin{align*} x = w + z \end{align*} $$ If \(\{u_1,...,u_k\}\) is an orthonormal basis for \(W\), then \(w = \langle x, u_1 \rangle + ... + \langle x, u_k \rangle u_k\) (TODO: Add pic) Proof We can always find \(\{u_1, ..., u_k\}\) using Gram-Schmidt. So we know \(w\) and we’re given \(x\). All we need is to verify that \(z = x - w\) and show that \(w\) and \(z\) are unique. In other words, it suffices to show for $$ \begin{align*} w = \sum_{j=1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ that \(z = x - w\) is in \(W^{\perp}\) plus the uniqueness of this decomposition. $$ \begin{align*} &amp;z = x - w \in W^{\perp} \\ \Leftrightarrow \ &amp;\langle z, y \rangle = 0 \ \forall y \in W \end{align*} $$ It isn’t easy to verify this for every vector \(y \in W\). But we do have a basis for \(W\) and any \(y\) can be written as a linear combination of the basis elements. So we can instead check this for every basis element $$ \begin{align*} \Leftrightarrow \ &amp;\langle z, u_j \rangle = 0 \ \forall j =1,...,k \end{align*} $$ So let’s check for every basis element that it’s orthogonal to \(x - w\). $$ \begin{align*} \Leftrightarrow \ \langle z, u_j \rangle &amp;= \langle x-w, u_j \rangle \\ &amp;= \langle x - \sum_{j=1}^k \langle x, u_j \rangle u_j , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \langle \sum_{i=1}^k \langle x, u_i \rangle u_i , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \langle u_i , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \delta_{ij} \\ &amp;= \langle x, u_j \rangle - \langle x, u_j \rangle \\ &amp;= 0 \end{align*} $$ So now we see that \(x = w + z\) where \(w \in W\) and \(z \in W^{\perp}\). We still need to show that this decomposition is unique. Suppose it’s not and that \(x = w + z = \tilde{w} + \tilde{z}\). This implies $$ \begin{align*} w - \tilde{w} = z - \tilde{z} = \bar{0}_V \end{align*} $$ But \(w,\tilde{w} \in W\) and \(z, \tilde{z} \in Z\) and since \(W \cap W^{\perp} = \bar{0}_V\), then \(w = \tilde{w}\) and \(z = \tilde{z}\). \(\ \blacksquare\) Projection of \(x\) onto \(W\) The \(w\) vector we found in the last theorem has a special name. It is the projection of \(x\) onto the subspace \(W\). Definition \(w\) above is called the orthogonal projection of \(x\) onto \(W\) and is denoted as $$ \begin{align*} proj_W(x) = w \end{align*} $$ It also has a special geometric interpretation. It is the closest vector to \(x\) in \(W\). Theorem 2 \(w = proj_W(x)\) is the vector in \(W\) closest to \(x\) in the following sense $$ \begin{align*} \Vert x - w \Vert \leq \Vert x - y \Vert \ \forall y \in W \end{align*} $$ Proof It’s easier to square things since we don’t have deal with squareroots so $$ \begin{align*} \Vert x - y \Vert^2 &amp;= \Vert (w + z) - y \Vert^2 \text{ (by theorem 1)} \\ &amp;= \Vert (w - y) + z \Vert^2 \text{ (w and y are both in $W$)} \\ &amp;= \langle (w - y) + z, (w - y) + z \rangle \\ &amp;= \langle (w - y), (w - y) \rangle + \langle (w - y), z \rangle + \langle z, z \rangle + \langle z, (w - y) \rangle \\ &amp;= \Vert w - y \Vert^2 + \langle (w - y), z \rangle + \langle z, (w - y) \rangle + \Vert z \Vert^2 \\ &amp;= \Vert w - y \Vert^2 + \Vert z \Vert^2 \text{ ($z \in W^{\perp}$ and $w, y \in W$)} \\ &amp;\geq \Vert z \Vert^2 \\ &amp;= \Vert x - w \Vert^2. \ \blacksquare \end{align*} $$ Projection as a Map In general we can think of the projection onto \(W\) as a map. $$ \begin{align*} proj_W: \ &amp; V \rightarrow W \\ &amp;x \rightarrow proj_W(x) \end{align*} $$ where the formula is $$ \begin{align*} w = \sum_{j = 1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ This formula tells us that the projection is linear in \(x\). References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 31/32: Orthonormal Sets, Gram Schmidt and Fourier coefficients</title><link href="http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets.html" rel="alternate" type="text/html" title="Lecture 31/32: Orthonormal Sets, Gram Schmidt and Fourier coefficients" /><published>2024-09-01T01:01:36-07:00</published><updated>2024-09-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
<ul>
	<li>\(x, y \in V\) are orthogonal (perpendicular) if \(\langle x, y \rangle = 0\)</li>
	<li>\(S \in V\) is orthogonal if for any \(x, y\) distinct in \(S\), \(\langle x, y \rangle = 0\)</li>
	<li>\(S \subset V\) is orthonormal if it is orthogonal and for each \(x \in S, \Vert x \Vert = 1\)</li>
</ul>
</div>
<p><br />
Remark: \(x \in V\) is a unit vector if \(\Vert x \Vert = 1\). If \(x \neq \bar{0}_V\), then \(\frac{x}{\Vert x \Vert}\) is a unit vector. This process is normalization.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div> 
$$
\begin{align*}
V = C^0([0,1]), \langle f,g \rangle = \int_0^1 f(t)g(t)dt
\end{align*}
$$
</div>
<p>Show \(\sin(2\pi t)\) and \(\cos(2\pi t)\) are orthogonal. To do this, we’ll compute their inner product.</p>
<div> 
$$
\begin{align*}
\langle \sin(2\pi t), \cos(2\pi t) \rangle &amp;= \int_0^1 \sin(2\pi t) \cos(2\pi t) \\
                                           &amp;= \frac{\sin^2 2 \pi t}{4\pi} \Big|^1_0 = 0
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The standard basis \(\beta = \{e_1, e_2, ..., e_n \}\) of \(\mathbf{R}^n\) is an orthonormal subset. For every distinct two vectors in \(\beta\), their inner product is zero. Moreover, for any vector \(e_i \in \beta\), \(\langle e_i, e_i \rangle = 1\). In general,</p>
<div> 
$$
\begin{align*}
\langle e_i, e_j \rangle &amp;= \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j\end{cases}\\
\end{align*}
$$
</div>
<p>In general, if \(\{v_1,...,v_k\}\) is orthonormal, then \(\langle v_i, v_j \rangle = \delta_{ij}\). The next theorem tells us why orthonormal sets are useful.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(S = \{v_1,...,v_k\} \subseteq V\) is orthonormal. If \(y \in span(S)\), then
$$
\begin{align*}
y = \sum_{j=1}^k \langle y, v_j \rangle v_j
\end{align*}
$$
</div>
<p><br />
So we don’t need to solve a system of linear equations to figure out the coefficients if \(y\) written with respect to \(S\). 
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>:
<br />
<br />
We know that \(y \in span(S)\) Therefore, we can write \(y\) as</p>
<div>
$$
\begin{align*}
y = \sum_{j=1}^k a_j v_j
\end{align*}
$$
</div>
<p>for scalars \(a_1,...,a_k\). We also know that the \(S\) is an orthonormal set. Taking the inner product of both sides with respect to one vector from \(S\), we see that</p>
<div>
$$
\begin{align*}
\langle y, v_i \rangle &amp;= \langle \sum_{j=1}^k a_j v_j, v_i \rangle \text{ for all $i = 1,...,k$} \\
 &amp;= \sum_{j=1}^k a_j \langle v_j, v_i \rangle \text{ (the inner product is linear in its first argument)} \\
 &amp;= \sum_{j=1}^k a_j \delta_{ij} \\
 &amp;= a_i \text{ ($\delta_{ij}$ is 1 only for $i = j$)}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
y = \sum_{j=1}^k \langle y, v_j \rangle v_j. \quad \blacksquare
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
What about orthogonal subsets, can we say anything about them? Yes!
<br />
<br /></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\). If \(y \in span(S)\), then
$$
\begin{align*}
y = \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>:
<br />
<br />
If \(S = \{v_1,...,v_k\}\) is orthogonal. We can turn this set into an orthonormal set by normalizing the set so</p>
<div>
$$
\begin{align*}
\{\frac{v_1}{\Vert v_1 \Vert^2},...,\frac{v_k}{\Vert v_k \Vert^2}\}
\end{align*}
$$
</div>
<p>By the previous theorem, then</p>
<div>
$$
\begin{align*}
y &amp;= \sum_{j=1}^k \langle y, \frac{v_j}{\Vert v_j \Vert }\rangle \frac{v_j}{\Vert v_j \Vert}  \\
  &amp;= \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j. \quad \blacksquare
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------------->
<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\), then \(S\) is linearly independent. 
</div>
<p><br />
<br />
<b>Proof:</b>
To see that it’s linearly independent, then the only solution to the equation</p>
<div>
	$$
	\begin{align*}
	a_1v_1 + ... + a_kv_k = \bar{0}_V
	\end{align*}
	$$
</div>
<p>is the trivial solution. But by corollary 1, if \(\bar{0}_V \in span(S)\), then we know the coefficients when it’s written relative to (S). Specifically the \(j\)’s coefficient is</p>
<div>
	$$
	\begin{align*}
	a_j = \frac{\langle \bar{0}_V, v_j \rangle}{\Vert v_j \Vert^2} = 0. \quad \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<!---------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) is finite dimensional inner product space, then it has an orthonormal basis.
</div>
<p><br />
<br />
This will follow from the procedure we will study next …
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Gram-Schmidt Process</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(\{w_1,...,w_k\}\) be a linearly independent subset of \(V\). Set 
	$$
	\begin{align*}
	u_1 &amp;= \frac{w_1}{\Vert w_1 \Vert} \\
	u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle}{\Vert w_2 - \langle w_2, u_1 \rangle \Vert} \\
	&amp;\vdots \\
	u_k &amp;= \frac{w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j}{\Vert w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j \Vert}
	\end{align*}
	$$
Then \(\{u_1,...,u_k\}\) is orthonormal and has same span as \(\{w_1,...,w_k\}\).
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
<br />
The basic idea of the proof (by induction) is that given \(\{w_1,...,w_k\}\) is linearly independent and given than \(\{u_1, u_2\}\) is orthonormal with the same span as \(\{w_1, w_2\}\), we want \(u_3\) such that \(\{u_1, u_2, u_3\}\) is orthonormal and \(span(\{u_1, u_2, u_3\}) = span(\{w_1,w_2,w_3\})\)
<br />
<br />
To show that the two spans are the same, it suffices to show that \(w_3 \in span(\{u_1, u_2, u_3\})\). In this case, we know by the theorem above what the coefficients should be:</p>
<div>
	$$
	\begin{align*}
	w_3 = \langle w_3, u_1 \rangle u_1 + \langle w_3, u_2 \rangle u_2 + \langle w_3, u_3 \rangle u_3
	\end{align*}
	$$
</div>
<p>Therefore, we can use the above equation to solve for \(u_3\). But we don’t want to divide by \(\langle w_3, u_3 \rangle\) since we’re trying to solve for \(u_3\) so we can think of this term as a constant we’re multiplying with:</p>
<div>
	$$
	\begin{align*}
	u_3 = c( w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2  )
	\end{align*}
	$$
</div>
<p>But we know we want \(u_3\) to be a unit vector. So we can just divide by the length of it.</p>
<div>
	$$
	\begin{align*}
	u_3 = \frac{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2}{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 \Vert}
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Find an orthonormal basis for \(P_2 \in C^0([-1, 1])\). equipped with</p>
<div>
	$$
	\begin{align*}
	\langle f, g \rangle = \int_{-1}^{1} f(x)g(x) dx
	\end{align*}
	$$
</div>
<p>Choose \(\{1, x, x^2\}\). Apply Gram-Schmidt. So</p>
<div>
	$$
	\begin{align*}
	\Vert 1\Vert^2 &amp;= \langle 1, 1 \rangle = \int_{-1}^{1} 1dx = 2 \\
	u_1 &amp;= \frac{1}{\Vert 1\Vert} = \frac{1}{\sqrt{2}}
	\end{align*}
	$$
</div>
<p>Next, we’ll find \(u_2\)</p>
<div>
	$$
	\begin{align*}
	\Vert w_2 \Vert^2 &amp;= \langle x, x \rangle = \int_{-1}^{1} x^2 dx = \frac{x^2}{3} \Big|^1_{-1} = \frac{2}{3} \\
	\langle w_2, u_1 \rangle &amp;=  \int_{-1}^{1} x\frac{1}{\sqrt{2}} dx = 0 \\
	u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle u_1}{\Vert w_2 - \langle w_2, u_1 \rangle u_1 \Vert} = \frac{\sqrt{3}}{\sqrt{2}}x
	\end{align*}
	$$
</div>
<p>And finally \(u_3\)</p>
<div>
	$$
	\begin{align*}
	u_3 &amp;= \frac{w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1}{\Vert w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1 \Vert}
	\\
	&amp;= \sqrt{\frac{5}{8}}(3x^2 - 1)
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Fourier coefficients</b></h4>
<p>The coefficients with respect to an orthonormal spanning set that we studied last time have a special name:
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(S \subseteq V\) be an (possibly infinite) orthonormal subset. The scalars \(\langle x, u\rangle\) for \(u \in S\) are called the Fourier coefficients of \(x\) with respect to \(S\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div> 
$$
\begin{align*}
V &amp;= C^0([-1,1]), \langle f,g \rangle = \int_{-1}^1 f(t)g(t)dt \\
S &amp;= \big\{\frac{1}{\sqrt{2}}\big\} \cup \{\sin n \pi t\}_{n=1}^{\infty} \cup \{\cos n \pi t \}_{n=1}^{\infty}
\end{align*}
$$
</div>
<p>We can easily check that \(S\) is an orthonormal set. Each of the two vectors is orthogonal to each other and each vector is of unit length. Find the Fourier coefficients of \(f = |t| \in C^0([-1,1])\)</p>
<div> 
$$
\begin{align*}
\langle f, \frac{1}{\sqrt{2}} \rangle &amp;= \int_{-1}^1 \frac{1}{\sqrt{2}} |t| dt = \frac{1}{\sqrt{2}} \\
\langle f, \sin n \pi t \rangle &amp;= \int_{-1}^1 \sin n \pi t |t| dt = 0 \\
\langle f, \cos n \pi t \rangle &amp;= \int_{-1}^1 \cos n \pi t |t| dt = 
\begin{cases} 0 \quad \ \ \quad \text{if $n$ even } \\ \frac{-4}{(n\pi)^2} \quad \text{if $n$ odd } \end{cases}
\end{align*}
$$
</div>
<p>When \(S = \{u_1,...,u_k\}\) is finite, then we can write</p>
<div> 
$$
\begin{align*}
x = \sum_{j=1}^{k} \langle x, u_j \rangle u_j
\end{align*}
$$
</div>
<p>But now in the infinite case, Is</p>
<div> 
$$
\begin{align*}
|t| = \frac{1}{2} = \sum_{n\text{ odd}} \frac{-4}{(n\pi)^2} \cos n \pi t
\end{align*}
$$
</div>
<p>Yes it is true but this is an infinite sum that converges to a number that is the absolute value of \(t\). This is basically the beginning of studying Fourier Analysis where any sufficiently nice function can be written as an infinite sum of sines and cosines. 
<br />
<br />
But one thing we know here is that \(S\) is not a basis for \(C^0[-1,1]\). \(|t| \neq\) finite set of elements of \(S\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(x, y \in V\) are orthogonal (perpendicular) if \(\langle x, y \rangle = 0\) \(S \in V\) is orthogonal if for any \(x, y\) distinct in \(S\), \(\langle x, y \rangle = 0\) \(S \subset V\) is orthonormal if it is orthogonal and for each \(x \in S, \Vert x \Vert = 1\) Remark: \(x \in V\) is a unit vector if \(\Vert x \Vert = 1\). If \(x \neq \bar{0}_V\), then \(\frac{x}{\Vert x \Vert}\) is a unit vector. This process is normalization. Example 1 $$ \begin{align*} V = C^0([0,1]), \langle f,g \rangle = \int_0^1 f(t)g(t)dt \end{align*} $$ Show \(\sin(2\pi t)\) and \(\cos(2\pi t)\) are orthogonal. To do this, we’ll compute their inner product. $$ \begin{align*} \langle \sin(2\pi t), \cos(2\pi t) \rangle &amp;= \int_0^1 \sin(2\pi t) \cos(2\pi t) \\ &amp;= \frac{\sin^2 2 \pi t}{4\pi} \Big|^1_0 = 0 \end{align*} $$ Example 2 The standard basis \(\beta = \{e_1, e_2, ..., e_n \}\) of \(\mathbf{R}^n\) is an orthonormal subset. For every distinct two vectors in \(\beta\), their inner product is zero. Moreover, for any vector \(e_i \in \beta\), \(\langle e_i, e_i \rangle = 1\). In general, $$ \begin{align*} \langle e_i, e_j \rangle &amp;= \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j\end{cases}\\ \end{align*} $$ In general, if \(\{v_1,...,v_k\}\) is orthonormal, then \(\langle v_i, v_j \rangle = \delta_{ij}\). The next theorem tells us why orthonormal sets are useful. Theorem Suppose \(S = \{v_1,...,v_k\} \subseteq V\) is orthonormal. If \(y \in span(S)\), then $$ \begin{align*} y = \sum_{j=1}^k \langle y, v_j \rangle v_j \end{align*} $$ So we don’t need to solve a system of linear equations to figure out the coefficients if \(y\) written with respect to \(S\). Proof: We know that \(y \in span(S)\) Therefore, we can write \(y\) as $$ \begin{align*} y = \sum_{j=1}^k a_j v_j \end{align*} $$ for scalars \(a_1,...,a_k\). We also know that the \(S\) is an orthonormal set. Taking the inner product of both sides with respect to one vector from \(S\), we see that $$ \begin{align*} \langle y, v_i \rangle &amp;= \langle \sum_{j=1}^k a_j v_j, v_i \rangle \text{ for all $i = 1,...,k$} \\ &amp;= \sum_{j=1}^k a_j \langle v_j, v_i \rangle \text{ (the inner product is linear in its first argument)} \\ &amp;= \sum_{j=1}^k a_j \delta_{ij} \\ &amp;= a_i \text{ ($\delta_{ij}$ is 1 only for $i = j$)} \end{align*} $$ Therefore, $$ \begin{align*} y = \sum_{j=1}^k \langle y, v_j \rangle v_j. \quad \blacksquare \end{align*} $$ What about orthogonal subsets, can we say anything about them? Yes! Corollary 1 If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\). If \(y \in span(S)\), then $$ \begin{align*} y = \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j \end{align*} $$ Proof: If \(S = \{v_1,...,v_k\}\) is orthogonal. We can turn this set into an orthonormal set by normalizing the set so $$ \begin{align*} \{\frac{v_1}{\Vert v_1 \Vert^2},...,\frac{v_k}{\Vert v_k \Vert^2}\} \end{align*} $$ By the previous theorem, then $$ \begin{align*} y &amp;= \sum_{j=1}^k \langle y, \frac{v_j}{\Vert v_j \Vert }\rangle \frac{v_j}{\Vert v_j \Vert} \\ &amp;= \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j. \quad \blacksquare \end{align*} $$ Corollary 2 If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\), then \(S\) is linearly independent. Proof: To see that it’s linearly independent, then the only solution to the equation $$ \begin{align*} a_1v_1 + ... + a_kv_k = \bar{0}_V \end{align*} $$ is the trivial solution. But by corollary 1, if \(\bar{0}_V \in span(S)\), then we know the coefficients when it’s written relative to (S). Specifically the \(j\)’s coefficient is $$ \begin{align*} a_j = \frac{\langle \bar{0}_V, v_j \rangle}{\Vert v_j \Vert^2} = 0. \quad \blacksquare \end{align*} $$ Theorem If \(V\) is finite dimensional inner product space, then it has an orthonormal basis. This will follow from the procedure we will study next … Gram-Schmidt Process Theorem Let \(\{w_1,...,w_k\}\) be a linearly independent subset of \(V\). Set $$ \begin{align*} u_1 &amp;= \frac{w_1}{\Vert w_1 \Vert} \\ u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle}{\Vert w_2 - \langle w_2, u_1 \rangle \Vert} \\ &amp;\vdots \\ u_k &amp;= \frac{w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j}{\Vert w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j \Vert} \end{align*} $$ Then \(\{u_1,...,u_k\}\) is orthonormal and has same span as \(\{w_1,...,w_k\}\). Proof The basic idea of the proof (by induction) is that given \(\{w_1,...,w_k\}\) is linearly independent and given than \(\{u_1, u_2\}\) is orthonormal with the same span as \(\{w_1, w_2\}\), we want \(u_3\) such that \(\{u_1, u_2, u_3\}\) is orthonormal and \(span(\{u_1, u_2, u_3\}) = span(\{w_1,w_2,w_3\})\) To show that the two spans are the same, it suffices to show that \(w_3 \in span(\{u_1, u_2, u_3\})\). In this case, we know by the theorem above what the coefficients should be: $$ \begin{align*} w_3 = \langle w_3, u_1 \rangle u_1 + \langle w_3, u_2 \rangle u_2 + \langle w_3, u_3 \rangle u_3 \end{align*} $$ Therefore, we can use the above equation to solve for \(u_3\). But we don’t want to divide by \(\langle w_3, u_3 \rangle\) since we’re trying to solve for \(u_3\) so we can think of this term as a constant we’re multiplying with: $$ \begin{align*} u_3 = c( w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 ) \end{align*} $$ But we know we want \(u_3\) to be a unit vector. So we can just divide by the length of it. $$ \begin{align*} u_3 = \frac{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2}{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 \Vert} \end{align*} $$ Example 3 Find an orthonormal basis for \(P_2 \in C^0([-1, 1])\). equipped with $$ \begin{align*} \langle f, g \rangle = \int_{-1}^{1} f(x)g(x) dx \end{align*} $$ Choose \(\{1, x, x^2\}\). Apply Gram-Schmidt. So $$ \begin{align*} \Vert 1\Vert^2 &amp;= \langle 1, 1 \rangle = \int_{-1}^{1} 1dx = 2 \\ u_1 &amp;= \frac{1}{\Vert 1\Vert} = \frac{1}{\sqrt{2}} \end{align*} $$ Next, we’ll find \(u_2\) $$ \begin{align*} \Vert w_2 \Vert^2 &amp;= \langle x, x \rangle = \int_{-1}^{1} x^2 dx = \frac{x^2}{3} \Big|^1_{-1} = \frac{2}{3} \\ \langle w_2, u_1 \rangle &amp;= \int_{-1}^{1} x\frac{1}{\sqrt{2}} dx = 0 \\ u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle u_1}{\Vert w_2 - \langle w_2, u_1 \rangle u_1 \Vert} = \frac{\sqrt{3}}{\sqrt{2}}x \end{align*} $$ And finally \(u_3\) $$ \begin{align*} u_3 &amp;= \frac{w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1}{\Vert w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1 \Vert} \\ &amp;= \sqrt{\frac{5}{8}}(3x^2 - 1) \end{align*} $$ Fourier coefficients The coefficients with respect to an orthonormal spanning set that we studied last time have a special name: Definition Let \(S \subseteq V\) be an (possibly infinite) orthonormal subset. The scalars \(\langle x, u\rangle\) for \(u \in S\) are called the Fourier coefficients of \(x\) with respect to \(S\). Example 1 $$ \begin{align*} V &amp;= C^0([-1,1]), \langle f,g \rangle = \int_{-1}^1 f(t)g(t)dt \\ S &amp;= \big\{\frac{1}{\sqrt{2}}\big\} \cup \{\sin n \pi t\}_{n=1}^{\infty} \cup \{\cos n \pi t \}_{n=1}^{\infty} \end{align*} $$ We can easily check that \(S\) is an orthonormal set. Each of the two vectors is orthogonal to each other and each vector is of unit length. Find the Fourier coefficients of \(f = |t| \in C^0([-1,1])\) $$ \begin{align*} \langle f, \frac{1}{\sqrt{2}} \rangle &amp;= \int_{-1}^1 \frac{1}{\sqrt{2}} |t| dt = \frac{1}{\sqrt{2}} \\ \langle f, \sin n \pi t \rangle &amp;= \int_{-1}^1 \sin n \pi t |t| dt = 0 \\ \langle f, \cos n \pi t \rangle &amp;= \int_{-1}^1 \cos n \pi t |t| dt = \begin{cases} 0 \quad \ \ \quad \text{if $n$ even } \\ \frac{-4}{(n\pi)^2} \quad \text{if $n$ odd } \end{cases} \end{align*} $$ When \(S = \{u_1,...,u_k\}\) is finite, then we can write $$ \begin{align*} x = \sum_{j=1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ But now in the infinite case, Is $$ \begin{align*} |t| = \frac{1}{2} = \sum_{n\text{ odd}} \frac{-4}{(n\pi)^2} \cos n \pi t \end{align*} $$ Yes it is true but this is an infinite sum that converges to a number that is the absolute value of \(t\). This is basically the beginning of studying Fourier Analysis where any sufficiently nice function can be written as an infinite sum of sines and cosines. But one thing we know here is that \(S\) is not a basis for \(C^0[-1,1]\). \(|t| \neq\) finite set of elements of \(S\). References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Section 4.3: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21.html" rel="alternate" type="text/html" title="Section 4.3: Exercise 21" /><published>2024-08-31T01:01:36-07:00</published><updated>2024-08-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21.html"><![CDATA[<div class="ydiv">
4.3: Exercise 21
</div>
<div class="ybdiv">
Prove that if \(M \in M_{n \times n}(\mathbf{F})\) can be written in the form
	$$
	\begin{align*}
	M = \begin{pmatrix}
	A &amp; B \\
	O &amp; C
	\end{pmatrix}
	\end{align*}
	$$
where \(A\) and \(C\) are square square matrices. Prove that \(\det(M) = \det(A)\det(C)\)
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(A\) is of size \(k \times k\) and \(C\) is of size \(m \times m\), then we know that \(k+m=n\) and \(O\) is of size \(m \times k\). We have two cases. If \(A\) is not invertible, then we know that \(\det(A) = 0\). Moreover, the columns of \(A\) are linearly dependent and the rank of \(A\) is less than \(k\). But \(M\) contains \(k+m\) columns and since the first \(k\) columns are linearly dependent, then the rank of \(M\) is less than \(k+m\). Therefore \(\det(M) = 0 = \det(A)\det(C)\) and we are done.
<br />
<br />
Suppose that \(A\) is invertible. Then define</p>
<div>
	$$
	\begin{align*}
     P &amp;= 
 	 \begin{pmatrix}
 	A &amp; O' \\
 	O &amp; I_m
 	\end{pmatrix},
	Q = 
	\begin{pmatrix}
	I_k &amp; A^{-1}B \\
	O &amp; C
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Where \(O'\) is of size \(k \times m\). Note here that \(PQ = M\). We claim that \(\det(P) = \det(A)\) and \(\det(Q)=C\). Once we show this claim is true, then we can conclude that \(\det(M) = \det(P)\det(Q) = \det(A)\det(Q)\).
<br />
<br />
To show that \(\det(P)=\det(O)\), We’ll prove this by induction on \(m\).<br />
Base Case \(m=1\): \(I_m\) is of size \(1 \times 1\). \(O\) is of size \(1 \times k\). The last row of \(P\) therefore contains a row of zeros followed by 1. Computing the determinant of \(P\) by cofactor expansion along this last row yields exactly \(\det(A)\) as required.
<br />
<br />
Inductive Case: Suppose this is true for \(m-1\). We will show that this is true for \(m\). In this case \(O\) is of size \(m \times k\) and \(I_m\) is of size \(m \times m\). The last row of this matrix is also a sequence of zeros followed by a 1 by the definition of \(O\) and \(I_m\). So compute the determinant by cofactor expansion along the last row to see that \(\det(P) = 1 \det(P')\). where \(P'\) is the matrix \(P\) but with the last column and the last row. We can now apply the inductive hypothesis to conclude that \(\det(P') = \det(A)\) and therefore \(\det(P) = 1\det(A) = \det(A)\) as we wanted to show.
<br />
<br />
We can use the same inductive proof to prove that \(\det(Q) = C\). Once we do so, we are done.</p>

<p>\(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[4.3: Exercise 21 Prove that if \(M \in M_{n \times n}(\mathbf{F})\) can be written in the form $$ \begin{align*} M = \begin{pmatrix} A &amp; B \\ O &amp; C \end{pmatrix} \end{align*} $$ where \(A\) and \(C\) are square square matrices. Prove that \(\det(M) = \det(A)\det(C)\) Proof: Suppose \(A\) is of size \(k \times k\) and \(C\) is of size \(m \times m\), then we know that \(k+m=n\) and \(O\) is of size \(m \times k\). We have two cases. If \(A\) is not invertible, then we know that \(\det(A) = 0\). Moreover, the columns of \(A\) are linearly dependent and the rank of \(A\) is less than \(k\). But \(M\) contains \(k+m\) columns and since the first \(k\) columns are linearly dependent, then the rank of \(M\) is less than \(k+m\). Therefore \(\det(M) = 0 = \det(A)\det(C)\) and we are done. Suppose that \(A\) is invertible. Then define $$ \begin{align*} P &amp;= \begin{pmatrix} A &amp; O' \\ O &amp; I_m \end{pmatrix}, Q = \begin{pmatrix} I_k &amp; A^{-1}B \\ O &amp; C \end{pmatrix} \end{align*} $$ Where \(O'\) is of size \(k \times m\). Note here that \(PQ = M\). We claim that \(\det(P) = \det(A)\) and \(\det(Q)=C\). Once we show this claim is true, then we can conclude that \(\det(M) = \det(P)\det(Q) = \det(A)\det(Q)\). To show that \(\det(P)=\det(O)\), We’ll prove this by induction on \(m\). Base Case \(m=1\): \(I_m\) is of size \(1 \times 1\). \(O\) is of size \(1 \times k\). The last row of \(P\) therefore contains a row of zeros followed by 1. Computing the determinant of \(P\) by cofactor expansion along this last row yields exactly \(\det(A)\) as required. Inductive Case: Suppose this is true for \(m-1\). We will show that this is true for \(m\). In this case \(O\) is of size \(m \times k\) and \(I_m\) is of size \(m \times m\). The last row of this matrix is also a sequence of zeros followed by a 1 by the definition of \(O\) and \(I_m\). So compute the determinant by cofactor expansion along the last row to see that \(\det(P) = 1 \det(P')\). where \(P'\) is the matrix \(P\) but with the last column and the last row. We can now apply the inductive hypothesis to conclude that \(\det(P') = \det(A)\) and therefore \(\det(P) = 1\det(A) = \det(A)\) as we wanted to show. We can use the same inductive proof to prove that \(\det(Q) = C\). Once we do so, we are done.]]></summary></entry><entry><title type="html">Section 5.3: Transition Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices.html" rel="alternate" type="text/html" title="Section 5.3: Transition Matrices" /><published>2024-08-30T01:01:36-07:00</published><updated>2024-08-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices.html"><![CDATA[<!---------------------------------------5.12--------------------------------------------->
<div class="purdiv">
Theorem 5.12
</div>
<div class="purbdiv">
Let \(A\) be a square matrix with complex entries. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists if and only if both of the following conditions hold:
<ol style="list-style-type:lower-alpha">
	<li>Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\)</li>
	<li>If 1 is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to 1 equals the multiplicity of 1 as an eigenvalue of 1. </li>
</ol>
</div>
<p><br />
<b>Proof:</b> [TODO]
<br />
<br />
<!----------------------------------------5.13--------------------------------------------></p>
<div class="purdiv">
Theorem 5.13
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) satisfy the following two conditions
<ol type="i"> 
	<li>Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\)</li>
	<li>\(A\) is diagonalizable. </li>
</ol>
Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Since \(A\) is diagonalizable, then we know that there exists an invertible matrix \(Q\) such that \(D = Q^{-1}AQ\). Suppose that</p>
<div> 
$$
\begin{align*}
D = 
\begin{pmatrix} 
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>\(\lambda_1,...,\lambda_n\) are the eigenvalues of \(A\). But condition (i) requires that for each \(i\), \(\lambda_i = 1\) or \(|\lambda_i| &lt; 1\). Thus</p>
<div>
$$
\begin{align*}
\lim\limits_{m \rightarrow \infty} {\lambda_i}^m &amp;= \begin{cases} 1 \quad \text{if } \lambda_i = 1 \\ 0 \quad \text{otherwise } \end{cases}
\end{align*}
$$
</div>
<p>so</p>
<div> 
$$
\begin{align*}
D^m = 
\begin{pmatrix} 
\lambda_1^m &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^m &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n^m
\end{pmatrix}
\end{align*}
$$
</div>
<p>and so the sequence \(D, D^2 ...\) converges to a limit \(L\). Therefore,</p>
<div> 
$$
\begin{align*}
\lim\limits_{m \rightarrow \infty} A^m = \lim\limits_{m \rightarrow \infty} (QDQ^{-1}) = QLQ^{-1}

\end{align*}
$$
</div>
<p><br />
<!----------------------------------------5.14--------------------------------------------></p>
<div class="purdiv">
Theorem 5.14
</div>
<div class="purbdiv">
Let \(M\) be an \(n \times n\) matrix having real nonnegative entries, let \(v\) be a column vector in \(\mathbf{R}^n\) having nonnegative coordinates, and let \(u \in \mathbf{R}^n\) be the column vector in which each coordinate equals 1. Then
<ol type="a"> 
	<li>\(M\) is a transition matrix if and only if \(u^tM = u^t\);</li>
	<li>\(v\) is a probability vector if and only if \(u^tv = (1)\). </li>
</ol>
</div>
<p><br />
<b>Proof (a):</b>
<br />
<br />
\(\Rightarrow\): Suppose \(M\) is a transition matrix. Then by the definition of matrix-vector multiplication,</p>
<div> 
$$
\begin{align*}
u^tM &amp;= 
\begin{pmatrix} 
1 &amp; 1 &amp; \cdots &amp; 1
\end{pmatrix}
\begin{pmatrix} 
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{m2} \\
\end{pmatrix}
=
\begin{pmatrix}
w_1 &amp; w_2 &amp; \cdots &amp; w_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>where \(w_i\) is the sum</p>
<div> 
$$
\begin{align*}
w_i &amp;= 1a_{1i} + 1a_{2i} + ... + 1a_{ni} \\
    &amp;= a_{1i} + a_{2i} + ... + a_{ni} \\
	&amp;= 1 \quad \text{(since $M$ is a transition matrix)}
\end{align*}
$$
</div>
<p><br />
\(\Leftarrow\): [TODO: same argument as \(\Rightarrow\)]
<br />
<br /><b>Proof (b):</b> [TODO: also very similar to \((a)\)]
<br />
<br />
<!-----------------------------------5.14 (Corollary)-----------------------------------------></p>
<div class="purdiv">
Theorem 5.14 (Corollary)
</div>
<div class="purbdiv">
<ol type="a"> 
	<li>The product of two \(n \times n \) transition matrices is an \(n \times n\) transition matrix. In particular, any power of a transition matrix is a transition matrix.</li>
	<li>The product of a transition matrix a probability vector is a probability vector.</li>
</ol>
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
[TODO]
<br />
<br />
<!-----------------------------------Definition-----------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A transition matrix is called regular if some power of the matrix contains only nonzero (i.e., positive) entries.
</div>
<p><br />
<br />
<!-----------------------------------Definition-----------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\). For \(1 \leq i,j \leq n\), define \(\rho_i(A)\) to be the sum of the absolute values of the entries of row \(i\) of \(A\), and define \(\nu_j(A)\) to be equal to the sum of the absolute values of the entries of column \(j\) of \(A\). Thus
$$
\begin{align*}
\rho_i(A) = \sum_{j=1}^n |A_{ij}| \quad \text{ for $i = 1,2,...,n$}
\end{align*}
$$
and
$$
\begin{align*}
\nu_j(A) = \sum_{i=1}^n |A_{ij}| \quad \text{ for $j = 1,2,...,n$}
\end{align*}
$$
The row sum of \(A\) denoted \(\rho(A)\), and the column sum of \(A\), denoted \(\nu(A)\), are defined as
$$
\begin{align*}
\rho(A) = \max\{\rho_i(A): 1 \leq i \leq n\}
\text{ and }
\nu(A) = \max\{\nu_j(A): 1 \leq j \leq n\}
\end{align*}
$$
</div>
<p><br />
<br />
TODO: definition of Gershgorin’s circle
<br />
<br />
<!-----------------------------------5.15-----------------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Gershgorin's Circle Theorem)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\). Then every eigenvalue of \(A\) is contained in a Greshgorin disk.
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 1)
</div>
<div class="purbdiv">
Let \(\lambda\) be any eigenvalue of \(A \in M_{n \times n}(\mathbf{C})\). Then \(|\lambda| \leq \rho(A)\).
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 2)
</div>
<div class="purbdiv">
Let \(\lambda\) be any eigenvalue of \(A \in M_{n \times n}(\mathbf{C})\). Then \(|\lambda| \leq 
min\{\rho(A),\nu(A)\}\).
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 3)
</div>
<div class="purbdiv">
If \(\lambda\) is an eigenvalue of a transition matrix, then \(|\lambda| \leq 1\).
</div>
<p><br />
<br />
<!--------------------------------------5.16-----------------------------------------></p>
<div class="purdiv">
Theorem 5.16
</div>
<div class="purbdiv">
Every transition matrix has 1 as an eigenvalue.
</div>
<p><br />
<br />
<!--------------------------------------5.17-----------------------------------------></p>
<div class="purdiv">
Theorem 5.17
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a matrix in which each entry is a positive real number, and let \(\lambda\) be a complex eigenvalue of \(A\) such that \(|\lambda| = \rho(A)\). Then \(\lambda = \rho(A)\) and \(\{u\}\) is a basis for \(E_{\lambda}\), where \(u \in \mathbf{C}^n\) is the column vector in which each coordinate equals 1.
</div>
<p><br />
<br />
<b>Proof</b>
[TODO: we did some version of this in class]
<br />
<br />
<!-----------------------------------5.17 (Corollary 1)--------------------------------------></p>
<div class="purdiv">
Theorem 5.17 (Corollary 1)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a matrix in which each entry is a positive, and let \(\lambda\) be an eigenvalue of \(A\) such that \(|\lambda| = \nu(A)\). Then \(\lambda = \nu(A)\) and the dimension of \(E_{\lambda}\) equals 1.
</div>
<p><br />
<br />
<!----------------------------------- 5.17 (Corollary 2) --------------------------------------></p>
<div class="purdiv">
Theorem 5.17 (Corollary 2)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a transition matrix in which each entry is positive, and let \(\lambda\) be an eigenvalue of \(A\) other than 1. Then \(|\lambda| &lt; 1\). Moreover, the eigenspace corresponding to the eigenvalue 1 has dimension 1.
</div>
<p><br />
<br />
Okay, so only if the transition matrix itself has positive entries (regular is not enough), then \(\lambda &lt; 1\) for eigenvalues that are not 1 AND the dimension of the eigenspace corresponding to eigenvalue 1 is 1.
<br />
<br />
<!-------------------------------------- 5.18 ----------------------------------------></p>
<div class="purdiv">
Theorem 5.18
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix, and let \(\lambda\) be an eigenvalue of \(A\). 
<ol type="a"> 
	<li>\(|\lambda| \leq 1\).</li>
	<li>If \(|\lambda| = 1\), then \(\lambda = 1\), and \(\dim(E_{\lambda}) = 1.\)</li>
</ol>
</div>
<p><br />
<br />
This is the other result I was looking for. If \(A\) is regular, then we’ll have the normal restriction of \(|\lambda| \leq 1\).
<br />
<br />
<!-------------------------------- 5.18 (Corollary) ---------------------------------></p>
<div class="purdiv">
Theorem 5.18 (Corollary)
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix that is diagonalizable. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists.
</div>
<p><br />
<br />
<!-------------------------------------- 5.19 ----------------------------------------></p>
<div class="purdiv">
Theorem 5.19
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix, and let \(\lambda\) be an eigenvalue of \(A\). 
<ol type="a"> 
	<!--------(a)------------>
	<li>The multiplicity of 1 as an eigenvalue of \(A\) is 1.</li>
	<!--------(b)------------>
	<li>\(\lim\limits_{m \rightarrow \infty} A^m\) exists.</li>
	<!--------(c)------------>
	<li>\(L = \lim\limits_{m \rightarrow \infty} A^m\) is a transition matrix.</li>
	<!--------(d)------------>
	<li>\(AL = LA = L\)</li>
	<!--------(e)------------>
	<li>The columns of \(L\) are identical. In fact, each column of \(L\) is equal to the unique probability vector \(v\) that is also an eigenvector of \(A\) corresponding to the eigenvalue 1.</li>
	<!--------(f)------------>
	<li>For any probability vector \(w\), \(\lim\limits_{m \rightarrow \infty} (A^m w) = v.\)</li>
</ol>
</div>
<p><br />
<br /></p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.12 Let \(A\) be a square matrix with complex entries. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists if and only if both of the following conditions hold: Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\) If 1 is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to 1 equals the multiplicity of 1 as an eigenvalue of 1. Proof: [TODO] Theorem 5.13 Let \(A \in M_{n \times n}(\mathbf{C})\) satisfy the following two conditions Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\) \(A\) is diagonalizable. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists Proof: Since \(A\) is diagonalizable, then we know that there exists an invertible matrix \(Q\) such that \(D = Q^{-1}AQ\). Suppose that $$ \begin{align*} D = \begin{pmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n \end{pmatrix} \end{align*} $$ \(\lambda_1,...,\lambda_n\) are the eigenvalues of \(A\). But condition (i) requires that for each \(i\), \(\lambda_i = 1\) or \(|\lambda_i| &lt; 1\). Thus $$ \begin{align*} \lim\limits_{m \rightarrow \infty} {\lambda_i}^m &amp;= \begin{cases} 1 \quad \text{if } \lambda_i = 1 \\ 0 \quad \text{otherwise } \end{cases} \end{align*} $$ so $$ \begin{align*} D^m = \begin{pmatrix} \lambda_1^m &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2^m &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n^m \end{pmatrix} \end{align*} $$ and so the sequence \(D, D^2 ...\) converges to a limit \(L\). Therefore, $$ \begin{align*} \lim\limits_{m \rightarrow \infty} A^m = \lim\limits_{m \rightarrow \infty} (QDQ^{-1}) = QLQ^{-1}]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5 (Corollary)</title><link href="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5 (Corollary)" /><published>2024-08-29T01:01:36-07:00</published><updated>2024-08-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html"><![CDATA[<div class="purdiv">
Theorem 5.5 Corollary
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable.
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\)
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Corollary Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable. Proof: Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5</title><link href="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5" /><published>2024-08-28T01:01:36-07:00</published><updated>2024-08-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html"><![CDATA[<div class="purdiv">
Theorem 5.5
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent.
</div>
<p><br />
Proof:
<br />
<br />
By induction on \(k\). 
<br />
<br />
Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done.
<br />
<br />
Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent.
<br />
<br /> 
Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0
	\end{align}
	$$
</div>
<p>We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that</p>
<div>
	$$
	\begin{align*}
	(T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\end{align*}
	$$
</div>
<p>But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means  that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0
	\end{align}
	$$
</div>
<p>The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). 
<br />
<br />
I really don’t like this proof!
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent. Proof: By induction on \(k\). Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done. Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent. Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0 \end{align} $$ We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that $$ \begin{align*} (T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \end{align*} $$ But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0 \end{align} $$ The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). I really don’t like this proof! References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.1: Exercise 15</title><link href="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html" rel="alternate" type="text/html" title="Section 5.1: Exercise 15" /><published>2024-08-27T01:01:36-07:00</published><updated>2024-08-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html"><![CDATA[<div class="ydiv">
Exercise 15
</div>
<div class="ybdiv">
For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues).
</div>
<p><br />
Proof:
<br />
<br />
Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det(A^t - (\lambda I_n)^t) \\
	                  &amp;= \det(A^t - \lambda I_n).
	\end{align*}
	$$
</div>
<p>From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 15 For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues). Proof: Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore, $$ \begin{align*} \det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det(A^t - (\lambda I_n)^t) \\ &amp;= \det(A^t - \lambda I_n). \end{align*} $$ From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry></feed>