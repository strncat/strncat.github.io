I"ô<p>These are notes I took while watching the series <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">The Essence of Linear Algebra by 3Blue1Brown</a>.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Introduction</h3>
<p>One of the main uses of linear algebra is that it lets us solve a linear system of equations. So if weâ€™re given for example the following set of equations,</p>
<div>
$$
\begin{align*}
6x - 3y + 2z = 7 \\
x + 2y + 5z = 0 \\
2x - 8y - z = -2
\end{align*}
$$
</div>
<p>We can group all the coefficients in a matrix \(A\), all the variables in a vector \(\vec{x}\) and all the constants in a vector \(\vec{v}\). We can then write,</p>
<div>
$$
\begin{align*}
A\vec{x} &amp;= \vec{y} \\
\begin{bmatrix}
6 &amp; -3 &amp; 2 \\
1 &amp; 2 &amp; 5 \\
2 &amp; -8 &amp; -1 
\end{bmatrix}
&amp;=
\begin{bmatrix}
x \\ y \\ z
\end{bmatrix}
=
\begin{bmatrix}
7 \\ 0 \\ -2
\end{bmatrix}
\end{align*}
$$
</div>
<p>This should look very familiar! The matrix \(A\) corresponds to some linear transformation so solving \(Ax=v\) means weâ€™re looking for a vector \(x\) which after applying the transformation \(A\), lands on the vector \(v\)!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>A 2D Example</h3>
<p>Letâ€™s start with a simple example in two dimensions such that the matrix \(A\) is a 2x2 matrix. Suppose we have two equations and two unknowns. The equations are</p>
<div>
$$
\begin{align*}
2x + 2y = 4 \\
1x + 3y = -1
\end{align*}
$$
</div>
<p>and after re-arranging everything, weâ€™ll have</p>
<div>
$$
\begin{align*}
A\vec{x} &amp;= \vec{y} \\
\begin{bmatrix}
2 &amp; 2 \\
1 &amp; 3
\end{bmatrix}
&amp;=
\begin{bmatrix}
x \\ y
\end{bmatrix}
=
\begin{bmatrix}
-4 \\ -1
\end{bmatrix}
\end{align*}
$$
</div>
<p>The solution to this system depends on whether A squishes all of the space into a lower dimension or whether it leaves everything still in two dimensions where it started. One way to determine this to use what we learned in the lastâ€™s lecture and compute the determinant of this matrix.</p>
<ul>
	 <li>If it's zero, then we know, we're squished to a lower dimension.</li>
	 <li>If it's non-zero, then we know we have a unique solution.</li>
 </ul>
<p>Weâ€™ll analyze each case separately next.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Case 1: The Determinant is not zero (The Inverse of a Transformation)</h3>
<p>In case 1, the determinant is not zero. This means the area is not zero and weâ€™re not squished to a lower dimension. Therefore there will be only one vector \(x\) such that \(Ax = v\). To find it, we really want to play the transformation in reverse going from \(v\) to \(x\) (imagine playing it slowly from \(\vec{v}\) back to \(\vec{x}\)). In other words, we want the inverse transformation applied to \(v\). The inverse of a transformation \(A\) is called the <b>inverse of \(A\)</b> and is denoted by \(A^{-1}\). \(A^{-1}\) is the unique transformation with the property that if you first apply \(A\) and then follow it with \(A^{-1}\), you end up back where you started. And we know that applying multiple transformations is captured algebraically with matrix multiplication. So \(AA^{-1}\) corresponds to the transformation that does nothing! The transformation that does nothing is called the <b>identity</b> transformation. It leaves both \(\widehat{i}\) and \(\widehat{j}\) unchanged so \(I = matrix (1,0,0,1)\).</p>
<div>
$$
\begin{align*}
I &amp;=
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\end{align*}
$$
</div>
<p>So now to find the unique solution, we can just apply the inverse to both sides of the equation</p>
<div>
$$
\begin{align*}
A\vec{x} &amp;= \vec{v} \\
A^{-1}A\vec{x} &amp;= A^{-1}\vec{v} \\
\vec{x} &amp;= A^{-1}\vec{v}
\end{align*}
$$
</div>
<p>Again itâ€™s important to stress the geometric meaning of this equation. Weâ€™re playing the transformation \(A\) in reverse from \(v\) and following it back to \(x\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Case 2: The Determinant is Zero</h3>
<p>So we know when the determinant is zero, this means that the two dimensions are squished onto a lower dimension. So there is no inverse because we canâ€™t unsquish a line to turn it into a plane. It is though still possible that a solution exist even when the determinant is zero. Itâ€™s just when the whole space is squished into a line, we have to lucky that the vector \(v\) lives somewhere on that line. (green means there is a solution and yellow indicates no solution exists)</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/inverse/00.png" width="90%" class="center" /></p>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Rank</h3>
<p>If we take a 3x3 transformation in 3 dimensions where the determinant is zero, we will notice that some of these zero determinant cases feel a lot more restrictive than others. It seems that it will be harder to find a solution if weâ€™re squished to a line versus for example getting squished to a plane. This leads us to develop new terminology to distinguish these cases. Weâ€™ll say the transformation that squishes us to a 1 dimensional line has <b>rank 1</b>. If weâ€™re squished to 2 dimensions, then the tranformation has rank 2. 
<br />
<br />
So really <b>the rank is the number of dimensions in the output of a transformation.</b> So in the 3x3 case, rank 3 is the best rank the matrix can reach and it means that the basis vectors continue to span the full three dimensions of space and the determinant is not zero!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Column Space</h3>
<p>The set of all possible outputs of the 3x3 transformation is called the <b>column space of \(A\)</b> of the transformation whether itâ€™s a line, a plane or 3D space. 
<br />
<br />
Why did we call it the column space? We know that each column in the matrix \(A\) refers to where a vector from the basis vectors have landed. For example the first column is where \(\widehat{i}\) has landed, the second is where \(\widehat{j}\) has landed and so on. The <b>span</b> of those transformed basis vectors gives you all possible outputs. In other words, the <b>column space</b> is the <b>span of the columns</b> of your matrix. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Null Space</h3>
<p>Another important thing is that the zero vector will always be included in the column space since linear transformations must keep the origin fixed in place. If the matrix is full rank, then the zero vector is the only vector that lands at the origin. But for other non full rank matrices, we can have a bunch of vectors land on zero. These vectors are called the <b>null space</b> or the <b>kernel</b> of your matrix. Itâ€™s the set of all vectors that become null (land on the zero vector). 
<br />
<br />
If weâ€™re trying to solve \(A\vec{x}=\vec{0}\), then the null space gives you all of the possible solutions to the equation.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>References</h3>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>

:ET