I"ñ-<p>Recall three types of elementary row operations</p>
<ul style="list-style-type:lower-alpha">
	<li>\(R_i \leftrightarrow R_j\)</li>
	<li>\(R_i \rightarrow \lambda R_i\) where \(\lambda \neq 0\)</li>
	<li>\(R_i \rightarrow R_i + \lambda R_j\)</li>
</ul>
<p>Now that weâ€™ve studied matrix multiplication we can state the fact that performing an elementary row operation on \(A \in M_{2 \times 2}\) can actually be described using matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Elementary Matrices</h3>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h3>Example</h3>
<p>Applying the three types of elementary row operations results in the following matrices</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix} \rightarrow E_1 &amp;= 
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix} \\
E_2 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; \lambda
\end{pmatrix} \\
E_3 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
\lambda &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>This leads to the next theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	Let \(E\) be the elementary matrix obtained from \(I_n\) by performing row operation \(\mathcal{R}(E = E(\mathcal{R})))\) for any \(A \in M_{m \times n}\), the product
	$$
	\begin{align*}
	E(\mathcal{R}) \cdot A_{m \times n}
    \end{align*}
	$$
	is equal to the matrix obtained from \(A\) by performing \(\mathcal{R}\).
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Example</h3>
<p>Letâ€™s apply the elementary matrices on the following given matrix</p>
<div>
$$
\begin{align*}
E_1  \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
\\
E_2
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} 
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ \lambda c &amp; \lambda d \end{pmatrix} 
\\
E_3
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;=\begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix}
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ c + \lambda a &amp; d + \lambda b \end{pmatrix} 
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>RREF by Matrix Multiplication</h3>
<p>Since we can perform elementary row operations by matrix multiplication, then we can possibly see how we can put a matrix in reduced row echelon by multiplication. But first, there is an observation</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	Each elementary matrix is invertible.
	$$
	\begin{align*}
	&amp;\mathcal{R}: R_i \leftrightarrow R_j \quad \quad \quad \quad \ \ \mathcal{R}^{-1}: R_j \leftrightarrow R_i \\
	&amp;\mathcal{R}: R_i \rightarrow \lambda R_i \quad \quad \quad \quad \mathcal{R}^{-1}: R_i \rightarrow \frac{1}{\lambda} R_i \\
	&amp;\mathcal{R}: R_i \rightarrow R_i + \lambda R_j \quad \quad \mathcal{R}^{-1}: R_i \rightarrow R_i - \lambda R_j
    \end{align*}
	$$
</div>
<p><br />
Proof: apply the elementary row operation by multiplying by \(E(\mathcal{R})\) and then apply the inverse again by multiplying by \(E(\mathcal{R}^{-1})\). The result is the identity matrix. In other words, \(E(\mathcal{R})E(\mathcal{R}^{-1}) = I_n\). 
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For every \(A \in M_{m \times n}\), there is a finite set of elementary matrices \(E_1,...,E_k \in M_{m \times n}\) such that \(E_k ... E_2E_1\) is in RREF.  
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	\(A \in M_{m \times n}\) is invertible if and only iff there is a finite set of elementary matrices \(E_1,...,E_k \in M_{n \times n}\) such that \(E_k...E_2E_1A = I_n\).  \((A \ | \ I_n)\).
</div>
<p><br />
Note here that from the expression above we can see that \(A^{-1} = E_k...E_1\) and \(A = E_1^{-1}E_2^{-1}...E^{-1}_{k-1}E^{-1}_{k}\)
<br />
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	\(A\) is invertible if and only iff it can be written as a product of elementary matrices.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>The Rank of a Matrix</h3>
<p>Recall that the rank of \(T: V \rightarrow W\) is \(\dim(R(T))\).</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	The rank of \(A \in M_{n \times n}\), \(rank(A)\) is the rank of \(L_A: \mathbf{R}^m \rightarrow \mathbf{R}^n\).
</div>
<p><br />
This definition is kind of awkward and instead we want to find an expression for rank(\(A\)) in terms of \(A\) itself and not have to rely on \(L_A\). To figure this out, we need the following result</p>
<div class="purdiv">
Proposition
</div>
<div class="purbdiv">
If \(B \in M_{n \times n}\) is invertible, then rank(\(BA\)) = rank(\(A\)).
</div>
<p><br />
So multiplication by \(B\) doesnâ€™t change the rank.
<br />
<br />
Proof: <br />
By definition \(rank(BA)\) is the rank of the linear map \(L_{BA}\). But the rank of a linear map is the dimension of its range and so</p>
<div>
$$
\begin{align*}
rank(BA) &amp;= rank(L_{BA}) \\
         &amp;= \dim(R(L_{BA})).
\end{align*}
$$
</div>
<p>By definition, to get the range, we just apply the linear map on \(v \in \mathbf{R}^n\). This range is a subset of \(\mathbf{R}^m\). Applying the linear map is just multiplying \(v\) by the matrices \(A\) and \(B\). So we can see this below:</p>
<div>
$$
\begin{align*}
\dim(R(L_{BA})) &amp;= \{L_{BA}(v) \ | \ v \in \mathbf{R}^n \} \subset \mathbf{R}^m \\
                &amp;= \{BA(v) \ | \ v \in \mathbf{R}^n\} \\
				&amp;= \{B(A(v)) \ | \ v \in \mathbf{R}^n\} \\
				&amp;= \{B(A(v)) \ | \ v \in \mathbf{R}^n\}
\end{align*}
$$
</div>
<p>This is equivalent to applying the linear map \(L_{B}\) on the set that we get from applying \(A\) as follows</p>
<div>
$$
\begin{align*}
\dim(R(L_{BA})) &amp;= L_B(\{A(v) \ | \ v \in \mathbf{R}^n\}) \\
\end{align*}
$$
</div>
<p>But this internal set is the range of the linear map \(L_A\), \(R(L_A)\). So what we want to show is that applying \(L_B\) doesnâ€™t change the dinemsion of \((R(L_A))\) or multiplying by the matrix \(B\) above, doesnâ€™t change anything about the dimension of the internal set.
<br />
<br /> 
The idea is simple but subtle. We know that \(L_B\) is a map from \(\mathbf{R}^m\) to \(\mathbf{R}^m\). But here, \(L_B\) is not acting on \(\mathbf{R}^m\) but rather \(R(L_A)\) (the internal set)\(. The range of\)L_A\(is a subset of\)\mathbf{R}^m$$. So define the map</p>
<div>
$$
\begin{align*}
L_B&amp;: \mathbf{R}^m \rightarrow \mathbf{R}^m \\
\tilde{L_B}&amp;: R(L_A) \rightarrow L_B(R(L_A))
\end{align*}
$$
</div>
<p>We claim this new map is invertible. Itâ€™s onto because we restricted the target to the image \(R(L_A)\) and itâ€™s one-to-one because \(B\) is invertible. Therefore, the dimension of the domain and the target are the same (Itâ€™s an invertible map!). In other words,</p>
<div>
$$
\begin{align*}
\dim(R(L_A)) = \dim(L_B(R(L_A)))
\end{align*}
$$
</div>
<p>But we know that \(\dim(R(L_A))\) is the rank of \(A\). and \(\dim(L_B(R(L_A))\) is the rank of \(L_{BA}\) so \(rank(A) = rank(BA) \ \blacksquare\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>The Rank of a Matrix</h3>
<p>So now we can go back to our original goal of finding an expression for finding the rank of a matrix \(A\) without going back to the rank of the linear map \(L_A\). We just proved that \(\text{rank}(BA) = \text{rank}(A)\). for any invertible matrix \(B\). We further studied earlier that elementary matrices are invertible. So multiplying \(A\) by a set of elementary matrices will not change its rank. So we can get \(A\) in RREF without its rank changing. Based on this we have the following corollaries:</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	Elementary row operations don't change rank.
</div>
<p><br />
and</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	$$
	\begin{align*}
	rank(A) = rank(RREF(A))
    \end{align*}
	$$
</div>
<p><br />
Why do we want RREF? because itâ€™s easy to read off and we can easily figure out the dimension easily from seeing a matrix in its RREF.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Example</h3>
<p>What is the range of the following matrix?</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>We know the rank of \(A\). It is the dimension of the range of \(L_A\) or \(\dim(R(L_A))\). Notice for the above matrix, if we multiply \(A\) by a vector \(v\), the result is a linear combination of the columns of \(A\). Recall</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\begin{pmatrix} x \\ y \\ z \\ w \end{pmatrix}
=
x
\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}
+
y
\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
+
z
\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}
+
w
\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>In other words, the columns span the range. Here, we see that we have 3 non-zero columns and they are linearly independent. so \(rank(A) = \dim(R(L_A)) = 3\). This works here but itâ€™s not generally true!! weâ€™re missing something â€¦ so letâ€™s clarify with another example
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Example</h3>
<p>What is the range of the following matrix?</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>Itâ€™s still true that the range of this is still spanned by the columns (always true). But the rank of \(A\) in general is the number of columns with leading entries in \(REF\) of \(A\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h3>Nullity and The Dimension Theorem</h3>
<p>So we know that \(\text{nullity}(A) = \dim(N(A))\). We found a basis for the null space of \(A\) by solving \(Ax = 0\) and finding all the solutions and then writing a set that spans that solution set. From the basis we knew the dimension of the null space. Specifically when we solved \(Ax = 0\), it was the number of columns without leading entries. So we can write \(\text{nullity}(A) = \dim(N(A)) =\) # of columns without leading entries.
<br />
<br />
So now if we put together the number of columns without leading entries (nullity of \(A\)) and the number of columns with leading entries (rank of \(A\)), then we get \(n\). This is basically the dimension theorem.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h3>References</h3>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>

:ET