<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-19T09:45:12-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Section 5.2: Theorem 5.5 (Corollary)</title><link href="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5 (Corollary)" /><published>2024-08-29T01:01:36-07:00</published><updated>2024-08-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html"><![CDATA[<div class="purdiv">
Theorem 5.5 Corollary
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable.
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\)
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Corollary Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable. Proof: Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5</title><link href="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5" /><published>2024-08-28T01:01:36-07:00</published><updated>2024-08-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html"><![CDATA[<div class="purdiv">
Theorem 5.5
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent.
</div>
<p><br />
Proof:
<br />
<br />
By induction on \(k\). 
<br />
<br />
Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done.
<br />
<br />
Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent.
<br />
<br /> 
Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0
	\end{align}
	$$
</div>
<p>We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that</p>
<div>
	$$
	\begin{align*}
	(T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\end{align*}
	$$
</div>
<p>But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means  that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0
	\end{align}
	$$
</div>
<p>The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). 
<br />
<br />
I really don’t like this proof!
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent. Proof: By induction on \(k\). Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done. Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent. Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0 \end{align} $$ We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that $$ \begin{align*} (T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \end{align*} $$ But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0 \end{align} $$ The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). I really don’t like this proof! References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.1: Exercise 15</title><link href="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html" rel="alternate" type="text/html" title="Section 5.1: Exercise 15" /><published>2024-08-27T01:01:36-07:00</published><updated>2024-08-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html"><![CDATA[<div class="ydiv">
Exercise 15
</div>
<div class="ybdiv">
For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues).
</div>
<p><br />
Proof:
<br />
<br />
Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det(A^t - (\lambda I_n)^t) \\
	                  &amp;= \det(A^t - \lambda I_n).
	\end{align*}
	$$
</div>
<p>From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.geneseo.edu/~heap/courses/333/exam2_F2007_practice_sol.pdf">Practice Midterm</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 15 For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues). Proof: Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore, $$ \begin{align*} \det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det(A^t - (\lambda I_n)^t) \\ &amp;= \det(A^t - \lambda I_n). \end{align*} $$ From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\) References: Practice Midterm]]></summary></entry><entry><title type="html">Section 4.3: Theorem 4.8</title><link href="http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8.html" rel="alternate" type="text/html" title="Section 4.3: Theorem 4.8" /><published>2024-08-26T01:01:36-07:00</published><updated>2024-08-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8.html"><![CDATA[<div class="purdiv">
Theorem 4.8
</div>
<div class="purbdiv">
For any \(A \in M_{n \times n}(\mathbf{F})\), \(\det(A^t) = \det(A)\).
</div>
<p><br />
Proof:
<br />
<br />
If \(A\) is not invertible, then \(\text{rank}(A) &lt; n\). We know that \(\text{rank}(A) = \text{rank}(A^t)\) by theorem 3.6 (corollary 2). Therefore, \(A^t\) is not invertible and \(\det(A) = \det(A^t) = 0\).
<br />
<br />
Suppose now that \(A\) is invertible, then we can write \(A\) as a product of elementary matrices, \(A = E_k,E_{k-1}...,E_1\). Furthermore, we’ve proved that \(\det(E) = \det(E^t)\) for any elementary matrix. From this, notice that</p>

<div>
	$$
	\begin{align*}
	\det(A^t) &amp;= \det((E_k,E_{k-1}...,E_1)^t) \\
	          &amp;= \det(E_1^tE_2^t...E_k^t) \\
	          &amp;= \det(E_1^t)\det(E_2^t)...\det(E_k^t) \\
	          &amp;= \det(E_1)\det(E_2)...\det(E_k) \\
	          &amp;= \det(E_k)\det(E_{k-1})...\det(E_1) \\
	          &amp;= \det(E_k,E_{k-1}...,E_1) \\
			  &amp;= \det(A). \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 4.8 For any \(A \in M_{n \times n}(\mathbf{F})\), \(\det(A^t) = \det(A)\). Proof: If \(A\) is not invertible, then \(\text{rank}(A) &lt; n\). We know that \(\text{rank}(A) = \text{rank}(A^t)\) by theorem 3.6 (corollary 2). Therefore, \(A^t\) is not invertible and \(\det(A) = \det(A^t) = 0\). Suppose now that \(A\) is invertible, then we can write \(A\) as a product of elementary matrices, \(A = E_k,E_{k-1}...,E_1\). Furthermore, we’ve proved that \(\det(E) = \det(E^t)\) for any elementary matrix. From this, notice that]]></summary></entry><entry><title type="html">Lecture 29: Inner Product Spaces</title><link href="http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces.html" rel="alternate" type="text/html" title="Lecture 29: Inner Product Spaces" /><published>2024-08-25T01:01:36-07:00</published><updated>2024-08-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces.html"><![CDATA[<p>Let \(V\) be. vector space over \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\).</p>
<div> 
$$
\begin{align*}
\mathbf{C} = \{ z=a+ib \ | \ a, b \in \mathbf{R} \}
\end{align*}
$$
</div>
<p>The complex conjugate of \(z\) is \(\bar{z} = a - ib\).</p>
<div> 
$$
\begin{align*}
z\bar{z} = (a + ib)(a - ib) = a^2 + b^2 = |z|^2
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inner product \(\langle \ , \ \rangle\) on a vector \(V\) is a map
$$
\begin{align*}
\langle \ , \ \rangle : \ &amp;V \times V \rightarrow \mathbf{F} = \mathbf{R} \text{ or } \mathbf{C} \\
&amp;(x, y) \rightarrow \langle x , y \rangle
\end{align*}
$$
such that
<ol type="i">
	<li>\(\langle x+z, y \rangle = \langle x , y \rangle + \langle z , y \rangle\)</li>
	<li>\(\langle cx, y \rangle = c \langle x , y \rangle\)</li>
	<li>\(\langle x, y \rangle = \overline{\langle y, x \rangle}\). Note if \(\mathbf{F} = \mathbf{R}\), then \(\langle x, y \rangle = \langle y, x \rangle\) </li>
	<li>\(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\)</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>The simplest example is \(V = \mathbf{R}^1\) (vector space over \(\mathbf{R}\)) where</p>
<div> 
$$
\begin{align*}
\langle x, y \rangle &amp;= xy
\end{align*}
$$
</div>
<p>This map satisfies the inner product properties. Notice that</p>
<ol type="i">
	<li>\(\langle x+z, y \rangle = (x+z)y = xy + zx = \langle x , y \rangle + \langle z , y \rangle \)</li>
	<li>\(\langle cx, y \rangle = cxy = c\langle x, y \rangle\)</li>
	<li>\(\langle x, y \rangle = \overline{\langle y, x \rangle} = yx = xy\). </li>
	<li>\(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\)</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>We can also define this inner product</p>
<div> 
$$
\begin{align*}
\langle \langle x, y \rangle \rangle_c &amp;= c^2xy \quad c \neq 0
\end{align*}
$$
</div>
<p>which also satisfies the inner product properties (TODO)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3: Dot Product</b></h4>
<p>Another example is \(V = \mathbf{R}^n\) over \(\mathbf{R}\) where</p>
<div> 
$$
\begin{align*}
\langle x, y \rangle &amp;= x_1y_1 + ... + x_ny_n = \sum_j x_jy_j
\end{align*}
$$
</div>
<p>which is commonly known as the dot product.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Another example is \(V = \mathbf{R}^2\) over \(\mathbf{R}\) where</p>
<div> 
$$
\begin{align*}
\langle \langle x, y \rangle \rangle &amp;= 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2
\end{align*}
$$
</div>
<p>We claim that \(\langle \langle \ , \ \rangle \rangle\) is an inner product. To see this check each property in the definition. For example for property 4, we see that</p>
<div> 
$$
\begin{align*}
\langle \langle x, x \rangle \rangle &amp;= 2x_1x_1 + x_1x_2 + x_2x_1 + x_2x_2 \\
                                     &amp;= 2x_1^2 + 2x_1x_2 + x_2^2 \\
									 &amp;= x_1^2 + (x_1 + x_2)^2 &gt; 0
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>Define \(V = \mathbf{C}\) over \(\mathbf{C}\) where</p>
<div> 
$$
\begin{align*}
\langle z_1, z_2 \rangle &amp;= z_1\bar{z_2}
\end{align*}
$$
</div>
<p>Note here that if we defined the product as \(\langle z_1, z_2 \rangle = z_1z_2\). This will fail to satisfy the inner product conditions.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6: Frobenius Inner Product</b></h4>
<p>Define \(V = \mathbf{C}^0([0,1]) = \{f: [0,1] \rightarrow \mathbf{R} \ | \ f \text{ continuous}\}\)</p>
<div> 
$$
\begin{align*}
\langle f, g \rangle &amp;= \int_0^1 f(t)g(t)dt
\end{align*}
$$
</div>
<p>is an inner product. Checking property (4)</p>
<div> 
$$
\begin{align*}
\langle f, g \rangle &amp;= \int_0^1 f(t)f(t) dt \\
                    &amp;= \int_0^1 f^2(t) dt \\
					&amp;&gt; 0 \text{ unless $f(t)=0 \ \forall t \in [0,1]$ }				
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 7</b></h4>
<p>Define \(V = M_{n \times n}(\mathbf{R})\)</p>
<div> 
$$
\begin{align*}
\langle A, B \rangle &amp;= tr(B^tA)
\end{align*}
$$
</div>
<p>Recall \(tr(C) = C_{11} + C_{22} + ... + C_{nn} = \sum_j C_{jj}\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(V\) be. vector space over \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\). $$ \begin{align*} \mathbf{C} = \{ z=a+ib \ | \ a, b \in \mathbf{R} \} \end{align*} $$ The complex conjugate of \(z\) is \(\bar{z} = a - ib\). $$ \begin{align*} z\bar{z} = (a + ib)(a - ib) = a^2 + b^2 = |z|^2 \end{align*} $$ Definition An inner product \(\langle \ , \ \rangle\) on a vector \(V\) is a map $$ \begin{align*} \langle \ , \ \rangle : \ &amp;V \times V \rightarrow \mathbf{F} = \mathbf{R} \text{ or } \mathbf{C} \\ &amp;(x, y) \rightarrow \langle x , y \rangle \end{align*} $$ such that \(\langle x+z, y \rangle = \langle x , y \rangle + \langle z , y \rangle\) \(\langle cx, y \rangle = c \langle x , y \rangle\) \(\langle x, y \rangle = \overline{\langle y, x \rangle}\). Note if \(\mathbf{F} = \mathbf{R}\), then \(\langle x, y \rangle = \langle y, x \rangle\) \(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\) Example 1 The simplest example is \(V = \mathbf{R}^1\) (vector space over \(\mathbf{R}\)) where $$ \begin{align*} \langle x, y \rangle &amp;= xy \end{align*} $$ This map satisfies the inner product properties. Notice that \(\langle x+z, y \rangle = (x+z)y = xy + zx = \langle x , y \rangle + \langle z , y \rangle \) \(\langle cx, y \rangle = cxy = c\langle x, y \rangle\) \(\langle x, y \rangle = \overline{\langle y, x \rangle} = yx = xy\). \(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\) Example 2 We can also define this inner product $$ \begin{align*} \langle \langle x, y \rangle \rangle_c &amp;= c^2xy \quad c \neq 0 \end{align*} $$ which also satisfies the inner product properties (TODO) Example 3: Dot Product Another example is \(V = \mathbf{R}^n\) over \(\mathbf{R}\) where $$ \begin{align*} \langle x, y \rangle &amp;= x_1y_1 + ... + x_ny_n = \sum_j x_jy_j \end{align*} $$ which is commonly known as the dot product. Example 4 Another example is \(V = \mathbf{R}^2\) over \(\mathbf{R}\) where $$ \begin{align*} \langle \langle x, y \rangle \rangle &amp;= 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2 \end{align*} $$ We claim that \(\langle \langle \ , \ \rangle \rangle\) is an inner product. To see this check each property in the definition. For example for property 4, we see that $$ \begin{align*} \langle \langle x, x \rangle \rangle &amp;= 2x_1x_1 + x_1x_2 + x_2x_1 + x_2x_2 \\ &amp;= 2x_1^2 + 2x_1x_2 + x_2^2 \\ &amp;= x_1^2 + (x_1 + x_2)^2 &gt; 0 \end{align*} $$ Example 5 Define \(V = \mathbf{C}\) over \(\mathbf{C}\) where $$ \begin{align*} \langle z_1, z_2 \rangle &amp;= z_1\bar{z_2} \end{align*} $$ Note here that if we defined the product as \(\langle z_1, z_2 \rangle = z_1z_2\). This will fail to satisfy the inner product conditions. Example 6: Frobenius Inner Product Define \(V = \mathbf{C}^0([0,1]) = \{f: [0,1] \rightarrow \mathbf{R} \ | \ f \text{ continuous}\}\) $$ \begin{align*} \langle f, g \rangle &amp;= \int_0^1 f(t)g(t)dt \end{align*} $$ is an inner product. Checking property (4) $$ \begin{align*} \langle f, g \rangle &amp;= \int_0^1 f(t)f(t) dt \\ &amp;= \int_0^1 f^2(t) dt \\ &amp;&gt; 0 \text{ unless $f(t)=0 \ \forall t \in [0,1]$ } \end{align*} $$ Example 7 Define \(V = M_{n \times n}(\mathbf{R})\) $$ \begin{align*} \langle A, B \rangle &amp;= tr(B^tA) \end{align*} $$ Recall \(tr(C) = C_{11} + C_{22} + ... + C_{nn} = \sum_j C_{jj}\) References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 28: Invariant and T-cyclic Subspaces</title><link href="http://localhost:4000/jekyll/update/2024/08/24/lec28-invariant-subspaces.html" rel="alternate" type="text/html" title="Lecture 28: Invariant and T-cyclic Subspaces" /><published>2024-08-24T01:01:36-07:00</published><updated>2024-08-24T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/24/lec28-invariant-subspaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/24/lec28-invariant-subspaces.html"><![CDATA[<p>Consider a linear map \(T: V \rightarrow V\). Suppose \(v\) is an eigenvector of \(T\). We know by definition that \(Tv = \lambda v\) and \(v \neq 0\). We also know that \(span\{v\} \subset V\) is subspace. The observation here is that since \(v\) is an eigenvector, then when \(T\) acts on the subspace, then it stays inside the subspace meaning</p>
<div> 
$$
\begin{align*}
T(span\{v\}) \subset span\{v\}
\end{align*}
$$
</div>
<p>This is because \(T(cv) = cT(v) = (c\lambda) v\) which is in the span of \(v\). The span of an eigenvector is the simplest example an invariant subspace.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A subspace \(W \in V\) is \(T\)-invariant if \(T(W) \subseteq W\).
</div>
<p><br />
i.e. \(T(w) \in W \ \forall w \in W\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>Example 1: \(W = span\{v\}\) where \(v\) is an eigenvector of \(T\).
<br />
<br />
Example 2: \(I_V:V \rightarrow V\) Every subspace is \(I_V\)-invariant.
<br />
<br />
Example 3: \(0_V:V \rightarrow V\) Every subspace is \(0_V\)-invariant.
<br />
<br />
Example 4:</p>
<div> 
$$
\begin{align*}
T : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
&amp;(x, y) \rightarrow (x,0)
\end{align*}
$$
</div>
<p>\(W = span\{(1,0)\}\) is \(T\)-invariant
<br />
\(W = span\{(1,1)\}\) is not \(T\)-invariant
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Characteristic Polynomial of Invariant Subspaces</b></h4>
<p>So what is the point of invariant subspaces? It helps us break off pieces of our map. What does that mean? If \(T: V \rightarrow V\) and \(W\) is \(T\)-invariant, then the restriction of \(T\) to \(W\), \(T_W\) satisfies \(T_W: W \rightarrow W\). So now have a smaller set instead of the entire vector space \(V\). The following theorem describes the relationship between the characteristic polynomial of \(T:V \rightarrow V\) and the characteristic polynomial of \(T: W \rightarrow W\). 
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\). If \(W\) is \(T\)-invariant, then the characteristic polynomial of \(T_W\) divides that of \(T\).
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
<br />
Let \(\beta_w = \{v_1,...,v_k\}\) be a basis of \(W\). Extend this to a basis for \(V\). so</p>
<div> 
$$
\begin{align*}
\beta = \{v_1,...,v_k,v_{k+1},...,v_n\}
\end{align*}
$$
</div>
<p>We can express \(T\) with respect \(\beta\) to get</p>
<div> 
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= \begin{pmatrix} [T(v_1)]_{\beta_1} &amp; \cdots &amp; [T(v_n)]_{\beta_1} \end{pmatrix}
\end{align*}
$$
</div>
<p>We know that \(T(v_i) \in W\) for \(i = 1,...,k\) since \(W\) is \(T\)-invariant. Therefore, we can express \(T(v_i)\) as a linear combination of just the first \(k\) vectors in \(\beta\). The rest of the coefficients will be zero for the remaining vectors in \(\beta\). So you can imagine below that for the first \(k\) columns of \([T]_{\beta}^{\beta}\), we’ll have zero entries for anything below the \(k\)th entry. Let \(B_1\) be the coefficients that we know will not be zero (at least some).</p>
<div> 
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= 
\begin{pmatrix} 
B_1 &amp; | &amp; B_2 \\
---- &amp; - &amp; ---- \\
0 &amp; | &amp; B_3 \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>We claim that \(B_1 = [T]_{\beta_W}^{\beta_W}\). Now, we want to determine the characteristic polynomial of \([T]_{\beta}^{\beta}\).</p>
<div> 
$$
\begin{align*}
\det([T]_{\beta}^{\beta} - tI_n) &amp;= 
\begin{pmatrix} 
[T]_{\beta_W}^{\beta_W} - tI_k &amp; | &amp; B_2 \\
---- &amp; - &amp; ---- \\
0 &amp; | &amp; B_3 - tI_{n-k} \\
\end{pmatrix} \\
&amp;= \det([T]_{\beta_W}^{\beta_W} - tI_k)g(t)
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>T-cyclic Subspaces</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
For \(v \in V\) the \(T\)-cyclic subspace generated by \(v\) is \(W=span\{v, T(v), T^2(v),...\} \subset V\).
</div>
<p><br />
<br />
Observe here that \(W\) is \(T\)-invariant. Why? take any element \(w \in W\), \(T(w)\) is still in \(W\). To see this, notice that</p>
<div> 
$$
\begin{align*}
T(a_0v + a_1T(v) + ... + a_kT^k(v)) = a_0T(v) + a_1T^2(v)+...+a_kT^{k+1}(v) \in W
\end{align*}
$$
</div>
<p>Question: Are all \(T\)-invariant subspaces \(T\)-cyclic?
<br />
<br />
The answer is no. Suppose</p>
<div> 
$$
\begin{align*}
T \ &amp;: \mathbf{R}^3 \rightarrow \mathbf{R}^3  \\
 &amp;(x,y,z) \rightarrow (x,y,0)
\end{align*}
$$
</div>
<p>\(W = \{(x,y,0) \ | \ x, y \in \mathbf{R}\}\) is \(T\)-invariant. We just map it to itself. In fact \(T_W = I_W\).
<br />
<br />
So we see here that \(W\) is not \(T\)-cyclic. Take \((x, y, 0)\),</p>
<div> 
$$
\begin{align*}
span\{(x,y,0), T(x,y,0),...\} = span\{(x,y,0)\}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Theorem
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\).
Let \(W\) be a \(T\)-cyclic subspace generated by \(v\). Set \(\dim W = k \leq \dim V\). Then
<ul style="list-style-type:lower-alpha">
	<li>\(\{v,T(v),...,T^{k-1}(v)\}\) is a basis for \(W\)</li>
	<li>If \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\), then the characteristic polynomial of \(T_W\) \((-1)^{k+1}(a_0 + a_1t + ... + a_{k-1}t^{k-1} - t^k)\)</li>
</ul>
</div>
<p><br />
For (a). Since the dimension is not infinite anymore. Then it’s natural to ask if only \(k\) of the infinitely generated vectors is a span for \(W\) and the answer is yes.
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof:</b>
<br />
<br />
We’ll start with a proof of (b) given (a). To say anything about the characteristic polynomial, we need to find a basis and then compute the matrix with respect to the basis. A natural choice is the basis given to us in \(a\) so</p>
<div> 
$$
\begin{align*}
\beta_W = \{v,T(v),...,T^{k-1}(v)\}
\end{align*}
$$
</div>
<p>Next we need to compute \([T_W]_{\beta_W}^{\beta_W}\)</p>
<div> 
$$
\begin{align*}
[T_W]_{\beta_W}^{\beta_W} &amp;=
\begin{pmatrix} 
[T_W(v)]_{\beta_W} &amp; \cdots &amp; [T_W(T^{k-1}(v))]_{\beta_W}
\end{pmatrix}\\
&amp;= 
\begin{pmatrix} 
0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\
1 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1} \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>The first column is the coefficients of \(T_W(v)\) with respect to \(\beta_W\) so that’s just 1 for \(T(v)\) while the rest are 0. Similarly, we have the same thing for the rest of the \(k-1\) column vectors. But for the last column, we need to represent \(T_W(T^{k-1}(v)) = T^k(v)\). We’re given \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\) and so the rest of the coefficients are \(a_0,a_1 .... a_{k-1}\). 
<br />
<br />
Now, we can compute the determinant expanding across the first row</p>
<div> 
$$
\begin{align*}
&amp;\det([T_W]_{\beta_W}^{\beta_W} -tI_k)
= 
\begin{pmatrix} 
-t &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\
1 &amp; -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\
\end{pmatrix} \\
&amp;=
(-1)^{1+1}(-t)\det 
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
 \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\
\end{pmatrix}
+(-1)^{1+k}a_0 (1)
\end{align*}
$$
</div>
<p>The last determinant for the \(a_0\) component is 1 because that sub matrix is upper triangular so the determinant is the product of the entries on the diagonal which are all 1.
<br />
<br />
Next, we want to compute that new determinant but notice now that it has the same pattern so</p>
<div> 
$$
\begin{align*}
&amp;=
-t
\left(
\det 
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
 \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\
\end{pmatrix}

\right)
+(-1)^{1+k}a_0 (1)
\\
&amp;=
(-t)
\left(
(-t)
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_2 \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_3 \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; \cdots &amp; \ddots &amp; -t &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\
\end{pmatrix}
+ (-1)^k a_1
\right)
+(-1)^{1+k}a_0 (1)
\\
&amp;=
(-1)^2
t^2
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_2 \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_3 \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; \cdots &amp; \ddots &amp; -t &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\
\end{pmatrix}
+ (-1)^{k+1} (ta_1 + a_0)
\\
&amp;= (-1)^{k+1}(a_0 + ta_1 + ... + t^{k-1}a_{k-1} - t^k)
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Consider a linear map \(T: V \rightarrow V\). Suppose \(v\) is an eigenvector of \(T\). We know by definition that \(Tv = \lambda v\) and \(v \neq 0\). We also know that \(span\{v\} \subset V\) is subspace. The observation here is that since \(v\) is an eigenvector, then when \(T\) acts on the subspace, then it stays inside the subspace meaning $$ \begin{align*} T(span\{v\}) \subset span\{v\} \end{align*} $$ This is because \(T(cv) = cT(v) = (c\lambda) v\) which is in the span of \(v\). The span of an eigenvector is the simplest example an invariant subspace. Definition A subspace \(W \in V\) is \(T\)-invariant if \(T(W) \subseteq W\). i.e. \(T(w) \in W \ \forall w \in W\) Examples Example 1: \(W = span\{v\}\) where \(v\) is an eigenvector of \(T\). Example 2: \(I_V:V \rightarrow V\) Every subspace is \(I_V\)-invariant. Example 3: \(0_V:V \rightarrow V\) Every subspace is \(0_V\)-invariant. Example 4: $$ \begin{align*} T : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (x,0) \end{align*} $$ \(W = span\{(1,0)\}\) is \(T\)-invariant \(W = span\{(1,1)\}\) is not \(T\)-invariant The Characteristic Polynomial of Invariant Subspaces So what is the point of invariant subspaces? It helps us break off pieces of our map. What does that mean? If \(T: V \rightarrow V\) and \(W\) is \(T\)-invariant, then the restriction of \(T\) to \(W\), \(T_W\) satisfies \(T_W: W \rightarrow W\). So now have a smaller set instead of the entire vector space \(V\). The following theorem describes the relationship between the characteristic polynomial of \(T:V \rightarrow V\) and the characteristic polynomial of \(T: W \rightarrow W\). Theorem Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\). If \(W\) is \(T\)-invariant, then the characteristic polynomial of \(T_W\) divides that of \(T\). Proof Let \(\beta_w = \{v_1,...,v_k\}\) be a basis of \(W\). Extend this to a basis for \(V\). so $$ \begin{align*} \beta = \{v_1,...,v_k,v_{k+1},...,v_n\} \end{align*} $$ We can express \(T\) with respect \(\beta\) to get $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= \begin{pmatrix} [T(v_1)]_{\beta_1} &amp; \cdots &amp; [T(v_n)]_{\beta_1} \end{pmatrix} \end{align*} $$ We know that \(T(v_i) \in W\) for \(i = 1,...,k\) since \(W\) is \(T\)-invariant. Therefore, we can express \(T(v_i)\) as a linear combination of just the first \(k\) vectors in \(\beta\). The rest of the coefficients will be zero for the remaining vectors in \(\beta\). So you can imagine below that for the first \(k\) columns of \([T]_{\beta}^{\beta}\), we’ll have zero entries for anything below the \(k\)th entry. Let \(B_1\) be the coefficients that we know will not be zero (at least some). $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= \begin{pmatrix} B_1 &amp; | &amp; B_2 \\ ---- &amp; - &amp; ---- \\ 0 &amp; | &amp; B_3 \\ \end{pmatrix} \end{align*} $$ We claim that \(B_1 = [T]_{\beta_W}^{\beta_W}\). Now, we want to determine the characteristic polynomial of \([T]_{\beta}^{\beta}\). $$ \begin{align*} \det([T]_{\beta}^{\beta} - tI_n) &amp;= \begin{pmatrix} [T]_{\beta_W}^{\beta_W} - tI_k &amp; | &amp; B_2 \\ ---- &amp; - &amp; ---- \\ 0 &amp; | &amp; B_3 - tI_{n-k} \\ \end{pmatrix} \\ &amp;= \det([T]_{\beta_W}^{\beta_W} - tI_k)g(t) \end{align*} $$ T-cyclic Subspaces Definition For \(v \in V\) the \(T\)-cyclic subspace generated by \(v\) is \(W=span\{v, T(v), T^2(v),...\} \subset V\). Observe here that \(W\) is \(T\)-invariant. Why? take any element \(w \in W\), \(T(w)\) is still in \(W\). To see this, notice that $$ \begin{align*} T(a_0v + a_1T(v) + ... + a_kT^k(v)) = a_0T(v) + a_1T^2(v)+...+a_kT^{k+1}(v) \in W \end{align*} $$ Question: Are all \(T\)-invariant subspaces \(T\)-cyclic? The answer is no. Suppose $$ \begin{align*} T \ &amp;: \mathbf{R}^3 \rightarrow \mathbf{R}^3 \\ &amp;(x,y,z) \rightarrow (x,y,0) \end{align*} $$ \(W = \{(x,y,0) \ | \ x, y \in \mathbf{R}\}\) is \(T\)-invariant. We just map it to itself. In fact \(T_W = I_W\). So we see here that \(W\) is not \(T\)-cyclic. Take \((x, y, 0)\), $$ \begin{align*} span\{(x,y,0), T(x,y,0),...\} = span\{(x,y,0)\} \end{align*} $$ Theorem Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\). Let \(W\) be a \(T\)-cyclic subspace generated by \(v\). Set \(\dim W = k \leq \dim V\). Then \(\{v,T(v),...,T^{k-1}(v)\}\) is a basis for \(W\) If \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\), then the characteristic polynomial of \(T_W\) \((-1)^{k+1}(a_0 + a_1t + ... + a_{k-1}t^{k-1} - t^k)\) For (a). Since the dimension is not infinite anymore. Then it’s natural to ask if only \(k\) of the infinitely generated vectors is a span for \(W\) and the answer is yes. Proof: We’ll start with a proof of (b) given (a). To say anything about the characteristic polynomial, we need to find a basis and then compute the matrix with respect to the basis. A natural choice is the basis given to us in \(a\) so $$ \begin{align*} \beta_W = \{v,T(v),...,T^{k-1}(v)\} \end{align*} $$ Next we need to compute \([T_W]_{\beta_W}^{\beta_W}\) $$ \begin{align*} [T_W]_{\beta_W}^{\beta_W} &amp;= \begin{pmatrix} [T_W(v)]_{\beta_W} &amp; \cdots &amp; [T_W(T^{k-1}(v))]_{\beta_W} \end{pmatrix}\\ &amp;= \begin{pmatrix} 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\ 1 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1} \\ \end{pmatrix} \end{align*} $$ The first column is the coefficients of \(T_W(v)\) with respect to \(\beta_W\) so that’s just 1 for \(T(v)\) while the rest are 0. Similarly, we have the same thing for the rest of the \(k-1\) column vectors. But for the last column, we need to represent \(T_W(T^{k-1}(v)) = T^k(v)\). We’re given \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\) and so the rest of the coefficients are \(a_0,a_1 .... a_{k-1}\). Now, we can compute the determinant expanding across the first row $$ \begin{align*} &amp;\det([T_W]_{\beta_W}^{\beta_W} -tI_k) = \begin{pmatrix} -t &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\ 1 &amp; -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\ \end{pmatrix} \\ &amp;= (-1)^{1+1}(-t)\det \begin{pmatrix} -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\ \end{pmatrix} +(-1)^{1+k}a_0 (1) \end{align*} $$ The last determinant for the \(a_0\) component is 1 because that sub matrix is upper triangular so the determinant is the product of the entries on the diagonal which are all 1. Next, we want to compute that new determinant but notice now that it has the same pattern so $$ \begin{align*} &amp;= -t \left( \det \begin{pmatrix} -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\ \end{pmatrix}]]></summary></entry><entry><title type="html">Lecture 26/27: Markov Chains and Transition Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/23/lec26-diagonalizable-matrices-and-markov-chains.html" rel="alternate" type="text/html" title="Lecture 26/27: Markov Chains and Transition Matrices" /><published>2024-08-23T01:01:36-07:00</published><updated>2024-08-23T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/23/lec26-diagonalizable-matrices-and-markov-chains</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/23/lec26-diagonalizable-matrices-and-markov-chains.html"><![CDATA[<p>We have a set of ingredients:</p>
<ul style="list-style-type:lower-alpha">
	<li>Set of states \(S_1,...,S_n\). If we have 10m people in Chicagoland, then we can divide them into \(S_1\) living in the city and \(S_2\) living in a suburb</li>
	<li>Time Step. Example: we are interested in how the number of people living in these two states changes over time. So a time step could be 1 year</li>
	<li>Probability vector at time \(k\)
<div> 
$$
\begin{align*}
\bar{p_k} = 
\begin{pmatrix} 
(\bar{p_k})_1 \\
\vdots \\
(\bar{p_k})_n
\end{pmatrix}
\end{align*}
$$
</div>
where \(\bar{p_k}_1\) is the probability of being in state 1 at time \(k\). In general \(\bar{p_k}_j \geq 0\) and \(\sum_{j=1}^n \bar{p_k}_j = 1\). For example
<div> 
$$
\begin{align*}
(\bar{p_k})_0 = 
\begin{pmatrix} 
.7 \\
.3
\end{pmatrix}
\end{align*}
$$
</div>
</li>
<!------------------------>
<li>Transition matrix \(A \in M_{n \times n}\)
<div> 
$$
\begin{align*}
A = 
\begin{pmatrix} 
A_{11} &amp; \cdots &amp; A_{1n} \\
\vdots &amp; A_{ij} &amp; \vdots \\
A_{n1} &amp; \cdots &amp; A_{nn}
\end{pmatrix}
\end{align*}
$$
</div>
where \(A_{ij}\) is the probability of moving from state \(j\) to state \(i\) in one time step. This transition matrix doesn't change over time. For example
<div> 
$$
\begin{align*}
A = 
\begin{pmatrix} 
.9 &amp; .02 \\
.1 &amp; .98
\end{pmatrix}
\end{align*}
$$
</div>
Here \(A_{ij} \geq 0\) and \(\sum_{i=1}^nA_{ij} = 1\). So \(A_{12} = 0.02\) is the probability of moving from state 1 (city) to the state 2 (suburb).
</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Limit of a Markov Matrix</b></h4>

<p>So \(\bar{p_0}\) is the probability at time step 0. What is \(\bar{p_1}, \bar{p_2},...,\bar{p_n}\)?</p>
<div> 
$$
\begin{align*}
\bar{p_1} &amp;= A\bar{p_0} \\
\bar{p_2} &amp;= A\bar{p_1} = A^2\bar{p_0} \\
\vdots
\bar{p_n} &amp;= A^n\bar{p_0} \\
\lim_{n \rightarrow \infty} \bar{p_n} &amp;= (\lim_{n \rightarrow \infty} A^n) \bar{p_0}
\end{align*}
$$
</div>
<p>What is the limit of this matrix?? This often exists and is easy to describe.
<br />
<br />
Exercise: \(A^d\) is also a transition matrix!
<br />
<br />
What we want to know is what this limit is so we’re going to spend some time studying the properties of this transition matrix.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose for some \(d \geq 1, A^d\) has all positive entries. Then
<ul style="list-style-type:lower-alpha">
	<li>1 is an eigenvalue of \(\det(A)\). \(\dim(E_1) = 1\) and \(E_1 = span\{\bar{u}\}\) where \(\bar{u}\) is a probability vector</li>
	<li>For any other eigenvalue \(\lambda, |\lambda| &lt; 1\).</li>
	<li>
	\(\lim\limits_{n \rightarrow \infty} A^n = 
		\begin{pmatrix} 
		\bar{u} &amp; \bar{u} &amp; \cdots &amp; \bar{u} \\
		\end{pmatrix}
		\)<br />
	As a Consequence, \(\lim\limits_{n \rightarrow \infty} \bar{p}_n = 
		\begin{pmatrix} 
		\bar{u} &amp; \bar{u} &amp; \cdots &amp; \bar{u} \\
		\end{pmatrix}\bar{p}_0 = \bar{u} \text{ (for any $\bar{p}_0$)}
		\)
	</li>
</ul>
</div>
<p><br />
<!-------------------------------------------1----------------------------------------->
<b>Proof</b>
<br />
<br />
We’re going to prove these statements in several parts.
<br />
<br />
<b>1 is an eigenvalue of \(A\):</b>
<br />
<br />
FACT: \(A\) and \(A^t\) has the same eigenvalues. Why? This is because \(\det(B) = \det(B^t)\) which means that \(\det(A - tI_n) = \det(A^t - tI_n)\). We will use this fact to prove what we want.
<br />
<br />
Consider \(A^t\). We know that the rows now add to 1 (in \(A\), the columns add to 1). Let \(\bar{v} = \begin{pmatrix} 1 &amp; \cdots &amp; 1 \end{pmatrix}^t\). Now,</p>
<div> 
$$
\begin{align*}
A^t\bar{v} &amp;= \begin{pmatrix} a_1^t &amp; \cdots &amp; a_n^t \end{pmatrix}
\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = a_1^t + ... + a_n^t 
= \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \bar{v}
\end{align*}
$$
</div>
<p>The last step is true because we know in \(A^t\), the sum of each row adds to 1. Therefore, \(\bar{v}\) is an eigenvector of \(A^t\) with eigenvalue is 1. But we know that \(A\) and \(A^t\) have the same eigenvalues and so 1 is an eigenvalue of \(A\).
<!---------------------------------------2--------------------------------------------->
<br />
<br />
<b>If \(\lambda\) is an eigenvalue of \(A\), then \(|\lambda| \leq 1\):</b>
<br />
<br />
Since \(\lambda\) is an eigenvalue of \(A\), then \(\lambda\) is an eigenvalue of \(A^t\) and</p>
<div> 
$$
\begin{align*}
A^tv = \lambda v, v = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \neq \bar{0}
\end{align*}
$$
</div>
<p>\(v\) is an eigenvector of \(A^t\). Let \(|v_k| = \max_j \{|v_j|\}\). so \(|v_k|\) is the greatest entry in the vector \(v\). Now, let’s look at the \(k\)th entry of \(A^tv\). By the definition of matrix-vector multiplication,</p>
<div> 
$$
\begin{align*}
(A^tv)_k = \sum_j (A^t)_{kj}v_j
\end{align*}
$$
</div>
<p>Alternatively,</p>
<div> 
$$
\begin{align*}
(A^tv)_k = (\lambda v)_k = \lambda v_k
\end{align*}
$$
</div>
<p>Combining both equations</p>
<div> 
$$
\begin{align*}
\lambda v_k &amp;= \sum_j (A^t)_{kj}v_j \\
|\lambda v_k| &amp;= |\sum_j (A^t)_{kj}v_j| \\
&amp;= |\sum_j A_{jk}v_j| \text{ (replace $A^t$ with $A$)}\\
&amp;\leq \sum_j |A_{jk}v_j| \text{ (the absolute value of a sum $\leq$ sum of absolute values)} \\
&amp;\leq \sum_j A_{jk}|v_k| \text{ (the entries in $A$ are positive)} \\
&amp;\leq |v_k| \sum_j A_{jk} \text{ ($|v_k$ doesn't depend on the sum)} \\
&amp;\leq |v_k| \text{ (the sum is 1 by definition)} \\
|\lambda| &amp;\leq 1
\end{align*}
$$
</div>
<p>As required.
<!-------------------------------------3----------------------------------------------->
<br />
<br />
<b>If \(A_{ij} &gt; 0\) for all \(i,j\), then \(\dim E_1 = 1\) and \(\lambda \neq 1 \implies |\lambda| &lt; 1\):</b>
<br />
<br />
Suppose \(|\lambda| = 1\) and \(A^tv = \lambda v\) for \(v \neq \bar{0}\). It suffies to show that</p>
<div> 
$$
\begin{align*}
v = c\begin{pmatrix} 1 \\ \vdots \\ 1 
\end{pmatrix}
\end{align*}
$$
</div>
<p>Why? if the eigenvector was \(\begin{pmatrix} 1 &amp; \cdots &amp; 1 \end{pmatrix}^t\), then \(\lambda\) must be 1. Because</p>
<div> 
$$
\begin{align*}
A\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}  = \lambda \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>so \(\lambda = 1\). So we argue as before,</p>
<div> 
$$
\begin{align*}
|v_k| &amp;= | \lambda v_k | \text { ( because $ \lambda = 1)$}\\
&amp;= |\sum_j A_{jk} v_j | \\
&amp;\leq \sum_j A_{jk} |v_j| \\
&amp;\leq |v_k| \sum_j A_{jk} = |v_k| \\
\end{align*}
$$
</div>
<p>So these inequalities are equalities. \(A_{jk}v_j\) all have the same sign.
[TODO continue this proof .. I’m lost now]
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Diagonalizable Transition Matrix</b></h4>

<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(A\) is a transition matrix such that \(A_{ij} &gt; 0\) for \(i, j\) and \(A\) is diagonalizable. Then
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k = \begin{pmatrix} \bar{u} &amp; \cdots &amp; \bar{u} \end{pmatrix}
\end{align*}
$$
where \(\bar{u}\) is a probability vector and \(A\bar{u} = \bar{u}\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
<br />
We know 1 is an eigenvalue and \(\dim(E_1)=1\) and any other \(\lambda\) has \(|\lambda| &lt; 1\). We now have</p>
<div> 
$$
\begin{align*}
A = QDQ^{-1}, 
D = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>We may assume</p>
<div> 
$$
\begin{align*}
D = 
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>We know we can compute \(A^k\) easily using</p>
<div> 
$$
\begin{align*}
A^k = Q 
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^k &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n^k
\end{pmatrix}
Q^{-1}
\end{align*}
$$
</div>
<p>We can now apply the limit</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k =
Q
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
Q^{-1}
= L
\end{align*}
$$
</div>
<p>What do we know about \(L\)? What happens if we multiply \(A\) by \(L\)?</p>
<div> 
$$
\begin{align*}
AL &amp;= A\lim\limits_{k \rightarrow \infty} A^k \\
&amp;= A\lim\limits_{k \rightarrow \infty} A^{k+1} \\
&amp;= L
\end{align*}
$$
</div>
<p>We know that \(L\) is a limit of a transition matrix so it’s a transition matrix. So the sum of the entries in each column of \(L\) is 1. Moreover, we saw that \(AL = L\). If \(L\) was a vector, then it would be an eigenvector with \(\lambda = 1\). But \(L\) is not a vector. It is a matrix with column vectors. if we apply the matrix-vector definition, then</p>
<div> 
$$
\begin{align*}
AL &amp;= \begin{pmatrix} Al_1 &amp; \cdots &amp; Al_n \end{pmatrix} \\
   &amp;= \begin{pmatrix} l_1 &amp; \cdots &amp; l_n \end{pmatrix}
\end{align*}
$$
</div>
<p>But this means that each column vector \(l_j\) is in fact an eigenvector of \(A\) with eigenvalue 1. But we also know that the eigenspace of \(\lambda=1\) is one-dimensional so all of the eigenvectors \(l_1,...,l_n\) must be in \(E_{\lambda_1}\). Therefore, \(l_j = c_j\bar{u}\) for some \(c_j\). We additionally know that the sum of entries of \(l_j\) is 1. Similarly the sum of entries in \(\bar{u}\) is also 1. So \(c_j\) must be 1 for any \(j\) and so</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k = \begin{pmatrix} \bar{u} &amp; \cdots &amp; \bar{u} \end{pmatrix}
\end{align*}
$$
</div>
<p>and \(\bar{u}\) is a probability vector.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Continuing the same example from before where</p>
<div> 
$$
\begin{align*}
A = 
\begin{pmatrix} 
.9 &amp; .02 \\
.1 &amp; .98
\end{pmatrix}
\end{align*}
$$
</div>
<p>We want to check if \(A\) is diagonalizable. In fact it is</p>
<div> 
$$
\begin{align*}
A = QDQ^{-1}, Q = 
\begin{pmatrix} 
\frac{1}{6} &amp; -\frac{1}{6} \\
\frac{5}{6} &amp; \frac{1}{6}
\end{pmatrix},
D = 
\begin{pmatrix} 
1 &amp; 0 \\
0 &amp; 0.88
\end{pmatrix}
\end{align*}
$$
</div>
<p>To compute \(A^k\), we know from the previous theorem, that the answer has two copies of the eigenvector corresponding to eigenvalue \(\lambda = 1\). You can see above that the eigenvector corresponding to 1 is the first column vector in \(Q\) and so</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k =
\begin{pmatrix} 
\frac{1}{6} &amp; \frac{1}{6} \\
\frac{5}{6} &amp; \frac{5}{6}
\end{pmatrix}
\end{align*}
$$
</div>
<p>and no matter what is our initially probability vector is,</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} \bar{p}_k =
\begin{pmatrix} 
\frac{1}{6} \\
\frac{5}{6} 
\end{pmatrix}
\end{align*}
$$
</div>
<p>\(\frac{1}{6}\) corresponds to state 1 and \(\frac{5}{6}\) correspond to state 2.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We have a set of ingredients: Set of states \(S_1,...,S_n\). If we have 10m people in Chicagoland, then we can divide them into \(S_1\) living in the city and \(S_2\) living in a suburb Time Step. Example: we are interested in how the number of people living in these two states changes over time. So a time step could be 1 year Probability vector at time \(k\) $$ \begin{align*} \bar{p_k} = \begin{pmatrix} (\bar{p_k})_1 \\ \vdots \\ (\bar{p_k})_n \end{pmatrix} \end{align*} $$ where \(\bar{p_k}_1\) is the probability of being in state 1 at time \(k\). In general \(\bar{p_k}_j \geq 0\) and \(\sum_{j=1}^n \bar{p_k}_j = 1\). For example $$ \begin{align*} (\bar{p_k})_0 = \begin{pmatrix} .7 \\ .3 \end{pmatrix} \end{align*} $$ Transition matrix \(A \in M_{n \times n}\) $$ \begin{align*} A = \begin{pmatrix} A_{11} &amp; \cdots &amp; A_{1n} \\ \vdots &amp; A_{ij} &amp; \vdots \\ A_{n1} &amp; \cdots &amp; A_{nn} \end{pmatrix} \end{align*} $$ where \(A_{ij}\) is the probability of moving from state \(j\) to state \(i\) in one time step. This transition matrix doesn't change over time. For example $$ \begin{align*} A = \begin{pmatrix} .9 &amp; .02 \\ .1 &amp; .98 \end{pmatrix} \end{align*} $$ Here \(A_{ij} \geq 0\) and \(\sum_{i=1}^nA_{ij} = 1\). So \(A_{12} = 0.02\) is the probability of moving from state 1 (city) to the state 2 (suburb). The Limit of a Markov Matrix]]></summary></entry><entry><title type="html">Lecture 25: Diagonalization and Eigenvalues</title><link href="http://localhost:4000/jekyll/update/2024/08/22/lec25-diagonalization-eigenvalues.html" rel="alternate" type="text/html" title="Lecture 25: Diagonalization and Eigenvalues" /><published>2024-08-22T01:01:36-07:00</published><updated>2024-08-22T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/22/lec25-diagonalization-eigenvalues</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/22/lec25-diagonalization-eigenvalues.html"><![CDATA[<p>Last time we learned that a matrix \(A\) is diagonalizable if \(A\) has \(n\) linearly independent eigenvectors. In other words if we can form a basis consisting of eigenvectors \(\beta = \{v_1,...,v_n\}\). 
<br />
<br />
In that case, we will see that the matrix representative of \(L_A\) with respect to basis \(\beta\) is a diagonal matrix where the diagonal entries are those eigenvalues corresponding to the eigenvectors</p>
<div> 
$$
\begin{align*}
[L_A]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix},
Av_j = \lambda_j v_j
\end{align*}
$$
</div>
<p>\(v_j\) is an eigenvector and \(\lambda_j\) is called an eigenvalue. And so to diagonalize a matrix, we need to find these eigenvectors. Last time we developed a plan for this:</p>
<ol> 
	<li>Find these eigenvalues \(\lambda_1, ..., \lambda_2\) </li>
	<li>Find a basis for \(\beta_i\) for each eigenspace \(E_{\lambda_i}\)</li>
	<li>If \(\sum_{j=1}^{k}\dim(E_{\lambda_i}) = n\), then collect all the separate bases and that's our basis \(\beta = \beta_1 \cup ... \cup \beta_k\). This step works because we proved in the last lecture the corollary \(\lambda_1 \neq \lambda_2 \rightarrow \beta_1 \cup \beta_2\) is linearly independent.</li> 
</ol>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples of Non-Diagonalizable Matrices</b></h4>
<p>Of course the plan above might not work and there are different ways, a matrix could fail to be diagonalized.</p>
<div> 
$$
\begin{align*}
A = 
	\begin{pmatrix} 
	0 &amp; -1 \\
	1 &amp; 0
	\end{pmatrix} 
	\end{align*}
$$
</div>
<p>This matrix has no eigenvalues so it can’t be diagonalized.</p>
<div> 
$$
\begin{align*}
B = 
	\begin{pmatrix} 
	1 &amp; 1 \\
	0 &amp; 1
	\end{pmatrix} 
\end{align*}
$$
</div>
<p>This matrix has only one eigen value.</p>
<div>
$$
\begin{align*}
det(A - tI_2) = 
	\begin{pmatrix} 
	1-t &amp; 1 \\
	0 &amp; 1-t
	\end{pmatrix} = (1-t)^2 = 0 \rightarrow t = 1
\end{align*}
$$
</div>
<p>This implies</p>
<div>
$$
\begin{align*}
E_{\lambda_1} = N(B - 1I_n) = N \left(
	\begin{pmatrix} 
	0 &amp; 1 \\
	0 &amp; 0
	\end{pmatrix} 
	\right)
	= \{ (t, 0) \ | \ t \in \mathbf{R} \}
\end{align*}
$$
</div>
<p>So we don’t have enough linearly independent vectors in \(\beta\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Is There a Better Diagonalization Test?</b></h4>
<p>Today, we will refine our answer to the question “Is \(A\) diagonalizable?”</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A polynomial \(f(t)\) splits over \(\mathbf{R}\) if there are scalars \(c,a_1,...,a_k \in \mathbf{R}\) such that 
$$
\begin{align*}
f(t) = c(t-a_1)(t - a_2)...(t-a_k)
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
In other words, can completely factorize the polynomial?
<br />
<br />
Example 1: \(t^2 + 1\) doesn’t split over \(\mathbf{R}\). It does however split over \(\mathbf{C} as (t - i)(t + i)\).
<br />
<br />
Example 2: \(t^2 - 2t + 1 = (t - 1)(t - 1)\) splits over \(\mathbf{R}\).
<br />
<br />
So now what does splitting have to do with diagonalizability? The following theorem explains this
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
If \(A\) is diagonalizable, then its characteristic polynomial splits over \(\mathbf{R}\)
</div>
<p><br />
Note that the the converse is false. If the characteristic polynomial splits over \(\mathbf{R}\), it doesn’t necessarily means that \(A\) is diagonalizable. (See example 2 above)
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvalues of Similar Matrices</b></h4>
<p>Before going into the proof of theorem 1 above, the following proposition will be useful</p>
<div class="purdiv">
Proposition
</div>
<div class="purbdiv">
If \(B = Q^{-1}AQ\), then 
$$
\begin{align*}
\det(B - tI_n) = \det(A - tI_n)
\end{align*}
$$
</div>
<p><br />
This means that if \(A\) and \(B\) are similar, then they have the same eigenvalues.
<br />
<br />
<b>Proof:</b>
<br />
<br />
Observe that</p>
<div>
$$
\begin{align*}
1 = \det(I_n) = \det(QQ^{-1}) = \det(Q)\det(Q^{-1}).
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\det(B - tI_n) &amp;= \det(Q^{-1}AQ - tI_n) \\
              &amp;= \det(Q^{-1}AQ - QtI_nQ^{-1}) \\
              &amp;= \det(Q (A - tI_n)Q^{-1}) \text{ (factor out Q on the left ..) }\\ 
              &amp;= \det(Q) \det((A - tI_n)) \det(Q^{-1}) \\ 
              &amp;= \det(Q) \det(Q^{-1}) \det((A - tI_n)) \text{ (they are just real numbers)}\\ 			  
              &amp;= \det((A - tI_n)). \ \blacksquare \\ 			  
\end{align*}
$$
</div>
<p>Note here that in step 2, \(QtI_nQ^{-1} = tQQ^{-1} = tQtQ^{-1} = tI_n\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Proof of Theorem 1</b></h4>
<p>Suppose that \(A\) is diagonalizable. This means that there exists a basis \(\beta\) such that</p>
<div>
$$
\begin{align*}
[L_A]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix},
Av_j = \lambda_j v_j		  
\end{align*}
$$
</div>
<p>If we let \(\alpha\) be the standard basis, then know that there exists a change of coordinate matrix \(Q = [I]^{\alpha}_{\beta}\) such that</p>
<div>
$$
\begin{align*}
[L_A]_{\beta}^{\beta} &amp;= [I]_{\alpha}^{\beta}[L_A]_{\alpha}^{\alpha}[I]^{\alpha}_{\beta}  \\
                   &amp;= Q^{-1}AQ.
\end{align*}
$$
</div>
<p>This means that \([L_A]_{\beta}^{\beta}\) and \(A\) are similar matrices. But by the previous proposition, this means that we have</p>
<div>
$$
\begin{align*}
\det ([L_A]_{\beta}^{\beta} -tI_n) &amp;= \det(A - tI_n).
\end{align*}
$$
</div>
<p>On the other hand,</p>
<div>
$$
\begin{align*}
\det ([L_A]_{\beta}^{\beta} -tI_n) &amp;= 
\begin{pmatrix} 
\lambda_1 -t &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n -t
\end{pmatrix}	  
\\
&amp;= (\lambda_1 - t)...(\lambda_n - t). \ \blacksquare
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Algebraic and Geometric Multiplicities of Eigenvalues</b></h4>
<p>There is another condition for diagonalizability but we need a few more definitions.</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The algebraic multiplicity of \(\lambda\) is the number of times \(\lambda - t\) divides \(\det(A - tI_n)\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
An an example if \(A =  
\begin{pmatrix} 
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}\) then, \(\lambda = 1\) has algebraic multiplicity 2 because \(\det(A - I_n) = (1-t)^2\).</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The geometric multiplicity of \(\lambda\) is \(\dim(E_{\lambda})\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
For the same example above. The geometric multiplicity of \(\lambda = 1\) is 1 because the nullspace is spanned by one vector.
<br />
<br />
Given these definitions we can now introduce the following theorem.
<br /></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
Geometric multiplicity of \(\lambda \leq\) the algebraic multiplicity of \(\lambda\).
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Let \(\lambda\) be an eigenvalue of \(A \in M_{n \times n}\) and let the geometric multiplicity of \(\lambda\) be \(k\). Now the goal is to relate the characteristic polynomial of \(\lambda\) to the dimension of \(E_{\lambda}\). By definition, we know that \(\dim(E_{\lambda})\) is \(k\). (definition above)
<br />
<br />
Let \(\{v_1,...,v_k\}\) be a basis for \(E_{\lambda}\). We know that \(\dim(E_{\lambda})=k\). We also know that \(k \leq n\) so extend this basis to a basis for \(\mathbf{R}^n\) so</p>
<div>
$$
\begin{align*}
\beta = \{v_1, ..., v_k, v_{k+1},...,v_n\}.
\end{align*}
$$
</div>
<p>Again, we are seeking a relationship between the dimension of the eigenspace and the characteristic polynomial of \(A\). We don’t know if \([L_A]_{\beta}^{\beta}\) is diagonalizable yet. But we do know by definition that</p>
<div>
$$
\begin{align*}
[L_A]_{\beta}^{\beta} &amp;= 
\begin{pmatrix} 
[Av_1]_{\beta} &amp; \cdots &amp; [Av_n]_{\beta}
\end{pmatrix}
\\
&amp;= 
\begin{pmatrix} 
[\lambda v_1]_{\beta} &amp; \cdots &amp; [\lambda v_k]_{\beta} &amp; [Av_{k+1}]_{\beta} &amp; \cdots &amp; [Av_n]_{\beta}
\end{pmatrix}
\end{align*}
$$
</div>
<p>The top left section of this matrix is diagonal where it consists of the \(k\) eigenvectors. By the proposition we introduced earlier, since \(A\) and \([L_A]_{\beta}^{\beta}\) are similar, then</p>
<div>
$$
\begin{align*}
\det([L_A]_{\beta}^{\beta} - tI_n) = \det(A - tI_n)
\end{align*}
$$
</div>
<p>But we can write \(\det([L_A]_{\beta}^{\beta} - tI_n)\) as</p>
<div>
$$
\begin{align*}
\det([L_A]_{\beta}^{\beta} - tI_n) &amp;= (\lambda - t)^k \det(\text{unknown part of the matrix}) \\
&amp;= \det(A - tI_n).
\end{align*}
$$
</div>
<p>Why is this true? The top left section just consists of \(\lambda - t\) entries on the diagonal while the rest is zero. Hence, the algebraic multiplicity of \(\lambda\) is at least \(k\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>A More Refined Test for Diagonalizability</b></h4>
<p>We’re finally ready to present the more refined test for diagonalizability. 
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(A\) is diagonalizable if and only if
<ul style="list-style-type:lower-alpha">
	<li>\(\det(A - tI_n)\) splits over \(\mathbf{R}\)</li>
	<li>For each eigenvalue \(\lambda\), geometric multiplicity = algebraic multiplicity</li>
</ul>
</div>
<p><br />
<b>Proof</b>
<br />
<br />
\(\Rightarrow:\)
Suppose that \(A\) is diagonalizable. This means that we have a basis \(beta = \{v_1,...,v_n\}\) consisting entirely of eigenvectors. Now we need to prove that \(a\) and \(b\) both hold. Last time we proved that \(a\) holds. This means that the characteristic polynomial splits over \(R\) and so</p>
<div> 
$$
\begin{align*}
\det(A - tI_n) = (t - \lambda_1)^{m_1}...(t - \lambda_k)^{m_k}
\end{align*}
$$
</div>
<p>where \(\sum_{j=1}^{k} m_j = n\). Now, given \(\lambda_j\), let \(k_j\) be the geometric multiplicity of \(\lambda_j\). We know by definition that \(k_j = \dim(E_{\lambda_j})\). The goal is to prove that \(k_j = \dim(E_{\lambda_j}) = m_j\). 
<br />
<br />
Last time we proved that the geometric multiplicity is always less than or equal to the algebraic multiplicity of any \(\lambda\). Therefore, we have \(k_j \leq m_j\). Moreover, each of the eigenvectors in \(\beta\) must belong to one of our eigenspaces so let \(l_j = num(\{v_i \in \beta \ | \ v_i \in E_{\lambda_j}\})\). That is \(l_j\) is the number of eigenvectors in \(\beta\) that belong to the eigenspace \(E_{\lambda_j}\). But we only have \(n\) eigenvectors in \(\beta\), therefore,</p>
<div> 
$$
\begin{align*}
n = \sum_{j=1}^k l_j
\end{align*}
$$
</div>
<p>We also know that</p>
<div> 
$$
\begin{align*}
l_j \leq k_j
\end{align*}
$$
</div>
<p>This is because the number of vectors in the eigenspace that are selected from the basis can be at most the dimension of the eigenspace itself. Putting the above observations together, we see that</p>
<div> 
$$
\begin{align*}
n = \sum_{j=1}^k l_j \leq \sum_{j=1}^k k_j \leq \sum_{j=1}^k m_j = n
\end{align*}
$$
</div>
<p>Therefore, \(k_j = m_j\) for \(j = 1,2,...,k\). 
<br />
<br />
\(\Leftarrow:\) We’re given (a) and (b). One way to prove that \(A\) is diagonalizable is by constructing a basis of eigenvectors. So let \(\beta_j\) be a basis for \(E_{\lambda_j}\). Set \(\beta = \beta_1 \cup \beta_2 \cup ... \cup \beta_k\). By the homework, \(\beta\) is linearly independent and by construction, it consists of eigenvectors. It remains to show that the number of vectors in \(\beta\) is \(n\). But from (b), we know that</p>
<div> 
$$
\begin{align*}
num(\beta) = \sum_{j=1}^k l_j = \sum_{j=1}^k k_j = \sum_{j=1}^k m_j
\end{align*}
$$
</div>
<p>Because in (b) we said that the algebraic multiplicity must be equal to the geometric multiplicity. And from (a), we know that</p>
<div> 
$$
\begin{align*}
\sum_{j=1}^k m_j = n
\end{align*}
$$
</div>
<p>So \(\beta\) is the desired basis and \(A\) must be diagonalizable. \(\ \blacksquare\)
<br />
<br />
One final note here: we’ve developed several tests including the above one to test matrices for diagonalizability. What about general linear transformations \(T: V \rightarrow V\)? well we can find the matrix representable of \(T\) with respect to  basis \(\gamma\) so \([T]_{\gamma}^{\gamma}\). Check if this matrix is diagonalizable for any basis \(\gamma\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing \(A^k\)</b></h4>
<p>If \(A\) is diagonalizable, then</p>
<div> 
$$
\begin{align*}
A = QDQ^{-1} \text { where }
D = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>This allows us to compute \(A^k\) easily since</p>
<div> 
$$
\begin{align*}
A^k = QD^kQ^{-1} \text { where }
D^k = 
\begin{pmatrix} 
\lambda_1^k &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n^k
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we learned that a matrix \(A\) is diagonalizable if \(A\) has \(n\) linearly independent eigenvectors. In other words if we can form a basis consisting of eigenvectors \(\beta = \{v_1,...,v_n\}\). In that case, we will see that the matrix representative of \(L_A\) with respect to basis \(\beta\) is a diagonal matrix where the diagonal entries are those eigenvalues corresponding to the eigenvectors $$ \begin{align*} [L_A]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix}, Av_j = \lambda_j v_j \end{align*} $$ \(v_j\) is an eigenvector and \(\lambda_j\) is called an eigenvalue. And so to diagonalize a matrix, we need to find these eigenvectors. Last time we developed a plan for this: Find these eigenvalues \(\lambda_1, ..., \lambda_2\) Find a basis for \(\beta_i\) for each eigenspace \(E_{\lambda_i}\) If \(\sum_{j=1}^{k}\dim(E_{\lambda_i}) = n\), then collect all the separate bases and that's our basis \(\beta = \beta_1 \cup ... \cup \beta_k\). This step works because we proved in the last lecture the corollary \(\lambda_1 \neq \lambda_2 \rightarrow \beta_1 \cup \beta_2\) is linearly independent. Examples of Non-Diagonalizable Matrices Of course the plan above might not work and there are different ways, a matrix could fail to be diagonalized. $$ \begin{align*} A = \begin{pmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{pmatrix} \end{align*} $$ This matrix has no eigenvalues so it can’t be diagonalized. $$ \begin{align*} B = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix} \end{align*} $$ This matrix has only one eigen value. $$ \begin{align*} det(A - tI_2) = \begin{pmatrix} 1-t &amp; 1 \\ 0 &amp; 1-t \end{pmatrix} = (1-t)^2 = 0 \rightarrow t = 1 \end{align*} $$ This implies $$ \begin{align*} E_{\lambda_1} = N(B - 1I_n) = N \left( \begin{pmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{pmatrix} \right) = \{ (t, 0) \ | \ t \in \mathbf{R} \} \end{align*} $$ So we don’t have enough linearly independent vectors in \(\beta\). Is There a Better Diagonalization Test? Today, we will refine our answer to the question “Is \(A\) diagonalizable?” Definition A polynomial \(f(t)\) splits over \(\mathbf{R}\) if there are scalars \(c,a_1,...,a_k \in \mathbf{R}\) such that $$ \begin{align*} f(t) = c(t-a_1)(t - a_2)...(t-a_k) \end{align*} $$ In other words, can completely factorize the polynomial? Example 1: \(t^2 + 1\) doesn’t split over \(\mathbf{R}\). It does however split over \(\mathbf{C} as (t - i)(t + i)\). Example 2: \(t^2 - 2t + 1 = (t - 1)(t - 1)\) splits over \(\mathbf{R}\). So now what does splitting have to do with diagonalizability? The following theorem explains this Theorem 1 If \(A\) is diagonalizable, then its characteristic polynomial splits over \(\mathbf{R}\) Note that the the converse is false. If the characteristic polynomial splits over \(\mathbf{R}\), it doesn’t necessarily means that \(A\) is diagonalizable. (See example 2 above) Eigenvalues of Similar Matrices Before going into the proof of theorem 1 above, the following proposition will be useful Proposition If \(B = Q^{-1}AQ\), then $$ \begin{align*} \det(B - tI_n) = \det(A - tI_n) \end{align*} $$ This means that if \(A\) and \(B\) are similar, then they have the same eigenvalues. Proof: Observe that $$ \begin{align*} 1 = \det(I_n) = \det(QQ^{-1}) = \det(Q)\det(Q^{-1}). \end{align*} $$ Therefore, $$ \begin{align*} \det(B - tI_n) &amp;= \det(Q^{-1}AQ - tI_n) \\ &amp;= \det(Q^{-1}AQ - QtI_nQ^{-1}) \\ &amp;= \det(Q (A - tI_n)Q^{-1}) \text{ (factor out Q on the left ..) }\\ &amp;= \det(Q) \det((A - tI_n)) \det(Q^{-1}) \\ &amp;= \det(Q) \det(Q^{-1}) \det((A - tI_n)) \text{ (they are just real numbers)}\\ &amp;= \det((A - tI_n)). \ \blacksquare \\ \end{align*} $$ Note here that in step 2, \(QtI_nQ^{-1} = tQQ^{-1} = tQtQ^{-1} = tI_n\) Proof of Theorem 1 Suppose that \(A\) is diagonalizable. This means that there exists a basis \(\beta\) such that $$ \begin{align*} [L_A]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix}, Av_j = \lambda_j v_j \end{align*} $$ If we let \(\alpha\) be the standard basis, then know that there exists a change of coordinate matrix \(Q = [I]^{\alpha}_{\beta}\) such that $$ \begin{align*} [L_A]_{\beta}^{\beta} &amp;= [I]_{\alpha}^{\beta}[L_A]_{\alpha}^{\alpha}[I]^{\alpha}_{\beta} \\ &amp;= Q^{-1}AQ. \end{align*} $$ This means that \([L_A]_{\beta}^{\beta}\) and \(A\) are similar matrices. But by the previous proposition, this means that we have $$ \begin{align*} \det ([L_A]_{\beta}^{\beta} -tI_n) &amp;= \det(A - tI_n). \end{align*} $$ On the other hand, $$ \begin{align*} \det ([L_A]_{\beta}^{\beta} -tI_n) &amp;= \begin{pmatrix} \lambda_1 -t &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n -t \end{pmatrix} \\ &amp;= (\lambda_1 - t)...(\lambda_n - t). \ \blacksquare \end{align*} $$ Algebraic and Geometric Multiplicities of Eigenvalues There is another condition for diagonalizability but we need a few more definitions.]]></summary></entry><entry><title type="html">Lecture 24: More Diagonalizability</title><link href="http://localhost:4000/jekyll/update/2024/08/21/lec24-more-diagonalizability.html" rel="alternate" type="text/html" title="Lecture 24: More Diagonalizability" /><published>2024-08-21T01:01:36-07:00</published><updated>2024-08-21T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/21/lec24-more-diagonalizability</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/21/lec24-more-diagonalizability.html"><![CDATA[<p>We saw in the previous lecture that \(A \in M_{n \times n}\) is diagonalizable if there is a basis \(\beta = \{v_1, v_2,...,v_n\}\) of \(\mathbf{R}^n\) such that</p>
<div> 
$$
\begin{align*}
[L_A]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>Where each vector of this matrix is \(Av_j = \lambda_j v_j\). \(v_j\) is an eigenvector and \(\lambda_j\) is called an eigenvalue.
<br />
Let the standard basis of \(\mathbf{R}^n\) be \(\alpha\), then we know that \([L_A]_{\alpha}^{\alpha} = A\). We also know that we can find a change of coordinates matrix such that</p>
<div> 
$$
\begin{align*}
[L_A]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^n}]_{\alpha}^{\beta}[L_A]_{\alpha}^{\alpha}[I_{\mathbf{R}^n}]_{\beta}^{\alpha}
\\
&amp;= Q^{-1}AQ.
\end{align*}
$$
</div>
<ul> 
<li>
<!------------------------------------------------------------------------------------>
So now to diagonalize a matrix, we need to find these eigenvectors and eigenvalues that satisfy \(Av_j = \lambda_jv_j\) for \(j = 1,...,n\) and \(v \neq \bar{0}\).
</li>
<li>Finding such a basis is equivalent to saying that \(A\) is diagonalizable if and only if there is a basis \(\beta\) consisting of eigenvectors. 
</li>
<li>
In fact, to diagonalize \(A\), we need \(n\) linearly independent eigenvectors of \(A\). 
</li>
<li> Given an eigenvalue, it is easy to find the eigenvectors. \(E_{\lambda} = N(A - \lambda I_n) = \{ \text{ eigenvectors of $A$ with eigenvalue $\lambda$} \}\cup \{\bar{0}\} \)
</li>
<li>
	Given \(E_{\lambda} = N(A - \lambda I_n)\), we can find a basis of independent eigenvectors. The number of eigenvectors is the dimension of \(E_{\lambda}\).
</li>
<li>This is all good but it required us to know the eigenvalues. To find the eigenvalues, we discovered last time that the eigenvalues of \(A\) are the real roots of the characteristic polynomial \(f(t) = \det(A - tI_n)\).
</li>
</ul>
<p><br />
<!------------------------------------------------------------------------------------>
This is great and gives up the following plan to diagonalize \(A\):</p>
<ol>
	<li>Find all the eigenvalues of \(A\), \(\lambda_1,...,\lambda_k\).</li>
	<li>Find a basis for \(E_{\lambda_j}\).</li>
	<li>Determine if these bases combine to form a basis \(\beta\) for \(\mathbf{R}^n\)</li>
	<li>Compute \(Q, Q^{-1}\) etc.</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Diagonalize \(\begin{align*}
A = 
\begin{pmatrix} 
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0 \\
-2 &amp; 0 &amp; 3
\end{pmatrix}
\end{align*}\)
If possible.
<br />
<br />
Following the above plan:</p>
<ol>
	<!-----------------1------------------>
	<li>Find all the eigenvalues. To do this we need to find the roots of the polynomial \(f(t)\)
	<div>
	$$
	\begin{align*}
	\det (A - tI_3) &amp;= \det
	\begin{pmatrix} 
	-t &amp; 0 &amp; 1 \\
	0 &amp; 2-t &amp; 0 \\
	-2 &amp; 0 &amp; 3-t
	\end{pmatrix} \\
	&amp;= (-1)^{2+2} \det 
	\begin{pmatrix} 
	-t &amp; 1 \\
	-2 &amp; 3-t
	\end{pmatrix} \\
	&amp;= (2-t)((-t)(3-t) + 2) \\
	&amp;= (2-t)(t^2 -3t + 2) \\
	&amp;= (2-t)(t-1)(t-2) \\
	&amp;= -(t-2)^2(t-1)
	\end{align*}
	$$
	</div>
	The solutions are \(\lambda_1 = 1\) and \(\lambda_2 = 2\). 
	</li>
	<!-----------------2------------------>
	<li>
	We know from the example of the previous lecture that for \(\lambda_1 = 1\), the eigenspace consists of the vectors \(E_{\lambda_1} = span\{ (1,0,1)\}\). For \(\lambda_2 = 2\), we need again to put \(A - 2I_3\) in row echelon form to find the eigenvectors.
	<div>
	$$
	\begin{align*}
	A - 2I_3 = 
	\begin{pmatrix} 
	-2 &amp; 0 &amp; 1 \\
	0 &amp; 0 &amp; 0 \\
	-2 &amp; 0 &amp; 1
	\end{pmatrix} 
	&amp;R_3 \rightarrow -R_1 + R_3
	\begin{pmatrix} 
	-2 &amp; 0 &amp; 1 \\
	0 &amp; 0 &amp; 0 \\
	0 &amp; 0 &amp; 0
	\end{pmatrix}
	\\
	\begin{pmatrix} 
	-2 &amp; 0 &amp; 1 \\
	0 &amp; 0 &amp; 0 \\
	0 &amp; 0 &amp; 0
	\end{pmatrix} 
	&amp;R_1 \rightarrow -\frac{1}{2}R_1
	\begin{pmatrix} 
	1 &amp; 0 &amp; -\frac{1}{2} \\
	0 &amp; 0 &amp; 0 \\
	0 &amp; 0 &amp; 0
	\end{pmatrix}
	\end{align*}
	$$
	</div>
	Therefore, 
	<div>
	$$
	\begin{align*}
	N(A - 2I_3) &amp;= \{(x_1, x_2, x_3) \ | \ x_1 = \frac{1}{2}x_3\} \\
	        &amp;= \{(\frac{1}{2}t_2, t_1, t_2) \ | \ t_1, t_2 \in \mathbf{R}\} \\
	        &amp;= span\{(0,1,0), (\frac{1}{2},0,1)\}
	\end{align*}	
	$$
	</div>
</li>
	<!-----------------3------------------>
	<li>Next, we need to determine if \(\{(1,0,1),(0,1,0),(\frac{1}{2},0,1)\}\) is a basis for \(\mathbf{R}^3\). This means we want to know if they are linearly independent. We can do by seeing if 
		<div>
		$$
		\begin{align*}
		a(1,0,1) + b(0,1,0) + c(\frac{1}{2}, 0,1) = 0
		\end{align*}	
		$$
		</div>
		has only the zero vector as the solution. This translate to the following system of equations.
		<div>
		$$
		\begin{align*}
		a + \frac{1}{2}c &amp;= 0 \\
		b &amp;= 0 \\
		a + \frac{1}{2}c &amp;= 0
		\end{align*}	
		$$
		</div>
		We can put this in a matrix and reduce it to row echelon form.
		<div>
		$$
		\begin{align*}
		\begin{pmatrix} 
		1 &amp; 0 &amp; \frac{1}{2} \\
		0 &amp; 1 &amp; 0 \\
		1 &amp; 0 &amp; 1
		\end{pmatrix} 
		&amp;R_3 \rightarrow -R_1 + R_3
		\begin{pmatrix} 
		1 &amp; 0 &amp; \frac{1}{2} \\
		0 &amp; 1 &amp; 0 \\
		0 &amp; 0 &amp; \frac{1}{2}
		\end{pmatrix}
		\end{align*}
		$$
		</div>
		From this we see that the vectors are linearly independent and \(\beta\) is therefore a basis.
</li>
	<!-----------------4------------------>
<li> The last step is to compute the change of coordinate matrix \(Q = [I]_{\beta}^{\alpha}\). 
	This matrix is easy to compute. Taking each vector from the basis \(\beta\) and expressing it as a linear combination of the standard basis \(\alpha\) will just give us the same vector back. Therefore,
	<div>
	$$
	\begin{align*}
	Q = 
	\begin{pmatrix} 
	1 &amp; 0 &amp; \frac{1}{2} \\
	0 &amp; 1 &amp; 0 \\
	1 &amp; 0 &amp; 1
	\end{pmatrix} 
	\end{align*}
	$$
	</div>
	We also need to compute \(Q^{-1}\) but we can do saw using the usual method (row reduced echelon form with the identity matrix on the right). 
	<div>
	$$
	\begin{align*}
	Q^{-1} = 
	\begin{pmatrix} 
	2 &amp; 0 &amp; -1 \\
	0 &amp; 1 &amp; 0 \\
	-2 &amp; 0 &amp; 2
	\end{pmatrix} 
	\end{align*}
	$$
	</div>
</li>
</ol>
<p>Therefore, we finally have</p>
<div>
$$
\begin{align*}
B = [L_A]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^n}]_{\alpha}^{\beta}[L_A]_{\alpha}^{\alpha}[I_{\mathbf{R}^n}]_{\beta}^{\alpha}
\\
&amp;= Q^{-1}AQ \\
&amp;= 
\begin{pmatrix} 
2 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix} 
\begin{pmatrix} 
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0 \\
-2 &amp; 0 &amp; 3
\end{pmatrix}
\begin{pmatrix} 
1 &amp; 0 &amp; \frac{1}{2} \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1
\end{pmatrix}\\
&amp;=
\begin{pmatrix} 
1 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>Remarks:
<br />
Remark 1: We knew \(B\) already. We knew what it would look like since the diagonal entries should be the eigenvalues. 
<br />
<br />
Remark 2: We can solve for \(A\) to get this really nice factorization.</p>
<div>
$$
\begin{align*}
A^k
&amp;=
Q
\begin{pmatrix} 
1 &amp; 0 &amp; 0 \\
0 &amp; 2^k &amp; 0 \\
0 &amp; 0 &amp; 2^k
\end{pmatrix}
Q^{-1}
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors of distinct eigenvalues are Linearly Independent</b></h4>
<p>Question: is the strategy of combining bases of various eigenspaces \(E_{\lambda}\)’s to form larger linearly independent subsets always work or useful?
<br />
<br />
The answer is yes and is demonstrated by the next lemma,
<br /></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
Suppose \(\lambda_1, \lambda_2\) are distinct eigenvalues of \(A\). If \(w \neq \bar{0}\) is in \(E_{\lambda_2}\), then it is not in \(E_{\lambda_1}\).
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Let \(\beta = \{v_1,...,v_k\}\) be a basis of \(E_{\lambda_1}\) consisting of eigenvectors. Suppose for the sake of contradiction that \(w \neq 0 \in E_{\lambda_1}\) for some \(w\in E_{\lambda_2}\). Since \(w\) is in the eigenspace of \(\lambda_1\), this means that \(w\) is in the span of of the basis \(\beta\) by the definition of a basis. Therefore, we can write \(w\) can be written as a linear combination of the vectors in \(\beta\) for some unique scalars \(a_1, a_2, ...,a_k\). (unique because it’s a basis)</p>
<div>
$$
\begin{align*}
w &amp;= a_1v_1 + ... + a_kv_k \\
Aw &amp;= a_1Av_1 + ... + a_kAv_k \\
\lambda_2w &amp;= a_1\lambda_1 v_1 + ... + a_k\lambda_1 v_k \text{ (because $Av = \lambda v$ for eigenvectors)} \\
w &amp;= \frac{\lambda_1}{\lambda_2} a_1v_1 + ... + \frac{\lambda_1}{\lambda_2} a_k v_k \\
\end{align*}
$$
</div>
<p>But since \(a_1,...,a_k\) are unique scalars then we must have \(\lambda_1 = \lambda_2\). But this is a contradiction since \(\lambda_1 \neq \lambda_2. \ \blacksquare\)
<br />
<br />
This lemma implies that the eigenvectors corresponding to distinct eigenvalues always form linearly independent sets when combined. The following corollary states exactly this.
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
If \(\lambda_1 \neq \lambda_2\) and \(\beta_1, \beta_2\) are basis for \(E_{\lambda_1}\) and \(E_{\lambda_2}\) respectively, then \(\beta_1 \cup \beta_2\) is linearly independent. 
</div>
<p><br />
This means that we didn’t need to do step 3 from our algorithm to check that our eigenvectors are linearly independent. They will be! The proof for the corollary is a homework question!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Is Every Matrix Diagonalizable?</b></h4>
<p>The answer is no! For some matrices, we won’t be able to find \(n\) linearly independent eigenvectors. One test that we did last time is to check the characteristic polynomial’s roots or in other words, solve \(\det(A - tI_2) = 0\).</p>
<div>
$$
\begin{align*}
\det(A - tI_2) = 
\det
\begin{pmatrix} 
-t &amp; -1 \\
1 &amp; -t \\
\end{pmatrix}
= t^2 + 1
\end{align*}
$$
</div>
<p>This polynomial has no real roots and so we don’t have eigenvalues. Is the only test? No, we will develop a better test for this.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We saw in the previous lecture that \(A \in M_{n \times n}\) is diagonalizable if there is a basis \(\beta = \{v_1, v_2,...,v_n\}\) of \(\mathbf{R}^n\) such that $$ \begin{align*} [L_A]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix} \end{align*} $$ Where each vector of this matrix is \(Av_j = \lambda_j v_j\). \(v_j\) is an eigenvector and \(\lambda_j\) is called an eigenvalue. Let the standard basis of \(\mathbf{R}^n\) be \(\alpha\), then we know that \([L_A]_{\alpha}^{\alpha} = A\). We also know that we can find a change of coordinates matrix such that $$ \begin{align*} [L_A]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^n}]_{\alpha}^{\beta}[L_A]_{\alpha}^{\alpha}[I_{\mathbf{R}^n}]_{\beta}^{\alpha} \\ &amp;= Q^{-1}AQ. \end{align*} $$ So now to diagonalize a matrix, we need to find these eigenvectors and eigenvalues that satisfy \(Av_j = \lambda_jv_j\) for \(j = 1,...,n\) and \(v \neq \bar{0}\). Finding such a basis is equivalent to saying that \(A\) is diagonalizable if and only if there is a basis \(\beta\) consisting of eigenvectors. In fact, to diagonalize \(A\), we need \(n\) linearly independent eigenvectors of \(A\). Given an eigenvalue, it is easy to find the eigenvectors. \(E_{\lambda} = N(A - \lambda I_n) = \{ \text{ eigenvectors of $A$ with eigenvalue $\lambda$} \}\cup \{\bar{0}\} \) Given \(E_{\lambda} = N(A - \lambda I_n)\), we can find a basis of independent eigenvectors. The number of eigenvectors is the dimension of \(E_{\lambda}\). This is all good but it required us to know the eigenvalues. To find the eigenvalues, we discovered last time that the eigenvalues of \(A\) are the real roots of the characteristic polynomial \(f(t) = \det(A - tI_n)\). This is great and gives up the following plan to diagonalize \(A\): Find all the eigenvalues of \(A\), \(\lambda_1,...,\lambda_k\). Find a basis for \(E_{\lambda_j}\). Determine if these bases combine to form a basis \(\beta\) for \(\mathbf{R}^n\) Compute \(Q, Q^{-1}\) etc. Example Diagonalize \(\begin{align*} A = \begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 3 \end{pmatrix} \end{align*}\) If possible. Following the above plan: Find all the eigenvalues. To do this we need to find the roots of the polynomial \(f(t)\) $$ \begin{align*} \det (A - tI_3) &amp;= \det \begin{pmatrix} -t &amp; 0 &amp; 1 \\ 0 &amp; 2-t &amp; 0 \\ -2 &amp; 0 &amp; 3-t \end{pmatrix} \\ &amp;= (-1)^{2+2} \det \begin{pmatrix} -t &amp; 1 \\ -2 &amp; 3-t \end{pmatrix} \\ &amp;= (2-t)((-t)(3-t) + 2) \\ &amp;= (2-t)(t^2 -3t + 2) \\ &amp;= (2-t)(t-1)(t-2) \\ &amp;= -(t-2)^2(t-1) \end{align*} $$ The solutions are \(\lambda_1 = 1\) and \(\lambda_2 = 2\). We know from the example of the previous lecture that for \(\lambda_1 = 1\), the eigenspace consists of the vectors \(E_{\lambda_1} = span\{ (1,0,1)\}\). For \(\lambda_2 = 2\), we need again to put \(A - 2I_3\) in row echelon form to find the eigenvectors. $$ \begin{align*} A - 2I_3 = \begin{pmatrix} -2 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -2 &amp; 0 &amp; 1 \end{pmatrix} &amp;R_3 \rightarrow -R_1 + R_3 \begin{pmatrix} -2 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \\ \begin{pmatrix} -2 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} &amp;R_1 \rightarrow -\frac{1}{2}R_1 \begin{pmatrix} 1 &amp; 0 &amp; -\frac{1}{2} \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ Therefore, $$ \begin{align*} N(A - 2I_3) &amp;= \{(x_1, x_2, x_3) \ | \ x_1 = \frac{1}{2}x_3\} \\ &amp;= \{(\frac{1}{2}t_2, t_1, t_2) \ | \ t_1, t_2 \in \mathbf{R}\} \\ &amp;= span\{(0,1,0), (\frac{1}{2},0,1)\} \end{align*} $$ Next, we need to determine if \(\{(1,0,1),(0,1,0),(\frac{1}{2},0,1)\}\) is a basis for \(\mathbf{R}^3\). This means we want to know if they are linearly independent. We can do by seeing if $$ \begin{align*} a(1,0,1) + b(0,1,0) + c(\frac{1}{2}, 0,1) = 0 \end{align*} $$ has only the zero vector as the solution. This translate to the following system of equations. $$ \begin{align*} a + \frac{1}{2}c &amp;= 0 \\ b &amp;= 0 \\ a + \frac{1}{2}c &amp;= 0 \end{align*} $$ We can put this in a matrix and reduce it to row echelon form. $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; \frac{1}{2} \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix} &amp;R_3 \rightarrow -R_1 + R_3 \begin{pmatrix} 1 &amp; 0 &amp; \frac{1}{2} \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; \frac{1}{2} \end{pmatrix} \end{align*} $$ From this we see that the vectors are linearly independent and \(\beta\) is therefore a basis. The last step is to compute the change of coordinate matrix \(Q = [I]_{\beta}^{\alpha}\). This matrix is easy to compute. Taking each vector from the basis \(\beta\) and expressing it as a linear combination of the standard basis \(\alpha\) will just give us the same vector back. Therefore, $$ \begin{align*} Q = \begin{pmatrix} 1 &amp; 0 &amp; \frac{1}{2} \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix} \end{align*} $$ We also need to compute \(Q^{-1}\) but we can do saw using the usual method (row reduced echelon form with the identity matrix on the right). $$ \begin{align*} Q^{-1} = \begin{pmatrix} 2 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp; 0 \\ -2 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ Therefore, we finally have $$ \begin{align*} B = [L_A]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^n}]_{\alpha}^{\beta}[L_A]_{\alpha}^{\alpha}[I_{\mathbf{R}^n}]_{\beta}^{\alpha} \\ &amp;= Q^{-1}AQ \\ &amp;= \begin{pmatrix} 2 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp; 0 \\ -2 &amp; 0 &amp; 2 \end{pmatrix} \begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 3 \end{pmatrix} \begin{pmatrix} 1 &amp; 0 &amp; \frac{1}{2} \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{pmatrix}\\ &amp;= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ Remarks: Remark 1: We knew \(B\) already. We knew what it would look like since the diagonal entries should be the eigenvalues. Remark 2: We can solve for \(A\) to get this really nice factorization. $$ \begin{align*} A^k &amp;= Q \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 2^k &amp; 0 \\ 0 &amp; 0 &amp; 2^k \end{pmatrix} Q^{-1} \end{align*} $$ Eigenvectors of distinct eigenvalues are Linearly Independent Question: is the strategy of combining bases of various eigenspaces \(E_{\lambda}\)’s to form larger linearly independent subsets always work or useful? The answer is yes and is demonstrated by the next lemma, Lemma Suppose \(\lambda_1, \lambda_2\) are distinct eigenvalues of \(A\). If \(w \neq \bar{0}\) is in \(E_{\lambda_2}\), then it is not in \(E_{\lambda_1}\). Proof: Let \(\beta = \{v_1,...,v_k\}\) be a basis of \(E_{\lambda_1}\) consisting of eigenvectors. Suppose for the sake of contradiction that \(w \neq 0 \in E_{\lambda_1}\) for some \(w\in E_{\lambda_2}\). Since \(w\) is in the eigenspace of \(\lambda_1\), this means that \(w\) is in the span of of the basis \(\beta\) by the definition of a basis. Therefore, we can write \(w\) can be written as a linear combination of the vectors in \(\beta\) for some unique scalars \(a_1, a_2, ...,a_k\). (unique because it’s a basis) $$ \begin{align*} w &amp;= a_1v_1 + ... + a_kv_k \\ Aw &amp;= a_1Av_1 + ... + a_kAv_k \\ \lambda_2w &amp;= a_1\lambda_1 v_1 + ... + a_k\lambda_1 v_k \text{ (because $Av = \lambda v$ for eigenvectors)} \\ w &amp;= \frac{\lambda_1}{\lambda_2} a_1v_1 + ... + \frac{\lambda_1}{\lambda_2} a_k v_k \\ \end{align*} $$ But since \(a_1,...,a_k\) are unique scalars then we must have \(\lambda_1 = \lambda_2\). But this is a contradiction since \(\lambda_1 \neq \lambda_2. \ \blacksquare\) This lemma implies that the eigenvectors corresponding to distinct eigenvalues always form linearly independent sets when combined. The following corollary states exactly this. Corollary If \(\lambda_1 \neq \lambda_2\) and \(\beta_1, \beta_2\) are basis for \(E_{\lambda_1}\) and \(E_{\lambda_2}\) respectively, then \(\beta_1 \cup \beta_2\) is linearly independent. This means that we didn’t need to do step 3 from our algorithm to check that our eigenvectors are linearly independent. They will be! The proof for the corollary is a homework question! Is Every Matrix Diagonalizable? The answer is no! For some matrices, we won’t be able to find \(n\) linearly independent eigenvectors. One test that we did last time is to check the characteristic polynomial’s roots or in other words, solve \(\det(A - tI_2) = 0\). $$ \begin{align*} \det(A - tI_2) = \det \begin{pmatrix} -t &amp; -1 \\ 1 &amp; -t \\ \end{pmatrix} = t^2 + 1 \end{align*} $$ This polynomial has no real roots and so we don’t have eigenvalues. Is the only test? No, we will develop a better test for this. References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 23: Eigenvalues and Diagonalizability</title><link href="http://localhost:4000/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html" rel="alternate" type="text/html" title="Lecture 23: Eigenvalues and Diagonalizability" /><published>2024-08-20T01:01:36-07:00</published><updated>2024-08-20T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>Motivation</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonal if all entries off diagonal are 0. \(A_{ij} = 0\) if \(i \neq j\).
</div>
<p><br />
All computations involving diagonal matrices are simple. If \(A, B\) are diagonal, then</p>
<div>
$$
\begin{align*}
\det(A) = A_{11}...A_{nn}
\end{align*}
$$
</div>
<p>Similarly, matrix multiplication of diagonal matrices is simple. The \(AB_{ij}\)th entry is</p>
<div>
$$
\begin{align*}
AB_{ij} &amp;= \begin{cases} A_{ij}B_{ij} \quad \text{if } i = j \\ 0\phantom{A_{ij}B} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This can be generalized to computing \((A)^k\) where the \(ij\) entry is</p>
<div>
$$
\begin{align*}
(A^k)_{ij} &amp;= \begin{cases} (A_{ij})^k \quad \text{if } i = j \\ 0\phantom{(A_{ij})} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This leads to the question of whether we can transform any matrix to a diagonal matrix so we can perform these computations easily. In the next definition we formalize this.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>When is \(A\) Diagonalizable?</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \in V \rightarrow V\) is diagonalizable if there is a basis \(\beta = \{v_1,...,v_n\}\) of \(V\) such that 
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
Two questions arises from this definition:
<br />
Questions 1: Does such a basis exist?
<br />
Question 2: If it exists, how can we compute it?
<br />
<br />
Suppose we have a basis \(\beta = \{v_1,...,v_n\}\) such that</p>
<div>
$$
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
$$
</div>
<p><br />
This is equivalent to</p>
<ul style="list-style: none;">
	<li> \(\leftrightarrow\) This matrix above by defintion is 
<div>
$$
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
[T(v_1)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta}
\end{pmatrix}
$$
</div>
	</li>
    <li>\(\leftrightarrow\) We can factor \(\lambda\) to see that \([T(v_j)]_{\beta} =  \lambda_j \begin{pmatrix} 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \end{pmatrix}^t \). But this is just the \(j\)th vector of the standard basis so we can write it as \(\lambda_j[v_j]_{\beta}\). [TODO: why?].
	</li>
    <li>\(\leftrightarrow\) We can take the constant \(\lambda_j\) inside since \([ \quad ]_{\beta}\) is a linear map to see that
		<div>
		$$
		 [T(v_j)]_{\beta} = \lambda_j[v_j]_{\beta} = [\lambda_jv_j]_{\beta}  \text{ for } j = 1,...,n.
		$$
		</div>
	</li>
	<li>\(\leftrightarrow \). Because both sides of the equation are written with respect to basis \(\beta\), we can take it out and write
		<div>
		$$
		 T(v_j) = \lambda_jv_j \text { for } j = 1,...,n \text { and } \lambda_1,...,\lambda_n \in \mathbf{R}
		$$
		</div>
	</li>
	<li>\(\leftrightarrow T(v) = \lambda v\).
	</li>
	<li>\(\leftrightarrow T(v) = \lambda I_V(v)\). (Since the identity matrix does nothing)
	</li>
	<li>\(\leftrightarrow T(v) - \lambda I_V(v) = \bar{0}_V\).
	</li>
	<li>\(\leftrightarrow (T - \lambda I_V)(v) = \bar{0}_V\).
	</li>
</ul>
<p>The left hand side is a family of linear maps parameterized by \(\lambda\). The solution to this is the set of all the non-zero vectors \(v\) of the nullspace. We don’t care about the zero solution since we want to build a basis.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of a Linear Transformation</b></h4>
<p>So now we’ve seen that finding such a basis boils down to finding all vectors such that \(T(v) = \lambda v\). These vectors are called eigenvectors. More formally,</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(\ T \in V \rightarrow V\) is a \(v \neq \bar{0}_V\) such that
$$
\begin{align*}
T(v) = \lambda v
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
And the \(\lambda\)’s are called,
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(\ T \in V \rightarrow V\) if \(\exists v \neq \bar{0}_V\) such that \(T(v) = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
We can now restate the previous theorem as the following,
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \in V \rightarrow V\) is diagonalizable if and only if there is a basis \(\beta = \{v_1,...,v_n\}\) consisting of eigenvectors.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
As we’ve seen before, finding a basis \(\beta\) where</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}, \lambda_1,...,\lambda_n \text{ eigenvalues}
\end{align*}
$$
</div>
<p>is equivalent to find the set of eigenvectors that satisfy \(T(v) = \lambda v\). This is all great. But now instead of looking at general linear maps that satisfy these conditions, let’s turn our focus on matrices.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of Matrices</b></h4>
<p>When is a matrix diagonalizable? and what are the eigenvectors and eigenvalues of a given matrix \(A\)?
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonalizable if \(L_A\) is diagonalizable.
</div>
<p><br />
This is equivalent to “There is a \(Q \in M_{n \times n}\) such that \(Q^{-1}AQ\) is diagonal”.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(A \in M_{n \times n}\) is a \(v \neq \bar{0} \in \mathbf{R}^n\) such that \(Av = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and finally,
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(A \in M_{n \times n}\) if \(\exists v \neq \bar{0}_V\) such that \(Av = \lambda v\).
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Eigenvectors</b></h4>
<p>Okay now that we’ve narrowed down the discussion to matrices, how do we actually find these eigenvectors of \(A\)? Set the nullspace of \(A\) to \(N(A) = N(L_A)\). Next we will need the following lemma
<br /></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
\(v \in \mathbf{R}^n\) is an eigenvector of \(A\) with eigenvalue \(\lambda\) if and only if \(v \in N(A - \lambda I_n)\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(v\) is an eigenvector of (A). By definition this means that \(Av = \lambda v\). We can re-write this as,</p>
<div>
$$
\begin{align*}
Av &amp;= \lambda I_n v \\
Av - \lambda I_n v &amp;= \bar{0}
\end{align*}
$$
</div>
<p>But this precisely means that \(v \in N(A - \lambda I_n). \ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find all the eigen values of</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix} 
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0 \\
-2 &amp; 0 &amp; 3
\end{pmatrix}
\end{align*}
$$
</div>
<p>with eigenvalue 1.
<br />
<br />
By the lemma, we want all vectors in the null space of \(A - \lambda I_n\).</p>
<div>
$$
\begin{align*}
A - (1)I_n = 
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>We’ll put this matrix in row echelon form.</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
R_3 \rightarrow -2R_1 + R_3, R_1 \leftrightarrow -1R_1
\begin{pmatrix} 
1 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>From this we see that the null space consists of vectors of the form</p>
<div>
$$
\begin{align*}
&amp;= \{(x_1, x_2, x_3) \ | \ x_3 = t, x_1 = t, x_2 = 0\}. \\
&amp;= \{(t, 0, t) \ | \ t \in \mathbf{R} \} \\
&amp;= span\{ (1,0,1) \}
\end{align*}
$$
</div>
<p>This is easy because we are given the eigenvalue. But typically, we also need to find the eigenvalues too!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenspace</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
If \(\lambda\) is an eigenvalue of \(A\), then the eigenspace of \(A\) corresponding to \(\lambda\) is
$$
\begin{align*}
E_{\lambda} &amp;= N(A - \lambda I_n) \\
&amp;= \{ \text{eigenvectors for } \lambda \} \cup \{\bar{0}\}
\end{align*}
$$
</div>
<p><br />
[TODO: What is the difference between the eigenspace and the nullspace?]
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Eigenvalues</b></h4>
<p>Again, if we’re given an eigenvalue, then finding the eigenspace or eigenvectors is easy and simple. We’re just solving a system of linear equations like we did for finding the nullspace. The question is how can we find the eigenvalues? for this we need the following theorem</p>
<div class="purdiv">
Theorem 5.2
</div>
<div class="purbdiv">
\(\lambda\) is an eigenvalue of \(A\) if and only if \(\det(A - \lambda I_n) = 0\).
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
\(\lambda\) is an eigenvalue is equivalent to</p>
<div>
$$
\begin{align*}
&amp;\leftrightarrow \exists v \neq \bar{0} \text{ such that } Av = \lambda v \\
&amp;\leftrightarrow (A - \lambda I_n)(v) = \bar{0} \text{ for } v \neq 0\\
&amp;\leftrightarrow N(A - \lambda I_n)(v) \neq \bar{0} \\
&amp;\leftrightarrow A - \lambda I_n \text{ is not 1-1} \\
&amp;\leftrightarrow A - \lambda I_n \text{ is not invertible} \\
&amp;\leftrightarrow \det(A - \lambda I_n) = 0. \\
\end{align*}
$$
</div>
<p><br />
So we see now that we have the necessary and sufficient conditions for \(\lambda\) to be an eigenvalue of \(A\). So what’s next? \(A\) is given to us in this equation but we need a \(\lambda\) that would make the equation \(\det(A - \lambda I_n)\) equal to zero. Let’s look at the following definition
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(f(t) = \det(A - tI_n)\) is the characteristic polynomial of \(A\).
</div>
<p><br />
What is this saying? we can interpret the right hand side as a function. We’re given \(A\). We know the identity matrix. So the unknown is \(t\). So inside the determinant, we’ll have a matrix with entries that depend on \(A\) and \(t\). We know the determinant is a map / inductive formula. So this expression when expanded as a whole is some number that depends on it. In fact it shouldn’t be surprising that \(f(t)\) is a polynomial of degree \(n\) (FACT). 
<br />
<br />
Based on this, we can rephrase the previous theorem as the following corollary,
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
\(\lambda\) is an eigenvalue of \(A\) if and only if \(f(t) = \det(A - \lambda I_n) = 0\).
</div>
<p><br />
So eigenvalues are the roots of this polynomial so it’s not always easy to do. How many roots? We know the degree of \(f(t)\) is at most \(n\). Therefore,
<br /></p>
<div class="purdiv">
Corollary 2 (Theorem 5.3(b))
</div>
<div class="purbdiv">
\(A\) has at most \(n\) eigenvalues.
</div>
<p><br />
So we know at least that there can only be \(n\) roots/eigenvalues at most.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find the eigenvalues of
\(\begin{align*}
A =
\begin{pmatrix} 
0 &amp; -1 \\
1 &amp; 0
\end{pmatrix}.
\end{align*}\)
<br />
<br />
Let’s write the characteristic polynomial and find its roots so</p>
<div>
$$
\begin{align*}
f(t) = \det(A - tI_n) &amp;= 0 \\
\det
\begin{pmatrix} 
-t &amp; -1 \\
1 &amp; -t
\end{pmatrix} &amp;= 0 \\
t^2 + 1 &amp;= 0 \\
t^2 &amp;= -1 \\
\end{align*}
$$
</div>
<p>This polynomial has no real roots! and so the matrix \(A\) has no eigenvalues.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Motivation Definition \(A \in M_{n \times n}\) is diagonal if all entries off diagonal are 0. \(A_{ij} = 0\) if \(i \neq j\). All computations involving diagonal matrices are simple. If \(A, B\) are diagonal, then $$ \begin{align*} \det(A) = A_{11}...A_{nn} \end{align*} $$ Similarly, matrix multiplication of diagonal matrices is simple. The \(AB_{ij}\)th entry is $$ \begin{align*} AB_{ij} &amp;= \begin{cases} A_{ij}B_{ij} \quad \text{if } i = j \\ 0\phantom{A_{ij}B} \quad \text{if } i \neq j \end{cases} \end{align*} $$ This can be generalized to computing \((A)^k\) where the \(ij\) entry is $$ \begin{align*} (A^k)_{ij} &amp;= \begin{cases} (A_{ij})^k \quad \text{if } i = j \\ 0\phantom{(A_{ij})} \quad \text{if } i \neq j \end{cases} \end{align*} $$ This leads to the question of whether we can transform any matrix to a diagonal matrix so we can perform these computations easily. In the next definition we formalize this. When is \(A\) Diagonalizable? Theorem \(T \in V \rightarrow V\) is diagonalizable if there is a basis \(\beta = \{v_1,...,v_n\}\) of \(V\) such that $$ \begin{align*} [T]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix}. \end{align*} $$ Two questions arises from this definition: Questions 1: Does such a basis exist? Question 2: If it exists, how can we compute it? Suppose we have a basis \(\beta = \{v_1,...,v_n\}\) such that $$ [T]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix} $$ This is equivalent to \(\leftrightarrow\) This matrix above by defintion is $$ [T]_{\beta}^{\beta} = \begin{pmatrix} [T(v_1)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta} \end{pmatrix} $$ \(\leftrightarrow\) We can factor \(\lambda\) to see that \([T(v_j)]_{\beta} = \lambda_j \begin{pmatrix} 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \end{pmatrix}^t \). But this is just the \(j\)th vector of the standard basis so we can write it as \(\lambda_j[v_j]_{\beta}\). [TODO: why?]. \(\leftrightarrow\) We can take the constant \(\lambda_j\) inside since \([ \quad ]_{\beta}\) is a linear map to see that $$ [T(v_j)]_{\beta} = \lambda_j[v_j]_{\beta} = [\lambda_jv_j]_{\beta} \text{ for } j = 1,...,n. $$ \(\leftrightarrow \). Because both sides of the equation are written with respect to basis \(\beta\), we can take it out and write $$ T(v_j) = \lambda_jv_j \text { for } j = 1,...,n \text { and } \lambda_1,...,\lambda_n \in \mathbf{R} $$ \(\leftrightarrow T(v) = \lambda v\). \(\leftrightarrow T(v) = \lambda I_V(v)\). (Since the identity matrix does nothing) \(\leftrightarrow T(v) - \lambda I_V(v) = \bar{0}_V\). \(\leftrightarrow (T - \lambda I_V)(v) = \bar{0}_V\). The left hand side is a family of linear maps parameterized by \(\lambda\). The solution to this is the set of all the non-zero vectors \(v\) of the nullspace. We don’t care about the zero solution since we want to build a basis. Eigenvectors and Eigenvalues of a Linear Transformation So now we’ve seen that finding such a basis boils down to finding all vectors such that \(T(v) = \lambda v\). These vectors are called eigenvectors. More formally,]]></summary></entry></feed>