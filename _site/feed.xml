<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-11T14:38:55-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 11: Dihedral Groups</title><link href="http://localhost:4000/jekyll/update/2025/02/03/math417-11-dihedral-groups.html" rel="alternate" type="text/html" title="Lecture 11: Dihedral Groups" /><published>2025-02-03T00:01:36-08:00</published><updated>2025-02-03T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/03/math417-11-dihedral-groups</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/03/math417-11-dihedral-groups.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
The Dihedral group is the group of rotational symmetries of a regular n-gon in \(\mathbf{R}^3\). It is a subgroup of the special orthogonal matrices, \(SO(3)\).
</div>
<p><br />
<br />
Before discussing the Dihedral group, and given a regular polygon, we can inscribe it inside a circle. Notice here that the symmetries of the polygon are a subset of the symmetries of the circle/disk. So let’s start by analyzing these symmetries first
<br />
<br />
<!------------------------------------------------------------------------------></p>
<h4><b>Symmetries of the Disk</b></h4>
<p>Let \(D \leq SO(3)\) be the rotational symmetries of the unit disk: \(\{(x,y,0) \ | \ x^2 + y^2 = 1 \}\). There are two types of rotations</p>
<ol>
	<li>\(A(e_3) = e_3\). This is when we rotation the disk around the \(z-\)axis. We're fixing the \(z-\)axis and spinning the disk. So the \(z-\)axis remains the same. These are called Rotations (of the disk).</li>
	<li>\(A(e_3) = -e_3\). These are Flips. Take the disk flip it to face the other way.</li>
</ol>
<p><b>Rotations</b>
<br />
A rotation around an arbitrary angle which is \(r_{\theta} = Rot_{e_3}(\theta)\). It is always a symmetry around the unit disk. Note here that \(r_{\theta + 2\pi n} = r_{\theta}\) so for the complete list, we need to specify \(r_{\theta}\) where \(\theta \in [0, 2\pi)]\). Note also that \(r_{\theta}^{-1} = r_{-\theta}\).
<br />
<br />
<b>Flips</b>
<br />
Here the \(z-\)axis will get reversed since we’re flipping over it (so \(e_3\) will now be \(-e3\) facing the opposite the direction). So this means that the rotation axis is parallel to the \(xy\) plane. So the rotation vector is \(u_\theta = (\cos \theta)e_1 + (\sin \theta)e_2 + 0e_3\). We’re calling this kind of rotation \(j_\theta\). Note here that \(j_{\theta + \pi n} = j_{\theta}\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The Dihedral group is the group of rotational symmetries of a regular n-gon in \(\mathbf{R}^3\). It is a subgroup of the special orthogonal matrices, \(SO(3)\). Before discussing the Dihedral group, and given a regular polygon, we can inscribe it inside a circle. Notice here that the symmetries of the polygon are a subset of the symmetries of the circle/disk. So let’s start by analyzing these symmetries first Symmetries of the Disk Let \(D \leq SO(3)\) be the rotational symmetries of the unit disk: \(\{(x,y,0) \ | \ x^2 + y^2 = 1 \}\). There are two types of rotations \(A(e_3) = e_3\). This is when we rotation the disk around the \(z-\)axis. We're fixing the \(z-\)axis and spinning the disk. So the \(z-\)axis remains the same. These are called Rotations (of the disk). \(A(e_3) = -e_3\). These are Flips. Take the disk flip it to face the other way. Rotations A rotation around an arbitrary angle which is \(r_{\theta} = Rot_{e_3}(\theta)\). It is always a symmetry around the unit disk. Note here that \(r_{\theta + 2\pi n} = r_{\theta}\) so for the complete list, we need to specify \(r_{\theta}\) where \(\theta \in [0, 2\pi)]\). Note also that \(r_{\theta}^{-1} = r_{-\theta}\). Flips Here the \(z-\)axis will get reversed since we’re flipping over it (so \(e_3\) will now be \(-e3\) facing the opposite the direction). So this means that the rotation axis is parallel to the \(xy\) plane. So the rotation vector is \(u_\theta = (\cos \theta)e_1 + (\sin \theta)e_2 + 0e_3\). We’re calling this kind of rotation \(j_\theta\). Note here that \(j_{\theta + \pi n} = j_{\theta}\). References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 09/10: Subgroups</title><link href="http://localhost:4000/jekyll/update/2025/02/02/math417-09-subgroups.html" rel="alternate" type="text/html" title="Lecture 09/10: Subgroups" /><published>2025-02-02T00:01:36-08:00</published><updated>2025-02-02T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/02/math417-09-subgroups</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/02/math417-09-subgroups.html"><![CDATA[<div class="mintheaderdiv">
Definition 2.2.1
</div>
<div class="mintbodydiv">
A non-empty subset \(H\) of a group \(G\) is called a subgroup if \(H\) is itself a group with the group operation inherited from \(G\). We write \(H \leq G\) to indicate that \(H\) is a subgroup of \(G\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
Example:</p>
<ul>
	<li>The group \(SO(3)\), the special orthogonal matrices is a subgroup of \(GL_3(\mathbf{R}\), the group of invertible \(3 \times 3\) matrices. Both of these are under the same operation.</li>
	<li>Non-example: \(\mathbf{Z}_n\): congruence classes modulo \(n\) with addition. We also have \(\phi(n)\): congruence classes with multiplicative inverses. This has the operation multiplication so it's not a subgroup! Even when you define \(\phi(4) = \{[1],[3]\}\) with addition, it will not satisfy the group requirements, so it's not a subgroup.</li>
</ul>
<p>Since we need to check a lot for subgroups. We have a set of necessary conditions.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
The necessary conditions for a subset \(H\) of \(G\) to be a subgroup of \(G\) are:
<ol>
	<li>\(H\) is not empty</li>
	<li>\(H\) is closed under multiplication. That is, for all elements \(h_1\) and \(h_2\) of \(H\), the products \(h_1h_2\) is also an element of \(H\)</li>
	<li>\(H\) is closed under inverses. For all \(h \in H\), the inverse \(h^{-1}\) is an element of \(H\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br /></p>
<ul>
	<li>\(H\) is associative. For any \(a,b,c \in H\), \((ab)c = a(bc)\). We're inheriting the same operation from \(G\) so \(H\) must be associative.</li>
	<li>\(H\) has an identity element: We know \(H\) is non-empty by (1). We also know for any \(h \in H\), \(h^{-1} \in H\) by (3). By (2), \(H\) is closed under multiplication so \(hh^{-1} \in H\) but \(hh^{-1} = e\) so the identity element is in \(H\).  </li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Consider the group \(G = S_4\) (permutations of \(\{1,2,3,4\}\)). The order of this group is \(|G| = 4\). The following are subgroups of \(S_4\):</p>
<ul>
	<li>\(H=\{ \sigma \in S_4 \ | \ \sigma(4) = 4\}\). So this subgroup fixes \(4\) in the permutation. \(|H| = 3! = 6\). In fact, it is permuting only 3 numbers and it is isomorphic to \(S_3\).</li>
	<!---------------------------->
	<li>\(H'=\{e,(1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3)\}\). It's not empty. It has the identity element. Every element is its own inverse. The one thing not too obvious is proving it is closed. This subgroup is isomorphic to the symmetry group of the rectangle. What's an isomorphism in this case? Assign \(A\) to 1, \(B\) to 2, \(C\) to 3, \(D\) to 4. Recall that the symmetries of the rectangle has the symmetries \(r_1, r_2, r_3, e\) where \(r_1\) is rotating around the \(x-\)axis. Observe now that \(r_1\)
		<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec10/rect-r1.png" width="45%" class="center" /></p>
	</li>
	rotates the vertices such that \(A\) and \(B\) switch positions and \(C\) and \(D\) switch positions. This is equivalent to the permutation \((1 \ 2)(3 \ 4)\). Following this, we'll see that \(r_2\) gets mapped to \((1 \ 4)(2 \ 3)\) and \(r_3\) gets mapped to \((1 \ 3)(2 \ 4)\).
	<!---------------------------->
	<li>\(H''=\{e,(1 \ 2 \ 3 \ 4)(4 \ 3 \ 2 \ 1), (1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3), (1 \ 3),(2 \ 4)\}\). This one is isomorphic to \(D_4\) which is the symmetries of the square.</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Generated Subgroups</b></h4>
<p>We begin with the following lemma which states that the intersection of a collection of subgroups is another subgroup.
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma (Book Proposition 2.2.8)
</div>
<div class="yellowbodydiv">
Let \(G\) be a group and let \(H_1, H_2,...,H_n\) be subgroups of \(G\). Then 
	$$
	\begin{align*}
	H = H_1 \cap H_2 \ \cap ... \cap \ H_n
	\end{align*}
	$$
is a subgroup of \(G\). More generally, if \(\{H_{\alpha}\}\) is any collection of subgroups, then \(\cap_{\alpha}\) is a subgroup.
<!------------------------------------------------------------------------------>
</div>
<p><br />
<b>Proof</b>
<br /></p>
<ol>
<li>Every \(H_i\) is a subgroup. Therefore \(e \in H_i\) for each \(i\) and thus \(e \in H\).</li>
<li>For any two elements \(a, b \in H\), we must have \(a, b \in H_i\) for each \(i\). Their product \(ab\) must also be in every \(H_i\) since each \(H_i\) is a subgroup. Therefore \(ab \in H\).</li>
<li>For any \(a \in H\), \(a\) must be in \(H_i\) for each \(i\). Therefore \(a^{-1} \in H_i \ \forall i\) and thus \(a^{-1} \in H\).</li>
</ol>
<p>Since any intersection of subgroups is another subgroups, we can then define a generated subgroups as follow
<br />
<!------------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(G\) be a group and let \(S\) be a subset of \(G\). Then
	$$
	\begin{align*}
	 \langle S \rangle = \bigcap_{\text{All } H_i \leq G \text{ such that } S \subseteq H_i}
	\end{align*}
	$$
\(\langle S \rangle\) is called <b>the subgroup generated by</b> \(S\) and it is the smallest subgroup in \(G\) that contains \(S\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
Observe that</p>
<ul>
<li>\(\langle S \rangle\) is a subgroup by the previous lemma since it is the intersection of subgroups.</li>
<li>\(S \subseteq \langle S \rangle\).</li>
<li>It is the smallest group that contains \(S\). Because for any subgroup \(K \leq G\) such that \(S \subseteq K\), we must have \(\langle S \rangle \leq K\) (why? because if \(S\) is contained in \(K\) and \(K\) is a group, then all the products of the elements of \(S\) are in \(K\). Same for the inverses. So the whole subgroup \(\langle S \rangle\) must also be contained in \(K\).)</li>
</ul>
<p>Additionally from the book:</p>
<ul>
<li>If \(S\) contains a single element, \(S = \{a\}\), then the subgroup generated by \(S\) is denoted by \(\langle a \rangle\). </li>
<li>If \(G = \langle S \rangle\), then \(G\) is generated by \(S\) or \(S\) generates \(G\)</li>
</ul>
<p>Another way to describe \(\langle S \rangle\) is take a bottom up approach to describe the elements.
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Suppose \(G\) is a group and \(S\) is a subset of \(G\). Then \(\langle S \rangle\) is equal to the set of elements in \(G\) of the form
	$$
	\begin{align*}
	 g_1g_2...g_k \quad \text{ where either } g_i \in S \text{ or } g_i^{-1} \in S \text{ for all } i.
	\end{align*}
	$$
</div>
<!------------------------------------------------------------------------------>
<p><br />
In other words, \(\langle S \rangle\) is a subgroup such that that it contains all the possible products \(g_1g_2...g_n\) which are in \(S\) or an inverse to an element of \(S\). The set that contains such products is often called the “set of words in \(S\)” in group theory.
<br />
<br />
<b>Proof</b>
<br />
Let \(T\) be the set of words \(\{e\} \cup \{g_1...g_k\}\) where \(g_i \in S\) or \(g_i^{-1} \in S\) for all \(i = 1,...,k\). We need to show that</p>
<ol>
	<li>\(T\) is a subgroup of \(G\). [This is true because \(T\) has the identity element. For any two word, the product of these is just a longer word which is also in \(T\). It is closed under the inverses because \((g_1g_2...g_k)^{-1} = g_k^{-1}...g_1^{-1}\).]</li>
	<li>\(S \subseteq T\). [This is true because all the words of length 1 make up the set \(S\). It is true by construction.]</li>
	<li>If \(K \leq G\) is a subgroup such that \(S \subseteq K\), then \(T \subseteq K\). [This is true because if \(K\) contains \(S\) then it must contain all the inverses of these words and then also the products of all of these words since it's a group (closed) so it has all of these elements and it contains \(T\) itself.]</li>
</ol>
<p>Condition (1) and (2) imply that \(\langle S \rangle \subseteq T\). Conditions (1) and (3) imply that \(T \subseteq \langle S \rangle\). Therefore, \(T = \langle S \rangle\). \(\ \blacksquare\) We’ll use this proposition a lot. 
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
	<li>\(\langle (1 \ 2) \rangle = \{e, (1 \ 2)\}\). This says that the subgroup generated by \(\{(1 \ 2)\}\) is \(\{e, (1 \ 2)\} = \langle (1 \ 2) \rangle \)</li>
	<li>\(\langle (1 \ 2 \ 3) \rangle = \{e, (1 \ 2 \ 3), (1 \ 3 \ 2)\}\)</li>
	<li>\(S_3 = \langle (1 \ 2), (1 \ 2 \ 3) \rangle\).</li>
	<li>\(S_4 = \langle (1 \ 2 \ 3 \ 4), (2 \ 4) \rangle\). </li>	
</ul>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cyclic Groups and Cyclic Subgroups</b></h4>
<p>We’ll start with the definition
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A group \(G\) is cyclic if \(G = \langle a \rangle\) for some \(a \in G\). Similarly, a subgroup \(H \leq G\) is cyclic if \(H = \langle a \rangle\) for some \(a \in G\). 
</div>
<!----------------------------------------------------------------------------->
<p><br />
In fact, we can describe all the elements in a cyclic subgroup as follows
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book 2.2.9)
</div>
<div class="peachbodydiv">
Let \(a\) be an element of a group \(G\). The subgroup \(\langle a \rangle\) generated by \(a\) is \(\{a^k \ : \ k \in \mathbf{Z}\}\).
</div>
<p><br />
Note here that this subgroup is abelian!
<br />
<!----------------------------------------------------------------------------->
<br />
<b>Proof (book)</b>
<br />
Let \(H = \{a^k \ : \ k \in \mathbf{Z}\}\). We will show that \(\langle a \rangle = H\) as follows:
<br />
<br />
\(\langle a \rangle \subseteq H\): We know that \(G\) is a group and \(H\) is subset of \(G\). We claim that it is a subgroup of \(G\). It is closed under multiplication because for any \(a^k\) and \(a^l\) in \(H\), \(a^{k+l} \in H\). It is closed under inverses because for any \(a^k \in H\), \((a^{k})^{-1} = a^{-k} \in H\). Furthermore, \(H\) contains \(a\) and since \(H\) is a subgroup, then \(H\) contains all the powers of \(a\). Therefore, \(\langle a \rangle \subseteq H\). 
<br />
<br />
\(H \subseteq \langle a \rangle\): We showed that \(H = \{a^k \ : \ k \in \mathbf{Z}\}\) is a subgroup above. It is closed under multiplication and so it contains all powers of \(a\). Therefore, \(H \subseteq \langle a \rangle\). \(\ \blacksquare\) 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose \(G = (\mathbf{Z}, + )\). For any element \(a \in \mathbf{Z}\),</p>
<div>
	$$
	\begin{align*}
	 \langle a \rangle = \{ka \ : \ k \in \mathbf{Z}\} = \mathbf{Z}a.
	\end{align*}
	$$
</div>
<ul>
<li>The set of powers of \(a\) is the set of all multiples of \(a\) since the group operation is addition. So \(a+a+a+a+a\) is the fifth power.</li>

<li>If \(a = 0\), then \(\langle a \rangle = \mathbf{Z}0 = \{0\}\)  is the trivial subgroup. </li>

<li>If \(a = 0\), then \(\mathbf{Z}a\) is infinite. In fact it is isomorphic to \(\mathbf{Z}\) so \(\mathbf{Z}a \approx \mathbf{Z}\). </li>

<li>Note here that \(\mathbf{Z} \neq \mathbf{Z}a\) unless \(a = \pm 1\).</li>

<li>Also note, that \(\langle a \rangle  = \langle -a \rangle\).</li>
</ul>

<p>Now consider \(a, b \in \mathbf{Z}\). Suppose we want to find the subgroup generated by the set of \(\{a,b\}\), In other words, \(\langle a,b \rangle\). This is the set of all words composed of \(a\) and \(b\) and their inverses. In fact is the set of all integer combinations of \(a\) and \(b\)</p>
<div>
	$$
	\begin{align*}
	 \langle a, b \rangle = \{ma + nb \ : \ m,n \in \mathbf{Z}\} = I(a,b)
	\end{align*}
	$$
</div>
<p>This is also a cyclic subgroup because \(\langle a, b \rangle = I(a,b) = \mathbf{Z}d\) where \(d = gcd(a,b)\) So the gcd is the generator of this subgroup. In fact, we will see next that all subgroups of \(\mathbf{Z}\) are cyclic
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cyclic Subgroups in \(\mathbf{Z}\)</b></h4>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
<ol>
	<li>All subgroups of \((\mathbf{Z}, +)\) are cyclic except for the trivial subgroup \(\langle 0 \rangle = \{0\}\). </li>
	<li>All subgroups are infinite.</li>
	<li>Each subgroup is generated by  a unique \(d \geq 0\).</li>
	<li>Each non-trivial subgroup is isomorphic to \(\mathbf{Z}\).</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof (1)</b>
<br />
To prove that all subgroups are cyclic, we need to find a generator for each of the subgroups. We have two cases:
<br />
If \(H = \{0\}\), then \(H = \mathbf{Z}0\) and we are done.
<br />
If \(H \neq \{0\}\), then \(H\) has at least a non-zero element. By the well ordering principle of \(\mathbf{N}\), there is a smallest element \(d \in H \cap \mathbf{N}\) (\(d\) is the smallest positive element in \(H\)). We claim that \(H = \mathbf{Z}d\). 
<br />
<br />
\(\mathbf{Z}d \subseteq H\): We know \(d \in H\) and \(H\) is a group. Therefore, any multiple of \(d\) is in \(H\) since it must be closed so \(\mathbf{Z}d \subseteq H\) as required.
<br />
<br />
\(H \subseteq \mathbf{Z}d\): Suppose \(x \in H\). We want to show that \(x\) is a multiple of \(d\). Use division with remainder (\(x \div d\)) to get \(x = qd + r\) such that \(q, r \in \mathbf{Z}\) and \(0 \leq r &lt; d\). Observe now that</p>
<div>
	$$
	\begin{align*}
	r = x - qd.
	\end{align*}
	$$
</div>
<p>\(x \in H\) by assumption. \(qd \in H\) since \(d \in H\). Therefore, we must have \(r \in H\). But \(d\) is the smallest positive element in \(H\) by the hypothesis and \(r &lt; d\) so \(r\) must be zero. Therefore, \(x = qd \in \mathbf{Z}d\) and \(H = \mathbf{Z}d\) as we wanted to show. \(\ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Lattice of subgroups of \(\mathbf{Z}\)</b></h4>
<p>We can organize all the subgroups of \(\mathbf{Z}\) in a lattice ordered by the subset relation. For example the all the multiples of 4 are contained in the multiples of 2, so \(\mathbf{Z}4 \subset \mathbf{Z}4\) and it it goes under \(\mathbf{Z}2\) and so. This lattice is also a divisibility lattice meaning that if \(\mathbf{Z}a \subseteq \mathbf{Z}b\), then \(b \ | \ a\). For example we see that \(\mathbf{Z}6 \subset \mathbf{Z}3\) in the lattice and so this implies that \(3 \ | \ 6\).</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec1/01-1.png" width="40%" class="center" /></p>
<p>[TODO add pic]
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Order of an Element</b></h4>
<p>Suppose \(G\) is a group and \(a \in G\), then we denote the order of an element by \(o(a)\). The order of \(a\) is as follows
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
\(o(a) = |\langle a \rangle| \in \mathbf{N} \cap \{\infty\} \)
</div>
<p><br />
Alternatively
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book: 2.2.17)
</div>
<div class="peachbodydiv">
	<ul>
<li>If \(o(a) = n\), then \(n\) is the least positive integer such that \(a^n = e\). Furthermore, \(\langle a \rangle = \{a^k \ : \ 0 \leq k &lt; o(a)\}\).</li>
<li>If \(o(a) = \infty\), then \(a^n \neq e \ \forall n\).</li>
</ul>
</div>
<!----------------------------------------------------------------------------->
<p><br />
These two are equivalent.
<br />
<br />
<b>Proof</b>
<br />
By definition, the cyclic subgroup generated by \(a\) is \(\langle a \rangle = \{a^k \ | \ k \in \mathbf{Z} \}\). If \(\langle a \rangle\) is finite, then two powers of \(a\) must coincide (side note: why? think of the integers mod 7 with addition, \(2^2 = 2+2\) and \(2^9=18\) both leave a remainder of 4. While \(2^7\) is \(e\)). So \(a^i = a^{j}\) for some \(i &lt; j\). This means that \(a^{j-i} = e\) where \(k = j - i &gt; 0\). Therefore, let \(k\) be the smallest positive number such that \(a^k = e\). We claim that \(k = n\). To see this, suppose we have some power \(m\) of \(a\). Write \(m = kq + r\) where \(0 \leq r &lt; k\) and \(r = rem_k(m)\). This means that</p>
<div>
	$$
	\begin{align*}
	 \langle a \rangle = \{e, a, a^2, ..., a^{k-1}\}
	\end{align*}
	$$
</div>
<p>Since \(n\) is the size of the set by assumption, then \(n \leq k\). But that means that that we have two powers in the set above that must coincide where \(a\) raised to the power of their difference is the identity element. But this is a contradiction since we said that \(k\) is the smallest positive number such that \(a^k = e\). Therefore, we must have \(n = k\). So the minimality of \(k\) implies their equality. 
<br />
<br />
If \(\langle a \rangle\) is infinite, we can use the same argument to show that if \(a^k = e\) for some \(k &gt; 0\), then all elements can be written as \(\langle a \rangle = \{e, a, a^2, ..., a^{k-1}\}\) but this is a finite set which is a contradiction. \(\ \blacksquare\)
<br />
<br />
The notion of order leads to the following result
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book 2.2.20)
</div>
<div class="peachbodydiv">
Let \(a\) be an element of a group \(G\).
<ol type="a">
	<li>If \(a\) has infinte order, then \(\langle a \rangle\) is isomorphic to \(\mathbf{Z}\)</li>
	<li>If \(a\) has finite order, then \(\langle a \rangle\) is isomorphic to the group \(\mathbf{Z}_n\)</li>
</ol>
</div>
<p><br />
<!----------------------------------------------------------------------------->
<b>Proof (book)</b>
<br />
For \((a)\), we want to show this by finding an example of an isomorphism. So define the map</p>
<div>
	$$
	\begin{align*}
	 \varphi \ : \ &amp;\mathbf{Z} \rightarrow \langle a \rangle \\
	 &amp;\varphi(k) = a^k.
	\end{align*}
	$$
</div>
<p>To show that this map is an isomorphism, we need to show that it is a bijection and also that for any two elements \(a, b \in \mathbf{Z}\), \(\varphi(ab) = \varphi(a)\varphi(b)\) (Definition 2.1.13).
<br />
<br />
To show that it is a bijection, observe that this map is surjective or onto by the definition of \(\langle a \rangle\). (Recall that that \(\langle a \rangle\) is of infinite order). It is also injective or 1-1 because all the elements of \(\langle a \rangle\) (powers of \(a\)) are distinct. Furthermore, see that \(\varphi(k + l) = a^{k+l} = a^ka^k\). So \(\varphi\) is an isomorphism.
<br />
<br />
Similarly for \((b)\), we want to define an isomorphism. Note here that both \(\mathbf{Z}_n\) and \(\langle a \rangle\) have \(n\) elements so define the map</p>
<div>
	$$
	\begin{align*}
	 \varphi \ : \ &amp;\mathbf{Z}_n \rightarrow \langle a \rangle \\ 
	 &amp;\varphi([k]) = a^k.
	\end{align*}
	$$
</div>
<p>where \(0 \leq k \leq n-1\). In \(\mathbf{Z}_n\), the addition of \([k]\) and \([l]\) is \([r]\) where \(r\) is the remainder after dividing \(k+l\) by \(n\). While the multiplication in \(\langle a \rangle\) is given by \(a^ka^l = a^{k+l} = a^r\) where \(r\) is also the remainder after dividing \(k+l\) by \(n\) as we saw earlier. Therefore \(\varphi\) is an isomorphism.
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Subgroups of \(\mathbf{Z}_{12}\)</b></h4>
<p>TODO
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Cyclic Subgroups in \(\mathbf{Z}_n\)</b></h4>

<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(a\) be an element of a group \(G\).
<ol>
	<li>Every subgroup of \(\mathbf{Z}_n\) is cyclic.</li>
	<li>If \(a \in \mathbf{Z}\), then \(\langle [a] \rangle = \langle [d] \rangle\) where \(d = gcd(a,n)\).</li>
	<li>If \(a, b \ | \ n\), then \(\langle [a] \rangle \subseteq \langle [b] \rangle\) if and only if \(b \ | \ a\).</li>
	<li>Every subgroup of \(\mathbf{Z}\) is \(\langle d \rangle\) for a unique \(d &gt; 0\), \(d \ | \ n\) and \(|\langle d \rangle| = \frac{n}{d}\).</li>
</ol>
</div>
<!---------------------------------------------------------------------->
<p><br />
\((2)\) simply says that the standard representative/generator of the cyclic subgroup \(\langle a \rangle\) is \(gcd(a, n)\). We can replace \(a\) with \(gcd(a,n)\). They both represent the same congruence class.<br /> \((3)\) is similar to \(\mathbf{Z}\). Recall that \(\mathbf{Z}2\) (multiples of 2) contains the subgroup \(\mathbf{Z}4\) and we concluded that \(\mathbf{Z}4 \subseteq \mathbf{Z}2\) implies that \(2 \ | \ 4\). A similar thing here.
<br />
<br />
To prove this theorem we need the following lemma:
<br />
<!----------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
Given a subset \(H \subseteq \mathbf{Z}_n\). Define
	$$
	\begin{align*}
	\tilde{H} &amp;= \{a \in \mathbf{Z} \ | \ [a]_n \in H \} \subseteq \mathbf{Z} \\
	          &amp;= \bigcup_{S \in H} S
 	\end{align*}
	$$
Then, \(H\) is a subgroup of \(\mathbf{Z}_n\) if and only if \(\tilde{H}\) is a subgroup of \(\mathbf{Z}\).
</div>
<p><br />
So \(H\) is a set of integers such that the elements are the representatives of the congruence classes in \(\mathbf{Z}_n\). The proof of this lemma is in the lecture notes.
<br />
<br />
<!---------------------------------------------------------------------->
<b>Theorem Proof</b>
<br />
For (1), Let \(H \leq \mathbf{Z}_n\) be a subgroup. We want to show that \(H\) is cyclic by proving that \(H\) is a cyclic group generated by some element. Consider \(\tilde{H}\). Since \(H\) is a subgroup, then \(\tilde{H}\) is a subgroup of \(\mathbf{Z}_n\) by the lemma. Moreover, \(\tilde{H} = \mathbf{Z}d\) for some \(d \in \mathbf{Z}\) by the previous theorem (every subgroup of \(\mathbf{Z}\) is cyclic and is generated by some \(d\)). Therefore, by the definition of \(\tilde{H}\), \([d]\) must be a congruence class in \(H\). This implies that \(\langle [d] \rangle \subseteq H\) since \(H\) is a group and all multiples of \(d\) must be in \(H\). 
<br />
<br />
To prove the other direction \(H \subseteq \langle [d] \rangle\), consider \([a] \in H\). This means that \(a \in \tilde{H}\) by the definition of \(\tilde{H}\). But \(\tilde{H} = \mathbf{Z}d\) so \(a\) is a multiple of \(d\) and we can write \(a = sd\) for some \(s \in \mathbf{Z}\). Therefore, \([a] = s[d]\) and \([a] \in \langle [d] \rangle\). Therefore, \(H = \langle d \rangle\) and \(H\) is cyclic as we wanted to show. \(\ \blacksquare\)
<br />
<br />
For (2), suppose that \(d = gcd(a,n)\). We want to show that \(\langle [a] \rangle = \langle [d] \rangle\). Since \(d\) is the gcd, then \(d\) divides both \(a\) and \(n\). Also \(d = I(a,n)\) and we can write \(d = sa + tn\) for some \(s, t \in \mathbf{Z}\). 
<br />
<br />
Since \(a \ | \ d\), then \(a = sd\) and so \([a] = s[d]\) so \(\langle [a] \rangle \subseteq \langle [d] \rangle\). Since \(d = sa + tn\), then \([d] = s[d] + t[n] = s[d]\) because \([n] = [0]\) since we’re in \(\mathbf{Z}_n\). Therefore, \(\langle [d] \rangle \subseteq \langle [a] \rangle\). So we’ve shown that \(\langle [a] \rangle = \langle [d] \rangle\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Subgroups of Finite Cyclic Groups</b></h4>
<p>In fact, not just the subgroups of \(\mathbf{Z}_n\) that are cyclic. All subgroups of any finite cyclic group \(G\) are cyclic. We can generalize the previous theorem as follows
<br />
<br /></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let G = \(\langle g \rangle\) and \(o(g) = n &lt; \infty\). Then
<ol>
	<li>Every subgroup of \(G\) is cyclic.</li>
	<li>\(\forall \ a \in \mathbf{Z}\), then \(\langle g^a \rangle = \langle g^d \rangle\) where \(d = gcd(a,n)\).</li>
	<li>If \(a, b \ | \ n\), then \(\langle g^a \rangle \subseteq \langle g^b \rangle\) if and only if \(b \ | \ a\).</li>
	<li>Every subgroup of \(G\) is of the form \(\langle g^d \rangle\) for a unique \(d &gt; 0\), \(d \ | \ n\) and it has order \(\frac{n}{d}\).</li>
</ol>
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 2.2.1 A non-empty subset \(H\) of a group \(G\) is called a subgroup if \(H\) is itself a group with the group operation inherited from \(G\). We write \(H \leq G\) to indicate that \(H\) is a subgroup of \(G\). Example: The group \(SO(3)\), the special orthogonal matrices is a subgroup of \(GL_3(\mathbf{R}\), the group of invertible \(3 \times 3\) matrices. Both of these are under the same operation. Non-example: \(\mathbf{Z}_n\): congruence classes modulo \(n\) with addition. We also have \(\phi(n)\): congruence classes with multiplicative inverses. This has the operation multiplication so it's not a subgroup! Even when you define \(\phi(4) = \{[1],[3]\}\) with addition, it will not satisfy the group requirements, so it's not a subgroup. Since we need to check a lot for subgroups. We have a set of necessary conditions. Proposition The necessary conditions for a subset \(H\) of \(G\) to be a subgroup of \(G\) are: \(H\) is not empty \(H\) is closed under multiplication. That is, for all elements \(h_1\) and \(h_2\) of \(H\), the products \(h_1h_2\) is also an element of \(H\) \(H\) is closed under inverses. For all \(h \in H\), the inverse \(h^{-1}\) is an element of \(H\) Proof \(H\) is associative. For any \(a,b,c \in H\), \((ab)c = a(bc)\). We're inheriting the same operation from \(G\) so \(H\) must be associative. \(H\) has an identity element: We know \(H\) is non-empty by (1). We also know for any \(h \in H\), \(h^{-1} \in H\) by (3). By (2), \(H\) is closed under multiplication so \(hh^{-1} \in H\) but \(hh^{-1} = e\) so the identity element is in \(H\). Example Consider the group \(G = S_4\) (permutations of \(\{1,2,3,4\}\)). The order of this group is \(|G| = 4\). The following are subgroups of \(S_4\): \(H=\{ \sigma \in S_4 \ | \ \sigma(4) = 4\}\). So this subgroup fixes \(4\) in the permutation. \(|H| = 3! = 6\). In fact, it is permuting only 3 numbers and it is isomorphic to \(S_3\). \(H'=\{e,(1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3)\}\). It's not empty. It has the identity element. Every element is its own inverse. The one thing not too obvious is proving it is closed. This subgroup is isomorphic to the symmetry group of the rectangle. What's an isomorphism in this case? Assign \(A\) to 1, \(B\) to 2, \(C\) to 3, \(D\) to 4. Recall that the symmetries of the rectangle has the symmetries \(r_1, r_2, r_3, e\) where \(r_1\) is rotating around the \(x-\)axis. Observe now that \(r_1\) rotates the vertices such that \(A\) and \(B\) switch positions and \(C\) and \(D\) switch positions. This is equivalent to the permutation \((1 \ 2)(3 \ 4)\). Following this, we'll see that \(r_2\) gets mapped to \((1 \ 4)(2 \ 3)\) and \(r_3\) gets mapped to \((1 \ 3)(2 \ 4)\). \(H''=\{e,(1 \ 2 \ 3 \ 4)(4 \ 3 \ 2 \ 1), (1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3), (1 \ 3),(2 \ 4)\}\). This one is isomorphic to \(D_4\) which is the symmetries of the square. Generated Subgroups We begin with the following lemma which states that the intersection of a collection of subgroups is another subgroup. Lemma (Book Proposition 2.2.8) Let \(G\) be a group and let \(H_1, H_2,...,H_n\) be subgroups of \(G\). Then $$ \begin{align*} H = H_1 \cap H_2 \ \cap ... \cap \ H_n \end{align*} $$ is a subgroup of \(G\). More generally, if \(\{H_{\alpha}\}\) is any collection of subgroups, then \(\cap_{\alpha}\) is a subgroup. Proof Every \(H_i\) is a subgroup. Therefore \(e \in H_i\) for each \(i\) and thus \(e \in H\). For any two elements \(a, b \in H\), we must have \(a, b \in H_i\) for each \(i\). Their product \(ab\) must also be in every \(H_i\) since each \(H_i\) is a subgroup. Therefore \(ab \in H\). For any \(a \in H\), \(a\) must be in \(H_i\) for each \(i\). Therefore \(a^{-1} \in H_i \ \forall i\) and thus \(a^{-1} \in H\). Since any intersection of subgroups is another subgroups, we can then define a generated subgroups as follow Definition Let \(G\) be a group and let \(S\) be a subset of \(G\). Then $$ \begin{align*} \langle S \rangle = \bigcap_{\text{All } H_i \leq G \text{ such that } S \subseteq H_i} \end{align*} $$ \(\langle S \rangle\) is called the subgroup generated by \(S\) and it is the smallest subgroup in \(G\) that contains \(S\). Observe that \(\langle S \rangle\) is a subgroup by the previous lemma since it is the intersection of subgroups. \(S \subseteq \langle S \rangle\). It is the smallest group that contains \(S\). Because for any subgroup \(K \leq G\) such that \(S \subseteq K\), we must have \(\langle S \rangle \leq K\) (why? because if \(S\) is contained in \(K\) and \(K\) is a group, then all the products of the elements of \(S\) are in \(K\). Same for the inverses. So the whole subgroup \(\langle S \rangle\) must also be contained in \(K\).) Additionally from the book: If \(S\) contains a single element, \(S = \{a\}\), then the subgroup generated by \(S\) is denoted by \(\langle a \rangle\). If \(G = \langle S \rangle\), then \(G\) is generated by \(S\) or \(S\) generates \(G\) Another way to describe \(\langle S \rangle\) is take a bottom up approach to describe the elements. Proposition Suppose \(G\) is a group and \(S\) is a subset of \(G\). Then \(\langle S \rangle\) is equal to the set of elements in \(G\) of the form $$ \begin{align*} g_1g_2...g_k \quad \text{ where either } g_i \in S \text{ or } g_i^{-1} \in S \text{ for all } i. \end{align*} $$ In other words, \(\langle S \rangle\) is a subgroup such that that it contains all the possible products \(g_1g_2...g_n\) which are in \(S\) or an inverse to an element of \(S\). The set that contains such products is often called the “set of words in \(S\)” in group theory. Proof Let \(T\) be the set of words \(\{e\} \cup \{g_1...g_k\}\) where \(g_i \in S\) or \(g_i^{-1} \in S\) for all \(i = 1,...,k\). We need to show that \(T\) is a subgroup of \(G\). [This is true because \(T\) has the identity element. For any two word, the product of these is just a longer word which is also in \(T\). It is closed under the inverses because \((g_1g_2...g_k)^{-1} = g_k^{-1}...g_1^{-1}\).] \(S \subseteq T\). [This is true because all the words of length 1 make up the set \(S\). It is true by construction.] If \(K \leq G\) is a subgroup such that \(S \subseteq K\), then \(T \subseteq K\). [This is true because if \(K\) contains \(S\) then it must contain all the inverses of these words and then also the products of all of these words since it's a group (closed) so it has all of these elements and it contains \(T\) itself.] Condition (1) and (2) imply that \(\langle S \rangle \subseteq T\). Conditions (1) and (3) imply that \(T \subseteq \langle S \rangle\). Therefore, \(T = \langle S \rangle\). \(\ \blacksquare\) We’ll use this proposition a lot. Examples \(\langle (1 \ 2) \rangle = \{e, (1 \ 2)\}\). This says that the subgroup generated by \(\{(1 \ 2)\}\) is \(\{e, (1 \ 2)\} = \langle (1 \ 2) \rangle \) \(\langle (1 \ 2 \ 3) \rangle = \{e, (1 \ 2 \ 3), (1 \ 3 \ 2)\}\) \(S_3 = \langle (1 \ 2), (1 \ 2 \ 3) \rangle\). \(S_4 = \langle (1 \ 2 \ 3 \ 4), (2 \ 4) \rangle\). Cyclic Groups and Cyclic Subgroups We’ll start with the definition Definition A group \(G\) is cyclic if \(G = \langle a \rangle\) for some \(a \in G\). Similarly, a subgroup \(H \leq G\) is cyclic if \(H = \langle a \rangle\) for some \(a \in G\). In fact, we can describe all the elements in a cyclic subgroup as follows Proposition (Book 2.2.9) Let \(a\) be an element of a group \(G\). The subgroup \(\langle a \rangle\) generated by \(a\) is \(\{a^k \ : \ k \in \mathbf{Z}\}\). Note here that this subgroup is abelian! Proof (book) Let \(H = \{a^k \ : \ k \in \mathbf{Z}\}\). We will show that \(\langle a \rangle = H\) as follows: \(\langle a \rangle \subseteq H\): We know that \(G\) is a group and \(H\) is subset of \(G\). We claim that it is a subgroup of \(G\). It is closed under multiplication because for any \(a^k\) and \(a^l\) in \(H\), \(a^{k+l} \in H\). It is closed under inverses because for any \(a^k \in H\), \((a^{k})^{-1} = a^{-k} \in H\). Furthermore, \(H\) contains \(a\) and since \(H\) is a subgroup, then \(H\) contains all the powers of \(a\). Therefore, \(\langle a \rangle \subseteq H\). \(H \subseteq \langle a \rangle\): We showed that \(H = \{a^k \ : \ k \in \mathbf{Z}\}\) is a subgroup above. It is closed under multiplication and so it contains all powers of \(a\). Therefore, \(H \subseteq \langle a \rangle\). \(\ \blacksquare\) Example Suppose \(G = (\mathbf{Z}, + )\). For any element \(a \in \mathbf{Z}\), $$ \begin{align*} \langle a \rangle = \{ka \ : \ k \in \mathbf{Z}\} = \mathbf{Z}a. \end{align*} $$ The set of powers of \(a\) is the set of all multiples of \(a\) since the group operation is addition. So \(a+a+a+a+a\) is the fifth power.]]></summary></entry><entry><title type="html">Groups and Isomorphism</title><link href="http://localhost:4000/jekyll/update/2025/02/01/math417-08-groups-and-isomorphism.html" rel="alternate" type="text/html" title="Groups and Isomorphism" /><published>2025-02-01T00:01:36-08:00</published><updated>2025-02-01T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/01/math417-08-groups-and-isomorphism</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/01/math417-08-groups-and-isomorphism.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A group \((G, G \times G \rightarrow G)\) is a set with a binary operation on that set such that. 
<ol>
	<li>The operation is associative so that \((ab)c = a(bc), \forall a,b,c \in G\)</li>
	<li>There exists an identity element. \(\exists e \in G\) such that \(ae = a = ea \forall a \in G\).</li>
	<li>Every element has an inverse. \(\forall a \in G, \exists a^{-1} \in G\) such that \(aa^{-1} = e = a^{-1}a\).</li>
</ol>
Additionally, a group is commutative/abelian if \(ab = ba, \forall a, b \in G.\).
</div>
<!---------------------------------------------------------------------->
<p><br />
<br />
In addition to groups, we also have</p>
<ul> 
<li>Monoids which satisfy \((1)\) and \((2)\). </li>
<li>Semi-groups which satisfy \((1)\).</li>
<li>Magma \(G, \cdot\) with no additional properties. </li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Basic Properties of Groups</b></h4>
<p>In the next few propositions, we’ll prove that the identity element in a group is unique and similarly the inverses are unique.
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
The identity element is unique.
</div>
<p><br />
<b>Proof</b>
<br />
Let \(e\) and \(e'\) be identity elements. Then by definition for any \(x, y \in G\)</p>
<div>
	$$
	\begin{align*}
	 xe &amp;= x = ex \\
	 ye' &amp;= y = e'y
	\end{align*}
	$$
</div>
<p>If we let \(x = e'\) and let \(y = e\), then</p>
<div>
	$$
	\begin{align*}
	 e'e &amp;= e' = ee' \\
	 ee' &amp;= e = e'e
	\end{align*}
	$$
</div>
<p>From this we see that \(e = e'\) as desired. \(\ \blacksquare\)
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Inverses in a group are unique.
</div>
<p><br />
<!---------------------------------------------------------------------->
<b>Proof</b>
<br />
Let \(a \in G\) and suppose for the sake of contradiction that \(a\) has two inverses \(b\) and \(c\). That is \(ab = e = ba\) and \(ac = e = ca\). Then,</p>
<div>
	$$
	\begin{align*}
	 c = ec = (ba)c = b(ac) = be = b.
	\end{align*}
	$$
</div>
<p>Therefore, \(c = b\) which is a contradiction so the inverse must be unique. \(\ \blacksquare\)
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (2.1.2)
</div>
<div class="peachbodydiv">
Let \(G\) be a group and let \(a, b \in G\). If \(ab = e\), then \(a = b^{-1}\). Likewise, if \(ba = e\), then \(b = a^{-1}\).
</div>
<p><br />
By definition, \(b\) an inverse of \(a\) if \(ab = ba = e\) so it’s an inverse on both sides. This proposition proposes that checking only one side is enough. That is if \(ab = e\), then \(a = b^{-1}\) and \(b\) is the inverse. So what we want to show here is that given \(ba = e\), then \(b\) is the inverse of \(a\). We don’t check the other side. One side is enough to imply the other.
<br />
<br />
<!---------------------------------------------------------------------->
<b>Proof</b>
<br />
We know from the previous proposition that \(a\) has an inverse \(a^{-1} \in G\). Now consider the expression \(a^{-1}ab\). By associativity, we can reduce this expression in two ways</p>
<div>
	$$
	\begin{align*}
	 (a^{-1}a)b &amp;= a^{-1}(ab)
	\end{align*}
	$$
</div>
<p>The left hand side</p>
<div>
	$$
	\begin{align*}
	 (a^{-1}a)b &amp;= eb = b
	\end{align*}
	$$
</div>
<p>The right hand side is</p>
<div>
	$$
	\begin{align*}
	 a^{-1}(ab) &amp;= a^{-1}
	\end{align*}
	$$
</div>
<p>So \(b = a^{-1}\) and therefore, \(ba = a^{-1}a = e\). 
<br />
<br />
<!---------------------------------------------------------------------->
<b>Proof (Book)</b>
<br />
Suppose \(hg = e\), then</p>
<div>
	$$
	\begin{align*}
	 h = h(gg^{-1}) = (hg)g^{-1}= eg^{-1} = g^{-1}.
	\end{align*}
	$$
</div>
<p>Suppose now that \(gh = e\), then</p>
<div>
	$$
	\begin{align*}
	 g = g(hh^{-1}) = (gh)h^{-1} = eh^{-1} = h^{-1}. \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Corollary 2.1.3
</div>
<div class="peachbodydiv">
Let \(g\) be an element of a group \(G\). We have \(g = (g^{-1})^{-1}\)
</div>
<p><br />
<b>Proof</b>
<br />
We know \(gg^{-1} = e\). By (2.1.2) \(g = (g^{-1})^{-1}\). \(\ \blacksquare\)
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 2.1.4
</div>
<div class="peachbodydiv">
Let \(G\) be a group and let \(a,b \in G\). Then We have \((ab)^{-1} = b^{-1}a^{-1}\)
</div>
<p><br />
<b>Proof</b>
<br />
Notice that \((ab)(b^{-1}a^{-1}) = a(b((b^{-1}a^{-1})) = a((bb^{-1})a^{-1}) = a(ea^{-1}) = aa^{-1} = e\). Therefore, \((ab)^{-1} = b^{-1}a^{-1}\).
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>The Left and Right Multiplication Maps</b></h4>

<div class="peachheaderdiv">
Proposition 2.1.5
</div>
<div class="peachbodydiv">
Let \(G\) be a group and \(a \in G\). <br />
The map \(L_a: G \rightarrow G\) defined by \(L_a(x) = ax\) is a bijection. Similarly, <br />
the map \(R_a: G \rightarrow G\) defined by \(R_a(x) = xa\) is a bijection.
</div>
<!---------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
To show that the map \(L_a\) is a bijection, we need to show that \(L_a\) is both one to one and onto or equivalently that \(L_a\) has an inverse. We claim that the left multiplication by \(a^{-1}\) or \(L_{a^{-1}}\) is the inverse of \(L_a\). To see that,</p>
<div>
	$$
	\begin{align*}
	L_{a^{-1}}(L_a(x)) = a^{-1}(ax) = (a^{-1}a)x = ex = x.
	\end{align*}
	$$
</div>
<p>Similarly,</p>
<div>
	$$
	\begin{align*}
	L_a(L_{a^{-1}}(x)) = a(a^{-1}x) = (aa^{-1})x = ex = x.
	\end{align*}
	$$
</div>
<p>So \(L_{a^{-1}}\) and \(L_a\) are inverse maps so both are bijective. \(\ \blacksquare\)
<br />
<br /></p>

<!---------------------------------------------------------------------->
<div class="peachheaderdiv">
Corollary 2.1.6
</div>
<div class="peachbodydiv">
Let \(G\) be a group and let \(a\) and \(b\) be elements of \(G\). The equation \(ax = b\) has a unique solution \(x\) in \(G\). and likewise the equation \(xa = b\) has a unique solution in \(G\).
</div>
<p><br />
<b>Proof</b>
<br />
For \(ax = b\) to have a solution. The map \(L_a\) needs to be onto or surjective. For the solution to be unique, the map needs to be one to one or injective. Similarly for \(xa = b\) to have a solution, we want \(R_a\) to be a bijective. Since we proved earlier that \(L_a\) and \(R_a\) are bijections, then both equations have unique solutions. \(\ \blacksquare\)
<br />
<br /></p>

<!---------------------------------------------------------------------->
<div class="peachheaderdiv">
Corollary 2.1.7 (Cancellation)
</div>
<div class="peachbodydiv">
Suppose \(a, x, y\) are elements of a group \(G\). If \(ax = ay\), then \(x = y\). Similarly, if \(xa = ya\), then \(x = y\).
</div>
<p><br />
<b>Proof</b>
<br />
Suppose \(ax = ay\). We know that \(L_a(x) = ax\) is one to one. So for any elements \(x, y \in G\), \(ax = ay\) must imply that \(x = y\) by definition of a one to one or injective map. A similar arguments shows that if \(xa = ya\) must imply that \(x = y\) by the injectivity of \(R_a\). \(\ \blacksquare\)
<br />
<br /></p>

<!---------------------------------------------------------------------->
<div class="peachheaderdiv">
Corollary 2.1.8
</div>
<div class="peachbodydiv">
If \(G\) is a finite group, each row and each column of the multiplication table of \(G\) contains each element of \(G\) exactly once.
</div>
<p><br />
<b>Proof (book)</b>
<br />
A row in the multiplication table can be represented by a left multiplication map \(G \rightarrow G\) if you fix the element multiplied on the left. We know the left multiplication map is a bijection. Therefore every element/result must be unique and each element of \(G\) must show up in the row. Similarly, each column can be represented by a right multiplication map. The map is a bijection and so each element must be unique and shown exactly once. (TODO clean up this proof)
<br />
<br /></p>
<hr />

<p><br />
<!---------------------------------------------------------------------></p>
<h4><b>Associativity in Groups</b></h4>
<p>We know by definition that the product is associative so for all \(a, b, c \in G\), we have \((ab)c = a(bc)\). What about the product of 4 or more elements? is it associative? For example, there are five ways to group four elements</p>
<div>
	$$
	\begin{align*}
	a(b(cd)), a((bc)d), (ab)(cd), (a(bc))d, ((ab)c)d
	\end{align*}
	$$
</div>
<p>\(a(b(cd))\) and \(((ab)c)d\) follow from the definition. For the rest, see that</p>
<div>
	$$
	\begin{align*}
	a(bcd) = a(b(cd)) = (ab)(cd) = ((ab)c)d = (abc)d
	\end{align*}
	$$
</div>
<p>How many ways are there to associate an \(n\)-fold product?</p>
<div>
	$$
	\begin{align*}
	\frac{1}{2}\binom{2n - 2}{n - 1}
	\end{align*}
	$$
</div>
<p>In general this works for any number of elements. Formally,
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 2.1.19 (General Associative law)
</div>
<div class="peachbodydiv">
Let \(M\) be a set with an associative operation, \(M \times M \rightarrow M\), denoted by juxtaposition. For every \(n \geq 1\), there is a unique product \(M^n \rightarrow M\),
	$$
	\begin{align*}
	(a_1, a_2,...,a_n) \rightarrow a_1a_2...a_n,
	\end{align*}
	$$
such that
<ol type="a">
	<li> The product of one element is that element \((a) = a\).</li>
	<li> The product of two elements agrees with the given operation \((ab) = ab\).</li>
	<li>For all \(n \geq 2\), for all \(a_1,...,a_n \in M\), and for all \(1 \leq k \leq n-1\), 
		$$
		\begin{align*}
		a_1a_2...a_n = (a_1...a_k)(a_{k-1}...a_n)
		\end{align*}
		$$
	</li>
</ol>
</div>
<!---------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
By induction on \(n\). <br />
Base Case: For \(n \leq 3\), property \((c)\) holds by definition.
<br />
Inductive Case: Suppose this is true for all \(1 \leq r \leq n\) where a unique product of \(r\) elements satisfies the properties \((a)-(c)\) above. Suppose now that we have \(n\) elements. Fix elements \(a_1, ...,a_n \in M\). By the inductive hypothesis, the \(n-1\) products</p>
<div>
	$$
	\begin{align*}
	p_k = (a_1...a_k)(a_{k+1}...a_n),
	\end{align*}
	$$
</div>
<p>are defined since we have at most \(n-1\) elements. … [TODO]
<br />
<br /></p>
<hr />

<p><br />
<!---------------------------------------------------------------------></p>
<h4><b>General Powers</b></h4>
<p>Now, we turn into defining powers of an element in a group
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
For \(a \in G\) where \(G\) is a group and \(n \in \mathbf{Z}\). Define \(a^n \in G\) by
<ol>
	<li>\(a^0 = e\).</li>
	<li>\(a^1 = a\).</li>
	<li>\(a^{-1} \) is the inverse of \(a\).</li>
	<li>\(n \geq 1\), \(a^{n+1} = a^n a\).</li>
	<li>\(n \leq -1\), \(a^{n-1} = a^n a^{-1}\).</li>
</ol>
</div>
<!---------------------------------------------------------------------->
<p><br />
Based on this definition we have the following proposition
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
	$$
	\begin{align*}
	a^{m}a^{n} &amp;= a^{m + n} \\
	(a^m)^{n} &amp;= a^{mn} \\	
	\end{align*}
	$$
</div>
<p><br />
<b>Proof (1)</b>
<br />
By induction on \(n\). <br />
Base Case \((n = 1)\): \(a^m a = a^{m+1}\) by definition.
<br />
Inductive Case: Suppose the inductive hypothesis is true for \(n\). That is \(a^m a^n = a^{m+n}\). Now observe that</p>
<div>
	$$
	\begin{align*}
	a^m a^{n+1} &amp;= a^m (a^n a) \text{ (by definition)} \\
	            &amp;= (a^m a^n) a \text{ (by associativity)} \\
	            &amp;= a^{m+n} a \text{ (by the inductive hypothesis)} \\
	            &amp;= a^{m+n+1} \text{ (by definition)}. \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<b>Proof (2)</b>
By induction on \(n\). <br />
Base Case \((n = 1)\): \((a^{m})^1 = a^{m}\).
<br />
Inductive Case: Suppose the inductive hypothesis is true for \(n\). That is \((a^m)^n = a^{mn}\). Now observe that</p>
<div>
	$$
	\begin{align*}
	(a^{m})^{n+1} &amp;= (a^{m})^{n} (a^{m})^{1} \text{ (by definition)} \\
	              &amp;= (a^{m})^{n} a^{m} \\
	              &amp;= a^{mn} a^{m}  \text{ (by the inductive hypothesis)} \\
				  &amp;= a^{mn+m}  \text{ (by the previous proof)} \\
	            &amp;= a^{m(n+1)}. \ \blacksquare
	\end{align*}
	$$
</div>

<p><br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Isomorphism</b></h4>
<p>One thing that we want to do is to compare two groups. For example take \((\mathbf{Z}_4, +)\), the symmetries of the rectangle, \((\phi(5), \cdot)\) and \((\phi(8), \cdot)\). These are all groups with exactly 4 elements. To compare two groups, we want to see if we can construct a bijection between the two groups. Formally, this is called an isomorphism as follows
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition 2.1.13
</div>
<div class="mintbodydiv">
We say two groups \(G\) and \(H\) are isomorphic if there is a bijection \(\varphi: G \rightarrow H\) such that for all \(a, b \in G\)
	$$
	\begin{align*}
	\varphi(ab) = \varphi(a)\varphi(b)	
	\end{align*}
	$$
where the first multiplication is in \(G\) while the second is in \(H\).<br />
 The map \(\varphi\) is called an isomorphism.
</div>
<p><br />
<br />
We write \(H \approx G\) for \(G\) is isomorphic to \(H\).
<br />
<br />
An an example consider the group of symmetries of the equilateral triangle \(D_3\) and the group \(S_3\) (permutations of \(\{1,2,3\}\)). Both groups contain exactly 6 elements. They turn out to be isomorphic. Observe here that if we assign the vertices \(A\) to \(1\), \(B\) to \(2\) and \(C\) to \(3\), then the rotation \(a\) flips the vertices \(B\) and \(C\). This is exactly the same as the permutation \((2 \ 3)\). Similarly, if you think about the rotation \(r\), you’ll notice that \(r\) permutes the vertices in the exact way as the permutation \((1 \ 2 \ 3)\). In fact, all permutations. Another example is the \(b\) rotation. This rotation fixes \(b\) and rotates \(a\) and \(c\). This is also the same as the permutation \((1 \ 3)\). We can map the rest of the symmetries as follows</p>
<div>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(D_3\)</td>
    <td>\(S_3\)</td>
  </tr>
  <tr>
    <td>\(id\)</td>
    <td>\(id\)</td>
  </tr>
  <tr>
    <td>\(r\)</td>
    <td>\((1 \quad 2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(a\)</td>
    <td>\((2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(r^2\)</td>
    <td>\((1 \quad 3 \quad 2)\)</td>
  </tr>
  <tr>
    <td>\(b\)</td>
    <td>\((1 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(c\)</td>
    <td>\((1 \quad 2)\)</td>
  </tr>
</table>
</div>
<p><br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
	If \(\varphi:G \rightarrow H\) is an isomorphism, then \(\varphi^{-1}: G \rightarrow H\) is also an isomorphism.
</div>
<p><br />
<b>Proof</b>
<br />
Let \(a', b' \in H\). We want to show that</p>
<div>
	$$
	\begin{align*}
	\varphi^{-1}(a'b') = \varphi^{-1}(a') \varphi^{-1}(b')
	\end{align*}
	$$
</div>
<p>Let \(a' = \varphi(a)\) and \(b' = \varphi(b)\) for some \(a, b \in G\). Since \(\varphi\) is injective then it suffices to show that</p>
<div>
	$$
	\begin{align*}
	\varphi(\varphi^{-1}(a'b')) = \varphi(\varphi^{-1}(a') \varphi^{-1}(b'))
	\end{align*}
	$$
</div>
<p>The right hand side is clearly just \(a'b'\). \(\varphi\) is an isomorphism so the left hand side becomes</p>
<div>
	$$
	\begin{align*}
	\varphi(\varphi^{-1}(a') \varphi^{-1}(b')) &amp;= 
	\varphi(\varphi^{-1}(a')) \varphi(\varphi^{-1}(b')) \\
	                          &amp;= a'b'
	\end{align*}
	$$
</div>
<p>Therefore \(\varphi^{-1}\) is an isomorphism as desired. \(\ \blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Groups of Small Order</b></h4>
<p>The order of a group is the number of elements in it. Formally,
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition 2.1.10
</div>
<div class="mintbodydiv">
The order of a group is its size or cardinality. We will denote the order of a group \(G\) by \(|G|\).
</div>
<p><br />
<br />
One interesting thing to do is to classify all groups of a given finite order. If we do that for small sizes, we get
<!---------------------------------------------------------------------></p>
<ol>
	<li>Order 0: None because we need to at least have the identity element.</li>
	<li>Order 1: \(G = \{e\}\). Other groups of order 1 can be \(G'=\{1\}\). These two groups are isomorphic. Technically, it's a different set but to us, they're the same group. So "up to isomorphism", there is only one group of size 1.</li>
	<li>Order 2: Up to isomorphism, the only unique group is \(\mathbf{Z}_2 = \{[0], [1], [2]\}\).</li>
	<li>Order 3: Up to isomorphism, the only unique group is \(\mathbf{Z}_3\). All other groups of size 3 will be isomorphic to \(\mathbf{Z}_3\).</li>
	<li>order 4: Up to isomorphism, there are two unique groups. \(\mathbf{Z}_4 \) and \(\mathbf{Z}_2 \times \mathbf{Z}_2\) (symmetries of the rectangle). Groups of order 4 can be isomorphic to either one.</li>
	<li>Order 5: Up to isomorphism, the only unique group is \(\mathbf{Z}_5\)</li>
	<li>Order 6: Up to isomorphism, we have two unique groups. \(\mathbf{Z}_6\) and \(S_3 \approx D_3\). These two groups are not isomorphic because \(S_3\) is non-abelian while \(\mathbf{Z}_6\) is abelian. The proof for this fact is next.</li>
</ol>
<p><br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(G \approx H\), then \(G\) is abelian if and only if \(H\) is abelian.
</div>
<p><br />
<b>Proof (Lecture Notes)</b>
<br />
Suppose that \(\phi: G \rightarrow H\) is an isomorphism. Furthermore, let \(a, b \in G\) and \(a', b' \in H\) such that \(\phi^{-1}(a') = a\) and \(\phi^{-1}(b') = b'\). We will prove both directions of the statement.
<br />
<br />
\(\Rightarrow\): Suppose that \(G\) is abelian. We will show that \(H\) is abelian. Observe that</p>
<div>
	$$
	\begin{align*}
	\phi(ab) &amp;= \phi(a)\phi(b) = a'b' \\
	\phi(ba) &amp;= \phi(b)\phi(a) = b'a'.
	\end{align*}
	$$
</div>
<p>But we know that \(G\) is abelian and so \(ab = ba\) so \(\phi(ab) = \phi(ba)\) and therefore we must have \(a'b' = b'a'\).
<br />
<br />
\(\Leftarrow\): Now suppose that \(H\) is abelian. Then,</p>
<div>
	$$
	\begin{align*}
	\phi(ab) &amp;= \phi(a)\phi(b) = a'b' \\
	\phi(ba) &amp;= \phi(b)\phi(a) = b'a'.
	\end{align*}
	$$
</div>
<p>We know that \(a'b' = b'a'\) because \(H\) is abelian. But since \(\phi\) is injective, this implies that \(ab = ba\). (remember injective means that \(f(a)=f(b) \implies a = b\)). \(\ \blacksquare\)
<br />
<br />
Note that for this direction, we could’ve instead relied on \(\phi^{-1}\) being an isomorphism itself. and so observe that</p>
<div>
	$$
	\begin{align*}
	\phi^{-1}(a'b') &amp;= \phi^{-}(a')\phi^{-}(b') = ab \\
	\phi^{-1}(b'a') &amp;= \phi^{-}(b')\phi^{-}(a') = ba.
	\end{align*}
	$$
</div>
<p>\(H\) is abelian so \(a'b' = b'a'\) so \(\phi^{-1}(a'b') = \phi^{-1}(b'a')\) and thus \(ab = ba\).
<br />
<br />
<br /></p>
<hr />

<p><br />
<br />
<!---------------------------------------------------------------------->
Extra Notes from the book: Two groups are isomorphic means that the multiplication tables match up. In fact, not only that but the identity elements and inverses of elements match up as well. The following propositions state these facts.
<br /></p>
<div class="peachheaderdiv">
Proposition 2.1.18
</div>
<div class="peachbodydiv">
If \(\phi : G \rightarrow H\) is an isomorphism, then \(\phi(e_G) = e_H\), and for each \(g \in G\), \(\phi(g^{-1}) = \phi(g)^{-1}\)
</div>
<p><br />
<b>Proof</b>
<br />
Since \(\phi\) is an isomorphism, then we know that for each \(h \in H\), there is a \(g \in G\) such that \(\phi(g) = h\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\phi(e_G)h &amp;= \phi(e_G)\phi(g) \\
	           &amp;= \phi(e_Gg) \quad \text{$\phi$ is an isomorphism}\\
			   &amp;= \phi(g) \\
			   &amp;= h \\
	\end{align*}
	$$
</div>
<p>So \(\phi(e_G)\) is an identity element. But since the identity element is unique, then \(\phi(e_G) = \phi(e_H)\).
<br />
<br />
To show that \(\phi(g^{-1}) = \phi(g)^{-1}\), see that</p>
<div>
	$$
	\begin{align*}
	\phi(g^{-1})\phi(g) &amp;= \phi(g^{-1}g) \quad \text{$\phi$ is an isomorphism} \\
	        &amp;= \phi(e_G) \\
			&amp;= e_H \quad \text{by the previous result} 	
	\end{align*}
	$$
</div>
<p>So \(\phi(g)\) is the inverse of \(\phi(g^{-1})\) or in other words \(\phi(g)^{-1} = \phi(g^{-1})\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A group \((G, G \times G \rightarrow G)\) is a set with a binary operation on that set such that. The operation is associative so that \((ab)c = a(bc), \forall a,b,c \in G\) There exists an identity element. \(\exists e \in G\) such that \(ae = a = ea \forall a \in G\). Every element has an inverse. \(\forall a \in G, \exists a^{-1} \in G\) such that \(aa^{-1} = e = a^{-1}a\). Additionally, a group is commutative/abelian if \(ab = ba, \forall a, b \in G.\). In addition to groups, we also have Monoids which satisfy \((1)\) and \((2)\). Semi-groups which satisfy \((1)\). Magma \(G, \cdot\) with no additional properties. Basic Properties of Groups In the next few propositions, we’ll prove that the identity element in a group is unique and similarly the inverses are unique. Proposition The identity element is unique. Proof Let \(e\) and \(e'\) be identity elements. Then by definition for any \(x, y \in G\) $$ \begin{align*} xe &amp;= x = ex \\ ye' &amp;= y = e'y \end{align*} $$ If we let \(x = e'\) and let \(y = e\), then $$ \begin{align*} e'e &amp;= e' = ee' \\ ee' &amp;= e = e'e \end{align*} $$ From this we see that \(e = e'\) as desired. \(\ \blacksquare\) Proposition Inverses in a group are unique. Proof Let \(a \in G\) and suppose for the sake of contradiction that \(a\) has two inverses \(b\) and \(c\). That is \(ab = e = ba\) and \(ac = e = ca\). Then, $$ \begin{align*} c = ec = (ba)c = b(ac) = be = b. \end{align*} $$ Therefore, \(c = b\) which is a contradiction so the inverse must be unique. \(\ \blacksquare\) Proposition (2.1.2) Let \(G\) be a group and let \(a, b \in G\). If \(ab = e\), then \(a = b^{-1}\). Likewise, if \(ba = e\), then \(b = a^{-1}\). By definition, \(b\) an inverse of \(a\) if \(ab = ba = e\) so it’s an inverse on both sides. This proposition proposes that checking only one side is enough. That is if \(ab = e\), then \(a = b^{-1}\) and \(b\) is the inverse. So what we want to show here is that given \(ba = e\), then \(b\) is the inverse of \(a\). We don’t check the other side. One side is enough to imply the other. Proof We know from the previous proposition that \(a\) has an inverse \(a^{-1} \in G\). Now consider the expression \(a^{-1}ab\). By associativity, we can reduce this expression in two ways $$ \begin{align*} (a^{-1}a)b &amp;= a^{-1}(ab) \end{align*} $$ The left hand side $$ \begin{align*} (a^{-1}a)b &amp;= eb = b \end{align*} $$ The right hand side is $$ \begin{align*} a^{-1}(ab) &amp;= a^{-1} \end{align*} $$ So \(b = a^{-1}\) and therefore, \(ba = a^{-1}a = e\). Proof (Book) Suppose \(hg = e\), then $$ \begin{align*} h = h(gg^{-1}) = (hg)g^{-1}= eg^{-1} = g^{-1}. \end{align*} $$ Suppose now that \(gh = e\), then $$ \begin{align*} g = g(hh^{-1}) = (gh)h^{-1} = eh^{-1} = h^{-1}. \ \blacksquare \end{align*} $$ Corollary 2.1.3 Let \(g\) be an element of a group \(G\). We have \(g = (g^{-1})^{-1}\) Proof We know \(gg^{-1} = e\). By (2.1.2) \(g = (g^{-1})^{-1}\). \(\ \blacksquare\) Proposition 2.1.4 Let \(G\) be a group and let \(a,b \in G\). Then We have \((ab)^{-1} = b^{-1}a^{-1}\) Proof Notice that \((ab)(b^{-1}a^{-1}) = a(b((b^{-1}a^{-1})) = a((bb^{-1})a^{-1}) = a(ea^{-1}) = aa^{-1} = e\). Therefore, \((ab)^{-1} = b^{-1}a^{-1}\). The Left and Right Multiplication Maps]]></summary></entry><entry><title type="html">Lecture 06/07: Modular Arithmetic</title><link href="http://localhost:4000/jekyll/update/2025/01/30/math417-07-modular-arithmetic.html" rel="alternate" type="text/html" title="Lecture 06/07: Modular Arithmetic" /><published>2025-01-30T00:01:36-08:00</published><updated>2025-01-30T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/30/math417-07-modular-arithmetic</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/30/math417-07-modular-arithmetic.html"><![CDATA[<!------------------------------------------------------------------------------>
<div class="mintheaderdiv">
Definition (Book Definition 1.7.1)
</div>
<div class="mintbodydiv">
Given integers \(a\) and \(b\), and a natural number \(n\), we say that "\(a\) is congruent to \(b\) modulo \(n\)" and we write 
$$
\begin{align*}
a \equiv b \bmod n \quad \text{or} \quad a \equiv_n b
\end{align*}
$$
if \(a - b\) is divisible by \(n\) or \(n \ | \ a - b\). So there exists some \(t \in \mathbf{Z}\) such that \(a - b = tn\)
</div>
<!------------------------------------------------------------------------------>
<p><br />
<br />
Example: \((1 \bmod 7) = (8 \bmod 7) = 1\). Therefore, \(1 \equiv_7 8\).
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
For each integer \(a\), write
$$
\begin{align*}
[a] &amp;= [a]_n = \{x \in \mathbf{Z} \ | \ x \equiv a \bmod n\} \subseteq \mathbf{Z} \\
    &amp;= \{ a + ny \ | \ y \in \mathbf{Z} \}
\end{align*}
$$
The set \([a]\) is called the residue class or congruence class of \(a\) modulo \(n\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
<br />
Example: \([2]_6 = \{2 + 6y \ | \ y \in \mathbf{Z}\} = \{ ...,-10,-4,2,8,14,20,... \}\)
<br />
\([2]_6 = [8]_6 = [-10]_6 = [602]_6 = ....\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Remainder Function</b></h4>
<p>Another important definition that we need is the following
<br />
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
	\(rem_n \ : \ \mathbf{Z} \rightarrow \{0,1,2,...,n-1\}\) is the remainder function after dividing by \(n\).
	<br />
	\(rem_n(a) = r\) is the unique remainder of \(a \div n\) such that \(0 \leq r &lt; n\) and \(a = qn + r\) for some \(q \in \mathbf{Z}\). Note that \(a - r\) is divisible by \(n\) or \(a \equiv_n r\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
It’s important to note that the remainder is in the same congruence class as \(a\). In fact \([a]_n \cap \{0,...n-1\} = \{r\}\). Usually the remainder \(r\) is the standard/canonical name for the congruence class. So for example, we usually don’t write \([602]_6\) but write \([2]_6\). But we don’t have to put it in canonical form.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Properties of Congruence</b></h4>
<p>Congruence is an equivalence relation. The following properties show this.
<br />
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma (Book 1.7.2)
</div>
<div class="yellowbodydiv">
	Properties of Congruence:
<ol>
	<li>Reflexive: For all \(a \in \mathbf{Z}\), \(a \equiv a \bmod n\).</li>
	<li>Symmetric: For all \(a, b \in \mathbf{Z}\), \(a \equiv b \bmod n\) if and only if \(b \equiv a \bmod n\).</li>
	<li>Transitive: For all \(a, b, c \in \mathbf{Z}\), if \(a \equiv b \bmod n\) and \(b \equiv c \bmod n\), then \(a \equiv c \bmod n\).</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof (book)</b>
<br />
For \((a)\), \(a - a = 0\) is divisible by \(n\). For \((b)\), if \(a - b\) is divisible by \(n\), then \(b - a\) is also divisible by \(n\) and vice versa. For \((c)\), if \(a - b\) is divisible by \(n\) and \(b - c\) is divisible by \(4n\), then \((a - b) + (b - c) = a - c\) is also divisible by \(n\). \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------>
Based on these properties, we have the following proposition
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Proposition (Book Lemma 1.7.3)
</div>
<div class="yellowbodydiv">
For \(a, b \in \mathbf{Z}\), the following are equivalent:
<ol type="a">
	<li>\(a \equiv b \bmod n\).</li>
	<li>\([a]_n = [b]_n\).</li>
	<li>\(\text{rem}_n(a) = \text{rem}_n(b)\).</li>
	<li>\([a]_n \cap [b]_n \neq \emptyset\).</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof (Book):</b>
<br />
\((a) \implies (b)\):
<br />
Suppose \(a \equiv b \bmod n\). We want to show that \([a]_n = [b]_n\). 
<br />
\([a]_n \subseteq [b]_n\): Let \(c \in \mathbf{Z}\). If \(c \equiv a \bmod n\), then \(c \equiv b \bmod n\) by Lemma 1.7.2 (c). Therefore \([a]_n \subseteq [b]_n\).
<br />
\([b]_n \subseteq [a]_n\): If \(c \equiv b \bmod n\), then \(c \equiv a \bmod n\) and so \([a] = [b]\) as required.
<br />
<br />
\((b) \implies (c)\):
<br />
By definition, \(\text{rem}_n(x)\) is the unique element of \([x]\) that lies inside \(\{0,1,...,n-1\}\). So if \([a]_n=[b]_n\), then it must be the same element.
<br />
<br />
\((c) \implies (d)\):
<br />
\((d)\) is an immediate application of \((c)\)
<br />
<br />
\((d) \implies (a)\):
Suppose that \([a]_n \cap [b]_n \neq \emptyset\). Let \(c \in [a]_n \cap [b]_n\). Then \(a \equiv c \bmod n\) and \(b \equiv c \bmod n\). But this implies that \(a \equiv b \bmod n\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Modular Arithmetic</b></h4>
<p>The following lemma establishes how modular arithmetic is done.
<br /></p>
<div class="yellowheaderdiv">
Proposition (Book Lemma 1.7.5)
</div>
<div class="yellowbodydiv">
Let \(a, a', b, b'\) be integers with \(a \equiv a' \bmod n\) and \(b \equiv b' \bmod n\). Then 
$$
\begin{align*}
a + b &amp;\equiv a' + b' \bmod n \\
ab &amp;\equiv a'b' \bmod n
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
[TODO]
<br />
<br />
<!------------------------------------------------------------------------------>
We can now use these modular arithmetic properties to define algebraic structures on a set.
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
	$$
	\begin{align*}
	\mathbf{Z}_n &amp;= \{ \text{ set of congruence classes mod } n \} \\
	             &amp;= \{ [a]_n \ | \ a \in \mathbf{Z} \} \\
				 &amp;= \{ [0]_n, [1]_n,...,[n-1]_n \ | \ a \in \mathbf{Z} \} \\
	\end{align*}
	$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------>
So now we can use the operations we defined previously to turn this set into a commutative ring.
<br />
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Define operations \(+\), \(\cdot\) on \(\mathbf{Z}_n\) by
	$$
	\begin{align*}
	[a]_n + [b]_n &amp;= [a + b]_n \\
	[a]_n[b]_n &amp;= [ab]_n
	\end{align*}
	$$
</div>
<p><br />
\((\mathbf{Z}_n,+,\cdot)\) is commutative ring with identity while \((\mathbf{Z}_n,\cdot)\) is a commutative monoid. \([1]\) is the identity element.
<br />
<br />
Only some elements \([a] \in \mathbf{Z}_n\) have a multiplicative inverse. (such that \([a][b] = [1] = [b][a]\)). The question is when do we have a multiplicative inverse? The answer is in the following proposition.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(n \geq 1, a \in \mathbf{Z}\). Then \([a]\) has a multiplicative inverse in \(\mathbf{Z}\) if and only if \(gcd(a,n) = 1\).
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Suppose that \(gcd(a,n)=1\). Then there exists \(r, s \in \mathbf{Z}\) such that \(ar + ns = 1\). Re-writing this equation, we observe that</p>
<div>
$$
\begin{align*}
ar + ns = 1 &amp;\Longleftrightarrow ar = 1 + (-s)n \quad \text{ (so $ar$ and 1 differ by a multiple of $n$)}\\
             &amp;\Longleftrightarrow ar = 1 \bmod n \\
			 &amp;\Longleftrightarrow [a][r] = 1. \ \blacksquare \\
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------>
We will use \(\phi(n) = \{x \in \mathbf{Z}_n \ | \ x \text{ has a multiplicative inverse}\} \in \mathbf{Z}_n\). 
<br />
\((\phi(n))\) is a commutative group.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Binomial Theorem</b></h4>
<p><br />
Next we have the binomial theorem which we need to prove a proposition which will then lead to Fermat’s Little Theorem.
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem (Binomial Theorem)
</div>
<div class="yellowbodydiv">
	\(a, b \in \mathbf{R}, n \in \mathbf{N}\). Then
	$$
	\begin{align*}
	(a + b)^n = \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^k
	\end{align*}
	$$
	where \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) for \(a \leq k \leq n\)
</div>
<p><br />
<b>Proof</b>
<br />
Use Pascal’s Identity: \(\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}\)… TODO
<br />
<br />
As a consequence of the binomial theorem, we have the next proposition
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(p\) be prime. For all \(a, b \in \mathbf{Z}\):
	$$
	\begin{align*}
	(a + b)^p \equiv a^p + b^p \bmod p
	\end{align*}
	$$
</div>
<p><br />
<br />
First observe that \((a+b)^5 = a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + b^5\). So those middle terms all have coefficients divisible by 5. Therefore, they’ll go away if we apply mod \(5\). This happens when \(p\) is prime.
<br />
<br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Using the binomial theorem, we can first expand the sum \((a+b)^p\) as follows</p>
<div>
	$$
	\begin{align*}
	(a + b)^p &amp;= \sum_{k=0}^{n} \binom{p}{k} a^{p-k} b^k \\
	          &amp;= a^{p} + \binom{p}{1} a^{p-1} b + \binom{p}{2} a^{p-2} b^2 + ... + b^k \\
	\end{align*}
	$$
</div>
<p>What we need to show is that \(\binom{p}{k} \equiv 0 \bmod p\) if \(0 &lt; k &lt; p\). If we show this, then what’s left is the first and last terms only. Observe that</p>
<div>
	$$
	\begin{align*}
	\binom{p}{k} &amp;= \frac{p!}{k!(p-k)!} \\
	\binom{p}{k} k! (p-k)! &amp;= p! \\
	\end{align*}
	$$
</div>
<p>\(p! = p(p-1)...1\). So \(p \ | \ p!\). But \(p! = \binom{p}{k} k! (p-k)!\) so \(p\) divides this whole product. Since \(p\) is prime, then \(p\) must divide one of the factors. Now observe that \(p\) can’t divide \(k!\) since \(k &lt; p\). \(p\) doesn’t divide \((p - k)!\) either since \(p - k &lt; p\). Therefore, \(p\) must divide \(\binom{p}{k}\) as desired. \(\ \blacksquare\). 
<!------------------------------------------------------------------------------>
<br />
<br />
A consequence of this proof is Fermat’s Little Theorem
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Fermat's Little Theorem</b></h4>
<p>Next, we will prove Fermat’s Little Theorem.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
	Let \(p\) be prime, \(a \in \mathbf{Z}\)
	<ol>
		<li>\(a^p \equiv a \bmod p\)</li>
		<li>If \(p \nmid a\), then \(a^{p-1} \equiv 1 \bmod p\)</li>
	</ol>
</div>
<!------------------------------------------------------------------------------>
<p><br />
<b>Proof of (1)</b>
<br /></p>
<div class="proofdiv">
By Induction on \(a\) for \(a \geq 1\).
<br />
Base Case (\(a = 1\)): \(1^p \equiv 1 \bmod p\) and so we're done.
<br />
<br />
Inductive Case (\(a &gt; 1\)): 
Assume it is true for \(a\). We will show that it is true for \(a+1\). 
<div>
	$$
	\begin{align*}
	(a + 1)^p &amp;\equiv a^p + 1^p \quad \text{(By the previous proposition)} \\
	          &amp;\equiv a + 1 \quad \text{(By the inductive hypothesis)}
	\end{align*}
	$$
</div>
</div>
<!------------------------------------------------------------------------>
<p><br />
Note that if \(a = 0\), then \(0^p = 0\). 
<br />
<!------------------------------------------------------------------------>
For \(a \leq 0\), we can use downward induction.</p>
<div class="proofdiv">
By Induction on \(a\) for \(a &lt; 0\).
<br />
Base Case (\(a = -1\)):  We need to show that \((-1)^p \equiv -1 \bmod p\). \(p\) is prime so we have two cases. If \(p = 2\), then \((-1)^2 = 1 = -1 \bmod 2\). If \(p\) is odd, then \((-1)^p = -1\).
<br />
<br />
Inductive Case (\(a &lt; -1\)): We want to show that \(a^p \equiv a\) implies \((a - 1)^p = a - 1\).
</div>
<p><br />
<!------------------------------------------------------------------------>
<b>Proof of (2)</b>
<br />
We are given that \(a \nmid p\). From part (1), we know that \(a^p \equiv a \bmod p\). So \(p \ | \ a^p - a = a(a^{p-1} - 1)\). But since \(p\) is prime and since it doesn’t divide \(p\), then it must divide \(a^{p-1} - 1\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Two-Prime Fermat</b></h4>
<p>There is a more generalized version of Fermat’s Little Theorem. We will use Fermat’s theorem to prove it.
<br />
<br /></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
	Let \(p, q\) be distinct primes and let \(n = pq\). Let \(m\) be
	$$
	\begin{align*}
	   m = lcm(p-1, q-1) = \frac{(p-1)(q-1)}{gcd(p-1,q-1)}
	\end{align*}
	$$
If \(a \in \mathbf{Z}\), \(h \in \mathbf{N}\) such that \(h \equiv 1 \bmod m\), then \(a^h \equiv a \bmod n\)
</div>
<!------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
We know that \(h \equiv 1 \bmod m\) so \(h = 1 + tm\) for some \(t \in \mathbf{Z}\). We want to show that \(a^h \equiv a \bmod n\). In other words, we want to show that \(n \ | \ a^h - a\) which means that \(pq \ | \ a^h - a\). We can write \(a^h - a\) as follows</p>
<div>
	$$
	\begin{align*}
	   a^h - a &amp;= a^{1+tm} - a \\
	           &amp;= a(a^{tm} - 1).
	\end{align*}
	$$
</div>
<p>Therefore, want to show that \(pq\) divides \(a(a^{tm} - 1)\). Lecture 5 Proposition (Corollary 1.6.17 in the book) states that if \(a\) and \(b\) are relatively prime, \(a \ | \ n\) and \(b \ | \ n\), \(ab \ | \ n\). We’re given that \(p\) and \(q\) are distinct primes so they are relatively prime. So the goal is to prove that \(p\) divides \(a(a^{tm} - 1)\) and \(q\) divides \(a(a^{tm} - 1)\) to conclude that the product \(pq\) divides \(a(a^{tm} - 1)\).
<br />
<br />
To start, we want to show that \(p \ | \ a(a^{tm} - 1)\). But since \(p\) is prime, then it will have to divide \(a\) or \(a^{tm} - 1\). So we need to show that either \(p \ | \ a\) or \(p \ | \ a^{tm} - 1\). So suppose that \(p \nmid a\). We claim that \(p \ | \ a^{tm} - 1\). Recall that \(m = lcm(p-1, q-1)\). So \(m\) is a multiple of \(p-1\) and we can write \(m = (p - 1)s\) for some \(s \in \mathbf{N}\). So now we can write \(a^{tm}\) as follows</p>
<div>
	$$
	\begin{align*}
	   a^{tm} &amp;= a^{ts(p-1)} \\
	              &amp;= (a^{p-1})^{ts}.
	\end{align*}
	$$
</div>
<p>Since we assumed that \(p \nmid a\). Then we can use Fermat’s Little Theorem (part 2) to conclude that \(a^{p-1} \equiv 1 \bmod p\). Now we can use modular arithmetic to simplify the original expression.</p>
<div>
	$$
	\begin{align*}
	   (a^{p-1})^{ts} &amp;\equiv 1^{ts} \bmod p \text{ (because } a^{p-1} \equiv 1 \bmod p) \\
	                  &amp;\equiv 1 \bmod p.
	\end{align*}
	$$
</div>
<p>This implies that \(a^{tm} - 1\) must be divisible by \(p\) as we wanted to show. So \(p\) divides the product \(a(a^{tm} - 1)\). With a similar argument, we can show this for \(q\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>RSA Cryptosystem</b></h4>
<p>This encryption method is based on the two prime fermat theorem. It is widely used to encrypt many of the transactions that happen on the internet.
<br />
<br />
Symmetric Encryption:
We have some plain text (\(x \in \mathbf{Z}_n\)) that we want to encrypt. We have a function that takes a key (\(e \in \mathbf{Z}\)) to encrypt the plain text and turn it to encrypted text (\(y \in \mathbf{Z}_n\)). To decrypt it back, we have to use the same key again to turn it to plain text. The flaw in this method is that the encryption and decryption keys are the same and will need to be shared somehow. That’s why we have an alternative:
<br />
<br />
Asymmetric Encryption:
This is the same proces except that now we have a decryption key \((d \in \mathbf{Z})\). So now we have a pair of keys \((e, d)\). You will broadcast \(e\) so that anyone can encrypt a message and send it to you, but you are the only with the decryption key. The goal here is to design the pair such that one one can deduce \(d\) from \(e\). So how to design such a pair?
<br />
<br />
One way to implement this idea is the following (RSA):</p>
<ol>
	<li>Pick a prime number \(p,q\) large (100s of digits)</li>
	<li>\(n = pq\)</li>
	<li>\(m = lcm(p-1,q-1)\)</li>
	<li>Choose \(1 &lt; d, e &lt; m\) such that \(de \equiv 1 \bmod m\). You can first pick \(d\) and then you can pick a multiplicative inverse of \(d\) to be \(e\) using the Euclidean Algorithm.</li>
</ol>
<p>So now, you take the plain test \(x\) and raise it to the \(e\)th power. \(x^e\) is the encrypted text. \(e\) is the encryption key. The output \(y = x^e\) is the encrypted text. To decrypt it, raise the encrypted text to the \(d\)th power so \(y^d = x^{ed}\) and so now we have the following equation</p>
<div>
	$$
	\begin{align*}
	  x^{ed} \equiv x \bmod pq
	\end{align*}
	$$
</div>
<p>This is Two-prime Fermat.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition (Book Definition 1.7.1) Given integers \(a\) and \(b\), and a natural number \(n\), we say that "\(a\) is congruent to \(b\) modulo \(n\)" and we write $$ \begin{align*} a \equiv b \bmod n \quad \text{or} \quad a \equiv_n b \end{align*} $$ if \(a - b\) is divisible by \(n\) or \(n \ | \ a - b\). So there exists some \(t \in \mathbf{Z}\) such that \(a - b = tn\) Example: \((1 \bmod 7) = (8 \bmod 7) = 1\). Therefore, \(1 \equiv_7 8\). Definition For each integer \(a\), write $$ \begin{align*} [a] &amp;= [a]_n = \{x \in \mathbf{Z} \ | \ x \equiv a \bmod n\} \subseteq \mathbf{Z} \\ &amp;= \{ a + ny \ | \ y \in \mathbf{Z} \} \end{align*} $$ The set \([a]\) is called the residue class or congruence class of \(a\) modulo \(n\). Example: \([2]_6 = \{2 + 6y \ | \ y \in \mathbf{Z}\} = \{ ...,-10,-4,2,8,14,20,... \}\) \([2]_6 = [8]_6 = [-10]_6 = [602]_6 = ....\) The Remainder Function Another important definition that we need is the following Definition \(rem_n \ : \ \mathbf{Z} \rightarrow \{0,1,2,...,n-1\}\) is the remainder function after dividing by \(n\). \(rem_n(a) = r\) is the unique remainder of \(a \div n\) such that \(0 \leq r &lt; n\) and \(a = qn + r\) for some \(q \in \mathbf{Z}\). Note that \(a - r\) is divisible by \(n\) or \(a \equiv_n r\). It’s important to note that the remainder is in the same congruence class as \(a\). In fact \([a]_n \cap \{0,...n-1\} = \{r\}\). Usually the remainder \(r\) is the standard/canonical name for the congruence class. So for example, we usually don’t write \([602]_6\) but write \([2]_6\). But we don’t have to put it in canonical form. Properties of Congruence Congruence is an equivalence relation. The following properties show this. Lemma (Book 1.7.2) Properties of Congruence: Reflexive: For all \(a \in \mathbf{Z}\), \(a \equiv a \bmod n\). Symmetric: For all \(a, b \in \mathbf{Z}\), \(a \equiv b \bmod n\) if and only if \(b \equiv a \bmod n\). Transitive: For all \(a, b, c \in \mathbf{Z}\), if \(a \equiv b \bmod n\) and \(b \equiv c \bmod n\), then \(a \equiv c \bmod n\). Proof (book) For \((a)\), \(a - a = 0\) is divisible by \(n\). For \((b)\), if \(a - b\) is divisible by \(n\), then \(b - a\) is also divisible by \(n\) and vice versa. For \((c)\), if \(a - b\) is divisible by \(n\) and \(b - c\) is divisible by \(4n\), then \((a - b) + (b - c) = a - c\) is also divisible by \(n\). \(\ \blacksquare\) Based on these properties, we have the following proposition Proposition (Book Lemma 1.7.3) For \(a, b \in \mathbf{Z}\), the following are equivalent: \(a \equiv b \bmod n\). \([a]_n = [b]_n\). \(\text{rem}_n(a) = \text{rem}_n(b)\). \([a]_n \cap [b]_n \neq \emptyset\). Proof (Book): \((a) \implies (b)\): Suppose \(a \equiv b \bmod n\). We want to show that \([a]_n = [b]_n\). \([a]_n \subseteq [b]_n\): Let \(c \in \mathbf{Z}\). If \(c \equiv a \bmod n\), then \(c \equiv b \bmod n\) by Lemma 1.7.2 (c). Therefore \([a]_n \subseteq [b]_n\). \([b]_n \subseteq [a]_n\): If \(c \equiv b \bmod n\), then \(c \equiv a \bmod n\) and so \([a] = [b]\) as required. \((b) \implies (c)\): By definition, \(\text{rem}_n(x)\) is the unique element of \([x]\) that lies inside \(\{0,1,...,n-1\}\). So if \([a]_n=[b]_n\), then it must be the same element. \((c) \implies (d)\): \((d)\) is an immediate application of \((c)\) \((d) \implies (a)\): Suppose that \([a]_n \cap [b]_n \neq \emptyset\). Let \(c \in [a]_n \cap [b]_n\). Then \(a \equiv c \bmod n\) and \(b \equiv c \bmod n\). But this implies that \(a \equiv b \bmod n\). \(\ \blacksquare\) Modular Arithmetic The following lemma establishes how modular arithmetic is done. Proposition (Book Lemma 1.7.5) Let \(a, a', b, b'\) be integers with \(a \equiv a' \bmod n\) and \(b \equiv b' \bmod n\). Then $$ \begin{align*} a + b &amp;\equiv a' + b' \bmod n \\ ab &amp;\equiv a'b' \bmod n \end{align*} $$ Proof [TODO] We can now use these modular arithmetic properties to define algebraic structures on a set. Definition $$ \begin{align*} \mathbf{Z}_n &amp;= \{ \text{ set of congruence classes mod } n \} \\ &amp;= \{ [a]_n \ | \ a \in \mathbf{Z} \} \\ &amp;= \{ [0]_n, [1]_n,...,[n-1]_n \ | \ a \in \mathbf{Z} \} \\ \end{align*} $$ So now we can use the operations we defined previously to turn this set into a commutative ring. Definition Define operations \(+\), \(\cdot\) on \(\mathbf{Z}_n\) by $$ \begin{align*} [a]_n + [b]_n &amp;= [a + b]_n \\ [a]_n[b]_n &amp;= [ab]_n \end{align*} $$ \((\mathbf{Z}_n,+,\cdot)\) is commutative ring with identity while \((\mathbf{Z}_n,\cdot)\) is a commutative monoid. \([1]\) is the identity element. Only some elements \([a] \in \mathbf{Z}_n\) have a multiplicative inverse. (such that \([a][b] = [1] = [b][a]\)). The question is when do we have a multiplicative inverse? The answer is in the following proposition. Proposition Let \(n \geq 1, a \in \mathbf{Z}\). Then \([a]\) has a multiplicative inverse in \(\mathbf{Z}\) if and only if \(gcd(a,n) = 1\). Proof Suppose that \(gcd(a,n)=1\). Then there exists \(r, s \in \mathbf{Z}\) such that \(ar + ns = 1\). Re-writing this equation, we observe that $$ \begin{align*} ar + ns = 1 &amp;\Longleftrightarrow ar = 1 + (-s)n \quad \text{ (so $ar$ and 1 differ by a multiple of $n$)}\\ &amp;\Longleftrightarrow ar = 1 \bmod n \\ &amp;\Longleftrightarrow [a][r] = 1. \ \blacksquare \\ \end{align*} $$ We will use \(\phi(n) = \{x \in \mathbf{Z}_n \ | \ x \text{ has a multiplicative inverse}\} \in \mathbf{Z}_n\). \((\phi(n))\) is a commutative group. Binomial Theorem Next we have the binomial theorem which we need to prove a proposition which will then lead to Fermat’s Little Theorem. Theorem (Binomial Theorem) \(a, b \in \mathbf{R}, n \in \mathbf{N}\). Then $$ \begin{align*} (a + b)^n = \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^k \end{align*} $$ where \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) for \(a \leq k \leq n\) Proof Use Pascal’s Identity: \(\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}\)… TODO As a consequence of the binomial theorem, we have the next proposition Proposition Let \(p\) be prime. For all \(a, b \in \mathbf{Z}\): $$ \begin{align*} (a + b)^p \equiv a^p + b^p \bmod p \end{align*} $$ First observe that \((a+b)^5 = a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + b^5\). So those middle terms all have coefficients divisible by 5. Therefore, they’ll go away if we apply mod \(5\). This happens when \(p\) is prime. Proof Using the binomial theorem, we can first expand the sum \((a+b)^p\) as follows $$ \begin{align*} (a + b)^p &amp;= \sum_{k=0}^{n} \binom{p}{k} a^{p-k} b^k \\ &amp;= a^{p} + \binom{p}{1} a^{p-1} b + \binom{p}{2} a^{p-2} b^2 + ... + b^k \\ \end{align*} $$ What we need to show is that \(\binom{p}{k} \equiv 0 \bmod p\) if \(0 &lt; k &lt; p\). If we show this, then what’s left is the first and last terms only. Observe that $$ \begin{align*} \binom{p}{k} &amp;= \frac{p!}{k!(p-k)!} \\ \binom{p}{k} k! (p-k)! &amp;= p! \\ \end{align*} $$ \(p! = p(p-1)...1\). So \(p \ | \ p!\). But \(p! = \binom{p}{k} k! (p-k)!\) so \(p\) divides this whole product. Since \(p\) is prime, then \(p\) must divide one of the factors. Now observe that \(p\) can’t divide \(k!\) since \(k &lt; p\). \(p\) doesn’t divide \((p - k)!\) either since \(p - k &lt; p\). Therefore, \(p\) must divide \(\binom{p}{k}\) as desired. \(\ \blacksquare\). A consequence of this proof is Fermat’s Little Theorem Fermat's Little Theorem Next, we will prove Fermat’s Little Theorem. Theorem Let \(p\) be prime, \(a \in \mathbf{Z}\) \(a^p \equiv a \bmod p\) If \(p \nmid a\), then \(a^{p-1} \equiv 1 \bmod p\) Proof of (1) By Induction on \(a\) for \(a \geq 1\). Base Case (\(a = 1\)): \(1^p \equiv 1 \bmod p\) and so we're done. Inductive Case (\(a &gt; 1\)): Assume it is true for \(a\). We will show that it is true for \(a+1\). $$ \begin{align*} (a + 1)^p &amp;\equiv a^p + 1^p \quad \text{(By the previous proposition)} \\ &amp;\equiv a + 1 \quad \text{(By the inductive hypothesis)} \end{align*} $$ Note that if \(a = 0\), then \(0^p = 0\). For \(a \leq 0\), we can use downward induction. By Induction on \(a\) for \(a &lt; 0\). Base Case (\(a = -1\)): We need to show that \((-1)^p \equiv -1 \bmod p\). \(p\) is prime so we have two cases. If \(p = 2\), then \((-1)^2 = 1 = -1 \bmod 2\). If \(p\) is odd, then \((-1)^p = -1\). Inductive Case (\(a &lt; -1\)): We want to show that \(a^p \equiv a\) implies \((a - 1)^p = a - 1\). Proof of (2) We are given that \(a \nmid p\). From part (1), we know that \(a^p \equiv a \bmod p\). So \(p \ | \ a^p - a = a(a^{p-1} - 1)\). But since \(p\) is prime and since it doesn’t divide \(p\), then it must divide \(a^{p-1} - 1\). Two-Prime Fermat There is a more generalized version of Fermat’s Little Theorem. We will use Fermat’s theorem to prove it. Theorem Let \(p, q\) be distinct primes and let \(n = pq\). Let \(m\) be $$ \begin{align*} m = lcm(p-1, q-1) = \frac{(p-1)(q-1)}{gcd(p-1,q-1)} \end{align*} $$ If \(a \in \mathbf{Z}\), \(h \in \mathbf{N}\) such that \(h \equiv 1 \bmod m\), then \(a^h \equiv a \bmod n\) Proof We know that \(h \equiv 1 \bmod m\) so \(h = 1 + tm\) for some \(t \in \mathbf{Z}\). We want to show that \(a^h \equiv a \bmod n\). In other words, we want to show that \(n \ | \ a^h - a\) which means that \(pq \ | \ a^h - a\). We can write \(a^h - a\) as follows $$ \begin{align*} a^h - a &amp;= a^{1+tm} - a \\ &amp;= a(a^{tm} - 1). \end{align*} $$ Therefore, want to show that \(pq\) divides \(a(a^{tm} - 1)\). Lecture 5 Proposition (Corollary 1.6.17 in the book) states that if \(a\) and \(b\) are relatively prime, \(a \ | \ n\) and \(b \ | \ n\), \(ab \ | \ n\). We’re given that \(p\) and \(q\) are distinct primes so they are relatively prime. So the goal is to prove that \(p\) divides \(a(a^{tm} - 1)\) and \(q\) divides \(a(a^{tm} - 1)\) to conclude that the product \(pq\) divides \(a(a^{tm} - 1)\). To start, we want to show that \(p \ | \ a(a^{tm} - 1)\). But since \(p\) is prime, then it will have to divide \(a\) or \(a^{tm} - 1\). So we need to show that either \(p \ | \ a\) or \(p \ | \ a^{tm} - 1\). So suppose that \(p \nmid a\). We claim that \(p \ | \ a^{tm} - 1\). Recall that \(m = lcm(p-1, q-1)\). So \(m\) is a multiple of \(p-1\) and we can write \(m = (p - 1)s\) for some \(s \in \mathbf{N}\). So now we can write \(a^{tm}\) as follows $$ \begin{align*} a^{tm} &amp;= a^{ts(p-1)} \\ &amp;= (a^{p-1})^{ts}. \end{align*} $$ Since we assumed that \(p \nmid a\). Then we can use Fermat’s Little Theorem (part 2) to conclude that \(a^{p-1} \equiv 1 \bmod p\). Now we can use modular arithmetic to simplify the original expression. $$ \begin{align*} (a^{p-1})^{ts} &amp;\equiv 1^{ts} \bmod p \text{ (because } a^{p-1} \equiv 1 \bmod p) \\ &amp;\equiv 1 \bmod p. \end{align*} $$ This implies that \(a^{tm} - 1\) must be divisible by \(p\) as we wanted to show. So \(p\) divides the product \(a(a^{tm} - 1)\). With a similar argument, we can show this for \(q\). RSA Cryptosystem This encryption method is based on the two prime fermat theorem. It is widely used to encrypt many of the transactions that happen on the internet. Symmetric Encryption: We have some plain text (\(x \in \mathbf{Z}_n\)) that we want to encrypt. We have a function that takes a key (\(e \in \mathbf{Z}\)) to encrypt the plain text and turn it to encrypted text (\(y \in \mathbf{Z}_n\)). To decrypt it back, we have to use the same key again to turn it to plain text. The flaw in this method is that the encryption and decryption keys are the same and will need to be shared somehow. That’s why we have an alternative: Asymmetric Encryption: This is the same proces except that now we have a decryption key \((d \in \mathbf{Z})\). So now we have a pair of keys \((e, d)\). You will broadcast \(e\) so that anyone can encrypt a message and send it to you, but you are the only with the decryption key. The goal here is to design the pair such that one one can deduce \(d\) from \(e\). So how to design such a pair? One way to implement this idea is the following (RSA): Pick a prime number \(p,q\) large (100s of digits) \(n = pq\) \(m = lcm(p-1,q-1)\) Choose \(1 &lt; d, e &lt; m\) such that \(de \equiv 1 \bmod m\). You can first pick \(d\) and then you can pick a multiplicative inverse of \(d\) to be \(e\) using the Euclidean Algorithm. So now, you take the plain test \(x\) and raise it to the \(e\)th power. \(x^e\) is the encrypted text. \(e\) is the encryption key. The output \(y = x^e\) is the encrypted text. To decrypt it, raise the encrypted text to the \(d\)th power so \(y^d = x^{ed}\) and so now we have the following equation $$ \begin{align*} x^{ed} \equiv x \bmod pq \end{align*} $$ This is Two-prime Fermat. References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 06: Least Common Multiple</title><link href="http://localhost:4000/jekyll/update/2025/01/29/math417-06-lcm.html" rel="alternate" type="text/html" title="Lecture 06: Least Common Multiple" /><published>2025-01-29T00:01:36-08:00</published><updated>2025-01-29T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/29/math417-06-lcm</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/29/math417-06-lcm.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A least common multiple (LCM) is \(m \in \mathbf{Z_{\geq 0}}\) such that:
<ol type="a">
	<li>\(a \ | \ m\) and \(b \ | \ m\)</li>
	<li>If \(n \in \mathbf{Z}\), \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\).</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
In fact the LCM is unique if it exists. To prove it’s existence we’ll need the following definition
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition (Well Ordering Principle)
</div>
<div class="mintbodydiv">
A non-empty subset of \(\mathbf{N}\) has a smallest element
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
Next, we’ll prove the existence of the LCM.
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
The LCM always exists
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Let \(a, b \in \mathbf{Z}\). Let \(J = \mathbf{Z}a \cap \mathbf{Z}b\). We want to show that \(J\) itself is a set of multiples so we can write \(J = \mathbf{Z}m\) for some \(m \in \mathbf{Z}_{\geq 0}\). This means that \(m\) must be the LCM of \(a\) and \(b\) by definition. [Why? If \(J = \mathbf{Z}m\), then \(m\) is the least element in \(J\), all the other elements are multiples of \(m\). We have \(a \ | \ m\) and \(b \ | \ m\) because \(J\) also contains common multiples of \(a\) and \(b\). And for any \(n \in \mathbf{Z}\), if \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\). This is true because \(m\) is the smallest of all these common multiples.] We will use division by remainder to prove this!
<br />
<br />
Observe that \(J\) is closed under addition and subtraction. So if \(c, d \in J\), then any integer combination of \(c\) and \(d\) will be in \(J\). So for any \(m, n \in \mathbf{Z}\),  \(cm + dn \in J\).
<br />
<br />
We have two cases:
<br />
Case 1: If either \(a = 0\) or \(b = 0\), then \(J = \{0\} = \mathbf{Z}0\).
<br />
<br />
Case 2: If \(a \neq 0\) and \(b \neq 0\), then \(J\) is the set of common multiples of \(\mathbf{Z}a\) and \(\mathbf{Z}b\). Consider \(J \cap \mathbf{N}\). This is the set of positive common multiples. This set is not empty because it will at least include \(|ab|\). So \(J \cap \mathbf{N}\) is a non-empty subset of \(\mathbf{N}\). By the well-ordering principle, there is a smallest number in \(J \cap \mathbf{N}\). Let \(m \in J \cap \mathbf{N}\) be the smallest common multiple. (so we’re setting \(m\) to be the smallest positive common multiple of \(\mathbf{Z}a\) and \(\mathbf{Z}b\))
<br />
<br />
We claim that \(J = \mathbf{Z}m\) (the set of multiples of \(m\)). We will show this by proving that \(J \subseteq \mathbf{Z}m\) and that \(\mathbf{Z}m \subseteq J\). 
<br />
<br />
\(\mathbf{Z}m \subseteq J\): This is true since \(m \in J\) and we showed earlier that \(m\) is closed under addition and subtraction so any multiple of \(m\) is also in \(J\) and therefore \(\mathbf{Z}m \subseteq J\).
<br />
<br />
\(J \subseteq \mathbf{Z}m\): Suppose \(x \in J = \mathbf{Z}a \cap \mathbf{Z}b\). Consider what happens when we divide \(x\) by \(m\). We can write \(x = qm + r\). with \(q, r \in \mathbf{Z}\) and \(0 \leq r &lt; m\). Re-write the equation as</p>
<div>
$$
\begin{align*}
r = x - qm.
\end{align*}
$$
</div>
<p>Observe now that \(x \in J\) by assumption. \(qm\) is a multiple of \(m\) so it’s in \(J\). So their difference is also in \(J\) which means that \(r \in J\). If \(r = 0\), \(x = qm \in \mathbf{Z}m\) and we’re done. If \(r \neq 0\), then \(r \in J\), then \(r \in J \cap \mathbf{N}\) but also \(r &lt; m\). But that’s a contradiction because \(m\) is the smallest element in \(J \cap \mathbf{N}\) and so this case doesn’t happen. So \(J = \mathbf{Z}m\) as desired. \(\ \blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Consequences of the LCM</b></h4>
<p>The next few propositions are some consequences of the LCM.
<br />
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(d = gcd(a,b) = 1\), then \(m = lcm(a,b) = ab\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
We are given that \(d = gcd(a,b) = 1\). This means that \(a\) and \(b\) are relatively prime. Condition 1 is true since \(a \ | \ ab\) and \(b \ | \ ab\). For condition two of the LCM definition, let \(n \in \mathbf{Z}\) such that \(a \ | \ n\) and \(b \ | \ n\). By the proposition from lecture 5, \(ab \ | \ n\). (Proposition: If \(a\) and \(b\) are relatively prime, then if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | \ n\)). \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(e\) be a common divisor of \(a\) and \(b\) so \(e \ | \ a\) and \(e \ | \ b\). Then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\). 
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
[TODO] \(x\) is a common divisor of \(\frac{a}{e}\) and \(\frac{b}{e}\) if and only if \(xe\) is a common divisor of \(a\) and \(b\). 
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(a, b \in \mathbf{N}\), \(d = gcd(a,b)\) and \(m = lcm(a,b)\), then \(m = \frac{ab}{d}\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
We need the previous two propositions</p>
<ol>
	<li>If \(d = 1\), then \(m = lcm(a,b) = ab\)</li>
	<li>If \(e \ | \ a\) and \(e \ | \ b\), then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\)</li>
</ol>
<p>If we divide both \(a\) and \(b\) by \(d\), their gcd will become 1 so \(gcd(\frac{a}{d}, \frac{b}{d}) = 1\). Since the gcd is 1, then \(lcm(\frac{a}{d}, \frac{b}{d}) = \frac{b}{d}\frac{a}{d}\) by part \((1)\). So now let \(e = d\), so by \((2)\),</p>
<div>
$$
\begin{align*}
\frac{1}{d} lcm(a, b) &amp;= lcm(\frac{a}{d}, \frac{b}{d}) \\
\frac{m}{d}  &amp;= \frac{b}{d}\frac{a}{d} \\
m &amp;= \frac{ab}{d} \\
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Pairwise Relatively Prime</b></h4>
<p><br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
\(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime if \(gcd(a_i,a_j) = 1\) for all \(1 \leq i,j \leq k, i \neq j\)
</div>
<p><br />
<br />
Some facts based this:
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
If \(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime, then \(a_k,b = a_1,...,a_{k-1}\) are relatively prime so \(gcd(a_k,b) = 1\). 
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose for the sake of contradiction that \(a_k\) and \(b\) are not relatively prime so they have a common divisor. Let \(p\) be a prime that is a common divisor of \(a_k\) and \(b\). So \(p \ | \ b\) which means that \(p \ | \ a_1a_2...a_{k-1}\). But since \(p\) is prime, then by the proposition from the previous lecture, \(p\) must divide one of the factors in \(a_1a_2...a_{k-1}\). Let \(p\) divide \(a_i\) for some \(1 \leq i \leq k - 1\). However \(p \ | \ p_k\). This means that \(gcd(a_i,a_k) \neq 1\). This is a contradiction since we assumed that \(a_1,...,a_k\) are pairwise relatively prime. Therefore, we must have \(a_k\) and \(b\) be relatively prime. \(\ \blacksquare\)
<br />
<br />
The consequence of this lemma is the following
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(a_1, a_2,...,a_k\) are pairwise relatively prime and if \(a_i \ | \ n\) for all \(i = 1,...,k\), then \(a_1a_2,...,a_k \ | \ n\) 
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
By Induction on \(k\) 
<br />
Base Case: If \(k=1\), then there is nothing to prove. 
<br />
If \(k = 2\), then \(gcd(a_1,a_2)=1\). If \(a_1 \ | \ n\) and \(a_1 \ | \ n\), then by the proposition from lecture 5, we must have \(ab \ | \ n\).
<br />
<br />
Inductive Case \(k \geq 3\): <br />
Let \(b = a_1a_2...a_{k-1}\).  By the previous lemma we know that \(gcd(a_k,b) = 1\). Also by the inductive hypothesis if \(a_1,...,a_{k-1} \ | \ n\), then \(a_1...a_{k-1} = b \ | \ n\). but \(gcd(a_k,b) = 1\) so \(a_k\) and \(b\) are relatively prime. Since they are relatively prime and they both divide \(n\), then \(ba_k = a_1...a_{k-1}a_k\) also divides \(n\) as we wanted to show. \(\ \blacksquare\)</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A least common multiple (LCM) is \(m \in \mathbf{Z_{\geq 0}}\) such that: \(a \ | \ m\) and \(b \ | \ m\) If \(n \in \mathbf{Z}\), \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\). In fact the LCM is unique if it exists. To prove it’s existence we’ll need the following definition Definition (Well Ordering Principle) A non-empty subset of \(\mathbf{N}\) has a smallest element Next, we’ll prove the existence of the LCM. Proposition The LCM always exists Proof Let \(a, b \in \mathbf{Z}\). Let \(J = \mathbf{Z}a \cap \mathbf{Z}b\). We want to show that \(J\) itself is a set of multiples so we can write \(J = \mathbf{Z}m\) for some \(m \in \mathbf{Z}_{\geq 0}\). This means that \(m\) must be the LCM of \(a\) and \(b\) by definition. [Why? If \(J = \mathbf{Z}m\), then \(m\) is the least element in \(J\), all the other elements are multiples of \(m\). We have \(a \ | \ m\) and \(b \ | \ m\) because \(J\) also contains common multiples of \(a\) and \(b\). And for any \(n \in \mathbf{Z}\), if \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\). This is true because \(m\) is the smallest of all these common multiples.] We will use division by remainder to prove this! Observe that \(J\) is closed under addition and subtraction. So if \(c, d \in J\), then any integer combination of \(c\) and \(d\) will be in \(J\). So for any \(m, n \in \mathbf{Z}\), \(cm + dn \in J\). We have two cases: Case 1: If either \(a = 0\) or \(b = 0\), then \(J = \{0\} = \mathbf{Z}0\). Case 2: If \(a \neq 0\) and \(b \neq 0\), then \(J\) is the set of common multiples of \(\mathbf{Z}a\) and \(\mathbf{Z}b\). Consider \(J \cap \mathbf{N}\). This is the set of positive common multiples. This set is not empty because it will at least include \(|ab|\). So \(J \cap \mathbf{N}\) is a non-empty subset of \(\mathbf{N}\). By the well-ordering principle, there is a smallest number in \(J \cap \mathbf{N}\). Let \(m \in J \cap \mathbf{N}\) be the smallest common multiple. (so we’re setting \(m\) to be the smallest positive common multiple of \(\mathbf{Z}a\) and \(\mathbf{Z}b\)) We claim that \(J = \mathbf{Z}m\) (the set of multiples of \(m\)). We will show this by proving that \(J \subseteq \mathbf{Z}m\) and that \(\mathbf{Z}m \subseteq J\). \(\mathbf{Z}m \subseteq J\): This is true since \(m \in J\) and we showed earlier that \(m\) is closed under addition and subtraction so any multiple of \(m\) is also in \(J\) and therefore \(\mathbf{Z}m \subseteq J\). \(J \subseteq \mathbf{Z}m\): Suppose \(x \in J = \mathbf{Z}a \cap \mathbf{Z}b\). Consider what happens when we divide \(x\) by \(m\). We can write \(x = qm + r\). with \(q, r \in \mathbf{Z}\) and \(0 \leq r &lt; m\). Re-write the equation as $$ \begin{align*} r = x - qm. \end{align*} $$ Observe now that \(x \in J\) by assumption. \(qm\) is a multiple of \(m\) so it’s in \(J\). So their difference is also in \(J\) which means that \(r \in J\). If \(r = 0\), \(x = qm \in \mathbf{Z}m\) and we’re done. If \(r \neq 0\), then \(r \in J\), then \(r \in J \cap \mathbf{N}\) but also \(r &lt; m\). But that’s a contradiction because \(m\) is the smallest element in \(J \cap \mathbf{N}\) and so this case doesn’t happen. So \(J = \mathbf{Z}m\) as desired. \(\ \blacksquare\). Consequences of the LCM The next few propositions are some consequences of the LCM. Proposition If \(d = gcd(a,b) = 1\), then \(m = lcm(a,b) = ab\) Proof We are given that \(d = gcd(a,b) = 1\). This means that \(a\) and \(b\) are relatively prime. Condition 1 is true since \(a \ | \ ab\) and \(b \ | \ ab\). For condition two of the LCM definition, let \(n \in \mathbf{Z}\) such that \(a \ | \ n\) and \(b \ | \ n\). By the proposition from lecture 5, \(ab \ | \ n\). (Proposition: If \(a\) and \(b\) are relatively prime, then if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | \ n\)). \(\ \blacksquare\) Proposition Let \(e\) be a common divisor of \(a\) and \(b\) so \(e \ | \ a\) and \(e \ | \ b\). Then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\). Proof [TODO] \(x\) is a common divisor of \(\frac{a}{e}\) and \(\frac{b}{e}\) if and only if \(xe\) is a common divisor of \(a\) and \(b\). Proposition If \(a, b \in \mathbf{N}\), \(d = gcd(a,b)\) and \(m = lcm(a,b)\), then \(m = \frac{ab}{d}\) Proof We need the previous two propositions If \(d = 1\), then \(m = lcm(a,b) = ab\) If \(e \ | \ a\) and \(e \ | \ b\), then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\) If we divide both \(a\) and \(b\) by \(d\), their gcd will become 1 so \(gcd(\frac{a}{d}, \frac{b}{d}) = 1\). Since the gcd is 1, then \(lcm(\frac{a}{d}, \frac{b}{d}) = \frac{b}{d}\frac{a}{d}\) by part \((1)\). So now let \(e = d\), so by \((2)\), $$ \begin{align*} \frac{1}{d} lcm(a, b) &amp;= lcm(\frac{a}{d}, \frac{b}{d}) \\ \frac{m}{d} &amp;= \frac{b}{d}\frac{a}{d} \\ m &amp;= \frac{ab}{d} \\ \end{align*} $$ Pairwise Relatively Prime Definition \(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime if \(gcd(a_i,a_j) = 1\) for all \(1 \leq i,j \leq k, i \neq j\) Some facts based this: Lemma If \(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime, then \(a_k,b = a_1,...,a_{k-1}\) are relatively prime so \(gcd(a_k,b) = 1\). Proof Suppose for the sake of contradiction that \(a_k\) and \(b\) are not relatively prime so they have a common divisor. Let \(p\) be a prime that is a common divisor of \(a_k\) and \(b\). So \(p \ | \ b\) which means that \(p \ | \ a_1a_2...a_{k-1}\). But since \(p\) is prime, then by the proposition from the previous lecture, \(p\) must divide one of the factors in \(a_1a_2...a_{k-1}\). Let \(p\) divide \(a_i\) for some \(1 \leq i \leq k - 1\). However \(p \ | \ p_k\). This means that \(gcd(a_i,a_k) \neq 1\). This is a contradiction since we assumed that \(a_1,...,a_k\) are pairwise relatively prime. Therefore, we must have \(a_k\) and \(b\) be relatively prime. \(\ \blacksquare\) The consequence of this lemma is the following Proposition If \(a_1, a_2,...,a_k\) are pairwise relatively prime and if \(a_i \ | \ n\) for all \(i = 1,...,k\), then \(a_1a_2,...,a_k \ | \ n\) Proof By Induction on \(k\) Base Case: If \(k=1\), then there is nothing to prove. If \(k = 2\), then \(gcd(a_1,a_2)=1\). If \(a_1 \ | \ n\) and \(a_1 \ | \ n\), then by the proposition from lecture 5, we must have \(ab \ | \ n\). Inductive Case \(k \geq 3\): Let \(b = a_1a_2...a_{k-1}\). By the previous lemma we know that \(gcd(a_k,b) = 1\). Also by the inductive hypothesis if \(a_1,...,a_{k-1} \ | \ n\), then \(a_1...a_{k-1} = b \ | \ n\). but \(gcd(a_k,b) = 1\) so \(a_k\) and \(b\) are relatively prime. Since they are relatively prime and they both divide \(n\), then \(ba_k = a_1...a_{k-1}a_k\) also divides \(n\) as we wanted to show. \(\ \blacksquare\)]]></summary></entry><entry><title type="html">Lecture 05: Greatest Common Divisor</title><link href="http://localhost:4000/jekyll/update/2025/01/28/math417-05-gcd.html" rel="alternate" type="text/html" title="Lecture 05: Greatest Common Divisor" /><published>2025-01-28T00:01:36-08:00</published><updated>2025-01-28T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/28/math417-05-gcd</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/28/math417-05-gcd.html"><![CDATA[<div class="mintheaderdiv">
Definition 1.6.8
</div>
<div class="mintbodydiv">
A natural number \(d\) is the greatest common divisor of integers \(a\) and \(b\) if
<ol type="a">
	<li>\(d\) is a common divisor. So \(d\) divides \(a\) and \(b\)</li>
	<li>Every common divisor \(e\) also divides \(d\). In other words, if \(e \ | \ a\) and \(e \ | \ b\), then \(e \ | \ d\)</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
If the greatest common exists, then it is unique. Why? suppose \(d\) and \(d'\) are both greatest common divisors. Then by definition, \(d \ | \ d'\) and \(d' \ | \ d\) because every common divisor divides the gcd and they each must divide each other. But this means that \(d = \pm 'd\) by the divisibility properties from last lecture. But also by definition, \(d\) is non-negative and so \(d = 'd\). \(\ \blacksquare\)
<br />
<br />
Note that in the book \(a\) and \(b\) are nonzero in the definition. With this definition, the set of divisors of \(0\) is \(\mathbf{Z}\). If \(a = 0\) and \(b \neq 0\), then \(|a|\) is the gcd of \(a\) and \(0\). If both \(a\) and \(b\) are zero, then \(0\) is the gcd of \(0\) and \(0\). 
<br />
<br />
<!------------------------------------------------------------------------>
Before addressing the question of whether the greatest common divisor exists, we’ll to define one more thing
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(a, b \in \mathbf{Z}\). An integer combination of \(a\) and \(b\) is any integer of the form
$$
\begin{align*}
I(a,b) = \{ra + sb \ | \ r, s \in \mathbf{Z}\}.
\end{align*}
$$
</div>
<!------------------------------------------------------------------------>
<p><br />
For example \(I(4,6) = \{4s + 6t \ | \ s, t \in \mathbf{Z}\}\). If we let \(s = -1\) and \(t = 1\), then \(2 \in I(4,6)\). In fact, this set produces all of the even integers. It includes all multiples of 2. In other words, we can also write that \(I(4,6) = \mathbf{Z}2\). In fact, this turns out to always be true, the set of integer combinations of two integers is also the set of multiples of a number and that number is the greatest common divisor! even when one of the integers is \(0\). So yes the GCD exists and can even be computed. Before formally proving its existence, we’ll present the way it can be computed next.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Euclidean Algorithm</b></h4>
<p>Let \(\mathbf{Z}^2 = \{(a,b), a,b \in \mathbf{Z}\}\). 
<br />
Define \(F: \mathbf{Z}^2 \rightarrow \mathbf{Z}^2\) by</p>
<div class="ediv">
  $$
  \begin{equation*}
  F(m,n) = \begin{cases} (n,r), r = rem_n(m) \quad &amp;\text{if } n \neq 0 \\ (|m|,0) \quad \quad &amp;\text{if } n = 0\end{cases}
  \end{equation*}
  $$
</div>
<p><br />
\(rem_n(m)\) is the remainder of \(m \div n\) (recall that we can write \(m = qn + r\) where \(0 \leq r &lt; |n|)\)).
<br />
<br />
<b>Euclidean Algorithm:</b> Iterate \(F\) until stable.
<br />
<br />
Example:</p>
<div>
  $$
  \begin{align*}
  (42, -24) \xrightarrow{F} (-24, 18) \xrightarrow{F} (18, 12) \xrightarrow{F} (12, 6) \xrightarrow{F} (6, 0) \xrightarrow{F} (6, 0)
  \end{align*}
  $$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The GCD Theorem</b></h4>
<p>So now we have an algorithm to compute the GCD algorithm. Next, we will prove that it does compute the GCD and so the GCD does exist.
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(a, b \in \mathbf{Z}\). Then
<ol>
	<li>For \(a, b\) have a GCD \(d \geq 0\).</li>
	<li>\(I(a,b) = \mathbf{Z}d\).</li>
	<li>\(d\) is computed by the Euclidean algorithm.</li>
</ol>
</div>
<p><br />
We’ll start from (3) and then prove that the answer produced by the algorithm is in fact an integer combination of the original input (statement 2). Finally, we’ll show that this just means that we have computed the GCD (statement). To do all of this we will also need the following lemmas
<br />
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
If \(a, b \in I(c,d)\), then \(I(a,b) \subseteq I(c,d)\)
</div>
<p><br />
<b>Proof</b>: Homework Problem
<br />
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
If \(F(m,n) = (a,b)\), then \(I(m,n) = I(a,b)\)
</div>
<p><br />
<b>Proof</b>:
<br />
We have two cases:
<br />
<br />
Case 1: \(n \neq 0\). In this case, \(F(m,n) = (n, r)\) where \(r\) is the remainder after dividing \(m\) by \(n\). So \(m = qn + r\) where \(0 \leq r &lt; |n|\). We want to show that the integer combinations of \(m\) and \(n\) is the same as the integer combinations of \(n\) and \(r\), that is, \(I(m,n) = I(n,r)\). To do this, we will show that \(I(m,n) \subseteq I(n,r)\) and \(I(n,r) \subseteq I(m,n)\). To show that \(I(m,n) \subseteq I(n,r)\), observe that \(m\) is an integer combination of \(n\) and \(r\) because we can write \(m\) as</p>
<div>
  $$
  \begin{align*}
  m &amp;= qn + 1r.  
  \end{align*}
  $$
</div>
<p>Similarly, \(n\) is also an integer combination of \(n\) and \(r\) because we can write \(n\) as</p>
<div>
  $$
  \begin{align*}
  n &amp;= 1n + 0r.
  \end{align*}
  $$
</div>
<p>Since \(m\) and \(n\) can both be written as integer combinations of \(n\) and \(r\), that is \(m \in I(n,r)\) and \(n \in I(n,r)\), then by the previous lemma,  \(I(m,n) \subseteq I(n,r)\). To see that \(I(m,n) \subseteq I(n,r)\), observe that we can write \(n\) as \(n = 0m + 1n\) so \(n \in I(m,n)\). Similarly, \(r = 1m + (-q)m\) so \(r \in I(m,n)\). Then, by the previous lemma, \(I(m,n) \subseteq I(n,r)\). Therefore, \(I(m,n) = I(n,r)\). 
<br />
<br />
Case 2: \(n = 0\). In this case, \(F(m,n) = F(m,0) = (|m|,0)\). In this case the integer combinations of \(0\) and \(m\) are just multiples of \(m\) so \(I(m,0) = \mathbf{Z}m\). But this is the same as the integer combinations of \(|m|\) and \(0\) so \(\mathbf{Z}m = I(|m|,0) = I(m,0)\) which is what we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Proof of the GCD Theorem</b></h4>
<p>We iterate the algorithm until \(n = 0\) and \(F(m,n) = (|m|,0)\). By the previous lemma we saw that \(I(m,n) = \mathbf{Z}d\) for some \(d \geq 0\). 
<br />
<br />
<b>Claim:</b> \(d\) is a GCD.<br /></p>
<ol>
	<li>\(m,n \in I(m,n) = \mathbf{Z}d\). So \(m\) and \(n\) are both multiples of \(d\) and d divides both of them. \(d \ | \ m\) and \(d \ | \ n\).</li>
	<li>So now for condition 2, suppose \(e\) is a common divisor of \(m\) and \(n\). We want to show that \(e\) divides \(d\). Since \(e \ | \ m\) and \(e \ | \ n\), then \(m\) and \(n\) are both multiples of \(e\). So we can write \(m = eu\) and \(n = ev\) for some \(u,v \in \mathbf{Z}\). 
	<br />
	<br />
	But we know that any integer combination of \(m\) and \(n\) is a multiple of \(d\). So we can write for some \(r, s \in \mathbf{Z}\) 
	<div>
	  $$
	  \begin{align*}
	  d &amp;= rm + sn \\
	   &amp;= reu + sev \\
	   &amp;= e(ru + sv).
	  \end{align*}
	  $$
	</div>
	So \(e\) must divide \(d\).
</li>
</ol>
<p>Therefore, \(d = gcd(m,n)\) as desired. \(\ \blacksquare\) 
<br />
Note that the Euclidean Algorithm gives a method for computing \(r, s \in \mathbf{Z}\), so \(d = rm + sn\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>GCD Example</b></h4>
<p>TODO … all I have now is this</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec05/1.png" width="55%" class="center" /></p>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Relatively Prime Integers</b></h4>
<p>Next, we’ll see how the GCD is used in the definition of relatively prime numbers.
<br /></p>
<div class="mintheaderdiv">
Definition (Book Definition 1.6.14)
</div>
<div class="mintbodydiv">
\(a, b \in \mathbf{Z}\) are relatively prime if gcd\((a,b) = 1\).
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
For example \(4\) and \(9\) are relatively prime. gcd\((4,9)=1\). 
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book Proposition 1.6.15)
</div>
<div class="peachbodydiv">
\(a, b\) are relatively prime if and only if
$$
\begin{align*}
1 = ra + sb
\end{align*}
$$
for some \(r,s \in \mathbf{Z}\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
\(\Rightarrow:\) If \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). By the GCD Theorem, this means that \(I(a,b) = 1\) and so we’re done.
<br />
\(\Leftarrow:\) If \(1 = ra + sb\) for some \(r,s \in \mathbf{Z}\), then this means that \(1 \in I(a,b)\). But \(I(a,b)\) is also the set of multiples of some integer \(m\). But since \(1\) is in the set, then it must contains all multiples of \(1\). Therefore, we must have \(1 = I(a,b)\). By the GCD Theorem, \(1\) is therefore the gcd of \(a\) and \(b\) and so \(a\) and \(b\) are relatively prime. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book Corollary 1.6.17)
</div>
<div class="peachbodydiv">
If \(a, b\) are relatively prime and if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | n\).
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose \(a\) and \(b\) are relatively prime. We are given that \(a \ | \ n\) so \(n = au\) for some \(u \in \mathbf{Z}\). Similarly, \(b \ | \ n\) and so \(n = bv\) for some \(v \in \mathbf{Z}\). Since \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). This means that \(I(a,b) = 1\) and we can write \(1 = ra + sb\) for some integers \(s, r \in \mathbf{Z}\). Multiply this equation by \(b\) as follows</p>
<div>
$$
\begin{align*}
1 &amp;= ra + sb \\
n &amp;= n(ra + sb) \\
  &amp;= nra + nsb \\
  &amp;= (bv)ra + (au)sb \\
  &amp;= (ab)vr + (ab)us \\
  &amp;= (ab)(vr + us). \\
\end{align*}
$$
</div>
<p>From this we see that \(ab \ | \ n\) as desired. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(p\) is prime and \(p \ | \ ab\), then either \(p \ | \ a\) or \(p \ | \ b\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose that \(p \ | \ ab\). We’ll show that if \(p \nmid a\), then \(p \ | \ b\). Since \(p\) is prime and \(p \nmid a\), then gcd\((p,a)=1\). By Proposition (1.6.15), this implies that \(1 = pr + as\) for some \(r, s \in \mathbf{Z}\). Multiply this equation by \(b\) to see that</p>
<div>
$$
\begin{align*}
1 &amp;= pr + as \\
b &amp;= b(pr + as) \\
b &amp;= p(br) + (ab)s \\
\end{align*}
$$
</div>
<p>Clearly \(p\) divides the first term. \(p\) also divides the second term by the assumption we’re given. Therefore, \(p\) must divide \(b\) as we wanted to show. \(\ \blacksquare\)
<br />
<br />
Fact: If \(p \ | \ a_1a_2...a_k\), then \(p\) divides at least one of the factors. So \(p \ | \ a_i\) for some \(i \in \{1,...,k\}\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 1.6.8 A natural number \(d\) is the greatest common divisor of integers \(a\) and \(b\) if \(d\) is a common divisor. So \(d\) divides \(a\) and \(b\) Every common divisor \(e\) also divides \(d\). In other words, if \(e \ | \ a\) and \(e \ | \ b\), then \(e \ | \ d\) If the greatest common exists, then it is unique. Why? suppose \(d\) and \(d'\) are both greatest common divisors. Then by definition, \(d \ | \ d'\) and \(d' \ | \ d\) because every common divisor divides the gcd and they each must divide each other. But this means that \(d = \pm 'd\) by the divisibility properties from last lecture. But also by definition, \(d\) is non-negative and so \(d = 'd\). \(\ \blacksquare\) Note that in the book \(a\) and \(b\) are nonzero in the definition. With this definition, the set of divisors of \(0\) is \(\mathbf{Z}\). If \(a = 0\) and \(b \neq 0\), then \(|a|\) is the gcd of \(a\) and \(0\). If both \(a\) and \(b\) are zero, then \(0\) is the gcd of \(0\) and \(0\). Before addressing the question of whether the greatest common divisor exists, we’ll to define one more thing Definition Let \(a, b \in \mathbf{Z}\). An integer combination of \(a\) and \(b\) is any integer of the form $$ \begin{align*} I(a,b) = \{ra + sb \ | \ r, s \in \mathbf{Z}\}. \end{align*} $$ For example \(I(4,6) = \{4s + 6t \ | \ s, t \in \mathbf{Z}\}\). If we let \(s = -1\) and \(t = 1\), then \(2 \in I(4,6)\). In fact, this set produces all of the even integers. It includes all multiples of 2. In other words, we can also write that \(I(4,6) = \mathbf{Z}2\). In fact, this turns out to always be true, the set of integer combinations of two integers is also the set of multiples of a number and that number is the greatest common divisor! even when one of the integers is \(0\). So yes the GCD exists and can even be computed. Before formally proving its existence, we’ll present the way it can be computed next. The Euclidean Algorithm Let \(\mathbf{Z}^2 = \{(a,b), a,b \in \mathbf{Z}\}\). Define \(F: \mathbf{Z}^2 \rightarrow \mathbf{Z}^2\) by $$ \begin{equation*} F(m,n) = \begin{cases} (n,r), r = rem_n(m) \quad &amp;\text{if } n \neq 0 \\ (|m|,0) \quad \quad &amp;\text{if } n = 0\end{cases} \end{equation*} $$ \(rem_n(m)\) is the remainder of \(m \div n\) (recall that we can write \(m = qn + r\) where \(0 \leq r &lt; |n|)\)). Euclidean Algorithm: Iterate \(F\) until stable. Example: $$ \begin{align*} (42, -24) \xrightarrow{F} (-24, 18) \xrightarrow{F} (18, 12) \xrightarrow{F} (12, 6) \xrightarrow{F} (6, 0) \xrightarrow{F} (6, 0) \end{align*} $$ The GCD Theorem So now we have an algorithm to compute the GCD algorithm. Next, we will prove that it does compute the GCD and so the GCD does exist. Theorem Let \(a, b \in \mathbf{Z}\). Then For \(a, b\) have a GCD \(d \geq 0\). \(I(a,b) = \mathbf{Z}d\). \(d\) is computed by the Euclidean algorithm. We’ll start from (3) and then prove that the answer produced by the algorithm is in fact an integer combination of the original input (statement 2). Finally, we’ll show that this just means that we have computed the GCD (statement). To do all of this we will also need the following lemmas Lemma If \(a, b \in I(c,d)\), then \(I(a,b) \subseteq I(c,d)\) Proof: Homework Problem Lemma If \(F(m,n) = (a,b)\), then \(I(m,n) = I(a,b)\) Proof: We have two cases: Case 1: \(n \neq 0\). In this case, \(F(m,n) = (n, r)\) where \(r\) is the remainder after dividing \(m\) by \(n\). So \(m = qn + r\) where \(0 \leq r &lt; |n|\). We want to show that the integer combinations of \(m\) and \(n\) is the same as the integer combinations of \(n\) and \(r\), that is, \(I(m,n) = I(n,r)\). To do this, we will show that \(I(m,n) \subseteq I(n,r)\) and \(I(n,r) \subseteq I(m,n)\). To show that \(I(m,n) \subseteq I(n,r)\), observe that \(m\) is an integer combination of \(n\) and \(r\) because we can write \(m\) as $$ \begin{align*} m &amp;= qn + 1r. \end{align*} $$ Similarly, \(n\) is also an integer combination of \(n\) and \(r\) because we can write \(n\) as $$ \begin{align*} n &amp;= 1n + 0r. \end{align*} $$ Since \(m\) and \(n\) can both be written as integer combinations of \(n\) and \(r\), that is \(m \in I(n,r)\) and \(n \in I(n,r)\), then by the previous lemma, \(I(m,n) \subseteq I(n,r)\). To see that \(I(m,n) \subseteq I(n,r)\), observe that we can write \(n\) as \(n = 0m + 1n\) so \(n \in I(m,n)\). Similarly, \(r = 1m + (-q)m\) so \(r \in I(m,n)\). Then, by the previous lemma, \(I(m,n) \subseteq I(n,r)\). Therefore, \(I(m,n) = I(n,r)\). Case 2: \(n = 0\). In this case, \(F(m,n) = F(m,0) = (|m|,0)\). In this case the integer combinations of \(0\) and \(m\) are just multiples of \(m\) so \(I(m,0) = \mathbf{Z}m\). But this is the same as the integer combinations of \(|m|\) and \(0\) so \(\mathbf{Z}m = I(|m|,0) = I(m,0)\) which is what we wanted to show. \(\blacksquare\) Proof of the GCD Theorem We iterate the algorithm until \(n = 0\) and \(F(m,n) = (|m|,0)\). By the previous lemma we saw that \(I(m,n) = \mathbf{Z}d\) for some \(d \geq 0\). Claim: \(d\) is a GCD. \(m,n \in I(m,n) = \mathbf{Z}d\). So \(m\) and \(n\) are both multiples of \(d\) and d divides both of them. \(d \ | \ m\) and \(d \ | \ n\). So now for condition 2, suppose \(e\) is a common divisor of \(m\) and \(n\). We want to show that \(e\) divides \(d\). Since \(e \ | \ m\) and \(e \ | \ n\), then \(m\) and \(n\) are both multiples of \(e\). So we can write \(m = eu\) and \(n = ev\) for some \(u,v \in \mathbf{Z}\). But we know that any integer combination of \(m\) and \(n\) is a multiple of \(d\). So we can write for some \(r, s \in \mathbf{Z}\) $$ \begin{align*} d &amp;= rm + sn \\ &amp;= reu + sev \\ &amp;= e(ru + sv). \end{align*} $$ So \(e\) must divide \(d\). Therefore, \(d = gcd(m,n)\) as desired. \(\ \blacksquare\) Note that the Euclidean Algorithm gives a method for computing \(r, s \in \mathbf{Z}\), so \(d = rm + sn\). GCD Example TODO … all I have now is this Relatively Prime Integers Next, we’ll see how the GCD is used in the definition of relatively prime numbers. Definition (Book Definition 1.6.14) \(a, b \in \mathbf{Z}\) are relatively prime if gcd\((a,b) = 1\). For example \(4\) and \(9\) are relatively prime. gcd\((4,9)=1\). Proposition (Book Proposition 1.6.15) \(a, b\) are relatively prime if and only if $$ \begin{align*} 1 = ra + sb \end{align*} $$ for some \(r,s \in \mathbf{Z}\) Proof \(\Rightarrow:\) If \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). By the GCD Theorem, this means that \(I(a,b) = 1\) and so we’re done. \(\Leftarrow:\) If \(1 = ra + sb\) for some \(r,s \in \mathbf{Z}\), then this means that \(1 \in I(a,b)\). But \(I(a,b)\) is also the set of multiples of some integer \(m\). But since \(1\) is in the set, then it must contains all multiples of \(1\). Therefore, we must have \(1 = I(a,b)\). By the GCD Theorem, \(1\) is therefore the gcd of \(a\) and \(b\) and so \(a\) and \(b\) are relatively prime. \(\ \blacksquare\) Proposition (Book Corollary 1.6.17) If \(a, b\) are relatively prime and if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | n\). Proof Suppose \(a\) and \(b\) are relatively prime. We are given that \(a \ | \ n\) so \(n = au\) for some \(u \in \mathbf{Z}\). Similarly, \(b \ | \ n\) and so \(n = bv\) for some \(v \in \mathbf{Z}\). Since \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). This means that \(I(a,b) = 1\) and we can write \(1 = ra + sb\) for some integers \(s, r \in \mathbf{Z}\). Multiply this equation by \(b\) as follows $$ \begin{align*} 1 &amp;= ra + sb \\ n &amp;= n(ra + sb) \\ &amp;= nra + nsb \\ &amp;= (bv)ra + (au)sb \\ &amp;= (ab)vr + (ab)us \\ &amp;= (ab)(vr + us). \\ \end{align*} $$ From this we see that \(ab \ | \ n\) as desired. \(\ \blacksquare\) Proposition If \(p\) is prime and \(p \ | \ ab\), then either \(p \ | \ a\) or \(p \ | \ b\) Proof Suppose that \(p \ | \ ab\). We’ll show that if \(p \nmid a\), then \(p \ | \ b\). Since \(p\) is prime and \(p \nmid a\), then gcd\((p,a)=1\). By Proposition (1.6.15), this implies that \(1 = pr + as\) for some \(r, s \in \mathbf{Z}\). Multiply this equation by \(b\) to see that $$ \begin{align*} 1 &amp;= pr + as \\ b &amp;= b(pr + as) \\ b &amp;= p(br) + (ab)s \\ \end{align*} $$ Clearly \(p\) divides the first term. \(p\) also divides the second term by the assumption we’re given. Therefore, \(p\) must divide \(b\) as we wanted to show. \(\ \blacksquare\) Fact: If \(p \ | \ a_1a_2...a_k\), then \(p\) divides at least one of the factors. So \(p \ | \ a_i\) for some \(i \in \{1,...,k\}\) References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 04: Integers</title><link href="http://localhost:4000/jekyll/update/2025/01/27/math417-04-integers.html" rel="alternate" type="text/html" title="Lecture 04: Integers" /><published>2025-01-27T00:01:36-08:00</published><updated>2025-01-27T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/27/math417-04-integers</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/27/math417-04-integers.html"><![CDATA[<p>Some facts about the integers</p>
<ul>
	<li>\((\mathbf{Z}, +)\) is a commutative group. \(+\) is associative and commutative. The identity element is \(0\) and each element \(a\)'s inverse is \(-a\).</li>
	<li>\((\mathbf{Z}, \cdot)\) is a commutative monoid. \(\cdot\) is associative and commutative. The identity element is \(1\). (No inverses required)</li>
	<li>Distributive Law: \(a(b+c) = (ab) + (ac), \forall a,b,c \in \mathbf{Z}\). (side note: the list of properties so far for reference imply that \(\mathbf{Z}\) is a commutative ring with unit.</li>
	<li>\(\mathbf{Z} / \{0\}\) is closed under multiplication. In other words, if \(a \neq, b \neq 0\), then \(ab \neq 0\) We can also re-write this by taking its contrapositive so \(ab = 0\) implies that \(a = 0\) or \(b = 0\).</li>
	<li>\(\mathbf{N} = \mathbf{Z}_{&gt;0}\) is closed under \(+\) and \(\cdot\).</li>
	<li>If \(a, b \in \mathbf{Z} / \{0\}\), then \( |ab| \geq \max\{|a|,|b|\} \). (Note that \(a &gt; b\) means that \(a - b \in \mathbf{Z}_{&gt;0}\).)</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Divisibility</b></h4>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(a, b \in \mathbf{Z}\). \(a\) <b>divides</b> \(b\) (write \(a | b\)) if there exists \(m \in \mathbf{Z}\) such that \(am = b\).
<br />
(We also say that \(a\) is a factor of \(b\) or \(b\) is a multiple of \(a\)).
<br />
In fact, \(a|b\) if and only if \(\frac{b}{a} \in \mathbf{Z} \text{if $a \neq 0$}\).
</div>
<p><br />
The factors or divisors of \(6\) for example are \(\{\pm 1, \pm 2, \pm 3, \pm 6\}\) while the divisors of \(0\) are all of \(\mathbf{Z}\). We also have the set of all multiplies of integer \(a\) so if \(a\) is 7, then the set is \(\{...,-14,-7,0,7,14,21,...\}\). Notation: we will denote the set of all multiples as</p>
<div> 
$$
\begin{align*}
\mathbf{Z}a = \{na \ | \ n \in \mathbf{Z}\}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Divisibility Properties</b></h4>
<p>The following are some well known propositions about divisibility.</p>
<ol>
	<li>If \(a | b\) and \(b | a\), then \(b \in \{\pm a\}\)</li>
	<li>If \(a | 1\), then \(a \in \{\pm 1\}\)</li>
	<li>If \(a | b\) and \(b | c\), then \(a | c\)</li>
	<li>If \(a | b\) and \(a | c\), then \(a | (b + c)\)</li>
	<li>If \(a | b\) and \(a | c\), then \(a | (mb + nc) \quad \forall m, n \in \mathbf{Z}\)</li>
</ol>
<p>Note that</p>
<div> 
$$
\begin{align*}
a | b \Leftrightarrow b \in \mathbf{Z}a \Leftrightarrow \mathbf{Z}b \subseteq \mathbf{Z}c
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Prime Numbers</b></h4>
<div class="mintheaderdiv">
Definition 1.6.3
</div>
<div class="mintbodydiv">
A natural number is prime if (i) \(n &lt; 1\) and (ii) the only positive divisors of \(n\) are \(\{1, n\}\).
</div>
<p><br />
<br />
<!-------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.4
</div>
<div class="peachbodydiv">
Every \(n \in \mathbf{N}\) greater than 1 is equal to a product of primes (at least one).
</div>
<p><br />
<!------------------------------------------------------------------------------->
<b>Proof</b><br />
By Induction on \(n \geq 2\). 
<br />
Base Case: \(n = 2:\) 2 is a prime so it’s a product of one prime.
<br />
<br />
Inductive Case \(n &gt; 2\): 
<br />
Suppose that inductive hypothesis is true for any number \(r\) where \(2 \leq r &lt; n\). Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of primes and we’re done. Otherwise, \(n\) is not a prime and has divisors other than \(1\) or itself. Let these divisors be \(a\) and \(b\) so that \(n = ab\). \(a\) and \(b\) are not \(n\) or \(1\) so we know that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of primes. Therefore, \(n = ab\) is also a product of primes. \(\ \blacksquare\) 
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem 1.6.6 (Euclid)
</div>
<div class="yellowbodydiv">
There are infinitely many prime numbers.
</div>
<p><br />
<b>Proof</b>
<br />
Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1 \in \mathbf{N}\). We know that \(p\) is greater than any of the primes \(p_1,p_2,...,p_k\). Observe now that none of these primes \(p_1, p_2,...,p_k\) can divide \(p\). Why? because if any of these primes did (say it was \(p_i\)), then this means that \(p_i \ | \ n\). But \(p_i\) also divides the product \(p_1p_2...p_k\). By the last fact of integers above, then \(p_i \ | \ (n - p_1p_2...p_k)\). So \(p_i \ | \ 1\). But that’s impossible so \(p_i\) can’t divide \(n\). But by Proposition 1.6.4, \(p\) must be a product of primes. Therefore, there must be another prime or \(p\) itself is a prime. This is a contradiction and so we can conclude that there are infinitely many primes. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Divison with Remainder</b></h4>
<p>The way we learn division is this. Given \(a, d \in \mathbf{Z}, d &gt; 0\), then \(\frac{a}{d} = q + \frac{r}{d}\). The quotient, \(q \in \mathbf{Z}\) and the remainder \(r \in \mathbf{Z}\). Moreover, \(\frac{r}{d} \in [0,1)\). Stated differently,
<br /></p>
<div class="peachheaderdiv">
Proposition 1.6.7
</div>
<div class="peachbodydiv">
Given integers \(a\) and \(d \in \mathbf{Z}\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that 
$$
\begin{align*}
a = qd + r
\end{align*}
$$
where \(0 \leq r &lt; d\). 
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof: (from my own notes (reading the book))</b>
<br />
<br />
We are given \(a\) and \(d\) such that \(d \geq 1\). <br />
We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\).
<br />
<br />
We have two cases:
<br />
\(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\).<br />
Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that</p>
<div> 
$$
\begin{align*}
(a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\
a &amp;= q'd + d + r \\
a &amp;= q'(d + 1) + r 
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done.
<br />
Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So</p>
<div> 
$$
\begin{align*}
-a &amp;= q'd + r' \\
a &amp;= -q'd - r' \\
a &amp;= -q'd - d + d - r' \\
a &amp;= (-q' - 1)d + (d - r') \\
  &amp;= (-q' - 1)d + (d - r').
\end{align*}
$$
</div>
<p>So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done.
<br />
<br />
To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that</p>
<div> 
$$
\begin{align*}
(q - q')d = r - r'
\end{align*}
$$
</div>
<p>But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Some facts about the integers \((\mathbf{Z}, +)\) is a commutative group. \(+\) is associative and commutative. The identity element is \(0\) and each element \(a\)'s inverse is \(-a\). \((\mathbf{Z}, \cdot)\) is a commutative monoid. \(\cdot\) is associative and commutative. The identity element is \(1\). (No inverses required) Distributive Law: \(a(b+c) = (ab) + (ac), \forall a,b,c \in \mathbf{Z}\). (side note: the list of properties so far for reference imply that \(\mathbf{Z}\) is a commutative ring with unit. \(\mathbf{Z} / \{0\}\) is closed under multiplication. In other words, if \(a \neq, b \neq 0\), then \(ab \neq 0\) We can also re-write this by taking its contrapositive so \(ab = 0\) implies that \(a = 0\) or \(b = 0\). \(\mathbf{N} = \mathbf{Z}_{&gt;0}\) is closed under \(+\) and \(\cdot\). If \(a, b \in \mathbf{Z} / \{0\}\), then \( |ab| \geq \max\{|a|,|b|\} \). (Note that \(a &gt; b\) means that \(a - b \in \mathbf{Z}_{&gt;0}\).) Divisibility Definition Let \(a, b \in \mathbf{Z}\). \(a\) divides \(b\) (write \(a | b\)) if there exists \(m \in \mathbf{Z}\) such that \(am = b\). (We also say that \(a\) is a factor of \(b\) or \(b\) is a multiple of \(a\)). In fact, \(a|b\) if and only if \(\frac{b}{a} \in \mathbf{Z} \text{if $a \neq 0$}\). The factors or divisors of \(6\) for example are \(\{\pm 1, \pm 2, \pm 3, \pm 6\}\) while the divisors of \(0\) are all of \(\mathbf{Z}\). We also have the set of all multiplies of integer \(a\) so if \(a\) is 7, then the set is \(\{...,-14,-7,0,7,14,21,...\}\). Notation: we will denote the set of all multiples as $$ \begin{align*} \mathbf{Z}a = \{na \ | \ n \in \mathbf{Z}\} \end{align*} $$ Divisibility Properties The following are some well known propositions about divisibility. If \(a | b\) and \(b | a\), then \(b \in \{\pm a\}\) If \(a | 1\), then \(a \in \{\pm 1\}\) If \(a | b\) and \(b | c\), then \(a | c\) If \(a | b\) and \(a | c\), then \(a | (b + c)\) If \(a | b\) and \(a | c\), then \(a | (mb + nc) \quad \forall m, n \in \mathbf{Z}\) Note that $$ \begin{align*} a | b \Leftrightarrow b \in \mathbf{Z}a \Leftrightarrow \mathbf{Z}b \subseteq \mathbf{Z}c \end{align*} $$ Prime Numbers Definition 1.6.3 A natural number is prime if (i) \(n &lt; 1\) and (ii) the only positive divisors of \(n\) are \(\{1, n\}\). Proposition 1.6.4 Every \(n \in \mathbf{N}\) greater than 1 is equal to a product of primes (at least one). Proof By Induction on \(n \geq 2\). Base Case: \(n = 2:\) 2 is a prime so it’s a product of one prime. Inductive Case \(n &gt; 2\): Suppose that inductive hypothesis is true for any number \(r\) where \(2 \leq r &lt; n\). Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of primes and we’re done. Otherwise, \(n\) is not a prime and has divisors other than \(1\) or itself. Let these divisors be \(a\) and \(b\) so that \(n = ab\). \(a\) and \(b\) are not \(n\) or \(1\) so we know that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of primes. Therefore, \(n = ab\) is also a product of primes. \(\ \blacksquare\) Theorem 1.6.6 (Euclid) There are infinitely many prime numbers. Proof Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1 \in \mathbf{N}\). We know that \(p\) is greater than any of the primes \(p_1,p_2,...,p_k\). Observe now that none of these primes \(p_1, p_2,...,p_k\) can divide \(p\). Why? because if any of these primes did (say it was \(p_i\)), then this means that \(p_i \ | \ n\). But \(p_i\) also divides the product \(p_1p_2...p_k\). By the last fact of integers above, then \(p_i \ | \ (n - p_1p_2...p_k)\). So \(p_i \ | \ 1\). But that’s impossible so \(p_i\) can’t divide \(n\). But by Proposition 1.6.4, \(p\) must be a product of primes. Therefore, there must be another prime or \(p\) itself is a prime. This is a contradiction and so we can conclude that there are infinitely many primes. \(\ \blacksquare\) Divison with Remainder The way we learn division is this. Given \(a, d \in \mathbf{Z}, d &gt; 0\), then \(\frac{a}{d} = q + \frac{r}{d}\). The quotient, \(q \in \mathbf{Z}\) and the remainder \(r \in \mathbf{Z}\). Moreover, \(\frac{r}{d} \in [0,1)\). Stated differently, Proposition 1.6.7 Given integers \(a\) and \(d \in \mathbf{Z}\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that $$ \begin{align*} a = qd + r \end{align*} $$ where \(0 \leq r &lt; d\). Proof: (from my own notes (reading the book)) We are given \(a\) and \(d\) such that \(d \geq 1\). We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\). We have two cases: \(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\). Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that $$ \begin{align*} (a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\ a &amp;= q'd + d + r \\ a &amp;= q'(d + 1) + r \end{align*} $$ And we are done. Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done. Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So $$ \begin{align*} -a &amp;= q'd + r' \\ a &amp;= -q'd - r' \\ a &amp;= -q'd - d + d - r' \\ a &amp;= (-q' - 1)d + (d - r') \\ &amp;= (-q' - 1)d + (d - r'). \end{align*} $$ So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done. To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that $$ \begin{align*} (q - q')d = r - r' \end{align*} $$ But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\) References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 03: Permutation Groups, Cycle Decomposition</title><link href="http://localhost:4000/jekyll/update/2025/01/26/math417-03-permutation-groups-cycle-decomposition.html" rel="alternate" type="text/html" title="Lecture 03: Permutation Groups, Cycle Decomposition" /><published>2025-01-26T00:01:36-08:00</published><updated>2025-01-26T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/26/math417-03-permutation-groups-cycle-decomposition</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/26/math417-03-permutation-groups-cycle-decomposition.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(X\) be a set. Define the <b>permutation</b> of \(X\) as a bijection from the set to itself (\(f: X \rightarrow X\)). Let \(Sym(X)\) be the set of all bijections from \(X\) to itself (\(X \rightarrow X\)).
</div>
<!------------------------------------------------------------------------>
<p><br />
\(Sym(X)\) equipped with the composition of functions operation is a group. To see this observe that</p>
<ul>
<li>\(id: X \rightarrow X, id(x) = x\) is the identity element.</li>
<li>Composition of functions is associative.</li>
<li>The composition of two permutations is another permutation.</li>
<li>Bijections have inverses and the inverse of a bijection is another bijection.</li>
</ul>
<p>This group is often denoted by the <b>Permutation Group</b> or more often the <b>Symmetric Group</b>.
<br />
<br />
The standard example for the case of finite sets is the standard set \(X = \{1,2,...,n\}\). The symmetric group has a special name called \(S_n\),</p>
<div>
$$
\begin{align*}
S_n = Sym(\{1,2,...,n\})
\end{align*}
$$
</div>
<p>The size of \(S_n\) is \(|S_n| = n!\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Two Line Notation</b></h4>
<p>Let \(\rho, \psi \in S_4\), then we’ll write</p>
<div>
	$$
	\begin{align*}
	 \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix}
	\end{align*}
	$$
</div>
<p>shows that \(\rho(1) = 1\), \(\rho(2) = 4\), \(\rho(3) = 3\) and \(\rho(4) = 2\). Similarly,</p>
<div>
	$$
	\begin{align*}
	 \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix}
	\end{align*}
	$$
</div>
<p>shows that \(\psi(1) = 4\), \(\psi(2) = 2\), \(\psi(3) = 1\) and \(\psi(4) = 3\). Finally,</p>
<div>
	$$
	\begin{align*}
	 \rho\psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 2 &amp; 4 &amp; 1 &amp; 3\end{pmatrix} \in S_4
	\end{align*}
	$$
</div>
<p>so \(\rho\psi(1) = 2\), \(\rho\psi(2) = 4\), \(\rho\psi(3) = 1\) and \(\rho\psi(4) = 3\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Cycle Notation</b></h4>
<p>We can decompose the cycles in each permutation above. For example for the first permutation</p>
<div>
	$$
	\begin{align*}
	 \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix}
	\end{align*}
	$$
</div>
<p>We can decompose this permutation into the following</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec03/1.png" width="20%" class="center" /></p>
<p>In this permutation, we have a cycle of length \(2\) For the second permutation</p>
<div>
	$$
	\begin{align*}
	 \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix}
	\end{align*}
	$$
</div>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec03/2.png" width="20%" class="center" /></p>
<p>we have a cycle of length \(3\). 
<br />
<br />
To describe a cycle, we can use <b>cycle notation</b> which is defined as follows
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) where \(a_1,...,a_k \in X\) are distinct. \(\sigma\) is defined by
	$$
	\begin{align*}
	 \sigma(a_i) &amp;= a_{i+1}, \quad i = 1,...,k-1 \\
	 \sigma(a_k) &amp;= a_1 \\
	 \sigma(x) &amp;= x, \quad \text{if} x \notin \{a_1,...,a_k\} \\
	\end{align*}
	$$
</div>
<p><br />
So we can write the first permutation with cycle notation as \((2,4)\) only since by definition for the other elements \(\sigma(x)=x\), while the second permutation can be written as \((1,4,3)\). 
<br />
<br />
What about the following example?</p>
<div>
	$$
	\begin{align*}
	 \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\ 3 &amp; 5 &amp; 4 &amp; 1 &amp; 2\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Note here that we have two disjoint cycles. \((1 \quad 4 \quad 3)\) where \(1\) goes to \(3\) and \(3\) goes to \(4\) and \(4\) goes back to \(1\) and \((2 \quad 5)\) where \(2\) goes to \(5\) and \(5\) goes back to \(2\). This permutation can be written as</p>
<div>
	$$
	\begin{align*}
	 (1 \quad 3 \quad 4)(2 \quad 5)
	\end{align*}
	$$
</div>
<!------------------------------------------------------------------------>
<h4><b>Facts</b></h4>
<p>Some facts about the cycle notation</p>
<ul>
<li>\((1) = (2) = ... (n) = id\). All of these represent the identity permutations.</li>
<li>The order of the elements in a cycle matters up to "cyclic permutation". \((1 \quad 2 \quad 3 \quad 4) = (2 \quad 3 \quad 4 \quad 1) \neq (1 \quad 3 \quad 4 \quad 2)\)</li>
<li>For the inverse of a permutation, we just reverse the order of elements so \((a_1 \quad a_2 \quad ... \quad a_k)^{-1} = (a_k \quad a_{k-1} \quad ... \quad a_1)\)</li>
<li>Two cycles \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) and \(\tau = (b_1 \quad b_{2} \quad ... \quad b_l) \in Sym(X)\) are disjoint if \(\{a_1,a_2,...,a_k\} \cap \{b_1,b_2,...,b_l\} = \emptyset\). Moreover, if the two sets are disjoint, then \(\sigma \circ \tau = \tau \circ \sigma\). This means that the permutation above that we described with \((1 \quad 3 \quad 4)(2 \quad 5)\) can also be written as \((2 \quad 5)(1 \quad 3 \quad 4)\)</li>
</ul>
<p><br />
<!------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Every non-id element in \(Sym(X)\) where \(X\) is finite can be written as a product of pairwise disjoint cycles (of length \(\geq 2\) uniquely up to re-ordering.
</div>
<p><br />
Suppose we have \(\sigma = (1 \quad 3)(2 \quad 7 \quad 8)(4 \quad 5 \quad 6)(9) \in S_9\). Any of the cycles in this notation are pairwise disjoint. We have 3 disjoint cycles where \(9\) is fixed.
<br />
<br />
The proof for this theorem is in the class notes.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Cycle Type</b></h4>
<p>We can classify permutations of a finite set into groups corresponding to the number of cycles of various lengths in their cycle decomposition.
<br />
<br />
For example for \(S_2\), we have two elements and so we have two permutations</p>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(1 + 1\)</td>
    <td>\(id = (1)(2)\)</td>
  </tr>
  <tr>
    <td>\(2\)</td>
    <td>\((1 \quad 2)\)</td>
  </tr>
</table>
<p>For \(S_3\),</p>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(1 + 1 + 1\)</td>
    <td>\(id = (1)(2)(3)\)</td>
  </tr>
  <tr>
    <td>\(2 + 1\)</td>
    <td>\((1 \quad 2),(1 \quad 3),(2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(3\)</td>
    <td>\((1 \quad 2 \quad 3),(1 \quad 3 \quad 2)\)</td>
  </tr>
</table>
<p>For \(S_4\),</p>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(1 + 1 + 1 + 1\)</td>
    <td>\(id = (1)(2)(3)(4)\)</td>
  </tr>
  <tr>
    <td>\(2 + 1 + 1\)</td>
    <td>\((1 \quad 2),(1 \quad 3),(1 \quad 4),(2 \quad 3),(2 \quad 4),(3 \quad 4)\)</td>
  </tr>
  <tr>
    <td>\(2 + 2\)</td>
    <td>\((1 \quad 2)(3 \quad 4),(1 \quad 3)(2 \quad 4),(1 \quad 4)(2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(3 + 1\)</td>
    <td>\((1 \quad 2 \quad 3),(1 \quad 2 \quad 4)\),\((1 \quad 3 \quad 4),(2 \quad 3 \quad 4),(1 \quad 3 \quad 2)\),\((1 \quad 4 \quad 2),(1 \quad 4 \quad 3),(2 \quad 4 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(4\)</td>
    <td>\((1 \quad 2 \quad 3 \quad 4),(1 \quad 2 \quad 4 \quad 3)\),\((1 \quad 3 \quad 2 \quad 4),(1 \quad 3 \quad 4 \quad 2)\),\((1 \quad 4 \quad 2 \quad 3),(1 \quad 4 \quad 3 \quad 2)\)</td>
  </tr>
</table>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Order of an Element</b></h4>
<p>(Start of lecture 4). The first concept that we will talk about is the order of an element in a group.
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Suppose we have a group \((G, \cdot)\) and let \(a \in G\). The order of \(a\) is the smallest positive integer \(n\) such that \(a^n = e\) (or infinite). \(\text{order}(a) \in \mathbf{N} \cup \{\infty\}\).
</div>
<!------------------------------------------------------------------------>
<p><br />
For example let \(\sigma = (1 \quad 2 \quad 3 \quad 4) \in S_5\) The order of \(\sigma\), \(\text{order}(\sigma)\) is \(4\). This is because it will take \(\sigma^4\) will finally get us back to the identity permutation. In fact that the order of a \(k-\)cycle is \(k\).
<br />
<br />
What about \(\tau =  (1 \quad 2)(3 \quad 4 \quad 5)\)? \(\text{order}(\tau) = 6\) because we have to iterate the operation \(6\) times to get to the identity element. Specifically, we have two disjoint cycles and since they are disjoint, then they operate independently. So we need to find \(n\) such that both of the cycles will return to the identity permutation</p>
<div>
$$
\begin{align*}
(1 \quad 2)^n &amp;= e \\
(3 \quad 4 \quad 5)^n &amp;= e
\end{align*}
$$
</div>
<p>This \(n\) must be then the least common multiple of \(2\) and \(3\) which is \(6\).
<br />
<br />
What about \(\nu =  (1 \quad 2)(2 \quad 3 \quad 4)\)? Note here that the two cycles are not disjoint. It’s not clear here what it would be so drawing this permutation might make this very obvious.</p>

<p>[TODO:PIC]</p>

<p>From this we see that \(\text{order}(\nu) = 4\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Parity of a Permutation</b></h4>
<p>The second concept that we want to talk about is the parity of a permutation but first we’ll start with the following proposition.
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Every permutation of a finite set is equal to some product of transpositions (a transposition is  2-cycle) not necessarily disjoint.
</div>
<p><br />
<!------------------------------------------------------------------------>
For example we can write \((1 \quad 2 \quad 3 \quad 4)\) as \((1 \quad 2)(2 \quad 3)(3 \quad 4)\). 
<br />
<br />
Next, we have the following proposition
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(\sigma\) is a permutation of a finite set and if \(\sigma = \tau_1\tau_2...\tau_k = \nu_1\nu_2...\nu_l\) where each of \(\tau_i\) and \(\nu_j\) are transpositions, then either \(k,l\) are both even, or are both odd. i.e. \((-1)^k = (-1)^l\)
</div>
<!------------------------------------------------------------------------>
<p><br />
Because of this we can classify permutations as even or odd. We also get the fact that composing two even permutations is another even permutations and composing an even permutation with an odd permutation is an odd permutation.
<br />
<br />
<!------------------------------------------------------------------------>
<b>Proof</b> (there is an alternative proof in the notes)
<br />
Define a function \(sgn: S_n \rightarrow \{\pm 1\}\) such that</p>
<ol>
	<li>\(sgn(id) = +1\)</li>
	<li>\(sgn(\sigma \circ \tau) = sgn(\sigma)sgn(\tau)\)</li>
	<li>If \(\tau\) is a transposition, then \(sgn(\tau) = -1\)</li>
</ol>
<p>So now given a permutation \(\sigma \in S_n\), there is a corresponding permutation matrix \(A_{\sigma} \in GL_{n}(\mathbf{R})\). \(A_{\sigma}\) has the standard basis vectors but permuted according to the permutation \(\sigma\). For example</p>
<div>
$$
\begin{align*}
A_{(1 \quad 2 \quad 3)}
&amp;= 
\begin{pmatrix}
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>So</p>
<div>
$$
\begin{align*}
A_{\sigma}
&amp;= 
\begin{pmatrix}
e_{\sigma(1)} &amp; e_{\sigma(2)} &amp; ... &amp; e_{\sigma(n)}
\end{pmatrix}
\end{align*}
$$
</div>
<p>where \(e_{\sigma(k)}\) is the standard column vector \(e_k\) permuted according to \(\sigma(k)\). 
<br />
<br />
The permutation matrix \(A_{\sigma}\) is useful in that left multiplication by \(A_{\sigma}\) permutes the subset \(\{e_1,...,e_n\} \in \mathbf{R}^n\) according to \(\sigma\). So</p>
<div>
$$
\begin{align*}
A_{\sigma}e_k = e_{\sigma(k)}
\end{align*}
$$
</div>
<p>This actually leads to a formula</p>
<div>
$$
\begin{align*}
A_{\sigma}A_{\tau} = A_{\sigma \circ \tau}
\end{align*}
$$
</div>
<p>This is because</p>
<div>
$$
\begin{align*}
A_{\sigma}A_{\tau}e_k = A_{\sigma}e_{\tau(k)} = e_{\sigma(\tau(k))} \leftrightarrow A_{\sigma \circ \tau}e_k = e_{(\sigma \circ \tau)(k)}
\end{align*}
$$
</div>
<p>So now define: \(sgn(\sigma) = \det(A_{\sigma})\) where \(sgn: S_n \rightarrow \{\pm 1\}\). This function satisfies the three properties we defined for the \(sgn\) function above.</p>
<ol>
	<li>\(sgn(id) = +1\). This is true because the permutation matrix for the identity permutation is the identity matrix.</li>
	<li>\(sgn(\sigma \circ \tau) = sgn(\sigma)sgn(\tau)\). This follows from the product property of the determinant. \(\det(AB) = \det(A)\det(B)\).</li>
	<li>If \(\tau\) is a transposition, then \(sgn(\tau) = -1\). This also follows from the fact that interchanging any two columns from a matrix results in switching the sign of the determinant.</li>
</ol>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition Let \(X\) be a set. Define the permutation of \(X\) as a bijection from the set to itself (\(f: X \rightarrow X\)). Let \(Sym(X)\) be the set of all bijections from \(X\) to itself (\(X \rightarrow X\)). \(Sym(X)\) equipped with the composition of functions operation is a group. To see this observe that \(id: X \rightarrow X, id(x) = x\) is the identity element. Composition of functions is associative. The composition of two permutations is another permutation. Bijections have inverses and the inverse of a bijection is another bijection. This group is often denoted by the Permutation Group or more often the Symmetric Group. The standard example for the case of finite sets is the standard set \(X = \{1,2,...,n\}\). The symmetric group has a special name called \(S_n\), $$ \begin{align*} S_n = Sym(\{1,2,...,n\}) \end{align*} $$ The size of \(S_n\) is \(|S_n| = n!\). Two Line Notation Let \(\rho, \psi \in S_4\), then we’ll write $$ \begin{align*} \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix} \end{align*} $$ shows that \(\rho(1) = 1\), \(\rho(2) = 4\), \(\rho(3) = 3\) and \(\rho(4) = 2\). Similarly, $$ \begin{align*} \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix} \end{align*} $$ shows that \(\psi(1) = 4\), \(\psi(2) = 2\), \(\psi(3) = 1\) and \(\psi(4) = 3\). Finally, $$ \begin{align*} \rho\psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 2 &amp; 4 &amp; 1 &amp; 3\end{pmatrix} \in S_4 \end{align*} $$ so \(\rho\psi(1) = 2\), \(\rho\psi(2) = 4\), \(\rho\psi(3) = 1\) and \(\rho\psi(4) = 3\). Cycle Notation We can decompose the cycles in each permutation above. For example for the first permutation $$ \begin{align*} \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix} \end{align*} $$ We can decompose this permutation into the following In this permutation, we have a cycle of length \(2\) For the second permutation $$ \begin{align*} \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix} \end{align*} $$ we have a cycle of length \(3\). To describe a cycle, we can use cycle notation which is defined as follows Definition Let \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) where \(a_1,...,a_k \in X\) are distinct. \(\sigma\) is defined by $$ \begin{align*} \sigma(a_i) &amp;= a_{i+1}, \quad i = 1,...,k-1 \\ \sigma(a_k) &amp;= a_1 \\ \sigma(x) &amp;= x, \quad \text{if} x \notin \{a_1,...,a_k\} \\ \end{align*} $$ So we can write the first permutation with cycle notation as \((2,4)\) only since by definition for the other elements \(\sigma(x)=x\), while the second permutation can be written as \((1,4,3)\). What about the following example? $$ \begin{align*} \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\ 3 &amp; 5 &amp; 4 &amp; 1 &amp; 2\end{pmatrix} \end{align*} $$ Note here that we have two disjoint cycles. \((1 \quad 4 \quad 3)\) where \(1\) goes to \(3\) and \(3\) goes to \(4\) and \(4\) goes back to \(1\) and \((2 \quad 5)\) where \(2\) goes to \(5\) and \(5\) goes back to \(2\). This permutation can be written as $$ \begin{align*} (1 \quad 3 \quad 4)(2 \quad 5) \end{align*} $$ Facts Some facts about the cycle notation \((1) = (2) = ... (n) = id\). All of these represent the identity permutations. The order of the elements in a cycle matters up to "cyclic permutation". \((1 \quad 2 \quad 3 \quad 4) = (2 \quad 3 \quad 4 \quad 1) \neq (1 \quad 3 \quad 4 \quad 2)\) For the inverse of a permutation, we just reverse the order of elements so \((a_1 \quad a_2 \quad ... \quad a_k)^{-1} = (a_k \quad a_{k-1} \quad ... \quad a_1)\) Two cycles \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) and \(\tau = (b_1 \quad b_{2} \quad ... \quad b_l) \in Sym(X)\) are disjoint if \(\{a_1,a_2,...,a_k\} \cap \{b_1,b_2,...,b_l\} = \emptyset\). Moreover, if the two sets are disjoint, then \(\sigma \circ \tau = \tau \circ \sigma\). This means that the permutation above that we described with \((1 \quad 3 \quad 4)(2 \quad 5)\) can also be written as \((2 \quad 5)(1 \quad 3 \quad 4)\) Theorem Every non-id element in \(Sym(X)\) where \(X\) is finite can be written as a product of pairwise disjoint cycles (of length \(\geq 2\) uniquely up to re-ordering. Suppose we have \(\sigma = (1 \quad 3)(2 \quad 7 \quad 8)(4 \quad 5 \quad 6)(9) \in S_9\). Any of the cycles in this notation are pairwise disjoint. We have 3 disjoint cycles where \(9\) is fixed. The proof for this theorem is in the class notes. Cycle Type We can classify permutations of a finite set into groups corresponding to the number of cycles of various lengths in their cycle decomposition. For example for \(S_2\), we have two elements and so we have two permutations \(1 + 1\) \(id = (1)(2)\) \(2\) \((1 \quad 2)\) For \(S_3\), \(1 + 1 + 1\) \(id = (1)(2)(3)\) \(2 + 1\) \((1 \quad 2),(1 \quad 3),(2 \quad 3)\) \(3\) \((1 \quad 2 \quad 3),(1 \quad 3 \quad 2)\) For \(S_4\), \(1 + 1 + 1 + 1\) \(id = (1)(2)(3)(4)\) \(2 + 1 + 1\) \((1 \quad 2),(1 \quad 3),(1 \quad 4),(2 \quad 3),(2 \quad 4),(3 \quad 4)\) \(2 + 2\) \((1 \quad 2)(3 \quad 4),(1 \quad 3)(2 \quad 4),(1 \quad 4)(2 \quad 3)\) \(3 + 1\) \((1 \quad 2 \quad 3),(1 \quad 2 \quad 4)\),\((1 \quad 3 \quad 4),(2 \quad 3 \quad 4),(1 \quad 3 \quad 2)\),\((1 \quad 4 \quad 2),(1 \quad 4 \quad 3),(2 \quad 4 \quad 3)\) \(4\) \((1 \quad 2 \quad 3 \quad 4),(1 \quad 2 \quad 4 \quad 3)\),\((1 \quad 3 \quad 2 \quad 4),(1 \quad 3 \quad 4 \quad 2)\),\((1 \quad 4 \quad 2 \quad 3),(1 \quad 4 \quad 3 \quad 2)\) The Order of an Element (Start of lecture 4). The first concept that we will talk about is the order of an element in a group. Definition Suppose we have a group \((G, \cdot)\) and let \(a \in G\). The order of \(a\) is the smallest positive integer \(n\) such that \(a^n = e\) (or infinite). \(\text{order}(a) \in \mathbf{N} \cup \{\infty\}\). For example let \(\sigma = (1 \quad 2 \quad 3 \quad 4) \in S_5\) The order of \(\sigma\), \(\text{order}(\sigma)\) is \(4\). This is because it will take \(\sigma^4\) will finally get us back to the identity permutation. In fact that the order of a \(k-\)cycle is \(k\). What about \(\tau = (1 \quad 2)(3 \quad 4 \quad 5)\)? \(\text{order}(\tau) = 6\) because we have to iterate the operation \(6\) times to get to the identity element. Specifically, we have two disjoint cycles and since they are disjoint, then they operate independently. So we need to find \(n\) such that both of the cycles will return to the identity permutation $$ \begin{align*} (1 \quad 2)^n &amp;= e \\ (3 \quad 4 \quad 5)^n &amp;= e \end{align*} $$ This \(n\) must be then the least common multiple of \(2\) and \(3\) which is \(6\). What about \(\nu = (1 \quad 2)(2 \quad 3 \quad 4)\)? Note here that the two cycles are not disjoint. It’s not clear here what it would be so drawing this permutation might make this very obvious.]]></summary></entry><entry><title type="html">Lecture 02: Rotations in Space</title><link href="http://localhost:4000/jekyll/update/2025/01/25/math417-02-rotations-in-space.html" rel="alternate" type="text/html" title="Lecture 02: Rotations in Space" /><published>2025-01-25T00:01:36-08:00</published><updated>2025-01-25T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/25/math417-02-rotations-in-space</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/25/math417-02-rotations-in-space.html"><![CDATA[<p>A group is a set with a defined product. This product is associative. The group is closed under this product. We have an identity element and each element has an inverse. Formally,
<br /></p>
<div class="mintheaderdiv">
Definition (1.10.1)
</div>
<div class="mintbodydiv">
A group is a (nonempty) set \(G\) with a binary operation (a product) \(G \times G \rightarrow G\) satisfying the following properties:
<ol type="a">
	<li>The group is closed under the operation. </li>
	<li>The product is associative. For all \(a, b, c \in G\), we have \((ab)c = a(bc)\).</li>
	<li>There is an identity element \(e \in G\) with property that for all \(a \in G\), \(ea = ae = a\).</li>
	<li>For each element \(a \in G\), there is an element \(a^{-1}\ \in G\) satisfying \(aa^{-1} = a^{-1}a = e\) (The inverse).</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
What are examples of the groups?</p>
<ol type="a">
	<li>\(\mathbf{R}\) with the addition operation.</li>
	<li>\(\mathbf{R}^{x} = \mathbf{R} - \{0\}\) with the multiplication operation.</li>
	<li>\(\mathbf{Z}\) with the addition operation.</li>
	<li>The set of invertible \(n \times n\) matrices with entries in \(\mathbf{R}\) with matrix multiplication as the product.</li>
	<li>Any vector space is a group if you forget about the scalar multiplication.</li>
	<li>For any set \(T\), define the set of all bijections \(g: T \rightarrow T\). The set of all bijections is the symmetric group of \(T\). The operation here is the composition of these maps.</li>
</ol>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose we define the group \((\mathbf{R}, \ast)\) where \(\ast\) is defined as</p>
<div>
$$
\begin{align*}
x \ast y = \sqrt[3]{x^3 + y^3}
\end{align*}
$$
</div>
<p>We can further check that this product satisfies the three axioms of a group.
<br />
<br />
<!------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(F\) be a field (for example \(\mathbf{R}\) or \(\mathbf{C}\)). Let \(Mat_{n \times n}(F)\) be the set of matrices with entries in \(F\). Then define \(GL_n(F)\) (General Linear Group) as a subset of \(A \in M_{n \times n}(F)\) where \(A\) is an invertible matrix.<br />
<br />
<br />
The set \(GL_n(F)\) equipped with matrix multiplication is a group. It satisfies the three axioms</p>
<ol type="a">
	<li>The group is closed under multiplication because multiplying two invertible matrices is another invertible matrix. \((AB)^{-1} = B^{-1}A^{-1}\)</li>
	<li>The identity element is the identity matrix.</li>
	<li>Matrix multiplication is associative.</li>
</ol>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Rotations in Plane</b></h4>
<p>We can define a plane rotation by an angle counter clockwise by the left multiplication by the following rotation matrix</p>
<div>
$$
\begin{align*}
Rot(\theta) &amp;= 
\begin{pmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{pmatrix}
\end{align*}
$$
</div>
<p>The rotation matrix has some properties</p>
<ol type="a">
	<li>\(\text{Rot}(0) = I\) is the identity matrix.</li>
	<li>\(\text{Rot}(\alpha)Rot(\beta) = \text{Rot}(\alpha + \beta)\).</li>
	<li>\(\text{Rot}(\alpha)^{-1} = \text{Rot}(-\alpha)\).</li>
	<li>\(\text{Rot}(\alpha + 2\pi n) = \text{Rot}(\alpha)\) where \(n \in \mathbf{Z}\).</li>
</ol>
<p>From this we see that the collection of rotation matrices forms a group with matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------></p>
<h4><b>Rotations in Space</b></h4>
<div>
$$
\begin{align*}
\text{Rot}_{e_3}(\theta) &amp;= 
\begin{pmatrix}
\cos\theta &amp; -\sin\theta &amp; 0 \\
\sin\theta &amp; \cos\theta &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>This defines a rotation around the \(z-\)axis by angle \(\theta\) counter clockwise viewed from the head of the vector \(e_3\) where \(e_3 = (0,0,1)\). But instead of using \(e_3\), we’ll use a more general vector \(u \in \mathbf{R}^3\) such that \(\lVert {u} \rVert = 1\). So let \(\text{Rot}_u(\theta)\) be a rotation matrix around the axis through \(u\) by angle \(\theta\) counterclockwise when viewed from the head of \(u\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>How Do you Compute The Rotation Matrix?</b></h4>
<p>Recall from Linear Algebra that</p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
An orthogonal matrix is a square matrix \(P \in Mat_{n \times n}(\mathbf{R})\) such that \(P^{T}P = I\) or \(P^{-1} = P^{T}\).
</div>
<p><br />
Equivalently the columns of \(P\) are an orthonormal basis of \(\mathbf{R}^n\) and the rows of \(P\) are also orthonormal basis of \(\mathbf{R}^n\).
<br />
<br />
Given the above definition, there is a useful formula where for \(u \in \mathbf{R}^3\), \(\lVert u \rVert = 1\), \(\theta \in \mathbf{R}\) and \(P\) an orthogonal matrix such the product</p>
<div>
$$
\begin{align*}
P \text{Rot}_u(\theta) P^{-1}
\end{align*}
$$
</div>
<p>is another rotation matrix but instead of having \(u\) as the axis, we have \(Pu\) as the axis so</p>
<div>
$$
\begin{align*}
\text{Rot}_{Pu}(\theta) = P \text{Rot}_u(\theta) P^{-1}
\end{align*}
$$
</div>
<p>Why is this a useful formula? Given \(\lVert u \rVert = 1\) and some angle \(\theta\), construct an orthonormal basis using Gram-Schmidt by setting \(u_3 = u\) and finding two more perpendicular vectors (normalized) \(u_1, u_2 \in \mathbf{R}^3\). Let \(P = [u_1 \quad u_2 \quad u_3]\). This means that \(Pe_3 = u_3 = u\). Therefore</p>
<div>
$$
\begin{align*}
\text{Rot}_{u}(\theta) = P \text{Rot}_{e_3}(\theta) P^{T}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------>
<h4><b>Example</b></h4>
<p>So now suppose we want to rotate around \(\frac{(e_1 + e_2)}{\sqrt{2}}\) (which is a unit vector) by \(\theta = \frac{\pi}{3}\). Then, we’ll need two more orthonormal vectors in addition to \(\frac{(e_1 + e_2)}{\sqrt{2}}\) to construct an orthonormal basis. We can use Gram-Schmidt to come up with the following orthonormal vectors and set them to be the column vectors of \(P\) as follows</p>
<div>
$$
\begin{align*}
P &amp;= 
\begin{pmatrix}
0 &amp; \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
0 &amp; -\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
1 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>The rotation \(\text{Rot}_{e_3}\) is as follows</p>
<div>
$$
\begin{align*}
\text{Rot}_{e_3}(\frac{\pi}{3}\big) &amp;= 
\begin{pmatrix}
\frac{1}{2} &amp; -\frac{\sqrt{3}}{2} &amp; 0 \\
\frac{\sqrt{3}}{2} &amp; \frac{1}{2} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\text{Rot}_{\frac{e_1 + e_2}{\sqrt{2}}}\big(\frac{\pi}{3}\big) &amp;= P \text{Rot}_{e_3}(\frac{\pi}{3}\big) P^{T}
\\
&amp;= 
\begin{pmatrix}
\frac{3}{4} &amp; \frac{1}{4} &amp; \frac{\sqrt{3}}{8} \\
\frac{3}{4} &amp; \frac{3}{4} &amp; -\frac{\sqrt{3}}{8} \\
-\frac{\sqrt{3}}{8} &amp; \frac{\sqrt{3}}{8} &amp; \frac{1}{2}
\end{pmatrix}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Rotations in Space</b></h4>
<p>From last lecture, we saw the symmetries of the square. These symmetries can be represented with rotation matrices. We have the identity rotation \(I\). We can also define the rotation around the axis coming through the centroid of the face (\(z-\)axis in the lecture) as \(R = \text{Rot}_{e_3}(\frac{\pi}{2})\). Therefore, \(R^2 = \text{Rot}_{e_3}(\pi)\) and \(R^3 = \text{Rot}_{e_3}(\frac{3\pi}{2})\).
<br />
<br />
What about the \(180\) degrees rotation around the \(x-axis\) labeled as \(a\) from last time? We can define it as \(A = \text{Rot}_{e_1}(\pi)\). Similarly, \(B = \text{Rot}_{e_2}(\pi)\), \(C = \text{Rot}_{\frac{e_1 - e_2}{\sqrt{2}}}(\pi)\) and \(D = \text{Rot}_{\frac{e_1 + e_2}{\sqrt{2}}}(\pi)\)
<br />
<br />
Rotations in space have also the following properties / identities</p>
<ol type="a">
	<li>\(\text{Rot}_u(0) = I\) is the identity matrix where \(u\) is any unit vector.</li>
	<li>\(\text{Rot}_u(\theta + 2\pi n) = \text{Rot}_u(\theta)\) where \(n \in \mathbf{Z}\).</li>
	<li>\(\text{Rot}_u(\theta) = \text{Rot}_{-u}(-\theta)\).</li>
	<li>\(\text{Rot}_u(\theta)^{-1} = \text{Rot}_{u}(-\theta) = \text{Rot}_{-u}(\theta)\).</li>
	<li>\(\text{Rot}_u(\alpha)Rot_v(\beta) = a\) is a rotation matrix.</li>
	<li>\(\text{Rot}_u(\alpha)Rot_u(\beta) = Rot_u(\alpha + \beta)\).</li>
</ol>
<p>But why is the last statement true? To figure it out, we need to introduce another definition
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A special orthogonal matrix is an \(A \in Mat_{n \times n}(\mathbf{R})\) such that
<ol type="a">
	<li>\(A^TA = I\). (The orthogonal property)</li>
	<li>\(\det(A) = \pm 1\). (This is as a result of the fact that \(det(A^{-1}) = \frac{1}{\det(A)}\) and that \(\det(A) = \det(A^T)\)</li> 
</ol>
</div>
<p><br />
And now we have the following proposition
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
\(A \in Mat_{3 \times 3}(\mathbf{R})\) is a rotation matrix if and only if it is special orthogonal.
</div>
<p><br />
We denote these matrices special matrices with</p>
<div>
$$
\begin{align*}
SO(n) = \{ A \in Mat_{n \times n} \text{ special orthogonal} \} \subseteq O(n) \subseteq GL_n(\mathbf{R}) 
\end{align*}
$$
</div>
<p>Observation: \(A, B \in SO(n)\) implies that \(AB \in SO(n)\). To show that this is true, we need to show that properties of special orthogonal matrices hold. To see that, observe that</p>
<ul>
	<li>\( (AB)^TAB = B^TA^TAB = B^TB = I \)</li>
	<li>\( \det(AB) = \det(A)\det(B) = 1 \)</li>
</ul>
<p><br />
Also observe that the identity matrix is in \(SO(n)\) and that for any \(A \in SO(n)\), \(A^{-1} = A^{T} \in SO(n)\). Therefore \(SO(n)\) is a group with matrix multiplication.
<br />
<br />
So now we know that the collection of special orthogonal matrices is a group. Therefore, if the proposition we introduced earlier holds (where we said that \(A \in Mat_{3 \times 3}(\mathbf{R})\) is a rotation matrix if and only if it is special orthogonal), then we can also conclude that the product of two rotation matrices is also a rotation matrix. So let’s sketch the proof of the proposition
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------>
<b>Proof</b>
<br />
\(\Rightarrow\) (\(\text{Rot}_u(\theta) \in SO(3)\)):
<br />
Let \(\text{Rot}_u(\theta) = P\text{Rot}_{e_3}(\theta)P^{-1}\) for some chosen \(P \in O(3)\). We want to show that \(P\text{Rot}_{e_3}(\theta)P^{-1} \in SO(3)\). To do so we need to show that if \(A \in SO(n)\) and \(P \in O(n)\), then \(PAP^{-1} = PAP^{T} \in SO(3)\). We can easily show this by verifying the two properties of special orthogonal matrices. For example,</p>
<div>
$$
\begin{align*}
(PAP^{T})^{T}PAP^{T} &amp;= PA^TP^TPAP^{T} \\
                     &amp;= PA^TAP^{T} \quad \text{ (because $P \in O(n)$)} \\
                     &amp;= PP^{T} = I.
\end{align*}
$$
</div>
<p>The verification that the determinat is 1 is also the same. So now what’s left is to show that \(\text{Rot}_{e_3}(\theta)\) is in \(SO(3)\). We can verify this because we know the exact matrix of \(\text{Rot}_{e_3}(\theta)\) so we can computationally verify that it is a special orthogonal matrix. Finally, since \(P \in O(n)\) and we just showed that \(\text{Rot}_{e_3}(\theta) \in SO(3)\), then \(P\text{Rot}_{e_3}(\theta)P^{T}\) is in \(SO(3)\) as we wanted to show.
<br />
<br />
\(\Leftarrow\) (\(A \in SO(3) \implies A = \text{Rot}_u(\theta)\)):
<br />
For this we’ll use the fact that if \(A \in SO(3)\), then \(1\) is an eigenvalue of \(A\). So let \(u\) be the eigenvector of \(A\) corresponding to \(\lambda = 1\) so \(Au = u\). Choose \(\lVert u \rVert = 1\). Now let \(u_3 = u\) and form an orthonormal basis \(\{u_1, u_2, u_3\}\). Now Let \(P = [u_1 \quad u_2 \quad u_3]\) and let \(B = P^{-1}AP = P^{T}AP\). We know that \(Pe_3 = u_3\) but \(u_3\) is an eigenvector of \(A\).</p>
<div>
$$
\begin{align*}
Be_3 = P^{-1}APe_3 = P^{-1}Au_3 = P^{-1}u_3 = e_3
\end{align*}
$$
</div>
<p>So \(e_3\) is an eigenvector of \(B\) … then by the fact we used in the left direction, we can conclude that \(B \in SO(3)\) (Why?). So this means that \(B\) has orthonormal columns since it’s in \(SO(3)\) and the third column specifically is \(e_3\). So</p>
<div>
$$
\begin{align*}
B
&amp;= 
\begin{pmatrix}
a &amp; b &amp; 0 \\
c &amp; d &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>By an algebraic argument we can show that \(a = d = \cos\theta\) and \(c = -b = \sin\theta\). So \(B\) must be \(\text{Rot}_{e_3}\). Therefore, \(A = P\text{Rot}_{e_3}P^{T}\)….
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[A group is a set with a defined product. This product is associative. The group is closed under this product. We have an identity element and each element has an inverse. Formally, Definition (1.10.1) A group is a (nonempty) set \(G\) with a binary operation (a product) \(G \times G \rightarrow G\) satisfying the following properties: The group is closed under the operation. The product is associative. For all \(a, b, c \in G\), we have \((ab)c = a(bc)\). There is an identity element \(e \in G\) with property that for all \(a \in G\), \(ea = ae = a\). For each element \(a \in G\), there is an element \(a^{-1}\ \in G\) satisfying \(aa^{-1} = a^{-1}a = e\) (The inverse). What are examples of the groups? \(\mathbf{R}\) with the addition operation. \(\mathbf{R}^{x} = \mathbf{R} - \{0\}\) with the multiplication operation. \(\mathbf{Z}\) with the addition operation. The set of invertible \(n \times n\) matrices with entries in \(\mathbf{R}\) with matrix multiplication as the product. Any vector space is a group if you forget about the scalar multiplication. For any set \(T\), define the set of all bijections \(g: T \rightarrow T\). The set of all bijections is the symmetric group of \(T\). The operation here is the composition of these maps. Example Suppose we define the group \((\mathbf{R}, \ast)\) where \(\ast\) is defined as $$ \begin{align*} x \ast y = \sqrt[3]{x^3 + y^3} \end{align*} $$ We can further check that this product satisfies the three axioms of a group. Example Let \(F\) be a field (for example \(\mathbf{R}\) or \(\mathbf{C}\)). Let \(Mat_{n \times n}(F)\) be the set of matrices with entries in \(F\). Then define \(GL_n(F)\) (General Linear Group) as a subset of \(A \in M_{n \times n}(F)\) where \(A\) is an invertible matrix. The set \(GL_n(F)\) equipped with matrix multiplication is a group. It satisfies the three axioms The group is closed under multiplication because multiplying two invertible matrices is another invertible matrix. \((AB)^{-1} = B^{-1}A^{-1}\) The identity element is the identity matrix. Matrix multiplication is associative. Rotations in Plane We can define a plane rotation by an angle counter clockwise by the left multiplication by the following rotation matrix $$ \begin{align*} Rot(\theta) &amp;= \begin{pmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix} \end{align*} $$ The rotation matrix has some properties \(\text{Rot}(0) = I\) is the identity matrix. \(\text{Rot}(\alpha)Rot(\beta) = \text{Rot}(\alpha + \beta)\). \(\text{Rot}(\alpha)^{-1} = \text{Rot}(-\alpha)\). \(\text{Rot}(\alpha + 2\pi n) = \text{Rot}(\alpha)\) where \(n \in \mathbf{Z}\). From this we see that the collection of rotation matrices forms a group with matrix multiplication. Rotations in Space $$ \begin{align*} \text{Rot}_{e_3}(\theta) &amp;= \begin{pmatrix} \cos\theta &amp; -\sin\theta &amp; 0 \\ \sin\theta &amp; \cos\theta &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} \end{align*} $$ This defines a rotation around the \(z-\)axis by angle \(\theta\) counter clockwise viewed from the head of the vector \(e_3\) where \(e_3 = (0,0,1)\). But instead of using \(e_3\), we’ll use a more general vector \(u \in \mathbf{R}^3\) such that \(\lVert {u} \rVert = 1\). So let \(\text{Rot}_u(\theta)\) be a rotation matrix around the axis through \(u\) by angle \(\theta\) counterclockwise when viewed from the head of \(u\). How Do you Compute The Rotation Matrix? Recall from Linear Algebra that Definition An orthogonal matrix is a square matrix \(P \in Mat_{n \times n}(\mathbf{R})\) such that \(P^{T}P = I\) or \(P^{-1} = P^{T}\). Equivalently the columns of \(P\) are an orthonormal basis of \(\mathbf{R}^n\) and the rows of \(P\) are also orthonormal basis of \(\mathbf{R}^n\). Given the above definition, there is a useful formula where for \(u \in \mathbf{R}^3\), \(\lVert u \rVert = 1\), \(\theta \in \mathbf{R}\) and \(P\) an orthogonal matrix such the product $$ \begin{align*} P \text{Rot}_u(\theta) P^{-1} \end{align*} $$ is another rotation matrix but instead of having \(u\) as the axis, we have \(Pu\) as the axis so $$ \begin{align*} \text{Rot}_{Pu}(\theta) = P \text{Rot}_u(\theta) P^{-1} \end{align*} $$ Why is this a useful formula? Given \(\lVert u \rVert = 1\) and some angle \(\theta\), construct an orthonormal basis using Gram-Schmidt by setting \(u_3 = u\) and finding two more perpendicular vectors (normalized) \(u_1, u_2 \in \mathbf{R}^3\). Let \(P = [u_1 \quad u_2 \quad u_3]\). This means that \(Pe_3 = u_3 = u\). Therefore $$ \begin{align*} \text{Rot}_{u}(\theta) = P \text{Rot}_{e_3}(\theta) P^{T} \end{align*} $$ Example So now suppose we want to rotate around \(\frac{(e_1 + e_2)}{\sqrt{2}}\) (which is a unit vector) by \(\theta = \frac{\pi}{3}\). Then, we’ll need two more orthonormal vectors in addition to \(\frac{(e_1 + e_2)}{\sqrt{2}}\) to construct an orthonormal basis. We can use Gram-Schmidt to come up with the following orthonormal vectors and set them to be the column vectors of \(P\) as follows $$ \begin{align*} P &amp;= \begin{pmatrix} 0 &amp; \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ 0 &amp; -\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ 1 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The rotation \(\text{Rot}_{e_3}\) is as follows $$ \begin{align*} \text{Rot}_{e_3}(\frac{\pi}{3}\big) &amp;= \begin{pmatrix} \frac{1}{2} &amp; -\frac{\sqrt{3}}{2} &amp; 0 \\ \frac{\sqrt{3}}{2} &amp; \frac{1}{2} &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} \end{align*} $$ Therefore, $$ \begin{align*} \text{Rot}_{\frac{e_1 + e_2}{\sqrt{2}}}\big(\frac{\pi}{3}\big) &amp;= P \text{Rot}_{e_3}(\frac{\pi}{3}\big) P^{T} \\ &amp;= \begin{pmatrix} \frac{3}{4} &amp; \frac{1}{4} &amp; \frac{\sqrt{3}}{8} \\ \frac{3}{4} &amp; \frac{3}{4} &amp; -\frac{\sqrt{3}}{8} \\ -\frac{\sqrt{3}}{8} &amp; \frac{\sqrt{3}}{8} &amp; \frac{1}{2} \end{pmatrix} \end{align*} $$ Rotations in Space From last lecture, we saw the symmetries of the square. These symmetries can be represented with rotation matrices. We have the identity rotation \(I\). We can also define the rotation around the axis coming through the centroid of the face (\(z-\)axis in the lecture) as \(R = \text{Rot}_{e_3}(\frac{\pi}{2})\). Therefore, \(R^2 = \text{Rot}_{e_3}(\pi)\) and \(R^3 = \text{Rot}_{e_3}(\frac{3\pi}{2})\). What about the \(180\) degrees rotation around the \(x-axis\) labeled as \(a\) from last time? We can define it as \(A = \text{Rot}_{e_1}(\pi)\). Similarly, \(B = \text{Rot}_{e_2}(\pi)\), \(C = \text{Rot}_{\frac{e_1 - e_2}{\sqrt{2}}}(\pi)\) and \(D = \text{Rot}_{\frac{e_1 + e_2}{\sqrt{2}}}(\pi)\) Rotations in space have also the following properties / identities \(\text{Rot}_u(0) = I\) is the identity matrix where \(u\) is any unit vector. \(\text{Rot}_u(\theta + 2\pi n) = \text{Rot}_u(\theta)\) where \(n \in \mathbf{Z}\). \(\text{Rot}_u(\theta) = \text{Rot}_{-u}(-\theta)\). \(\text{Rot}_u(\theta)^{-1} = \text{Rot}_{u}(-\theta) = \text{Rot}_{-u}(\theta)\). \(\text{Rot}_u(\alpha)Rot_v(\beta) = a\) is a rotation matrix. \(\text{Rot}_u(\alpha)Rot_u(\beta) = Rot_u(\alpha + \beta)\). But why is the last statement true? To figure it out, we need to introduce another definition Definition A special orthogonal matrix is an \(A \in Mat_{n \times n}(\mathbf{R})\) such that \(A^TA = I\). (The orthogonal property) \(\det(A) = \pm 1\). (This is as a result of the fact that \(det(A^{-1}) = \frac{1}{\det(A)}\) and that \(\det(A) = \det(A^T)\) And now we have the following proposition Proposition \(A \in Mat_{3 \times 3}(\mathbf{R})\) is a rotation matrix if and only if it is special orthogonal. We denote these matrices special matrices with $$ \begin{align*} SO(n) = \{ A \in Mat_{n \times n} \text{ special orthogonal} \} \subseteq O(n) \subseteq GL_n(\mathbf{R}) \end{align*} $$ Observation: \(A, B \in SO(n)\) implies that \(AB \in SO(n)\). To show that this is true, we need to show that properties of special orthogonal matrices hold. To see that, observe that \( (AB)^TAB = B^TA^TAB = B^TB = I \) \( \det(AB) = \det(A)\det(B) = 1 \) Also observe that the identity matrix is in \(SO(n)\) and that for any \(A \in SO(n)\), \(A^{-1} = A^{T} \in SO(n)\). Therefore \(SO(n)\) is a group with matrix multiplication. So now we know that the collection of special orthogonal matrices is a group. Therefore, if the proposition we introduced earlier holds (where we said that \(A \in Mat_{3 \times 3}(\mathbf{R})\) is a rotation matrix if and only if it is special orthogonal), then we can also conclude that the product of two rotation matrices is also a rotation matrix. So let’s sketch the proof of the proposition Proof \(\Rightarrow\) (\(\text{Rot}_u(\theta) \in SO(3)\)): Let \(\text{Rot}_u(\theta) = P\text{Rot}_{e_3}(\theta)P^{-1}\) for some chosen \(P \in O(3)\). We want to show that \(P\text{Rot}_{e_3}(\theta)P^{-1} \in SO(3)\). To do so we need to show that if \(A \in SO(n)\) and \(P \in O(n)\), then \(PAP^{-1} = PAP^{T} \in SO(3)\). We can easily show this by verifying the two properties of special orthogonal matrices. For example, $$ \begin{align*} (PAP^{T})^{T}PAP^{T} &amp;= PA^TP^TPAP^{T} \\ &amp;= PA^TAP^{T} \quad \text{ (because $P \in O(n)$)} \\ &amp;= PP^{T} = I. \end{align*} $$ The verification that the determinat is 1 is also the same. So now what’s left is to show that \(\text{Rot}_{e_3}(\theta)\) is in \(SO(3)\). We can verify this because we know the exact matrix of \(\text{Rot}_{e_3}(\theta)\) so we can computationally verify that it is a special orthogonal matrix. Finally, since \(P \in O(n)\) and we just showed that \(\text{Rot}_{e_3}(\theta) \in SO(3)\), then \(P\text{Rot}_{e_3}(\theta)P^{T}\) is in \(SO(3)\) as we wanted to show. \(\Leftarrow\) (\(A \in SO(3) \implies A = \text{Rot}_u(\theta)\)): For this we’ll use the fact that if \(A \in SO(3)\), then \(1\) is an eigenvalue of \(A\). So let \(u\) be the eigenvector of \(A\) corresponding to \(\lambda = 1\) so \(Au = u\). Choose \(\lVert u \rVert = 1\). Now let \(u_3 = u\) and form an orthonormal basis \(\{u_1, u_2, u_3\}\). Now Let \(P = [u_1 \quad u_2 \quad u_3]\) and let \(B = P^{-1}AP = P^{T}AP\). We know that \(Pe_3 = u_3\) but \(u_3\) is an eigenvector of \(A\). $$ \begin{align*} Be_3 = P^{-1}APe_3 = P^{-1}Au_3 = P^{-1}u_3 = e_3 \end{align*} $$ So \(e_3\) is an eigenvector of \(B\) … then by the fact we used in the left direction, we can conclude that \(B \in SO(3)\) (Why?). So this means that \(B\) has orthonormal columns since it’s in \(SO(3)\) and the third column specifically is \(e_3\). So $$ \begin{align*} B &amp;= \begin{pmatrix} a &amp; b &amp; 0 \\ c &amp; d &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} \end{align*} $$ By an algebraic argument we can show that \(a = d = \cos\theta\) and \(c = -b = \sin\theta\). So \(B\) must be \(\text{Rot}_{e_3}\). Therefore, \(A = P\text{Rot}_{e_3}P^{T}\)…. References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 01: Groups and Symmetries</title><link href="http://localhost:4000/jekyll/update/2025/01/24/math417-01-groups-and-symmetries.html" rel="alternate" type="text/html" title="Lecture 01: Groups and Symmetries" /><published>2025-01-24T00:01:36-08:00</published><updated>2025-01-24T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/24/math417-01-groups-and-symmetries</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/24/math417-01-groups-and-symmetries.html"><![CDATA[<h4><b>Introduction</b></h4>
<p>Abstract Algebra lets us examine structures which allow for algebraic manipulations of the types we’ve used in high school. As an example, we have Fields. It’s a set with operations addition, subtraction, multiplication and division that satisfy a few rules. An example of a field is the field of real numbers \(\mathbf{R}\) or the field of complex numbers \(\mathbf{C}\). We also have the field of rational numbers \(\mathbf{Q}\). Or \(\mathbf{Z}_p\) which is a finite field of integers modulo some prime \(p\). It is finite with \(p\) elements.
<br />
<br />
Another example of important structures are Rings. Here we have the operations addition, subtraction and multiplication. The first example of a ring is \(\mathbf{Z}\). Another example of a ring is a polynomial ring in one variable over \(\mathbf{R}\), \(\mathbf{R}[x]\). The set of all square matrices. \(M_{n \times n}(\mathbf{R})\) is also another example. This example is non-commutative.<br />
<br />
<br />
Another structure is a Group. It is a set with one operation usually called “multiplication”. One example is the set of real numbers with just addition. Another example is \((\mathbf{R}^{x})\) (the set of real numbers with zero removed, \(\mathbf{R} - \{0\}\) along with multiplication. \(GL(n,\mathbf{R})\), the set of invertible matrices with multiplication is another example of a Group. This is an example of a non-abelian or non-commutative group.
<br />
<br />
One additional structure that comes up is a Monoid. These are sets with one operation but you don’t have to have inverses like Groups. So \(\mathbf{R}\) with multiplication is not a group because it includes zero and zero doesn’t have an inverse but this set is a Monoid. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Groups and Symmetries</b></h4>

<p>From the book, a symmetry is an undetectable motion. An object is symmetric if it has symmetries. Take an equilateral triangle.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-1.png" width="40%" class="center" /></p>

<p>What are the symmetries of this triangle? We want to rotate the triangle in such a way that we won’t tell if it was rotated. For example, if we rotate this triangle by \(120\) degrees counter clockwise, then we’ll get a triangle in the same exact orientation. We label this rotation with \(r_1\). Note here that the labels on the vertices are used to illustrate that we indeed have rotated the triangle. They are a visual aid.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-2.png" width="70%" class="center" /></p>

<p>We can also rotate this triangle \(120\) degrees clockwise to get the following \(r_2\) rotation.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-3.png" width="70%" class="center" /></p>

<p>Are there more rotations? Yes, consider the rotation around the axis that goes through the vertex \(A\) by 180 degrees so essentially it’s a flip around that axis.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-4.png" width="70%" class="center" /></p>

<p>Similarly, we can do a flip around the axis that goes through the vertex \(B\) and a flip around the axis that goes through the vertex \(C\).</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-5.png" width="40%" class="center" /></p>

<p>So now we have \(5\) symmetries total. Is that all? No, We have one more symmetry representing the identity symmetry \(e\) that does nothing. So the set of symmetries are then</p>
<div>
$$
\begin{align*}
G = \{e, r_1, r_2, a, b, c\}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Can We Compose Symmetries?</b></h4>

<p>Can we compose symmetries? Yes, in fact we’ll equip the group above with the operation compose. This means that for any \(x, y \in G\), \(xy\) means that we need to apply the rotation \(y\) first and then apply the rotation \(x\). Since we have \(6\) elements in \(G\), then we have \(36\) ways of composing these elements. 
<br />
<br />
For example composing \(r_1\) with \(r_1\) again results in the rotation \(r_2\) so \(r_1r_1 = r_2\).</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-6.png" width="70%" class="center" /></p>
<p>And if we compose \(r_1\) with \(r_2\), then we’ll get the identity rotation so \(r_1r_2 = e\).</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-7.png" width="70%" class="center" /></p>
<p>In fact since \(r_1r_1 = r_2\) and \(r_1r_2 = e\), we’ll change the notation to a much simpler notation. Let \(r_1 = r\). Then, \(r_2 = r_1r_1 = rr = r^2\). So now we can fill these in a table as follows
<br />
<!------------------------------------------------------------------------></p>
<div>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td></td>
    <td>\(e\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
  </tr>
  <tr>
    <td>\(e\)</td>
    <td>\(e\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
  </tr>
  <tr>
    <td>\(r\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(e\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
  </tr>
  <tr>
    <td>\(r^2\)</td>
    <td>\(r^2\)</td>
	<td>\(e\)</td>
	<td>\(r\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
  </tr>
  <tr>
    <td>\(a\)</td>
    <td>\(a\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
  </tr>
 <tr>
   <td>\(b\)</td>
   <td>\(b\)</td>
   <td>\(\)</td>
   <td>\(\)</td>	
   <td>\(\)</td>
   <td>\(\)</td>
   <td>\(\)</td>
 </tr>
  <tr>
    <td>\(c\)</td>
    <td>\(c\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
  </tr>
</table>
</div>
<p><br />
So far so good. What about composing the flips around the axes? For example composing \(a\) with \(b\). What would that do? For starters, note that each flip is a \(180\) degrees flip. So if you compose ANY two flips, we’ll be back to the same face we started with. This means that any two flips will be equivalent to \(e, r\) or \(r^2\). For example, if we apply \(a\) and then apply \(b\), we’ll get</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-8.png" width="90%" class="center" /></p>

<p>Note here that when did the second flip, we still rotated through the \(b\) access which now goes through vertex \(C\) instead. Physically the axis will still be in the same position. So we see above doing two flips \(a\), then around \(b\) is equivalent to applying two rotations so \(ba = r^2\). 
<br />
<br />
What about two flips of the same type? What happens if you apply \(a\) and then again flip it around that same access? We’ll get the identity rotation since we just go back to the same exact orientation. Applying this same logic on the rest of flips, we can fill that forth quadrant of the table which are all about compositions of flips.
<br />
<br />
<!------------------------------------------------------------------------></p>
<div>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td></td>
    <td>\(e\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
  </tr>
  <tr>
    <td>\(e\)</td>
    <td>\(e\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
  </tr>
  <tr>
    <td>\(r\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(e\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
  </tr>
  <tr>
    <td>\(r^2\)</td>
    <td>\(r^2\)</td>
	<td>\(e\)</td>
	<td>\(r\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
  </tr>
  <tr>
    <td>\(a\)</td>
    <td>\(a\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(e\)</td>
	<td>\(r\)</td>
	<td>\(r^2\)</td>
  </tr>
 <tr>
   <td>\(b\)</td>
   <td>\(b\)</td>
   <td>\(\)</td>
   <td>\(\)</td>	
   <td>\(r^2\)</td>
   <td>\(e\)</td>
   <td>\(r\)</td>
 </tr>
  <tr>
    <td>\(c\)</td>
    <td>\(c\)</td>
	<td>\(\)</td>
	<td>\(\)</td>
	<td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(e\)</td>
  </tr>
</table>
</div>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Second and Third Quadrants</b></h4>
<p>What about applying a rotation \(r\) followed by a flip \(a\)? What is \(ar\)? We apply a rotation so we move \(120\) degrees anti-clockwise and then we do an \(a\) flip which is now going to be through the \(C\) vertex. The triangle will now be faced down which is equivalent to applying a single flip. Comparing the vertices in the original triangle and the outcome, we see that \(B\) is fixed while \(A\) and \(C\) have switched. This means that this is a flip around the \(b\) access.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-9.png" width="90%" class="center" /></p>

<!------------------------------------------------------------------------>
<div>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td></td>
    <td>\(e\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
  </tr>
  <tr>
    <td>\(e\)</td>
    <td>\(e\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
  </tr>
  <tr>
    <td>\(r\)</td>
    <td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(e\)</td>
	<td>\(c\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
  </tr>
  <tr>
    <td>\(r^2\)</td>
    <td>\(r^2\)</td>
	<td>\(e\)</td>
	<td>\(r\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
	<td>\(a\)</td>
  </tr>
  <tr>
    <td>\(a\)</td>
    <td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(c\)</td>
	<td>\(e\)</td>
	<td>\(r\)</td>
	<td>\(r^2\)</td>
  </tr>
 <tr>
   <td>\(b\)</td>
   <td>\(b\)</td>
   <td>\(c\)</td>
   <td>\(a\)</td>	
   <td>\(r^2\)</td>
   <td>\(e\)</td>
   <td>\(r\)</td>
 </tr>
  <tr>
    <td>\(c\)</td>
    <td>\(c\)</td>
	<td>\(a\)</td>
	<td>\(b\)</td>
	<td>\(r\)</td>
	<td>\(r^2\)</td>
	<td>\(e\)</td>
  </tr>
</table>
</div>
<p>We notice that each row has each rotation appearing exactly once and it’s the same for each column. Notice also that composition is not commutative above. So \(ar \neq ra\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of Symmetries</b></h4>
<p>We can actually represent these symmetries by matrices. For example, the following matrix will represent a \(120\) degrees rotation around the \(z-\) axis</p>
<div>
$$
\begin{align*}
r(x,y,z) &amp;= 
\begin{pmatrix}
\cos\frac{2\pi}{3} &amp; - \sin\frac{2\pi}{3} &amp; 0 \\
\sin\frac{2\pi}{3} &amp; + \cos\frac{2\pi}{3} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>or we can do the \(a\) flip so a \(180\) degree flip around the \(x\)-axis.</p>
<div>
$$
\begin{align*}
a(x,y,z) &amp;= 
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>What about something more complex? like \(ra\) which is equal to \(c\) using the multiplication table above. This will be</p>
<div>
$$
\begin{align*}
c(x,y,z) &amp;= ra(x,y,z) 
\\
&amp;= 
\begin{pmatrix}
\cos\frac{2\pi}{3} &amp; - \sin\frac{2\pi}{3} &amp; 0 \\
\sin\frac{2\pi}{3} &amp; + \cos\frac{2\pi}{3} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 0
\end{pmatrix}
\\
&amp;=
\begin{pmatrix}
\cos\frac{2\pi}{3} &amp; \sin\frac{2\pi}{3} &amp; 0 \\
\sin\frac{2\pi}{3} &amp; - \cos\frac{2\pi}{3} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Symmetries of the Rectangle</b></h4>
<p>How many symmetries does the rectangle have?</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-rect.png" width="30%" class="center" /></p>

<p>We can have a rotation \(r_1\) around the \(y-\)axis as follows</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-rect-r1.png" width="75%" class="center" /></p>

<p>We can also have a rotation \(r_2\) around the \(x-\)axis</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-rect-r2.png" width="75%" class="center" /></p>

<p>Finally, we have one more rotation \(r_3\) around the \(z-\)axis. (the axis coming off the screen toward us from the centroid of the face.)</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-rect-r3.png" width="75%" class="center" /></p>

<p>So we have four rotations which make the following group</p>
<div>
$$
\begin{align*}
G = \{e, r_1, r_2, r_3\}
\end{align*}
$$
</div>
<p>One thing to notice here that for any rotation in \(G\), if you apply it twice, you’ll get the identity rotation. So each rotation has an order \(2\). The multiplication table for the symmetries of the rectangle is as follows</p>
<div>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td></td>
    <td>\(e\)</td>
    <td>\(r_1\)</td>
	<td>\(r_2\)</td>
	<td>\(r_3\)</td>
  </tr>
  <tr>
    <td>\(e\)</td>
    <td>\(e\)</td>
    <td>\(r_1\)</td>
	<td>\(r_2\)</td>
	<td>\(r_3\)</td>
  </tr>
  <tr>
    <td>\(r_1\)</td>
    <td>\(r_1\)</td>
    <td>\(r_2\)</td>
	<td>\(r_3\)</td>
	<td>\(e\)</td>
  </tr>
  <tr>
    <td>\(r_2\)</td>
    <td>\(r_2\)</td>
    <td>\(r_3\)</td>
	<td>\(e\)</td>
	<td>\(r_1\)</td>
  </tr>
  <tr>
    <td>\(r_3\)</td>
    <td>\(r_3\)</td>
    <td>\(e\)</td>
	<td>\(r_1\)</td>
	<td>\(r_2\)</td>
  </tr>
</table>
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Symmetries of the Square</b></h4>
<p>What about the symmetries of the square? There are eight of them. First we consider the axis coming through the centroid of the face toward us. There are three rotations around this axis.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-r.png" width="55%" class="center" /></p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-r2.png" width="55%" class="center" /></p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-r3.png" width="55%" class="center" /></p>

<p>Next, we can consider the \(x-\)axis or the axis coming through the center of the edges. We can do a \(180\) degrees rotation around this access (a flip). Call this one \(a\).</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-a.png" width="55%" class="center" /></p>

<p>Similarly, we can do one around the \(y-\)axis.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-b.png" width="55%" class="center" /></p>

<p>And then two more through the diagonals of the square.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-c.png" width="55%" class="center" /></p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec01/01-square-d.png" width="55%" class="center" /></p>

<div>
$$
\begin{align*}
G = \{e, r, r^2, r^3, a, b, c, d\}
\end{align*}
$$
</div>
<p><br />
What about the cube? How many symmetries does it have? it has 24 symmetries!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Introduction Abstract Algebra lets us examine structures which allow for algebraic manipulations of the types we’ve used in high school. As an example, we have Fields. It’s a set with operations addition, subtraction, multiplication and division that satisfy a few rules. An example of a field is the field of real numbers \(\mathbf{R}\) or the field of complex numbers \(\mathbf{C}\). We also have the field of rational numbers \(\mathbf{Q}\). Or \(\mathbf{Z}_p\) which is a finite field of integers modulo some prime \(p\). It is finite with \(p\) elements. Another example of important structures are Rings. Here we have the operations addition, subtraction and multiplication. The first example of a ring is \(\mathbf{Z}\). Another example of a ring is a polynomial ring in one variable over \(\mathbf{R}\), \(\mathbf{R}[x]\). The set of all square matrices. \(M_{n \times n}(\mathbf{R})\) is also another example. This example is non-commutative. Another structure is a Group. It is a set with one operation usually called “multiplication”. One example is the set of real numbers with just addition. Another example is \((\mathbf{R}^{x})\) (the set of real numbers with zero removed, \(\mathbf{R} - \{0\}\) along with multiplication. \(GL(n,\mathbf{R})\), the set of invertible matrices with multiplication is another example of a Group. This is an example of a non-abelian or non-commutative group. One additional structure that comes up is a Monoid. These are sets with one operation but you don’t have to have inverses like Groups. So \(\mathbf{R}\) with multiplication is not a group because it includes zero and zero doesn’t have an inverse but this set is a Monoid. Groups and Symmetries]]></summary></entry></feed>