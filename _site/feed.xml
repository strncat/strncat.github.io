<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-04-25T15:59:41-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 35: Principal Ideal</title><link href="http://localhost:4000/jekyll/update/2025/02/27/math417-35-principal-ideal.html" rel="alternate" type="text/html" title="Lecture 35: Principal Ideal" /><published>2025-02-27T00:01:36-08:00</published><updated>2025-02-27T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/27/math417-35-principal-ideal</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/27/math417-35-principal-ideal.html"><![CDATA[<p>Last time we introduced the notion of an ideal in a ring. Stating it again
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Ideal
</div>
<div class="mintbodydiv">
An ideal in \(R\) is a subset \(I \subseteq R\) such that
<ol>
	<li>\(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well.</li>
	<li>If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication.</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
We said that usually we refer to this definition as the “Two sided ideal” since other variants can exists (left and right ideals).
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(\{I_{\alpha}\}\) is a collection of ideals, then \(I = \bigcap_{\alpha} I_{\alpha}\) is an ideal. 
</div>
<p><br />
As a consequence of this, we have the following defintion
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Proposition
</div>
<div class="mintbodydiv">
Given a subset \(S \subseteq R\) ring, define 
$$
\begin{align*}
(S) = \bigcap_{\text{all ideas such that }S \subseteq I} I
\end{align*}
$$
\((S)\) is an Ideal in \(R\) generated by \(S\). It is in fact the smallest Ideal containing the subset \(S\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
It is the smallest, since by definition for any \(I \subseteq R\), \(S\) is contained in \(I\), so \((S)\) is also in \(I\). This is a nice formal definition but it’s hard to use. An explicit description of the Ideal is as follows
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Proposition
</div>
<div class="mintbodydiv">
If \(R\) is a ring with 1 and \(S \subseteq R\) then,
$$
\begin{align*}
(S) = \{0\} \cup \{a_1s_2b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\}
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
Let \(T = \{a_1s_2b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\}\). Then we need to show</p>
<ol>
	<li>Step (1) is showing that \(T\) is an Ideal where \(S \subseteq T\)</li>
<li>Step (2) is if any \(I \subseteq R\) is any idea such that \(S \subseteq I\), then \(T \subseteq I\) </li>
</ol>
<p>(1) and (2) Together imply that \(T = (S)\). <br />
[TODO: Write full proof]
<br />
<br />
Special Case: If \(R\) is a commutative ring, then</p>
<div>
$$
\begin{align*}
(S) = \{0\} \cup \{a_1s_2 + ... + a_ks_k \ | \ k \geq 1, s_1,...s_k \in S, a_i \in R\}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Principle Ideal</b></h4>
<p>Another special case is the Principle Ideal.
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A Principle Ideal \(I\) is an ideal such that \(I = (r)\) for a single \(r = R\) so
$$
\begin{align*}
(r) = \{a_1rb_1 + ... + a_krb_k \ | \  a_i,b_i \in R\}
\end{align*}
$$
</div>
<p><br />
Note that if \(R\) is a commutative ring with identity then</p>
<div>
$$
\begin{align*}
(r) &amp;= \{a_1r + ... + a_kr \ | \  a_1,...a_k \in R\} \\
    &amp;= \{(a_1 + ... + a_k)r \ | \  a_1,...a_k \in R\} \\
	&amp;= \{ ar \ | \ a \in R \}.
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->

<p>Example: If \(K\) is a field. The only ideals are \(\{0\}\) and \(K\). \(\{0\}\) is generated by \((0)\) and \(K\) is generated by \((1)\).</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we introduced the notion of an ideal in a ring. Stating it again Definition: Ideal An ideal in \(R\) is a subset \(I \subseteq R\) such that \(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well. If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication. We said that usually we refer to this definition as the “Two sided ideal” since other variants can exists (left and right ideals). Proposition If \(\{I_{\alpha}\}\) is a collection of ideals, then \(I = \bigcap_{\alpha} I_{\alpha}\) is an ideal. As a consequence of this, we have the following defintion Proposition Given a subset \(S \subseteq R\) ring, define $$ \begin{align*} (S) = \bigcap_{\text{all ideas such that }S \subseteq I} I \end{align*} $$ \((S)\) is an Ideal in \(R\) generated by \(S\). It is in fact the smallest Ideal containing the subset \(S\). It is the smallest, since by definition for any \(I \subseteq R\), \(S\) is contained in \(I\), so \((S)\) is also in \(I\). This is a nice formal definition but it’s hard to use. An explicit description of the Ideal is as follows Proposition If \(R\) is a ring with 1 and \(S \subseteq R\) then, $$ \begin{align*} (S) = \{0\} \cup \{a_1s_2b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\} \end{align*} $$ Proof Let \(T = \{a_1s_2b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\}\). Then we need to show Step (1) is showing that \(T\) is an Ideal where \(S \subseteq T\) Step (2) is if any \(I \subseteq R\) is any idea such that \(S \subseteq I\), then \(T \subseteq I\) (1) and (2) Together imply that \(T = (S)\). [TODO: Write full proof] Special Case: If \(R\) is a commutative ring, then $$ \begin{align*} (S) = \{0\} \cup \{a_1s_2 + ... + a_ks_k \ | \ k \geq 1, s_1,...s_k \in S, a_i \in R\} \end{align*} $$ Principle Ideal Another special case is the Principle Ideal. Definition A Principle Ideal \(I\) is an ideal such that \(I = (r)\) for a single \(r = R\) so $$ \begin{align*} (r) = \{a_1rb_1 + ... + a_krb_k \ | \ a_i,b_i \in R\} \end{align*} $$ Note that if \(R\) is a commutative ring with identity then $$ \begin{align*} (r) &amp;= \{a_1r + ... + a_kr \ | \ a_1,...a_k \in R\} \\ &amp;= \{(a_1 + ... + a_k)r \ | \ a_1,...a_k \in R\} \\ &amp;= \{ ar \ | \ a \in R \}. \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 34: Homomorphisms of Rings and the Substitution Principle</title><link href="http://localhost:4000/jekyll/update/2025/02/26/math417-34-homomorphisms-rings.html" rel="alternate" type="text/html" title="Lecture 34: Homomorphisms of Rings and the Substitution Principle" /><published>2025-02-26T00:01:36-08:00</published><updated>2025-02-26T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/26/math417-34-homomorphisms-rings</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/26/math417-34-homomorphisms-rings.html"><![CDATA[<p>Last time we introduced polynomial rings. Given a commutative ring with 1, then \(R[x]\) is a commutative polynomial ring with 1 and coefficients in \(R\). Today, we will introduce a few more definitions
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Homomorphisms of Rings
</div>
<div class="mintbodydiv">
Let \(R\) and \(S\) be rings. A homomorphism \(\varphi: R \rightarrow S\) is a function such that
<ol>
	<li>\(\varphi: (R_1, +) \rightarrow (S, +)\) is a homomorphism of groups (ie: \(\varphi(a + b) = \varphi(a) + \varphi(b)\)</li>
	<li>\(\varphi(a,b) = \varphi(a)\varphi(b)\) for all \(a,b \in R\)</li>
</ol>
</div>
<!--------------------------------------------------------------------------->
<p><br />
Remark: If \(R\) and \(S\) have a multiplication identity, \(\varphi\) can fail to take 1 to 1. So \(\varphi(1)\) could be something other than 1. Unlike for zero, where zero needs to go zero. This is because rings are required to have an additive inverses and because we have additive inverses, then zero will be mapped to zero. But we don’t necessarily have multiplicative inverses for all elements (that’s a field). So 1 won’t necessarily get mapped to 1. Based on this define
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Homomorphisms of Rings
</div>
<div class="mintbodydiv">
Let \(R\) and \(S\) be rings. A unital homomorphism is a ring homomorphism such that also \(\varphi(1_R) = 1_S\)
</div>
<p><br /></p>
<h4><b>Example 1</b></h4>
<p>Let \(\pi: \mathbf{Z} \rightarrow \mathbf{Z}_n\) where \(\pi(a) = [a]_n\). Then \(\pi\) is a unital homomorphism of rings.
<br />
<br /></p>
<h4><b>Example 2</b></h4>
<p>If \(R\) is any ring with 1. Define \(\varphi \mathbf{Z} \rightarrow R\) by \(\varphi(n) = n1_R\). \(\varphi\) is a unital ring homomorphism. For example</p>
<div>
$$
\begin{align*}
\varphi(2) &amp;= 1_R + 1_R \\
\varphi(3) &amp;= 1_R + 1_R + 1_R \\
\varphi(2)\varphi(3) &amp;= (1_R + 1_R)(1_R + 1_R + 1_R) = \varphi(6).
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Isomorphism of Rings</b></h4>
<p>More simple definitions:
<br /></p>
<div class="mintheaderdiv">
Definition: Isomorphism of Rings
</div>
<div class="mintbodydiv">
Let \(R\) and \(S\) be rings. An Isomorphism of rings is a homomorphism of rings which is also a bijection.
</div>
<p><br />
Since an isomorphism is a bijection, this means that we have an inverse function \(\varphi^{-1}: S \rightarrow R\) that is also an isomorphism of rings
<!------------------------------------------------------------------------->
<br /></p>
<div class="mintheaderdiv">
Definition: Automorphism of Rings
</div>
<div class="mintbodydiv">
Let \(R\) be a ring. An Automorphism \(\varphi: R \rightarrow R\) is an isomorphism of the ring to itself.
</div>
<p><br />
<!------------------------------------------------------------------------->
For example, take the set of matrices such that</p>
<div>
$$
\begin{align*}
S = \big\{ \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix} \big\} 
\subseteq \text{Mat}_{2\times 2}(\mathbf{R})
\end{align*}
$$
</div>
<p>with \(a, b \in \mathbf{R}\). \(S\) is a subring. We have an isomorphism of rings between \(S\) and the complex numbers as follows</p>
<div>
$$
\begin{align*}
\varphi: \mathbf{C} &amp;\rightarrow S \\
         a + bi &amp;\rightarrow \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix}
\end{align*}
$$
</div>
<p>Another example is \(\varphi: \mathbf{C} \rightarrow \mathbf{C}\) where \(\varphi(z) = \bar{z}\). So \(\varphi(a+bi) = a - bi\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Substitution Principle</b></h4>
<p>The substitution principle is a method for constructing any ring homomorphism where the domain is a polynomial ring
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition: Substitution Principle
</div>
<div class="peachbodydiv">
Let \(R\) and \(S\) be commutative rings with identity. Given that
<ol> 
	<li>\(\varphi: R \rightarrow S\) is a unital homomorphism</li>
	<li>\(c \in S\)</li>
</ol>
Then there exists a unique ring homomorphism
$$
\begin{align*}
\varphi_c: R[x] \rightarrow S
\end{align*}
$$
such that
<ol> 
	<li>\(\varphi_c(r) = \varphi(r)\)</li>
	<li>\(\varphi_c(x) = c\)</li>
</ol>
Furthermore, if \(f = \sum_{k=0}^n a_kx^k \in R[x], a_k \in R\). Then
$$
\begin{align*}
\varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k
\end{align*}
$$
</div>
<p><br />
<!---------------------------------------------------------------------->
This means given some unital homomorphism between the two rings. Then</p>
<ul>
<li>If we know how the homomorphism act on constant polynomials (so these are the elements in \(R\) which will be the constant polynomials in \(R[x]\)</li> 
<li>And if we know how it acts on \(x\) (we're sending it to \(c\))</li>
</ul>
<p>Then we will get this new homomorphism \(\varphi_c\) and we will know how it acts on all the other polynomials using \(\varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k\).
<br />
<br />
But now you might ask that it’s kind of like evaluating the polynomial at \(c\)?? This is true for a special case when \(\phi\) is the identity function. So when \(R = S\) and \(\phi:R \rightarrow R\) is just the identity. Then, given any \(c\), we will see that</p>
<div>
$$
\begin{align*}
\phi_c(f) = ev_c(f) = \sum_{k=0}^n a_k c^k \in R \quad \text{ev for evaluation}
\end{align*}
$$
</div>
<p>which is basically like plugging in \(c\) for \(x\) so \(f(c)\). However, this \(ev_c: R[x] \rightarrow R\) is actually a ring homomorphism. This ring homomorphism has the following properties.</p>
<ol>
	<li>\(ev_c(r) = r\) if \(r \in R \subseteq R(x)\)</li>
	<li>\(ev_(f+g) = ev_c(f) + ev_c(g)\). This is equivalent to us doing \((f+g)(c) = f(c)+f(g)\)</li>
	<li>\(ev_c(fg) = ev_c(f)ev_c(g)\). This is equivalent to us doing \((fg)(c) = f(c)g(c)\)</li>
</ol>
<p>So again, if we have a unital homomorphism and we have those two conditions where we know what it does to \(c\) and we know what it does to constant polynomials. Then that formula must work. It’s the only way. But then we need to check that this formula is actually a ring homomorphism (Excercise)
<br />
<br />
Observation: \(R\) is a commutative ring with 1. Any polynomial \(f \in R[x]\) gives a function</p>
<div>
$$
\begin{align*}
func_c: R &amp;\rightarrow R \\
c &amp;\rightarrow ev_c(f) = "f(c)"
\end{align*}
$$
</div>
<p>Warning: We can have \(f \neq g\) but \(func_f = func_g\)!.
<br />
Example 1: Let \(R = \mathbf{Z}_p\) where \(p\) is prime. Then</p>
<div>
$$
\begin{align*}
f &amp;= x \in \mathbf{Z}_p[x] \implies func_f = id: \mathbf{Z}_p \rightarrow \mathbf{Z}_p \\
g &amp;= x^p \in \mathbf{R}_p[x] \implies func_g(c) = c^p: \mathbf{Z}_p \rightarrow \mathbf{Z}_p 
\end{align*}
$$
</div>
<p>These are two different polynomials but they are the same function because due to Fermat’s little theorem. \(c^p \equiv c (\bmod p)\) so \(c^p = c\) in \(\mathbf{Z}_p\).
<br />
<br />
Example 2: Let \(h = g-f \in \mathbf{Z}_p\) so \(h = x^p - x\) and \(func_h = 0\) for any \(x \in \mathbf{Z}_p\). So any element in \(\mathbf{Z}_p\) is a root of \(h(x)\) and \(h(x)\) has at least \(p\) roots in \(\mathbf{Z}_p\). In otherwords, we have a polynomial where any element we input, we get zero and so we can’t distinguish it as a function from the zero function.</p>
<ul>
<li>So as a polynomial, $$h(x) = x - x^p$$ is not the zero polynomial. Its coefficients are not all zero. </li>
<li>But as a function, it is the zero function. It evalutes to zero for any input in \(\mathbf{Z}_p\)</li>
</ul>
<p>This happens because we’re in \(\mathbf{Z}_p\). If we’re in an inifinite field, then when \(f \neq g\), we will always get \(func_g \neq func_f\). Also in general, polynomials give us functions but they are not exactly functions!
<br />
<br />
Side study note: A polynomial belongs to the ring \(R[x]\). A function is a mapping from \(R\) to \(R\). It assigns to each input \(r\) in \(R\), another value \(f(r)\) in \(R\). The function lives in the set of functions from \(R\) to \(R\). If \(R\) is finite, two different polynomials in \(R[x]\) may define the same function! So \(x^2\) and \(x\), will define the same function from \(\mathbf{Z}_2 \rightarrow \mathbf{Z}_2\). If \(R\) was an infinite field, then each distinct polynmoial will give a distinct function.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Ideals</b></h4>
<p>So far, we’ve seen groups, subgroups and normal subgroups. Normal subgroups were special since they show up as kernels of homomorphism and we can form quotient groups using them. 
<br />
<br />
Similarly, we have rings and subrings. And we also have ideals. Ideals show up as kernels of ring homomorphisms similar to nomral groups. And we can also form quotient rings using them. 
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Ideal
</div>
<div class="mintbodydiv">
An ideal in \(R\) is a subset \(I \subseteq R\) such that
<ol>
	<li>\(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well.</li>
	<li>If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication.</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
So it’s not just a subring. It’s more special because of the closure under the product. Warning: This definition is also reffered as a “Two Sided Ideal”. Because we do have another variant where we only require half the second condition (left and right ideals).
<br />
<br />
Example: In any ring \(R\), \(\{0\}\) and \(R\) are both ideas in \(R\). \(\{0\}\) is usually called the trivial ideal. \(R\) is called the unit ideal.
<br />
<br />
Observation: If \(R\) is a ring with 1 and \(I \subseteq R\) is an ideal. Then, if \(1 \in I\), then \(I = R\). Why? The ideal must be closed under multiplication so if the ideal contains 1, then it contains every element in \(R\). 
<br />
<br />
Observation: If \(I\) contains a unit \(a^x\), then \(I = R\). This is because if \(a \in I\), then \(a^{-1}a = 1 \in I\). so \(1 \in I\) and therefore, every element is in \(I\). So \(I = R\). 
<br />
<br />
Example: If \(K\) is a field, then we have exactly two ideals \(K\) and \(\{0\}\). Because fields have only units in them. 
<br />
<br />
Example: If \(K\) is a field and \(n \geq 1\), then \(S = \text{Mat}_{n \times n}(K)\) has two ideals: \(\{0\}\) and \(S\) (not trivial but not hard to prove).
<br />
<br />
Example: If \(R = \mathbf{Z}\), the ideals in \mathbf{Z}\(all have form\){\mathbf{Z}_n = {an \ | \ a \in \mathbf{Z}}$$. 
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
If \(\varphi: R \rightarrow S\) is a ring homomorphism. Its kernel is 
$$
\begin{align*}
\ker(\varphi) = \{r \in R \ | \ \varphi(r) = 0\}
\end{align*}
$$
</div>
<p><br />
Fact: \(\ker(\varphi)\) is an ideal in \(R\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we introduced polynomial rings. Given a commutative ring with 1, then \(R[x]\) is a commutative polynomial ring with 1 and coefficients in \(R\). Today, we will introduce a few more definitions Definition: Homomorphisms of Rings Let \(R\) and \(S\) be rings. A homomorphism \(\varphi: R \rightarrow S\) is a function such that \(\varphi: (R_1, +) \rightarrow (S, +)\) is a homomorphism of groups (ie: \(\varphi(a + b) = \varphi(a) + \varphi(b)\) \(\varphi(a,b) = \varphi(a)\varphi(b)\) for all \(a,b \in R\) Remark: If \(R\) and \(S\) have a multiplication identity, \(\varphi\) can fail to take 1 to 1. So \(\varphi(1)\) could be something other than 1. Unlike for zero, where zero needs to go zero. This is because rings are required to have an additive inverses and because we have additive inverses, then zero will be mapped to zero. But we don’t necessarily have multiplicative inverses for all elements (that’s a field). So 1 won’t necessarily get mapped to 1. Based on this define Definition: Homomorphisms of Rings Let \(R\) and \(S\) be rings. A unital homomorphism is a ring homomorphism such that also \(\varphi(1_R) = 1_S\) Example 1 Let \(\pi: \mathbf{Z} \rightarrow \mathbf{Z}_n\) where \(\pi(a) = [a]_n\). Then \(\pi\) is a unital homomorphism of rings. Example 2 If \(R\) is any ring with 1. Define \(\varphi \mathbf{Z} \rightarrow R\) by \(\varphi(n) = n1_R\). \(\varphi\) is a unital ring homomorphism. For example $$ \begin{align*} \varphi(2) &amp;= 1_R + 1_R \\ \varphi(3) &amp;= 1_R + 1_R + 1_R \\ \varphi(2)\varphi(3) &amp;= (1_R + 1_R)(1_R + 1_R + 1_R) = \varphi(6). \end{align*} $$ Isomorphism of Rings More simple definitions: Definition: Isomorphism of Rings Let \(R\) and \(S\) be rings. An Isomorphism of rings is a homomorphism of rings which is also a bijection. Since an isomorphism is a bijection, this means that we have an inverse function \(\varphi^{-1}: S \rightarrow R\) that is also an isomorphism of rings Definition: Automorphism of Rings Let \(R\) be a ring. An Automorphism \(\varphi: R \rightarrow R\) is an isomorphism of the ring to itself. For example, take the set of matrices such that $$ \begin{align*} S = \big\{ \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix} \big\} \subseteq \text{Mat}_{2\times 2}(\mathbf{R}) \end{align*} $$ with \(a, b \in \mathbf{R}\). \(S\) is a subring. We have an isomorphism of rings between \(S\) and the complex numbers as follows $$ \begin{align*} \varphi: \mathbf{C} &amp;\rightarrow S \\ a + bi &amp;\rightarrow \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix} \end{align*} $$ Another example is \(\varphi: \mathbf{C} \rightarrow \mathbf{C}\) where \(\varphi(z) = \bar{z}\). So \(\varphi(a+bi) = a - bi\). Substitution Principle The substitution principle is a method for constructing any ring homomorphism where the domain is a polynomial ring Proposition: Substitution Principle Let \(R\) and \(S\) be commutative rings with identity. Given that \(\varphi: R \rightarrow S\) is a unital homomorphism \(c \in S\) Then there exists a unique ring homomorphism $$ \begin{align*} \varphi_c: R[x] \rightarrow S \end{align*} $$ such that \(\varphi_c(r) = \varphi(r)\) \(\varphi_c(x) = c\) Furthermore, if \(f = \sum_{k=0}^n a_kx^k \in R[x], a_k \in R\). Then $$ \begin{align*} \varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k \end{align*} $$ This means given some unital homomorphism between the two rings. Then If we know how the homomorphism act on constant polynomials (so these are the elements in \(R\) which will be the constant polynomials in \(R[x]\) And if we know how it acts on \(x\) (we're sending it to \(c\)) Then we will get this new homomorphism \(\varphi_c\) and we will know how it acts on all the other polynomials using \(\varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k\). But now you might ask that it’s kind of like evaluating the polynomial at \(c\)?? This is true for a special case when \(\phi\) is the identity function. So when \(R = S\) and \(\phi:R \rightarrow R\) is just the identity. Then, given any \(c\), we will see that $$ \begin{align*} \phi_c(f) = ev_c(f) = \sum_{k=0}^n a_k c^k \in R \quad \text{ev for evaluation} \end{align*} $$ which is basically like plugging in \(c\) for \(x\) so \(f(c)\). However, this \(ev_c: R[x] \rightarrow R\) is actually a ring homomorphism. This ring homomorphism has the following properties. \(ev_c(r) = r\) if \(r \in R \subseteq R(x)\) \(ev_(f+g) = ev_c(f) + ev_c(g)\). This is equivalent to us doing \((f+g)(c) = f(c)+f(g)\) \(ev_c(fg) = ev_c(f)ev_c(g)\). This is equivalent to us doing \((fg)(c) = f(c)g(c)\) So again, if we have a unital homomorphism and we have those two conditions where we know what it does to \(c\) and we know what it does to constant polynomials. Then that formula must work. It’s the only way. But then we need to check that this formula is actually a ring homomorphism (Excercise) Observation: \(R\) is a commutative ring with 1. Any polynomial \(f \in R[x]\) gives a function $$ \begin{align*} func_c: R &amp;\rightarrow R \\ c &amp;\rightarrow ev_c(f) = "f(c)" \end{align*} $$ Warning: We can have \(f \neq g\) but \(func_f = func_g\)!. Example 1: Let \(R = \mathbf{Z}_p\) where \(p\) is prime. Then $$ \begin{align*} f &amp;= x \in \mathbf{Z}_p[x] \implies func_f = id: \mathbf{Z}_p \rightarrow \mathbf{Z}_p \\ g &amp;= x^p \in \mathbf{R}_p[x] \implies func_g(c) = c^p: \mathbf{Z}_p \rightarrow \mathbf{Z}_p \end{align*} $$ These are two different polynomials but they are the same function because due to Fermat’s little theorem. \(c^p \equiv c (\bmod p)\) so \(c^p = c\) in \(\mathbf{Z}_p\). Example 2: Let \(h = g-f \in \mathbf{Z}_p\) so \(h = x^p - x\) and \(func_h = 0\) for any \(x \in \mathbf{Z}_p\). So any element in \(\mathbf{Z}_p\) is a root of \(h(x)\) and \(h(x)\) has at least \(p\) roots in \(\mathbf{Z}_p\). In otherwords, we have a polynomial where any element we input, we get zero and so we can’t distinguish it as a function from the zero function. So as a polynomial, $$h(x) = x - x^p$$ is not the zero polynomial. Its coefficients are not all zero. But as a function, it is the zero function. It evalutes to zero for any input in \(\mathbf{Z}_p\) This happens because we’re in \(\mathbf{Z}_p\). If we’re in an inifinite field, then when \(f \neq g\), we will always get \(func_g \neq func_f\). Also in general, polynomials give us functions but they are not exactly functions! Side study note: A polynomial belongs to the ring \(R[x]\). A function is a mapping from \(R\) to \(R\). It assigns to each input \(r\) in \(R\), another value \(f(r)\) in \(R\). The function lives in the set of functions from \(R\) to \(R\). If \(R\) is finite, two different polynomials in \(R[x]\) may define the same function! So \(x^2\) and \(x\), will define the same function from \(\mathbf{Z}_2 \rightarrow \mathbf{Z}_2\). If \(R\) was an infinite field, then each distinct polynmoial will give a distinct function. Ideals So far, we’ve seen groups, subgroups and normal subgroups. Normal subgroups were special since they show up as kernels of homomorphism and we can form quotient groups using them. Similarly, we have rings and subrings. And we also have ideals. Ideals show up as kernels of ring homomorphisms similar to nomral groups. And we can also form quotient rings using them. Definition: Ideal An ideal in \(R\) is a subset \(I \subseteq R\) such that \(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well. If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication. So it’s not just a subring. It’s more special because of the closure under the product. Warning: This definition is also reffered as a “Two Sided Ideal”. Because we do have another variant where we only require half the second condition (left and right ideals). Example: In any ring \(R\), \(\{0\}\) and \(R\) are both ideas in \(R\). \(\{0\}\) is usually called the trivial ideal. \(R\) is called the unit ideal. Observation: If \(R\) is a ring with 1 and \(I \subseteq R\) is an ideal. Then, if \(1 \in I\), then \(I = R\). Why? The ideal must be closed under multiplication so if the ideal contains 1, then it contains every element in \(R\). Observation: If \(I\) contains a unit \(a^x\), then \(I = R\). This is because if \(a \in I\), then \(a^{-1}a = 1 \in I\). so \(1 \in I\) and therefore, every element is in \(I\). So \(I = R\). Example: If \(K\) is a field, then we have exactly two ideals \(K\) and \(\{0\}\). Because fields have only units in them. Example: If \(K\) is a field and \(n \geq 1\), then \(S = \text{Mat}_{n \times n}(K)\) has two ideals: \(\{0\}\) and \(S\) (not trivial but not hard to prove). Example: If \(R = \mathbf{Z}\), the ideals in \mathbf{Z}\(all have form\){\mathbf{Z}_n = {an \ | \ a \in \mathbf{Z}}$$. Definition If \(\varphi: R \rightarrow S\) is a ring homomorphism. Its kernel is $$ \begin{align*} \ker(\varphi) = \{r \in R \ | \ \varphi(r) = 0\} \end{align*} $$ Fact: \(\ker(\varphi)\) is an ideal in \(R\). References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 33: Polynomial Rings</title><link href="http://localhost:4000/jekyll/update/2025/02/25/math417-33-polynomial-rings.html" rel="alternate" type="text/html" title="Lecture 33: Polynomial Rings" /><published>2025-02-25T00:01:36-08:00</published><updated>2025-02-25T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/25/math417-33-polynomial-rings</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/25/math417-33-polynomial-rings.html"><![CDATA[<p>We’ve introduced rings last lecture and we said that rings can be commutative, contain a multiplicative identity or can also be a field. We saw the subset that includes elements with multiplicative inverses that’s also a group. This is the Units group or \(R^{\times}\) which contains an element in \(R\) such that it has a multiplication inverse. We also saw an example of a ring which is</p>
<div>
$$
\begin{align*}
R[i] = \{\text{Formal expressions } a+bi, a, b \in R \}, \quad i^2 = -1
\end{align*}
$$
</div>

<!----------------------------------------------------------------------------->
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Given a commutative ring with 1. Define \(R[x]\) to be the set of "Formal Expression"
$$
\begin{align*}
f = \sum_{k = 0}^n a_kx^k = a_0 + a_1x + ... + a_nx^n \quad a_0,...a_n \in R, \quad x \text{ new symbol}
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------->
<p><br />
Warning: We want to think of \(3 + 4x + 17x^2 + 0x^3 = 3 + 4x + 17x^2\). So if terms has a zero, then we can take it out. 
<br />
We really identify an \(f \in R[x]\) with an infinite sequence \((a_k)_{k \geq 0} = (a_0,a_1,a_2,...)\) where each \(a_i \in R\) and because this polynomial is finite and we want an infinite sequence, we’ll say that there exists an \(n\) such that \(a_i = 0\) for all \(i &gt; n\). So after some point \(n\), all the terms will be zero after.
<br />
<br />
The claim is that \(R[x]\) is a commutative ring with 1. So we need to define multiplication and addition. Therefore, let \(f = \sum_i a_ix^i, g = \sum_j b_jx^j\) where \(a_i,b_j \in R\). Then</p>
<div>
$$
\begin{align*}
f + g := \sum_k (a_k + b_k)x^k \\
fg := \sum_k (\sum_{i = 0}^k a_ib_{k-i})x^k
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------->
<p>The identity element is an example of a constant polynomial. A constant polynomial is \(f \in R[i]\) where \(f = \sum_ka_k x^k\) such that \(a_k = 0\) for all \(k \geq 1\) So \(f = a_0\).
<br />
<br />
Let \(C \subseteq R[i]\) be the subset of constant. Then \(C\) is a subring. Define a bijection from</p>
<div>
$$
\begin{align*}
\lambda: R &amp;\rightarrow C \\
         a &amp;\rightarrow \text{constant polynomial $a$}
\end{align*}
$$
</div>
<p>This bijection is in fact an isomorphism of rings between \(R\) and \(C\). 
<br />
<br />
Convention: We identify an element \(a\) in the original ring \(R\) with the corresponding constant polynomial in \(R[x]\). So we can think of \(R\) as a subring of \(R[x]\). 
<br />
<br />
Remark: In a similar way, we can form \(R[x,y]\) or \(R[x_1,...,x_n]\) (polynomial ring of several variables). In fact, \(R[x,y] = (R[x])[y]\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------></p>
<h4><b>The Degree of a Polynomial</b></h4>
<p>We now want to focus on a special case of a polynomial ring where the coefficient ring is actually a field. So let \(R = K\) be a field and let</p>
<div>
$$
\begin{align*}
f = \sum_k a_kx^k \in K[x], a_k \in K
\end{align*}
$$
</div>
<p>Then, the degree of \(f\), \(\deg(f)\) is the largest integer \(n\) such that \(a_n \neq 0\). For example if</p>
<div>
$$
\begin{align*}
f = 7x^3 + \frac{1}{2}x - 17
\end{align*}
$$
</div>
<p>Then, \(\deg(f) = 3\). Warning: if</p>
<div>
$$
\begin{align*}
f = \sum_{k=0}^{n} a_kx^k \in K[x]
\end{align*}
$$
</div>
<p>Then, we only know that \(\deg(f) \leq n\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Degree of the Zero Polynomial</b></h4>
<p>If \(f\) is a constant polynomial, what is the degree of \(f\)? We defined the degree as the largest \(n\) such that \(a_n \neq 0\). In a constant polynomial, all the terms are zero except for \(a_0\). If the constant is non-zero, then the degree is zero. But if the polynomial is zero itself so \(f = 0\), then now all the coefficients \(a_i\)’s are zero so in this case the degree is undefined but we’re going to let the degree in this case be \(-\infty\). So deg\((f)=0\) if and only if \(f = 0\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Degree of a Polynomial when the Coefficient Ring is a Field</b></h4>
<p>Next, we’re going to prove a proposition where we will see why we needed the coefficient ring to be a field.
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(K\) is a field and \(f,g \in K[x]\). Then
<ol> 
	<li>\(\text{deg}(fg) = \text{deg}(f) + \text{deg}(g)\)</li>
	<li>\(\text{deg}(f+g) \leq \max\{\text{deg}(f), \text{deg}(g)\}\)</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
Convention:</p>
<ul>
	<li>\(-\infty + \text{ anything } = -\infty\)</li>
	<li>\(-\infty \leq \text{ anything }\)</li>
</ul>
<p><b>Proof of (1)</b>
<br />
Let \(f = a_0 + a_1x + ... + a_mx^m\) and \(g = b_0 + b_1x + ... + b_nx^n\). Then</p>
<div>
$$
\begin{align*}
fg &amp;= a_0b_0 + (a_1b_0+a_0b_1)x + ... + (a_ma_n)x^{m+n} \\
\end{align*}
$$
</div>
<p>and if \(a_m \neq 0\) and \(b_n \neq 0\), then \(a_mb_n \neq 0\). This is because \(K\) is a field so if \(a,b \in K\) and \(a \neq 0\), \(b \neq 0\), then \(ab \neq 0\). (because \(K^{\times} = K\{0\}\)). 
<br />
<br /></p>
<hr />

<p><br />
Non-example: Take \(R = \mathbf{Z}_4 = \{0,1,2,3\}\). \(\mathbf{Z}_4\) is not a field since element 2 doesn’t have an inverse in \(\mathbf{Z}_4\). Note that only elements 1 and 3 have inverses. (\(3*3 = 1\) and \(1*1 = 1\) in \(\mathbf{Z}_4\). Now consider \(\mathbf{Z}_4[x]\) and let \(f = 1 + 2x \in \mathbf{Z}_4[x]\). Observe that</p>
<div>
$$
\begin{align*}
ff = f^2 = (1 + 2x)^2 = 1 + 2x + 2x + (2x)(2x) = 1 + 4x + 4x^2 = 1
\end{align*}
$$
</div>
<p>The proposition says the degree of \(fg\) should be \(1+1=2\) but here so the degree of \(f^2\) is zero. So we’re failing here because we’re not working in a field. Note that this shows that \(f \in \mathbf{Z_4}[x]^{\times}\) is a unit and it’s multiplicative inverse is itself.  Here is a consequence of the proposition:
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Corollary
</div>
<div class="peachbodydiv">
If \(K\) is a field, then the units in the polynomial ring \(K[x]\) are exactly the elements of \(K^{\times}\). In other words, \(K[x]^{\times} = K^{\times}\).
</div>
<!------------------------------------------------------------------------>
<p><br />
So if \(K\) is a field, then all the units in the ring \(K[x]\) will be the nonzero constant polynomials. Those units are called \(K[x]^{\times}\). They form the multiplicative group of the ring \(K[x]\). Reminder: \(K\) is a field so every element except zero has a multiplicative inverse. \(K^{\times}\) is not a field, it is a group \((K^{\times},\cdot)\) that excludes zero so every single element has a multiplicative inverse. 
<br />
<br />
<b>Proof</b>
<br />
One direction: If we have a constant non-zero polynomial and we’re working in a field, then it has an inverse that’s also a constant polynomial.
<br />
Other direction: If we have two polynomials \(f,g\) such that \(fg = 1\), then computing the degree of both sides we see that</p>
<div>
$$
\begin{align*}
\deg(f,g) &amp;= \deg(1) \\
\deg(f) + \deg(g) &amp;= 0 \\
\deg(f) &amp;= -\deg(g).
\end{align*}
$$
</div>
<p>But the only solution is that \(\deg(f) = \deg(g) = 0\). So the units always have to be constant.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Division Algorithm for \(K[x]\)</b></h4>
<!------------------------------------------------------------------------>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(K\) be a field. Let \(p, d \in K[x]\) where \(\deg(d) \geq 0\) so \(d\) is not a constant polynomial. Then there exists unique \(q,r \in K[x]\) such that
<ol>
	<li>\(p = qd + r\)</li>
	<li>\(\deg(r) &lt; \deg(d)\)</li>
</ol>
</div>
<!------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
Let</p>
<div>
$$
\begin{align*}
p &amp;= \sum_i a_ix^i \text{ with } \deg(p)=m, \ \text{ so } \ a_m \neq 0, a_k &gt; 0 \text{ if } k &gt; m\\
d &amp;= \sum_j b_ix^j, \text{ with } \deg(d)=n \geq 0, \ \text{ so } \ b_n \neq 0, b_k &gt; 0 \text{ if } k &gt; n
\end{align*}
$$
</div>
<p>First, show that if \(m &gt; n\) so the degree of \(p\) is greater than the degree of \(d\), then there exists a monomial \(cx^k\) such that</p>
<ol>
	<li>\(p = (cx^k)d + p'\)</li>
	<li>\(\deg(p') &lt; m = \deg(p)\)</li>
</ol>
<p>So we can in a sense subtract that extra monomial where its degree is less than \(m\). Now, re-write both polynomials such that highest term is in the front</p>
<div>
$$
\begin{align*}
p &amp;= a_mx^m + \text{ lower polynomial } \\
d &amp;= b_nx^n + \text{ lower polynomial }
\end{align*}
$$
</div>
<p>Let \(c = a_mb_n^{-1} = \frac{a_m}{b_n}\). This is fine becasuse \(b_n \neq 0\). Let \(k = m - n\). Then</p>
<div>
$$
\begin{align*}
p' &amp;= p - (cx^k)d = (a_mx^m + \text{ lower polynomial }) - (a_nb_n^{-1}x^{m-n})(b_nx^n + \text{ lower polynomial }) \\
&amp;= (a_mx^m - a_nx^m) \text{ lower polynomial }) \\
&amp;= \text{ lower polynomial }
\end{align*}
$$
</div>
<p>Therefore, \(\deg(p') &lt; m\). 
<br />
<br />
To prove the proposition, we use induction on \(m = \deg(p)\). <br />
If \(m &lt; n = \deg(d)\), let \(q = 0, r = p\) so that \(p = 0d + r\). Therefore, \(\deg(r) = \deg(p) &lt; n\). 
<br />
<br />
If \(m \geq n\), use induction. I can write \(p = (cx^k)d + p'\) where \(\deg(p') &lt; m\). By induction, there exists a \(q'\) such that \(q'd + r\) where \(\deg(r) &lt; n\) so</p>
<div>
$$
\begin{align*}
p &amp;= (cx^k + p')d + r.
\end{align*}
$$
</div>
<p>[TODO: This proof is unclear and a mess … ]
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’ve introduced rings last lecture and we said that rings can be commutative, contain a multiplicative identity or can also be a field. We saw the subset that includes elements with multiplicative inverses that’s also a group. This is the Units group or \(R^{\times}\) which contains an element in \(R\) such that it has a multiplication inverse. We also saw an example of a ring which is $$ \begin{align*} R[i] = \{\text{Formal expressions } a+bi, a, b \in R \}, \quad i^2 = -1 \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 32: Rings</title><link href="http://localhost:4000/jekyll/update/2025/02/24/math417-32-rings.html" rel="alternate" type="text/html" title="Lecture 32: Rings" /><published>2025-02-24T00:01:36-08:00</published><updated>2025-02-24T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/24/math417-32-rings</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/24/math417-32-rings.html"><![CDATA[<p>Let \(G\) be a group that acts on \(X\). Define
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A Ring: \(R, +, \cdot\) is a \(R\)-set with \(+, \cdot\) operations on \(R\) such that
<ul>
	<li>\(R, +\) is an abelian group with \(0\) as the identity and \(-a\) as the inverse of \(a \in R\).</li>
	<li>Multiplication is associative so \((ab)c = a(bc)\) for all \(a,b,c \in R\).</li>
	<li>Distributive Law: \((a + b)c = (ac) + (bc)\) and \((a(b + c) = (ab) + (ac)\).</li>
</ul>
</div>
<!----------------------------------------------------------------------------->
<p><br />
Warning: we don’t know if \(a + b \cdot c\) should be \((a + b) \cdot c\) or \(a + (b \cdot c)\). The convention is to use the second (operator precedence).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>More Terminology</b></h4>
<ul>
	<li><b>Ring with identity</b>: This means a rin with a multiplicative identity since a ring always has an additive identity but not necessarily a multiplicative identity. This is a ring \(R\) such that \(\exists 1 \in R\) so that \(1a = a1 = a\) for all \(a \in R\).</li>
	<!----->
	<li><b>Commutative Ring</b>: A ring \(R\) such that \(ab = ba\) for all \(a, b \in R\).</li>
	<!---->
	<li>If \(R\) has a multiplicative identity, then \(a \in R\) is a unit if there exists a \(b\) in \(R\) such that \(ab = 1 = ba\). We call \(b\) an inverse of \(a\) and write \(b = a^{-1}\). (So a unit is an element with a multiplicative inverse)</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Basic Facts in a Ring</b></h4>
<ol>
	<li>\(a0 = 0 = 0a\) for all \(a \in R\). To show this, use the distributive law to see that
		<div>
		$$
		\begin{align*}
		0a = (0 + 0)a = 0a + 0a. 
		\end{align*}
		$$
		</div>
	But \(R\) is abelian with respect to addition so we can cancel \(0a\) from both sides to see that \(0 = 0a\).</li>
	<!----->
	<li>If \(1 \in R\), then \(-a = (-1)a\). So this says the additive inverse is -1 multiplied by \(a\). To see why, observe that
		<div>
		$$
		\begin{align*}
		(1 + (-1))a &amp;= 1a + (-1)a \\
		     0a &amp;= 0.
		\end{align*}
		$$
		</div>
	  </li>
	  <!----->
	  <li>The multiplicative identity is unique if it exists.</li>
	  <!----->
	  <li>If \(1 \in R\) and \(a\) is unit in \(R\), then it has a unique inverse</li>
	  <!----->
	  <li>If \(1 \in R\), then define \((R^{\times},\cdot)\) as the set of units in \(R\) so \((R^{\times},\cdot) = \{\text{units }a \in R\}\). So this is set of the elements that have a multiplicative inverse. This set with the multiplication operation is a group. As a consequence, if \(a\) and \(b\) are units, then their product is in \(R^{\times}\). In fact, \((ab)^{-1} = b^{-1}a^{-1}\).</li>	  
</ol>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>The Zero Ring</b></h4>
<p>This is a ring \(R\) with element \(R = \{0\}\). \(0 + 0 = 0\) and \(0 \cdot 0 = 0\). This is a commutative ring with identity. The multiplicative identity is \(1 = 0\) in this ring. In fact, If \(R\) is a ring with 1, then \(1 = 0\) if and only if \(R = \{0\}\). Sometimes this ring is excluded from the definition of rings …
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Fields</b></h4>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A Field is a commutative ring with 1 such that every nonzero element is a unit and \(1 \neq 0\).
</div>
<p><br />
So we’re excluding the zero ring here.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
	<li>\(\mathbf{Z}\) with addition and multiplication is a commutative ring with identity.</li>
	<!----->
	<li>\(\mathbf{Z}2 = \{2n \ | \ n \in \mathbf{Z}\}\) is a commutative ring but it doesn't have the identity \(1 \not\in \mathbf{Z}2\).</li>
	<!---->
	<li>\(\mathbf{Q}, \mathbf{R}, \mathbf{C}\) are all fields.</li>
	<!---->
	<li>For \(n \geq 1\), \(\mathbf{Z}_n\) is a commutative ring with \(1 = [1]_n\). It is a field if and only if \(n\) is a prime.</li>
	<!---->
	<li>Let \(R\) be any ring. Then \(S = \text{Mat}_{n \times n}(R)\) the set of matrices with entries in \(R\) is 
	also a ring with matrix addition/multiplication.</li>
	    <ul>
	    <li>If \(1 \in R\) so \(R\) includes the multiplicative identity. Then, \(I \in S\).</li>
	    <li>If \(R\) is commutative. Then, \(S\) might not be commutative.</li>
	   </ul>
	<!---->
	<li>Rings of functions: if \(X\) is an arbitrary set and \(R\) is an arbitrary function, then \(F(X,R) = \{ \text{ all functions } \ | \ f: X \rightarrow R \ \}\) is a ring via a pointwise operation. What's a pointwise operation? Given two different functions \(f, g: X \rightarrow R\), then define 
		<div>
		$$
		\begin{align*}
		(f + g)(x) &amp;:= f(x) + g(x) \\
		(fg)(x) &amp;:= f(x)g(x) \quad \text{ for } x \in X
		\end{align*}
		$$
		</div>
	As long as the target is a ring this works. No restriction on \(X\).
	</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Subrings</b></h4>
<p>A subring \(S\) of ring \(R\) is a subset, which is a ring via operation inherited from \(R\). This implies that if \(a, b \in S\), then \(a + b, ab \in S\). To show \(S\) is a subring, we have the following proposition
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
\(R\) ring, a subset \(S \subseteq R\) is a subring if and only if
<ul>
	<li>\(0 \in S\) or \(S \neq \emptyset\).</li>
	<li>if \(a,b \in S\), then \(a + b, ab \in S\).</li>
	<li>If \(s \in S\) then \(-a \in S\).</li>
</ul>
</div>
<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
	<li>\(\mathbf{Z}\) is a ring and \(\mathbf{Z}2\) is a subring.</li>
	<!----->
	<li>\(\mathbf{Z} \subseteq \mathbf{Q} \subseteq \mathbf{R} \subseteq \mathbf{C}\) are subrings.</li>
	<!----->
	<li> \(R = \text{Mat}_{n \times n}(\mathbf{R})\) be the ring of 2 by 2 matrices with entries in \(\mathbf{R}\). Then, \(S = \big\{ \begin{pmatrix}a &amp; -b \\ b &amp; a \end{pmatrix} \big\}, a, b \in R\) is a subring. In fact, \(S \cong \mathbf{C}\) as rings</li>
</ul>
<p>Warning: we can have a subring \(S \subseteq R\) such that \(1_S \in S\) and \(1_R \in R\) but \(1_S \neq 1_S\).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Complex Numbers</b></h4>
<p>One way to describe the complex numbers is to say that the elements of \(\mathbf{C}\) are “formal expressions” \(a + bi\) where \(a, b \in \mathbf{R}\) and \(i\) is a new symbol. We add in the usual way and when we multiply, we use the identity \(i^2 = -1\). 
<br />
<br />
Another way to describe the complex numbers is to say that the elements of \(C\) are vectors \((a,b)\) in \(\mathbf{R}^2\) where we have a special notation for \(1 = (1, 0)\) and \(i = (0, 1)\). Now, define \(+\) by vector addition and define \(\cdot\) by</p>
<div>
$$
\begin{align*}
(a, b) \cdot (a', b') = (aa' - bb', ab' + ba')
\end{align*}
$$
</div>
<p>In terms of the older notation where \((a,b) = a+bi\), it is</p>
<div>
$$
\begin{align*}
(a + bi)(a' + b'i) = (aa' - bb') + (ab' + ba')i
\end{align*}
$$
</div>
<p>\(\mathbf{C}\) is a commutative ring with 1. In fact, \(\mathbf{C}\) is a field. Why? because we have straight forward formula to finding the inverse. First observe what happens when we multiply by the complex conjugate</p>
<div>
$$
\begin{align*}
(a + bi)(a' - b'i) = (a^2 + b^2) + 0i.
\end{align*}
$$
</div>
<p>Fact: if \(a, b \in \mathbf{R}\) and \((a,b) \neq (0,0)\), then \(a^2 + b^2 &gt; 0\). From this we get the inverse formula</p>
<div>
$$
\begin{align*}
\frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i = (a + bi)^{-1} 
\end{align*}
$$
</div>
<p>So every non-zero element has a multiplicative inverse and so it’s a field because we have this formula.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(R = \mathbf{Z}_3[i]\) is ring. This is the set of formal expressions \(a + bi\) where \(a, b \in \mathbf{Z}_3\). \(i\) is a symbol such that \(i^2 = [-1]_3 = -1\).
<br />
<br />
We can check very easily that \(R\) is a commutative ring with identity. Is \(R\) a field? It’s obvious but the answer is yes. So for any \(a, b \in \mathbf{Z}_3\) where \((a,b) \neq (0,0)\), then \(a^2 + b^2 \neq 0\). Why? observe that, \(0^2 = 0\), \(1^2 = 1\) but \(2^2 = 1\) in \(\mathbf{Z}_3\). So the only way to get \(a^2 + b^2 = 0\) is to have \(a = b = 0\). The formula for a multiplicative inverse is</p>
<div>
$$
\begin{align*}
(a + bi)^{-1} = \frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i 
\end{align*}
$$
</div>
<p>Therefore, \(\mathbf{Z}_3[i]\) is a field with 9 elements. Call this \(\mathbf{F}_9\).
<br />
<br />
What about \(\mathbf{Z}_5[i]\)? is it a field? No. Because \((2 + i)\) doesn’t have a multiplicative in \(\mathbf{Z}_5[i]\). To see this, observe that</p>
<div>
$$
\begin{align*}
(2 + i) \cdot (2 - i) = 2^2 + i^2 + 5 = 0 
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(G\) be a group that acts on \(X\). Define Definition A Ring: \(R, +, \cdot\) is a \(R\)-set with \(+, \cdot\) operations on \(R\) such that \(R, +\) is an abelian group with \(0\) as the identity and \(-a\) as the inverse of \(a \in R\). Multiplication is associative so \((ab)c = a(bc)\) for all \(a,b,c \in R\). Distributive Law: \((a + b)c = (ac) + (bc)\) and \((a(b + c) = (ab) + (ac)\). Warning: we don’t know if \(a + b \cdot c\) should be \((a + b) \cdot c\) or \(a + (b \cdot c)\). The convention is to use the second (operator precedence). More Terminology Ring with identity: This means a rin with a multiplicative identity since a ring always has an additive identity but not necessarily a multiplicative identity. This is a ring \(R\) such that \(\exists 1 \in R\) so that \(1a = a1 = a\) for all \(a \in R\). Commutative Ring: A ring \(R\) such that \(ab = ba\) for all \(a, b \in R\). If \(R\) has a multiplicative identity, then \(a \in R\) is a unit if there exists a \(b\) in \(R\) such that \(ab = 1 = ba\). We call \(b\) an inverse of \(a\) and write \(b = a^{-1}\). (So a unit is an element with a multiplicative inverse) Basic Facts in a Ring \(a0 = 0 = 0a\) for all \(a \in R\). To show this, use the distributive law to see that $$ \begin{align*} 0a = (0 + 0)a = 0a + 0a. \end{align*} $$ But \(R\) is abelian with respect to addition so we can cancel \(0a\) from both sides to see that \(0 = 0a\). If \(1 \in R\), then \(-a = (-1)a\). So this says the additive inverse is -1 multiplied by \(a\). To see why, observe that $$ \begin{align*} (1 + (-1))a &amp;= 1a + (-1)a \\ 0a &amp;= 0. \end{align*} $$ The multiplicative identity is unique if it exists. If \(1 \in R\) and \(a\) is unit in \(R\), then it has a unique inverse If \(1 \in R\), then define \((R^{\times},\cdot)\) as the set of units in \(R\) so \((R^{\times},\cdot) = \{\text{units }a \in R\}\). So this is set of the elements that have a multiplicative inverse. This set with the multiplication operation is a group. As a consequence, if \(a\) and \(b\) are units, then their product is in \(R^{\times}\). In fact, \((ab)^{-1} = b^{-1}a^{-1}\). The Zero Ring This is a ring \(R\) with element \(R = \{0\}\). \(0 + 0 = 0\) and \(0 \cdot 0 = 0\). This is a commutative ring with identity. The multiplicative identity is \(1 = 0\) in this ring. In fact, If \(R\) is a ring with 1, then \(1 = 0\) if and only if \(R = \{0\}\). Sometimes this ring is excluded from the definition of rings … Fields Definition A Field is a commutative ring with 1 such that every nonzero element is a unit and \(1 \neq 0\). So we’re excluding the zero ring here. Examples \(\mathbf{Z}\) with addition and multiplication is a commutative ring with identity. \(\mathbf{Z}2 = \{2n \ | \ n \in \mathbf{Z}\}\) is a commutative ring but it doesn't have the identity \(1 \not\in \mathbf{Z}2\). \(\mathbf{Q}, \mathbf{R}, \mathbf{C}\) are all fields. For \(n \geq 1\), \(\mathbf{Z}_n\) is a commutative ring with \(1 = [1]_n\). It is a field if and only if \(n\) is a prime. Let \(R\) be any ring. Then \(S = \text{Mat}_{n \times n}(R)\) the set of matrices with entries in \(R\) is also a ring with matrix addition/multiplication. If \(1 \in R\) so \(R\) includes the multiplicative identity. Then, \(I \in S\). If \(R\) is commutative. Then, \(S\) might not be commutative. Rings of functions: if \(X\) is an arbitrary set and \(R\) is an arbitrary function, then \(F(X,R) = \{ \text{ all functions } \ | \ f: X \rightarrow R \ \}\) is a ring via a pointwise operation. What's a pointwise operation? Given two different functions \(f, g: X \rightarrow R\), then define $$ \begin{align*} (f + g)(x) &amp;:= f(x) + g(x) \\ (fg)(x) &amp;:= f(x)g(x) \quad \text{ for } x \in X \end{align*} $$ As long as the target is a ring this works. No restriction on \(X\). Subrings A subring \(S\) of ring \(R\) is a subset, which is a ring via operation inherited from \(R\). This implies that if \(a, b \in S\), then \(a + b, ab \in S\). To show \(S\) is a subring, we have the following proposition Proposition \(R\) ring, a subset \(S \subseteq R\) is a subring if and only if \(0 \in S\) or \(S \neq \emptyset\). if \(a,b \in S\), then \(a + b, ab \in S\). If \(s \in S\) then \(-a \in S\). Examples \(\mathbf{Z}\) is a ring and \(\mathbf{Z}2\) is a subring. \(\mathbf{Z} \subseteq \mathbf{Q} \subseteq \mathbf{R} \subseteq \mathbf{C}\) are subrings. \(R = \text{Mat}_{n \times n}(\mathbf{R})\) be the ring of 2 by 2 matrices with entries in \(\mathbf{R}\). Then, \(S = \big\{ \begin{pmatrix}a &amp; -b \\ b &amp; a \end{pmatrix} \big\}, a, b \in R\) is a subring. In fact, \(S \cong \mathbf{C}\) as rings Warning: we can have a subring \(S \subseteq R\) such that \(1_S \in S\) and \(1_R \in R\) but \(1_S \neq 1_S\). Complex Numbers One way to describe the complex numbers is to say that the elements of \(\mathbf{C}\) are “formal expressions” \(a + bi\) where \(a, b \in \mathbf{R}\) and \(i\) is a new symbol. We add in the usual way and when we multiply, we use the identity \(i^2 = -1\). Another way to describe the complex numbers is to say that the elements of \(C\) are vectors \((a,b)\) in \(\mathbf{R}^2\) where we have a special notation for \(1 = (1, 0)\) and \(i = (0, 1)\). Now, define \(+\) by vector addition and define \(\cdot\) by $$ \begin{align*} (a, b) \cdot (a', b') = (aa' - bb', ab' + ba') \end{align*} $$ In terms of the older notation where \((a,b) = a+bi\), it is $$ \begin{align*} (a + bi)(a' + b'i) = (aa' - bb') + (ab' + ba')i \end{align*} $$ \(\mathbf{C}\) is a commutative ring with 1. In fact, \(\mathbf{C}\) is a field. Why? because we have straight forward formula to finding the inverse. First observe what happens when we multiply by the complex conjugate $$ \begin{align*} (a + bi)(a' - b'i) = (a^2 + b^2) + 0i. \end{align*} $$ Fact: if \(a, b \in \mathbf{R}\) and \((a,b) \neq (0,0)\), then \(a^2 + b^2 &gt; 0\). From this we get the inverse formula $$ \begin{align*} \frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i = (a + bi)^{-1} \end{align*} $$ So every non-zero element has a multiplicative inverse and so it’s a field because we have this formula. Example Let \(R = \mathbf{Z}_3[i]\) is ring. This is the set of formal expressions \(a + bi\) where \(a, b \in \mathbf{Z}_3\). \(i\) is a symbol such that \(i^2 = [-1]_3 = -1\). We can check very easily that \(R\) is a commutative ring with identity. Is \(R\) a field? It’s obvious but the answer is yes. So for any \(a, b \in \mathbf{Z}_3\) where \((a,b) \neq (0,0)\), then \(a^2 + b^2 \neq 0\). Why? observe that, \(0^2 = 0\), \(1^2 = 1\) but \(2^2 = 1\) in \(\mathbf{Z}_3\). So the only way to get \(a^2 + b^2 = 0\) is to have \(a = b = 0\). The formula for a multiplicative inverse is $$ \begin{align*} (a + bi)^{-1} = \frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i \end{align*} $$ Therefore, \(\mathbf{Z}_3[i]\) is a field with 9 elements. Call this \(\mathbf{F}_9\). What about \(\mathbf{Z}_5[i]\)? is it a field? No. Because \((2 + i)\) doesn’t have a multiplicative in \(\mathbf{Z}_5[i]\). To see this, observe that $$ \begin{align*} (2 + i) \cdot (2 - i) = 2^2 + i^2 + 5 = 0 \end{align*} $$ References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 31: Fixed Point Theorem and Cauchy Theorem</title><link href="http://localhost:4000/jekyll/update/2025/02/23/math417-31-fixed-point-theorem-cauchy-theorem.html" rel="alternate" type="text/html" title="Lecture 31: Fixed Point Theorem and Cauchy Theorem" /><published>2025-02-23T00:01:36-08:00</published><updated>2025-02-23T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/23/math417-31-fixed-point-theorem-cauchy-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/23/math417-31-fixed-point-theorem-cauchy-theorem.html"><![CDATA[<p>Let \(G\) be a group that acts on \(X\). Define
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Define \(X^G = \{x \in X \ | \ gx = x \text{ for all } g \in G\} \subseteq X \). This set is called the Fixed Point set of the action. 
</div>
<!----------------------------------------------------------------------------->
<p><br />
Compare this to the definition for any \(g \in G\), then.</p>
<div>
$$
\begin{align*}
\text{Fix}(g) = \{x \in X \ | \ gx = x\}
\end{align*}
$$
</div>
<p>So this is the set of elements fixed by \(g\) but what we defined above is the set of elements in \(X\) such that they’re always fixed by any \(g\). This means that we can re-write the definition to</p>
<div>
$$
\begin{align*}
X^G = \bigcap_{g \in G}\text{Fix}(g)
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->
<p>We can also describe this set another way. Recall that an element \(x\) is fixed by every element \(g \in G\) if and only if its orbit contains only the element \(x\) itself. So now we can re-write the definition to be</p>
<div>
$$
\begin{align*}
X^G = \{x \in X \ | \ O(x) = \{x\} \}
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->
<p>Recall now that \(\text{Stab}(x) = \{g \in G \ | \ gx = x\}\). So we can re-write this definition to say</p>
<div>
$$
\begin{align*}
X^G = \{x \in X \ | \ \text{Stab}(x) = G \}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Fixed Point Theorem</b></h4>
<p>We’ll study one theorem about this. But first define
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(p\) be a prime number. A \(p\)-group is a group of order \(p^k\) for some \(k \geq 1\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
For example \(\mathbf{Z}_{p^k}\) is a \(p\)-group. The cyclic group like \(\mathbf{Z}_{p^i} \times \mathbf{Z}_{p^j}\) is another \(p\)-group. Or the dihedral group \(D_{2^k}\) which has order \(2^{k+1}\) so this is a 2-group.
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(G\) be \(p\)-group which acts on a finite set \(X\). Then \(|X^G| \equiv |X| (\bmod p)\)
</div>
<!----------------------------------------------------------------------------->
<p><br /></p>
<p>Proof</p>
<p>The idea is that since \(G\) acts on \(X\), then it partitions \(X\) into non-empty disjoint orbits. So we can write \(O_1,O_2,...O_r\) for the orbits of the action. Then</p>
<div>
$$
\begin{align*}
|X| = |O_1| + |O_2| + ... + |O_r|
\end{align*}
$$
</div>
<p>But we know that \(|G|=p^k\) and we also know that any orbit size must divide the group order. Therefore, \(|O_i| \in \{1,p,p^2,...p^k\}\). Break the orbits into two types. Let</p>
<div>
$$
\begin{align*}
O_1, O_2, ..., O_d
\end{align*}
$$
</div>
<p>be orbits of size 1 and let</p>
<div>
$$
\begin{align*}
O_{d+1}, O_{d+2}, ..., O_r
\end{align*}
$$
</div>
<p>be orbits of size \(r\). So now</p>
<div>
$$
\begin{align*}
|X| = (|O_1| + |O_2| + ... + |O_d|) + (|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|)
\end{align*}
$$
</div>
<p>where \(|O_1| + |O_2| + ... + |O_d|=d\) and \(|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|\) is divisible by \(p\) so it’s some multiple \(k\) of \(p\). More precisely, \(d\) is the number of elements that are in orbits of size 1. By definition, this is the fixed set of the action so \(|X^G| = d\). Therefore</p>
<div>
$$
\begin{align*}
|X| &amp;= d + kp \\
|X| - d &amp;= kp \\
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
|X| &amp;\equiv d (\bmod p) \\
|X| &amp;\equiv |X^G| (\bmod p) 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Application of the Fixed Point Theorem</b></h4>
<p>Here are some application of this theorem
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(G\) be \(p\)-group. Then the center of the group \(Z(G) = \{ g \in G \ | \ gh = hg \text{ for all } h \in H\}\) is non-trivial so \(Z(G) \neq \{e\}\).
</div>
<!----------------------------------------------------------------------------->
<p><br /></p>
<p>Proof</p>
<p>Let \(G\) acts on \(X = G\) itself by conjugation. So we have \(c: G \rightarrow Sym(X)\). The fixed points of this action are</p>
<div>
$$
\begin{align*}
X^G &amp;= \{x \in X \ | \ gx = x \text{ for all } g \in G\} \\
 &amp;= \{x \in X \ | \ c(g)(x) = x \text{ for all } g \in G\} \quad \text{(the action is the conjugation action)} \\
&amp;= \{x \in X \ | gxg^{-1} = x \text{ for all } g \in G\} \\
&amp;= \{x \in X \ | gx = xg \text{ for all } g \in G\} 
\end{align*}
$$
</div>
<p>So in the conjugation action, the fixed point set is the center of the set. By the previous theorem we know that</p>
<div>
$$
\begin{align*}
|X^G| &amp;\equiv |X| (\bmod p) \\
|X^G| &amp;\equiv p^k (\bmod p) \quad \text{(order of $|G|$ is $p^k$)} \\
|X^G| &amp;\equiv 0 (\bmod p)  \quad \text{because ($p^k \equiv 0 (\bmod p)$)}\\
|Z(G)| &amp;\equiv 0 (\bmod p)  \quad \text{(we just showed this)}\\
\end{align*}
$$
</div>
<p>Therefore, \(Z(G) - 0 = pm\) for some \(m \in Z\). This means that \(p \ | \ Z(G)\). But \(Z(G)\) is a subgroup so it includes at least the identity element. So its size is at least 1. Therefore, \(|Z(G)| \geq p \geq 2\). So we must have at least one non-trivial element in the center. \(\ \blacksquare\)
<br />
<br />
So again, \(p-\)groups will always have a non-trivial center. Next, we have a corollary of this
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(p\) be a prime number. Then every group of order \(p^2\) is abelian.
</div>
<!----------------------------------------------------------------------------->
<p><br />
Using this, we can now use the elementary divisor theorem to classify these groups. In fact, \(G\) of order (p^2) is isomorphic to either \(\mathbf{Z}_{p^2}\) or \(\mathbf{Z}_p \times \mathbf{Z}_p\). 
<br />
<br />
<b>Proof</b>
Let \(|G| = p^2\). By the proposition, \(Z(G)\) is not trivial. But we also know that it is a subgroup. So its order must divide the order of the group. So its order must either be \(p\) or \(p^2\). If the order is \(p^2\), then every element commute with every other element so \(G\) must be abelian. So we only have case which is when \(|Z(G)| = p\). Now, recall that \(Z(G)\) is a normal subgroup in \(G\). Therefore, we can form the quotient group \(G/Z(G)\). The order of this quotient group is \(p^2/p = p\). But we also know that every group of prime order is cyclic so \(G/Z(G)\) is cyclic. By Homework ?, if we have a group \(G\) where its quotient group mod its center is cyclic (\(G/Z(G)\)), then \(G\) is abelian. So \(G\) is abelian in this case too. \(\ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cauchy Theorem</b></h4>
<p>There is one more application of the fixed point theorem. 
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(G\) be a finite group. If \(p\) is a prime number that divides \(|G|\), then \(G\) must have element of order \(p\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
A special case of this is the even-order theorem. As a reminder, let’s revisit this proof in terms of group actions before proving the actual theorem. 
<br />
<br />
<b>Proof (Even Order Theorem)</b>
<br />
Let \(|G| = n\) where \(n\) is even. Let \(C = \langle \varphi \rangle = \{e, \varphi\}\) be a cyclic group of order 2. Let \(C\) act on the set \(X = C\) which is the group itself. We know the identity elements acts as the identity function on \(X\). So we only have to define the action for \(\varphi\). So let’s define what \(\varphi(g)\) is for every element of the group. Let \(\varphi(g)\) be</p>
<div>
$$
\begin{align*}
\varphi(g) = g^{-1} \quad \text{for } g \in G
\end{align*}
$$
</div>
<p>This is in fact is a bijection. Note here, \(\varphi \circ \varphi = id\) so composing the action \(\varphi\) with itself gives us back the identity function. Now, \(C\) is a 2-group since its of order 2. So we can apply the fixed point theorem which states that</p>
<div>
$$
\begin{align*}
|X^C| &amp;\equiv |X| (\bmod 2) \\
|X^C| &amp;\equiv 2 (\bmod 2) \quad \text{(We know $|X| = 2$)}\\
|X^C| &amp;\equiv 0 (\bmod 2)
\end{align*}
$$
</div>
<p>So \(|X^C|\) must be even. \(X^C\) is the set of elements that are fixed by the action \(\varphi\) so</p>
<div>
$$
\begin{align*}
X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\
    &amp;= \{x \in X \ | \ x^{-1} = x\} \\
	&amp;= \{x \in X \ | \ x^2 = e\}.
\end{align*}
$$
</div>
<p>So this group has an even number of elements and must at least include the identity element. Therefore, it must have at least one more non-trivial element of order 2. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------->
<b>Proof (Cauchy’s Theorem)</b>
<br />
Let \(G\) be a group of order \(n\). Suppose \(p\) is a prime number such that \(p \ | \ n\). Let \(C = \langle \varphi \rangle\) be a cyclic group of order \(p\). Let \(X\) be the set</p>
<div>
$$
\begin{align*}
X = \{(a_1,...,a_p) \in G^p \ | \ a_1,...a_p \in G, a_1a_2...a_p = e\}
\end{align*}
$$
</div>
<p>So it’s the set of all \(p\) tuples such that when we multiply any tuple’s elements, we get the identity. But we can re-write this as</p>
<div>
$$
\begin{align*}
a_p =  (a_1,a_2...a_{p-1})^{-1}
\end{align*}
$$
</div>
<p>So the last element \(a_p\) is the inverse of the previous elements all multiplied. So for any \(g\) to be in \(G^p\), we can pick \(p-1\) elements from \(G\) and then form the last element by taking their product and taking the inverse of that product. 
<br />
<br />
What is \(|X|\)? we have \(n\) choices for the first \(p-1\) elements but only 1 choice for the last element. This implies that \(|X| = n^{p-1}\). Moreover, by assumption we know that \(p\) divides \(|G|=n\). So \(n = pk\) for some \(k\). So we can write \(|X| = (pk)^{p-1}\). But \(p\) is prime so it’s at least 2. Therefore, \(p\) must divide \(|X|\) as well.
<br />
<br />
Now, let \(C\) act on \(X\). Define</p>
<div>
$$
\begin{align*}
\varphi \cdot (a_1,a_2,...,a_{p-1},a_p) = (a_p,a_1,...,a_{p-1})
\end{align*}
$$
</div>
<p>So the action permutes the elements cyclicly. We need to verify that the product is in \(X\). This means if we multiply the first \(p-1\) elements and take their inverse, we should get the last element. To show this notice that \((a_1,a_2,...,a_{p-1},a_p)\) is in \(X\) by assumption so we know that the product of the elements is \(e\). Now conjugate this product by \(a^{p}\) to see that</p>
<div>
$$
\begin{align*}
(a_1a_2...a_{p-1}a_p) &amp;= e \\
a_p(a_1a_2...a_{p-1}a_p)a_p^{-1} &amp;= a_pea_p^{-1} \\
a_pa_1a_2...a_{p-1} &amp;= e.
\end{align*}
$$
</div>
<p>So we can see that \(a_pa_1a_2...a_{p-1} \in X\) which is what we wanted to show. Additionally, if we apply \(\varphi\) \(p\) times, we will see that \(\varphi \circ \varphi \circ ... \varphi = id\) it will take us to the identity function or action. So this action or permutation has order \(p\). 
<br />
<br />
So now we have \(|G| = n\) and \(|X| = n^{p-1}\). We know \(p\) divides both. We can apply the fixed point theorem but what is \(X^C\)? By definition, it’s the set of elements fixed by any \(g \in \langle \varphi \rangle\). But since \(\langle \varphi \rangle\) is cyclic, then if an element gets fixed by \(\varphi\), it get fixed by any power of \(\varphi\). Therefore</p>
<div>
$$
\begin{align*}
X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\
    &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and } \varphi \cdot (a_1,...,a_p) = (a_1,...,a_p)\} \\
    &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and }  (a_p,a_1...,a_{p-1}) = (a_1,a_2...,a_p)\}.
\end{align*}
$$
</div>
<p>This last condition says that \(a_p=a_1\), \(a_1=a_2\), … \(a_{p-1}=a_p\). This means that all the elements are the same. So we can write \(X^C\) as</p>
<div>
$$
\begin{align*}
X^C &amp;= \{ (a,a...,a) \ | \ a \in G, aa...a = a^p = e\}.
\end{align*}
$$
</div>
<p>Therefore, the size of this set, is the number of elements in \(G\) which have order \(p\). So</p>
<div>
$$
\begin{align*}
|X^C| &amp;= |\{ a \in G \ | \ a^p = e\}|.
\end{align*}
$$
</div>
<p>We know \(e \in X^C\) since e\(e^p = e\). Moreover, by the Fixed Point Theorem,</p>
<div>
$$
\begin{align*}
|X^C| &amp;\equiv |X| (\bmod p) \\
|X^C| &amp;\equiv 0 (\bmod p) \quad \text{(because $p \ | \ |X|$)}
\end{align*}
$$
</div>
<p>So \(|X^C|\) must be divisible by \(p\) and since \(|X^C| \geq 1\), then \(|X^C| \geq p\). But this means that \(G\) has at least one non-trivial element of order \(p\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Classification of Groups of Order 6</b></h4>
<p>We’ve seen before \(\mathbf{Z}_6 \cong \mathbf{Z}_2 \times \mathbf{Z}_3\) and we’ve also seen \(D_3 \cong S_3\). 
<br />
<br />
Suppose \(|G| = 6 = 2(3)\). These are prime factors, so we can use Cauchy’s Theorem twice to conclude that we must have an element of order 2 and another element of order 3. So let</p>
<div>
$$
\begin{align*}
A &amp;= \langle a \rangle \text{ where $|A| = 2$ } \\
N &amp;= \langle 3 \rangle \text{ where $|N| = 3$}
\end{align*}
$$
</div>
<p>This implies that</p>
<div>
$$
\begin{align*}
[G:N] = \frac{|G|}{|N|} = \frac{6}{3} = 2.
\end{align*}
$$
</div>
<p>But we’ve seen in one of the homeworks, that this implies that \(N\) is normal. We also know the following facts</p>
<ul>
	<li>\(N \cap A = \{e\}\) since elements of \(N\) have order 1 or 3 and elements of \(A\) have orders 1 and 2</li>
	<li>\(NA\) is a subgroup of \(G\) because one of the groups is normal by Corollary (26.4).</li>
	<li>By the Diamond Isomorphism Theorem, \(A/A \cap N \cong NA/N\). But since \(A \cap N = \{e\}\). Then \(A \cong NA/N\). This means that \(|NA|/|N| = |A|\). So \(|NA| = |A||N| = 6\)</li>
	<li>Since \(NA\) is a subgroup of size 6, then it's \(G\) so \(NA = G\)</li>
</ul>
<p>So we know 4 important things</p>
<ul>
	<li>\(A\) is a subgroup of \(G\).</li>
	<li>\(N\) is a normal subgroup of \(G\).</li>
	<li>\(G = NA\).</li>
	<li>\(A \cap N = \{e\}\).</li>
</ul>
<p>These are the 4 conditions so we can apply the the recognization theorem to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups</p>
<div>
$$
\begin{align*}
N \rtimes_{\gamma} A \cong G
\end{align*}
$$
</div>
<p>\(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order 3 so it’s isomorphic to \(\mathbf{Z}_3\). We know that \(\text{Aut}(\mathbf{Z}_3) \cong \Phi(3)\) But \(\Phi(3)\) is of order 2 so it’s isomorphic to \(\mathbf{Z}_2\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\) if both groups are cyclic of order 2? There are only two choices.</p>
<ul>
	<li>The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_3\)</li>
	<li>The non-trivial homomorphism gives us the dihedral group \(D_3\).</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Study Notes on \(D_3\)</b></h4>
<p>How does the non-trivial homomorphism gives us the dihedral group? how does this happen? We have</p>
<ul>
	<li>\(N = \langle n \rangle \cong \mathbf{Z}_3\) where \(n^3 = e\).</li>
	<li>\(A = \langle a \rangle \cong \mathbf{Z}_2\) where \(a^2 = e\)</li>
</ul>
<p>Define</p>
<div>
	$$
	\begin{align*}
	\gamma: A &amp;\rightarrow \text{Aut}(\mathbf{Z}_3) \\
	\gamma(a) &amp;= \alpha \text{ where } \alpha(n) = n^{-1} \\
	\gamma(a)(n) &amp;= n^{-1}
	\end{align*}
	$$
</div>
<p>So the element \(a \in A\) acts on \(n \in N\) by inverting it. Observe that \(a^2 = e\) and \(\alpha^2 = (n^{-1})^{-1} = \text{id}\) so \(\gamma(\alpha^2) = \gamma(\alpha)^2\). So now multiplication in semi-direct groups is defined as</p>
<div>
	$$
	\begin{align*}
	(n^i, a^j)(n^k, a^l) = (n^i \cdot \gamma_{a^j}(n^k), a^{j}a^{j})
	\end{align*}
	$$
</div>
<p>\(a\) has order 2 so \(\gamma_{a^j}(n^k)\) is defined as</p>
<ul>
	<li>When \(j = 0\), then \(a^0 = 0\), then \(\gamma_{e} = id\).</li>
	<li>When \(j = 1\), then \(a^1 = a\), then \(\gamma_{a} = \alpha\) where \(\alpha(n) = n^{-k}\).</li>
</ul>
<p>So now if we apply the semidirect product multiplication, using the homomorphism we defined, we will see that for any \(a\), that we get the relationship \(ana^{-1} = n^{-1}\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Classification of Groups of Order \(2p\)</b></h4>
<p>where \(p\) is an odd prime. We know two groups \(\mathbf{Z}_{2p} =\mathbf{Z}_{2} \times \mathbf{Z}_{p}\) and \(D_p\). 
<br />
<br />
Let \(|G| = 2p\). By Cauchy, \(2\) divides \(|G|\). Therefore, we have an element of order \(2\). From this, we get \(A = \langle a \rangle\) where \(|A| = 2\). We also have an element of order \(p\). From this we get \(N = \langle N \rangle\). Again, we will see that</p>
<div>
$$
\begin{align*}
[G:N] = \frac{|G|}{|N|} = \frac{2p}{p} = 2.
\end{align*}
$$
</div>
<p>Therefore \(N\) is normal. With a similar argument to the previous example. We will see that</p>
<ul>
	<li>\(A\) is a subgroup of \(G\).</li>
	<li>\(N\) is a normal subgroup of \(G\).</li>
	<li>\(G = NA\).</li>
	<li>\(A \cap N = \{e\}\).</li>
</ul>
<p>So we can use the recognization theorem again to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups</p>
<div>
$$
\begin{align*}
N \rtimes_{\gamma} A \cong G
\end{align*}
$$
</div>
<p>\(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order \(p\) so it’s isomorphic to \(\mathbf{Z}_p\) (cyclic). We know that \(\text{Aut}(\mathbf{Z}_p) \cong \Phi(p)\) where \(\Phi(p)\) is of order \(p-1\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\).</p>
<ul>
	<li>The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_p\)</li>
	<li>The non-trivial homomorphism gives us the dihedral group \(D_p\).</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Classification of Groups of Order \(pq\)</b></h4>
<p>\(p\) and \(q\) are distinct primes where \(p &gt; q\). So again we have \(|G|=pq\). By Cauchy, we have two cyclic subgroups such that</p>
<div>
$$
\begin{align*}
A &amp;= \langle a \rangle \text{ where $|A| = q$ } \\
N &amp;= \langle b \rangle \text{ where $|N| = p$}
\end{align*}
$$
</div>
<p>This implies that</p>
<div>
$$
\begin{align*}
[G:N] = \frac{|G|}{|N|} = q.
\end{align*}
$$
</div>
<p>\(q\) is the smallest prime dividing the order of \(|G|\). By some homework assignment that said that if we have a subgroup of order “the smallest prime dividing the order”, then this subgroup is normal. So now again,</p>
<ul>
	<li>\(A\) is a subgroup of \(G\).</li>
	<li>\(N\) is a normal subgroup of \(G\).</li>
	<li>\(G = NA\).</li>
	<li>\(A \cap N = \{e\}\) because \(p \neq q\).</li>
</ul>
<p>So we can use the recognization theorem AGAIN to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups</p>
<div>
$$
\begin{align*}
N \rtimes_{\gamma} A \cong G
\end{align*}
$$
</div>
<p>Where</p>
<div>
	$$
	\begin{align*}
	\gamma: A &amp;\rightarrow \text{Aut}(N) \\
	\gamma: \mathbf{Z}_p &amp;\rightarrow \Phi(p)
	\end{align*}
	$$
</div>
<p>There are two cases:</p>
<ul>
	<li>\(q \ \not\mid \ (p-1)\): In this case, the generator of the group \(A\)'s \(q\)th power has to go to the identity because \(q\) doesn't divide \(p - 1\) which is the order of \(\Phi(p)\). The only possible \(\gamma\) is \(\gamma(a) = e\) so send everything to the identity and we get the direct product \(\mathbf{Z}_p \times \mathbf{Z}_q\).</li>
	<li>\(q \ | \ (p-1)\): So we get the non-trivial homomorphism \(\gamma\). Because \(q \ (p-1)\) which is the order of \(\Phi(p-1)\), then by Cauchy there exists an element of order \(q\) in \(\Phi(p-1)\). So \(\mathbf{Z}_{pq}\) and another non abelian group.</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(G\) be a group that acts on \(X\). Define Definition Define \(X^G = \{x \in X \ | \ gx = x \text{ for all } g \in G\} \subseteq X \). This set is called the Fixed Point set of the action. Compare this to the definition for any \(g \in G\), then. $$ \begin{align*} \text{Fix}(g) = \{x \in X \ | \ gx = x\} \end{align*} $$ So this is the set of elements fixed by \(g\) but what we defined above is the set of elements in \(X\) such that they’re always fixed by any \(g\). This means that we can re-write the definition to $$ \begin{align*} X^G = \bigcap_{g \in G}\text{Fix}(g) \end{align*} $$ We can also describe this set another way. Recall that an element \(x\) is fixed by every element \(g \in G\) if and only if its orbit contains only the element \(x\) itself. So now we can re-write the definition to be $$ \begin{align*} X^G = \{x \in X \ | \ O(x) = \{x\} \} \end{align*} $$ Recall now that \(\text{Stab}(x) = \{g \in G \ | \ gx = x\}\). So we can re-write this definition to say $$ \begin{align*} X^G = \{x \in X \ | \ \text{Stab}(x) = G \} \end{align*} $$ Fixed Point Theorem We’ll study one theorem about this. But first define Definition Let \(p\) be a prime number. A \(p\)-group is a group of order \(p^k\) for some \(k \geq 1\). For example \(\mathbf{Z}_{p^k}\) is a \(p\)-group. The cyclic group like \(\mathbf{Z}_{p^i} \times \mathbf{Z}_{p^j}\) is another \(p\)-group. Or the dihedral group \(D_{2^k}\) which has order \(2^{k+1}\) so this is a 2-group. Theorem Let \(G\) be \(p\)-group which acts on a finite set \(X\). Then \(|X^G| \equiv |X| (\bmod p)\) Proof The idea is that since \(G\) acts on \(X\), then it partitions \(X\) into non-empty disjoint orbits. So we can write \(O_1,O_2,...O_r\) for the orbits of the action. Then $$ \begin{align*} |X| = |O_1| + |O_2| + ... + |O_r| \end{align*} $$ But we know that \(|G|=p^k\) and we also know that any orbit size must divide the group order. Therefore, \(|O_i| \in \{1,p,p^2,...p^k\}\). Break the orbits into two types. Let $$ \begin{align*} O_1, O_2, ..., O_d \end{align*} $$ be orbits of size 1 and let $$ \begin{align*} O_{d+1}, O_{d+2}, ..., O_r \end{align*} $$ be orbits of size \(r\). So now $$ \begin{align*} |X| = (|O_1| + |O_2| + ... + |O_d|) + (|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|) \end{align*} $$ where \(|O_1| + |O_2| + ... + |O_d|=d\) and \(|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|\) is divisible by \(p\) so it’s some multiple \(k\) of \(p\). More precisely, \(d\) is the number of elements that are in orbits of size 1. By definition, this is the fixed set of the action so \(|X^G| = d\). Therefore $$ \begin{align*} |X| &amp;= d + kp \\ |X| - d &amp;= kp \\ \end{align*} $$ Therefore, $$ \begin{align*} |X| &amp;\equiv d (\bmod p) \\ |X| &amp;\equiv |X^G| (\bmod p) \end{align*} $$ Application of the Fixed Point Theorem Here are some application of this theorem Proposition Let \(G\) be \(p\)-group. Then the center of the group \(Z(G) = \{ g \in G \ | \ gh = hg \text{ for all } h \in H\}\) is non-trivial so \(Z(G) \neq \{e\}\). Proof Let \(G\) acts on \(X = G\) itself by conjugation. So we have \(c: G \rightarrow Sym(X)\). The fixed points of this action are $$ \begin{align*} X^G &amp;= \{x \in X \ | \ gx = x \text{ for all } g \in G\} \\ &amp;= \{x \in X \ | \ c(g)(x) = x \text{ for all } g \in G\} \quad \text{(the action is the conjugation action)} \\ &amp;= \{x \in X \ | gxg^{-1} = x \text{ for all } g \in G\} \\ &amp;= \{x \in X \ | gx = xg \text{ for all } g \in G\} \end{align*} $$ So in the conjugation action, the fixed point set is the center of the set. By the previous theorem we know that $$ \begin{align*} |X^G| &amp;\equiv |X| (\bmod p) \\ |X^G| &amp;\equiv p^k (\bmod p) \quad \text{(order of $|G|$ is $p^k$)} \\ |X^G| &amp;\equiv 0 (\bmod p) \quad \text{because ($p^k \equiv 0 (\bmod p)$)}\\ |Z(G)| &amp;\equiv 0 (\bmod p) \quad \text{(we just showed this)}\\ \end{align*} $$ Therefore, \(Z(G) - 0 = pm\) for some \(m \in Z\). This means that \(p \ | \ Z(G)\). But \(Z(G)\) is a subgroup so it includes at least the identity element. So its size is at least 1. Therefore, \(|Z(G)| \geq p \geq 2\). So we must have at least one non-trivial element in the center. \(\ \blacksquare\) So again, \(p-\)groups will always have a non-trivial center. Next, we have a corollary of this Proposition Let \(p\) be a prime number. Then every group of order \(p^2\) is abelian. Using this, we can now use the elementary divisor theorem to classify these groups. In fact, \(G\) of order (p^2) is isomorphic to either \(\mathbf{Z}_{p^2}\) or \(\mathbf{Z}_p \times \mathbf{Z}_p\). Proof Let \(|G| = p^2\). By the proposition, \(Z(G)\) is not trivial. But we also know that it is a subgroup. So its order must divide the order of the group. So its order must either be \(p\) or \(p^2\). If the order is \(p^2\), then every element commute with every other element so \(G\) must be abelian. So we only have case which is when \(|Z(G)| = p\). Now, recall that \(Z(G)\) is a normal subgroup in \(G\). Therefore, we can form the quotient group \(G/Z(G)\). The order of this quotient group is \(p^2/p = p\). But we also know that every group of prime order is cyclic so \(G/Z(G)\) is cyclic. By Homework ?, if we have a group \(G\) where its quotient group mod its center is cyclic (\(G/Z(G)\)), then \(G\) is abelian. So \(G\) is abelian in this case too. \(\ \blacksquare\). Cauchy Theorem There is one more application of the fixed point theorem. Theorem Let \(G\) be a finite group. If \(p\) is a prime number that divides \(|G|\), then \(G\) must have element of order \(p\). A special case of this is the even-order theorem. As a reminder, let’s revisit this proof in terms of group actions before proving the actual theorem. Proof (Even Order Theorem) Let \(|G| = n\) where \(n\) is even. Let \(C = \langle \varphi \rangle = \{e, \varphi\}\) be a cyclic group of order 2. Let \(C\) act on the set \(X = C\) which is the group itself. We know the identity elements acts as the identity function on \(X\). So we only have to define the action for \(\varphi\). So let’s define what \(\varphi(g)\) is for every element of the group. Let \(\varphi(g)\) be $$ \begin{align*} \varphi(g) = g^{-1} \quad \text{for } g \in G \end{align*} $$ This is in fact is a bijection. Note here, \(\varphi \circ \varphi = id\) so composing the action \(\varphi\) with itself gives us back the identity function. Now, \(C\) is a 2-group since its of order 2. So we can apply the fixed point theorem which states that $$ \begin{align*} |X^C| &amp;\equiv |X| (\bmod 2) \\ |X^C| &amp;\equiv 2 (\bmod 2) \quad \text{(We know $|X| = 2$)}\\ |X^C| &amp;\equiv 0 (\bmod 2) \end{align*} $$ So \(|X^C|\) must be even. \(X^C\) is the set of elements that are fixed by the action \(\varphi\) so $$ \begin{align*} X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\ &amp;= \{x \in X \ | \ x^{-1} = x\} \\ &amp;= \{x \in X \ | \ x^2 = e\}. \end{align*} $$ So this group has an even number of elements and must at least include the identity element. Therefore, it must have at least one more non-trivial element of order 2. \(\ \blacksquare\) Proof (Cauchy’s Theorem) Let \(G\) be a group of order \(n\). Suppose \(p\) is a prime number such that \(p \ | \ n\). Let \(C = \langle \varphi \rangle\) be a cyclic group of order \(p\). Let \(X\) be the set $$ \begin{align*} X = \{(a_1,...,a_p) \in G^p \ | \ a_1,...a_p \in G, a_1a_2...a_p = e\} \end{align*} $$ So it’s the set of all \(p\) tuples such that when we multiply any tuple’s elements, we get the identity. But we can re-write this as $$ \begin{align*} a_p = (a_1,a_2...a_{p-1})^{-1} \end{align*} $$ So the last element \(a_p\) is the inverse of the previous elements all multiplied. So for any \(g\) to be in \(G^p\), we can pick \(p-1\) elements from \(G\) and then form the last element by taking their product and taking the inverse of that product. What is \(|X|\)? we have \(n\) choices for the first \(p-1\) elements but only 1 choice for the last element. This implies that \(|X| = n^{p-1}\). Moreover, by assumption we know that \(p\) divides \(|G|=n\). So \(n = pk\) for some \(k\). So we can write \(|X| = (pk)^{p-1}\). But \(p\) is prime so it’s at least 2. Therefore, \(p\) must divide \(|X|\) as well. Now, let \(C\) act on \(X\). Define $$ \begin{align*} \varphi \cdot (a_1,a_2,...,a_{p-1},a_p) = (a_p,a_1,...,a_{p-1}) \end{align*} $$ So the action permutes the elements cyclicly. We need to verify that the product is in \(X\). This means if we multiply the first \(p-1\) elements and take their inverse, we should get the last element. To show this notice that \((a_1,a_2,...,a_{p-1},a_p)\) is in \(X\) by assumption so we know that the product of the elements is \(e\). Now conjugate this product by \(a^{p}\) to see that $$ \begin{align*} (a_1a_2...a_{p-1}a_p) &amp;= e \\ a_p(a_1a_2...a_{p-1}a_p)a_p^{-1} &amp;= a_pea_p^{-1} \\ a_pa_1a_2...a_{p-1} &amp;= e. \end{align*} $$ So we can see that \(a_pa_1a_2...a_{p-1} \in X\) which is what we wanted to show. Additionally, if we apply \(\varphi\) \(p\) times, we will see that \(\varphi \circ \varphi \circ ... \varphi = id\) it will take us to the identity function or action. So this action or permutation has order \(p\). So now we have \(|G| = n\) and \(|X| = n^{p-1}\). We know \(p\) divides both. We can apply the fixed point theorem but what is \(X^C\)? By definition, it’s the set of elements fixed by any \(g \in \langle \varphi \rangle\). But since \(\langle \varphi \rangle\) is cyclic, then if an element gets fixed by \(\varphi\), it get fixed by any power of \(\varphi\). Therefore $$ \begin{align*} X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\ &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and } \varphi \cdot (a_1,...,a_p) = (a_1,...,a_p)\} \\ &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and } (a_p,a_1...,a_{p-1}) = (a_1,a_2...,a_p)\}. \end{align*} $$ This last condition says that \(a_p=a_1\), \(a_1=a_2\), … \(a_{p-1}=a_p\). This means that all the elements are the same. So we can write \(X^C\) as $$ \begin{align*} X^C &amp;= \{ (a,a...,a) \ | \ a \in G, aa...a = a^p = e\}. \end{align*} $$ Therefore, the size of this set, is the number of elements in \(G\) which have order \(p\). So $$ \begin{align*} |X^C| &amp;= |\{ a \in G \ | \ a^p = e\}|. \end{align*} $$ We know \(e \in X^C\) since e\(e^p = e\). Moreover, by the Fixed Point Theorem, $$ \begin{align*} |X^C| &amp;\equiv |X| (\bmod p) \\ |X^C| &amp;\equiv 0 (\bmod p) \quad \text{(because $p \ | \ |X|$)} \end{align*} $$ So \(|X^C|\) must be divisible by \(p\) and since \(|X^C| \geq 1\), then \(|X^C| \geq p\). But this means that \(G\) has at least one non-trivial element of order \(p\). \(\ \blacksquare\) Classification of Groups of Order 6 We’ve seen before \(\mathbf{Z}_6 \cong \mathbf{Z}_2 \times \mathbf{Z}_3\) and we’ve also seen \(D_3 \cong S_3\). Suppose \(|G| = 6 = 2(3)\). These are prime factors, so we can use Cauchy’s Theorem twice to conclude that we must have an element of order 2 and another element of order 3. So let $$ \begin{align*} A &amp;= \langle a \rangle \text{ where $|A| = 2$ } \\ N &amp;= \langle 3 \rangle \text{ where $|N| = 3$} \end{align*} $$ This implies that $$ \begin{align*} [G:N] = \frac{|G|}{|N|} = \frac{6}{3} = 2. \end{align*} $$ But we’ve seen in one of the homeworks, that this implies that \(N\) is normal. We also know the following facts \(N \cap A = \{e\}\) since elements of \(N\) have order 1 or 3 and elements of \(A\) have orders 1 and 2 \(NA\) is a subgroup of \(G\) because one of the groups is normal by Corollary (26.4). By the Diamond Isomorphism Theorem, \(A/A \cap N \cong NA/N\). But since \(A \cap N = \{e\}\). Then \(A \cong NA/N\). This means that \(|NA|/|N| = |A|\). So \(|NA| = |A||N| = 6\) Since \(NA\) is a subgroup of size 6, then it's \(G\) so \(NA = G\) So we know 4 important things \(A\) is a subgroup of \(G\). \(N\) is a normal subgroup of \(G\). \(G = NA\). \(A \cap N = \{e\}\). These are the 4 conditions so we can apply the the recognization theorem to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups $$ \begin{align*} N \rtimes_{\gamma} A \cong G \end{align*} $$ \(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order 3 so it’s isomorphic to \(\mathbf{Z}_3\). We know that \(\text{Aut}(\mathbf{Z}_3) \cong \Phi(3)\) But \(\Phi(3)\) is of order 2 so it’s isomorphic to \(\mathbf{Z}_2\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\) if both groups are cyclic of order 2? There are only two choices. The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_3\) The non-trivial homomorphism gives us the dihedral group \(D_3\). Study Notes on \(D_3\) How does the non-trivial homomorphism gives us the dihedral group? how does this happen? We have \(N = \langle n \rangle \cong \mathbf{Z}_3\) where \(n^3 = e\). \(A = \langle a \rangle \cong \mathbf{Z}_2\) where \(a^2 = e\) Define $$ \begin{align*} \gamma: A &amp;\rightarrow \text{Aut}(\mathbf{Z}_3) \\ \gamma(a) &amp;= \alpha \text{ where } \alpha(n) = n^{-1} \\ \gamma(a)(n) &amp;= n^{-1} \end{align*} $$ So the element \(a \in A\) acts on \(n \in N\) by inverting it. Observe that \(a^2 = e\) and \(\alpha^2 = (n^{-1})^{-1} = \text{id}\) so \(\gamma(\alpha^2) = \gamma(\alpha)^2\). So now multiplication in semi-direct groups is defined as $$ \begin{align*} (n^i, a^j)(n^k, a^l) = (n^i \cdot \gamma_{a^j}(n^k), a^{j}a^{j}) \end{align*} $$ \(a\) has order 2 so \(\gamma_{a^j}(n^k)\) is defined as When \(j = 0\), then \(a^0 = 0\), then \(\gamma_{e} = id\). When \(j = 1\), then \(a^1 = a\), then \(\gamma_{a} = \alpha\) where \(\alpha(n) = n^{-k}\). So now if we apply the semidirect product multiplication, using the homomorphism we defined, we will see that for any \(a\), that we get the relationship \(ana^{-1} = n^{-1}\). Classification of Groups of Order \(2p\) where \(p\) is an odd prime. We know two groups \(\mathbf{Z}_{2p} =\mathbf{Z}_{2} \times \mathbf{Z}_{p}\) and \(D_p\). Let \(|G| = 2p\). By Cauchy, \(2\) divides \(|G|\). Therefore, we have an element of order \(2\). From this, we get \(A = \langle a \rangle\) where \(|A| = 2\). We also have an element of order \(p\). From this we get \(N = \langle N \rangle\). Again, we will see that $$ \begin{align*} [G:N] = \frac{|G|}{|N|} = \frac{2p}{p} = 2. \end{align*} $$ Therefore \(N\) is normal. With a similar argument to the previous example. We will see that \(A\) is a subgroup of \(G\). \(N\) is a normal subgroup of \(G\). \(G = NA\). \(A \cap N = \{e\}\). So we can use the recognization theorem again to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups $$ \begin{align*} N \rtimes_{\gamma} A \cong G \end{align*} $$ \(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order \(p\) so it’s isomorphic to \(\mathbf{Z}_p\) (cyclic). We know that \(\text{Aut}(\mathbf{Z}_p) \cong \Phi(p)\) where \(\Phi(p)\) is of order \(p-1\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\). The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_p\) The non-trivial homomorphism gives us the dihedral group \(D_p\). Classification of Groups of Order \(pq\) \(p\) and \(q\) are distinct primes where \(p &gt; q\). So again we have \(|G|=pq\). By Cauchy, we have two cyclic subgroups such that $$ \begin{align*} A &amp;= \langle a \rangle \text{ where $|A| = q$ } \\ N &amp;= \langle b \rangle \text{ where $|N| = p$} \end{align*} $$ This implies that $$ \begin{align*} [G:N] = \frac{|G|}{|N|} = q. \end{align*} $$ \(q\) is the smallest prime dividing the order of \(|G|\). By some homework assignment that said that if we have a subgroup of order “the smallest prime dividing the order”, then this subgroup is normal. So now again, \(A\) is a subgroup of \(G\). \(N\) is a normal subgroup of \(G\). \(G = NA\). \(A \cap N = \{e\}\) because \(p \neq q\). So we can use the recognization theorem AGAIN to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups $$ \begin{align*} N \rtimes_{\gamma} A \cong G \end{align*} $$ Where $$ \begin{align*} \gamma: A &amp;\rightarrow \text{Aut}(N) \\ \gamma: \mathbf{Z}_p &amp;\rightarrow \Phi(p) \end{align*} $$ There are two cases: \(q \ \not\mid \ (p-1)\): In this case, the generator of the group \(A\)'s \(q\)th power has to go to the identity because \(q\) doesn't divide \(p - 1\) which is the order of \(\Phi(p)\). The only possible \(\gamma\) is \(\gamma(a) = e\) so send everything to the identity and we get the direct product \(\mathbf{Z}_p \times \mathbf{Z}_q\). \(q \ | \ (p-1)\): So we get the non-trivial homomorphism \(\gamma\). Because \(q \ (p-1)\) which is the order of \(\Phi(p-1)\), then by Cauchy there exists an element of order \(q\) in \(\Phi(p-1)\). So \(\mathbf{Z}_{pq}\) and another non abelian group. References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 30: Finite Subgroups of SO(3)</title><link href="http://localhost:4000/jekyll/update/2025/02/22/math417-30-finite-subgroups.html" rel="alternate" type="text/html" title="Lecture 30: Finite Subgroups of SO(3)" /><published>2025-02-22T00:01:36-08:00</published><updated>2025-02-22T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/22/math417-30-finite-subgroups</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/22/math417-30-finite-subgroups.html"><![CDATA[<p>Today, we’ll analyze finite subgroups of \(SO(3)\). Let’s describe the ones we already know about</p>
<ul>
	<li>The trivial subgroup \(\{I\}\).</li>
	<!-------------------------------------->
	<li>Subgroups that are cyclic so these subgroups are isomorphic to \(\mathbf{Z}_n\). For each \(n\), we have infinitely many subgroups. For example, there are infinitely many cyclic subgroups of order 3 in \(SO(3)\). Pick any axis and consider the rotation by angle \(\frac{2\pi}{3}\) around this axis. This makes a subgroup of order 3. So you can get a new subgroup for whatever axis you pick. Therefore, we have infinitely many. In fact, all these subgroups that are rotations by \(\frac{2\pi}{3}\) are conjugate (see the lecture before last. If we have a rotation by \(\theta\) around a unit vector \(v\) and we have the same rotation around another unit vector \(u\), then we change \(u\) to \(v\) by some rotation \(A \in SO(3)\).</li>
	<!-------------------------------------->
	<li>Subgroups isomorphic to the dihedral groups \(D_n\) where \(n \geq 2\). These are symmetries of a regular \(n-\)gon. Pick any \(n-\) and fix one of the vertices on the \(x-\)axis. The symmetries of the \(n-\) is a group isomorphic to \(D_n\).</li>
	<!-------------------------------------->
	<li>Symmetry group of the tetrahedron which is isomorphic to \(A_4\).</li>
	<!-------------------------------------->
	<li>Symmetry group of the cube/octahedron which is isomorphic to \(S_4\).</li>
	<!-------------------------------------->
	<li>Symmetry group of the icosahedron/dodecahedron which is isomorphic to \(A_5\).</li>
</ul>
<p>In fact we have the following theorem
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Every finite subgroup of \(SO(3)\) is in the list above.
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
\(SO(3)\) acts on \(\mathbf{R}^3\) (we multiply a matrix in \(SO(3)\) by a vector in \(\mathbf{R}^3\)). So now suppose that \(G\) is a subgroup of \(SO(3)\). \(G\) acts on \(\mathbf{R}^3\). We’ll focus on the pole subset in \(\mathbf{R}^3\). What is a poll?
<br />
<br />
A pole for \(G \subseteq SO(3)\) is a unit vector \((u) \in \mathbf{R}^3\) where \(\lVert u \rVert = 1\) such that \(|Stab_G(u)|\geq 2\).
<br />
<br />
In other words, there exists an element \(g\) in \(\text{Stab}_G(u)\) other than the identity such that \(gu = u\). If \(g \neq I\), then \(g = Rot_u(\theta)\) where \(\theta\) is not a multiple of \(2\pi\). Recall, that a rotation in space only fixes vectors along the axis of rotation. But since we constrained this to unit vectors, then it will only fix the unit vector on the axis of rotation. So \(g\) fixes only \(\pm u\). As a consequence, poles come in pairs. If \(u\) is a pole, then \(-u\) is a pole too. So again, a pole is a unit vector in \(\mathbf{R}^3\) that gets fixed by some rotation in \(G\). In other words, \(u\) is a unit vector along the axis of some rotation in \(G\). As a consequence, every non-identity element in \(G\) can contribute 2 poles to the set of poles. If we let \(X\) to be the set of poles, then</p>
<div>
$$
\begin{align*}
|X| \leq 2(|G| - 1) \quad \text{(-1 for the identity)}
\end{align*}
$$
</div>
<p>So the set of poles \(X\) is the collection of unit vectors along the axes of rotations of elements in the group \(G\). \(G\) acts on the subset \(X \in \mathbf{R}^3\). For example, if \(g \in G\) and \(x \in X\), then we know that \(gx\) and \(g^{-1}x\) are both in \(X\). Why?
<br />
If \(x\) is a pole, we want to show that \(gx\) is a pole. Since \(x\) is a pole, then we know that \(|\text{Stab}_G(x)| \geq 2\). But we also know that \(\text{Stab}(gx) = g\text{Stab}(x)g^{-1}\) (By HW10, Problem 2). As a consequence, they both have the same size. Therefore, \(|\text{Stab}_G(gx)| \geq 2\). Therefore, \(gx\) is also a pole.
<br />
<br />
Now, since \(G\) acts on the set of poles \(X\), then the set \(X\) decomposes have orbits \(O_1, O_2,...,O_k \subset X\) of the \(G\) action. In the following table, we’re going to list the orbits in decreasing order of their sizes so \(|O_1| \geq |O_2| ... \geq |O_k|\). We will also define \(c_i\) to be the size of the stabilizer in \(G\) of \(x\) for any \(x \in O_i\). Because elements in the same orbit have stabilizers of the same order. We also know that \(|O| = \frac{n}{c_i}\) by the orbits stabilizer theorem.</p>
<ol>
	<li>\(Z_n\): Cyclic groups are rotations around a single axis. We have 2 poles for each direction. The poles end up being in separate orbits. So each orbit is of size 1. So the corresponding stabilizer groups are the whole groups</li>
	<!---------------------------------->
	<li>\(D_m\): \(D_m\) is acting on the set of poles here where a pole is a unit vector such that there is a non-trivial element of \(D_m\) that fixes it. The rotations in \(D_m\) are rotations around the \(z-\)axis. The unit vector along the \(z-\)axis is fixed by all rotations in \(D_m\). So this unit vector \((0,0,1)\) must be in the set of poles and its stabilizer consists of all the rotations in \(D_m\). So it has size \(m\). By the orbits-stabilizer theorem, the size of the orbit is 2. We know it's 2 because \(-u\) is also a pole.
	<br /><br />
	For the reflections, we know from assignment 10 that they are in one orbit. For the flips, these are axes in the \(xy\) plane that we flip around. Each flip, will have two poles associated to them. They live in orbits each of size 2. Notice here that the orbits will be of size \(m\).</li>
	<!---------------------------------->
	<li>\(A_4\): For the regular polyhedra, we also have 3 orbits in each. For the tetrahedron, the orbits are of size 6 (midpoint of edges), 4 (centroid of faces) and 4 (vertex-vertex). We can divide by the $$|G|$$ to get the sizes for the stabilizer. Note here the the number of poles is basically the sum of the orbit sizes since the action breaks the set into disjoint orbits.</li>
</ol>

<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>Groups \(G\)</td>
    <td>\(n = |G|\)</td>
    <td>\(k = \) orbits</td>
    <td>\(|O_1| \geq ... \geq |O_k|\)</td>
    <td>\(c_i \leq ... \leq c_k\)</td>
  </tr>
  <tr>
	<td>\(\{I\}\)</td>
	<td>1</td>
	<td>1</td>
	<td>0</td>
	<td></td>
  </tr>
  <tr>
	<td>\(\mathbf{Z}_n, n \geq 2\)</td>
	<td>\(n\)</td>
	<td>2</td>
	<td>1,1</td>
	<td>\(n,n\)</td>
  </tr>
  <tr>
	<td>\(D_m, m \geq 2\)</td>
	<td>\(2m\)</td>
	<td>3</td>
	<td>\(m,m,2\)</td>
	<td>\(2,2,m\)</td>
  </tr>
  <tr>
	<td>\(A_4\)</td>
	<td>\(12\)</td>
	<td>3</td>
	<td>\(6,4,2\)</td>
	<td>\(2,3,3\)</td>
  </tr>
  <tr>
	<td>\(S_4\)</td>
	<td>\(24\)</td>
	<td>3</td>
	<td>\(12,8,6\)</td>
	<td>\(2,3,4\)</td>
  </tr>
  <tr>
	<td>\(A_5\)</td>
	<td>\(60\)</td>
	<td>3</td>
	<td>\(30,20,12\)</td>
	<td>\(2,3,5\)</td>
  </tr>
<!-------------------->
</table>
<p>So now we want to know if there is another subgroup \(G\) missing from the table. Let’s use the burnside theorem. We have an action by \(G \leq SO(3)\) on the set of poles \(X\). We don’t know what \(G\) is. The burnside theorem says that number of orbits of this action is</p>
<div>
$$
\begin{align*}
\frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)|
\end{align*}
$$
</div>
<p>where \(\text{Fix}(g) = \{x \in X \ | \ gx = x \} \subseteq X\) is the set of elements fixed by \(g\). We’ve calculate the number of orbits in the above table. What about \(\text{Fix}(e)\)? It’s everything in \(X\) so</p>
<div>
$$
\begin{align*}
\text{Fix}(e) = |G| = |O_1| + |O_2| + ... + |O_k|
\end{align*}
$$
</div>
<p>What about any other element in \(G\)? It’s not the identity element so it’s a rotation around some axis. Therefore, it fixes two unit vectors along the axis \(\pm u\) where \(g = Rot_u(\theta)\). In particular, \(\text{Fix}(g) = 2\). So now let’s apply the formula to see that</p>
<div>
$$
\begin{align*}
k = \frac{1}{n}\big[ |O_1| + ... + |O_k|  + 2(n-1) \big]
\end{align*}
$$
</div>
<p>We can simplify this because we know that \(|O_i| = \frac{n}{c_i}\). Therefore</p>
<div>
$$
\begin{align*}
k &amp;= \frac{1}{n}\big[ \frac{n}{c_1} + ... + \frac{n}{c_k} + 2(n-1) \big] \\
&amp;= c_1 + ... + c_k + 2 - \frac{2}{n}. 
\end{align*}
$$
</div>
<p>We know \(k \geq 0\). We know \(n \geq 1\). We also know that \(c_1,...,c_k\) are integers and they’re all greater than 2. Finally, \(c_i \ | \ n\) because each \(c_i\) is a subgroup. With these constraints, the only solutions are the rows in the table we constructed!!! the is the main step in classifying the finite subgroups in \(SO(3)\).
<br />
<br />
So how do we solve it? For example, say we know we have two orbits each of size 1, so we have two poles. This means that they must be opposite to each other since poles come in pairs. Therefore, everything is rotation around a single axis. Therefore, it must be cyclic. 
<br />
<br />
Step 1: We will show that we can only have 0, 2 or 3 orbits so \(k \in \{0,2,3\}\). Re-write the equations such that</p>
<div>
$$
\begin{align*}
k &amp;= \frac{1}{n}\big[ \frac{n}{c_1} + ... + \frac{n}{c_k} + 2(n-1) \big] \\
&amp;= c_1 + ... + c_k + 2 - \frac{2}{n} \\
 (1 - \frac{1}{c_i}) + ... + (1 - \frac{1}{c_k}) &amp;= 2 - \frac{2}{n}.
\end{align*}
$$
</div>
<p>The observation is that \(\frac{2}{n}\) can be between 0 and less than 2. The right hand side, each \(1 - \frac{1}{c}\) needs to be in \([\frac{1}{2},1)\). So \(k\) can’t be 1. If \(k = 4\), then the left hand side is greater than 2 but the right hand side must be less than 2. So \(k\) can’t be 4.
<br />
<br />
Step 2: If \(k = 0\), this gives us \(n = 1\). That’s the trivial group.
<br />
<br />
Step 3: Re-write the equation so that</p>
<div>
$$
\begin{align*}
\frac{2}{n} = \frac{1}{c_1} + \frac{1}{c_2}
\end{align*}
$$
</div>
<p>This shows …. [TODO .. I’m lost now]
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Today, we’ll analyze finite subgroups of \(SO(3)\). Let’s describe the ones we already know about The trivial subgroup \(\{I\}\). Subgroups that are cyclic so these subgroups are isomorphic to \(\mathbf{Z}_n\). For each \(n\), we have infinitely many subgroups. For example, there are infinitely many cyclic subgroups of order 3 in \(SO(3)\). Pick any axis and consider the rotation by angle \(\frac{2\pi}{3}\) around this axis. This makes a subgroup of order 3. So you can get a new subgroup for whatever axis you pick. Therefore, we have infinitely many. In fact, all these subgroups that are rotations by \(\frac{2\pi}{3}\) are conjugate (see the lecture before last. If we have a rotation by \(\theta\) around a unit vector \(v\) and we have the same rotation around another unit vector \(u\), then we change \(u\) to \(v\) by some rotation \(A \in SO(3)\). Subgroups isomorphic to the dihedral groups \(D_n\) where \(n \geq 2\). These are symmetries of a regular \(n-\)gon. Pick any \(n-\) and fix one of the vertices on the \(x-\)axis. The symmetries of the \(n-\) is a group isomorphic to \(D_n\). Symmetry group of the tetrahedron which is isomorphic to \(A_4\). Symmetry group of the cube/octahedron which is isomorphic to \(S_4\). Symmetry group of the icosahedron/dodecahedron which is isomorphic to \(A_5\). In fact we have the following theorem Theorem Every finite subgroup of \(SO(3)\) is in the list above. Proof \(SO(3)\) acts on \(\mathbf{R}^3\) (we multiply a matrix in \(SO(3)\) by a vector in \(\mathbf{R}^3\)). So now suppose that \(G\) is a subgroup of \(SO(3)\). \(G\) acts on \(\mathbf{R}^3\). We’ll focus on the pole subset in \(\mathbf{R}^3\). What is a poll? A pole for \(G \subseteq SO(3)\) is a unit vector \((u) \in \mathbf{R}^3\) where \(\lVert u \rVert = 1\) such that \(|Stab_G(u)|\geq 2\). In other words, there exists an element \(g\) in \(\text{Stab}_G(u)\) other than the identity such that \(gu = u\). If \(g \neq I\), then \(g = Rot_u(\theta)\) where \(\theta\) is not a multiple of \(2\pi\). Recall, that a rotation in space only fixes vectors along the axis of rotation. But since we constrained this to unit vectors, then it will only fix the unit vector on the axis of rotation. So \(g\) fixes only \(\pm u\). As a consequence, poles come in pairs. If \(u\) is a pole, then \(-u\) is a pole too. So again, a pole is a unit vector in \(\mathbf{R}^3\) that gets fixed by some rotation in \(G\). In other words, \(u\) is a unit vector along the axis of some rotation in \(G\). As a consequence, every non-identity element in \(G\) can contribute 2 poles to the set of poles. If we let \(X\) to be the set of poles, then $$ \begin{align*} |X| \leq 2(|G| - 1) \quad \text{(-1 for the identity)} \end{align*} $$ So the set of poles \(X\) is the collection of unit vectors along the axes of rotations of elements in the group \(G\). \(G\) acts on the subset \(X \in \mathbf{R}^3\). For example, if \(g \in G\) and \(x \in X\), then we know that \(gx\) and \(g^{-1}x\) are both in \(X\). Why? If \(x\) is a pole, we want to show that \(gx\) is a pole. Since \(x\) is a pole, then we know that \(|\text{Stab}_G(x)| \geq 2\). But we also know that \(\text{Stab}(gx) = g\text{Stab}(x)g^{-1}\) (By HW10, Problem 2). As a consequence, they both have the same size. Therefore, \(|\text{Stab}_G(gx)| \geq 2\). Therefore, \(gx\) is also a pole. Now, since \(G\) acts on the set of poles \(X\), then the set \(X\) decomposes have orbits \(O_1, O_2,...,O_k \subset X\) of the \(G\) action. In the following table, we’re going to list the orbits in decreasing order of their sizes so \(|O_1| \geq |O_2| ... \geq |O_k|\). We will also define \(c_i\) to be the size of the stabilizer in \(G\) of \(x\) for any \(x \in O_i\). Because elements in the same orbit have stabilizers of the same order. We also know that \(|O| = \frac{n}{c_i}\) by the orbits stabilizer theorem. \(Z_n\): Cyclic groups are rotations around a single axis. We have 2 poles for each direction. The poles end up being in separate orbits. So each orbit is of size 1. So the corresponding stabilizer groups are the whole groups \(D_m\): \(D_m\) is acting on the set of poles here where a pole is a unit vector such that there is a non-trivial element of \(D_m\) that fixes it. The rotations in \(D_m\) are rotations around the \(z-\)axis. The unit vector along the \(z-\)axis is fixed by all rotations in \(D_m\). So this unit vector \((0,0,1)\) must be in the set of poles and its stabilizer consists of all the rotations in \(D_m\). So it has size \(m\). By the orbits-stabilizer theorem, the size of the orbit is 2. We know it's 2 because \(-u\) is also a pole. For the reflections, we know from assignment 10 that they are in one orbit. For the flips, these are axes in the \(xy\) plane that we flip around. Each flip, will have two poles associated to them. They live in orbits each of size 2. Notice here that the orbits will be of size \(m\). \(A_4\): For the regular polyhedra, we also have 3 orbits in each. For the tetrahedron, the orbits are of size 6 (midpoint of edges), 4 (centroid of faces) and 4 (vertex-vertex). We can divide by the $$|G|$$ to get the sizes for the stabilizer. Note here the the number of poles is basically the sum of the orbit sizes since the action breaks the set into disjoint orbits.]]></summary></entry><entry><title type="html">Lecture 29: Burnside Formula</title><link href="http://localhost:4000/jekyll/update/2025/02/21/math417-29-burnside-formula.html" rel="alternate" type="text/html" title="Lecture 29: Burnside Formula" /><published>2025-02-21T00:01:36-08:00</published><updated>2025-02-21T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/21/math417-29-burnside-formula</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/21/math417-29-burnside-formula.html"><![CDATA[<p>Today we will discuss a formula for counting orbits calling the burnside formula. The book illustrates this with a necklace beads counting example. So how many necklaces are there with two orange, two blue beads? If we count naively, we have 4 slots. The two blue beads will go into two of the slots. So there are \(\binom{4}{2} = 6\) to do this. Then the orange beads will get the remaining two spots. So</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/1.png" width="90%" class="center" /></p>
<div>
$$
\begin{align*}
X = \{OOBB, OBOB, OBBO, BOBO, BOOB, BBOO\}
\end{align*}
$$
</div>
<p>But that’s not right. There are only 2 distinct elements. This in fact is the same as counting orbits of a group action. 2 here is the number of orbits of a group action. The group action is</p>
<div>
$$
\begin{align*}
G = \{ D_4 \}, |G| = 8 \quad \text{acts on } X = \text{ The set of linear arrangements where $|X|=6$}
\end{align*}
$$
</div>
<p>To see this, imagine the beads moving by 90 degrees (which is the rotation \(r\)) to get the next shape in the picture. The symmetries of the beads are the same as the symmetries of the dihedral group. We can also flip the necklace on the \(x\)-axis which is the same as the flip \(j \in D_4\). Now, the orbits of this action are as follows</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/2.png" width="90%" class="center" /></p>
<div>
$$
\begin{align*}
A &amp;= \{OOBB, OBBO, BOOB, BBOO\} \\
B &amp;= \{OBOB, BOBO\}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>How many necklaces with 9 beads: 4 red, 3 white, 2 yellow? This isn’t an easy question. But there is an easier question which is counting the linear arrangements of the 9 beads with the colors we have. Let \(X\) be the set of linear arrangements of 9 beads with 4 red, 3 while, 2 yellow. If they were all of different colors, then there will be \(9!\) arrangements. But only have 3 colors so some of the beads are indistinguishable. Therefore, we have</p>
<div>
$$
\begin{align*}
|X| = \frac{9!}{4!3!2!} = 1260.
\end{align*}
$$
</div>
<p>So it’s impossible to physically see which arrangements will unique. We said earlier that this is equivalent to having an action by \(D_9\) on the set \(X\). So we just to count the orbits of this action with something called the burnside formula.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Burnside Formula</b></h4>
<p>First, we need the following notation</p>
<div class="mintheaderdiv">
Action by \(G\) on \(X\)
</div>
<div class="mintbodydiv">
Given \(g \in G\), let \(\text{Fix}(g) = \{x \in X \ | \ gx = x \} \subseteq X \) be the set of elements fixed by \(g\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
For example \(Fix(e) = X\). Now,
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Burnside Formula
</div>
<div class="peachbodydiv">
If \(G\) acts on a set \(X\), and \(|G| &lt; \infty, |X| &lt; \infty\). Then, the number of orbits is
$$
\begin{align*}
\frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)|
\end{align*}
$$
</div>
<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let’s apply the formula on Example 1. We had \(G = D_4 = \{e,r,r^2,j,rj,r^2j\}\) where \(|G| = 8\) and \(|X| = 6\). For example, take \(r\). We want to know the arrangements that are fixed by \(r\) (the rotation by 90 degrees). What does it mean for an arrangement to be fixed by \(r\). It means that the arrangement when rotated by 90 degrees, it will still be the same arrangement. Observe what happens to the following arrangement.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/3.png" width="55%" class="center" /></p>
<p>This arrangement is not fixed by \(r\). In fact, all the beads have to have the same color for \(r\) to be able to fix them. So the fix set is zero for \(r\). Note here that \(r^{-1} = r^{3}\) will have the same result for the same reason.
<br />
<br />
What about \(r^2\) so a rotation by 180 degrees. Observe what happens to the following arrangement</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/4.png" width="55%" class="center" /></p>
<p>So the beads on the opposite sides have to have the same color for the arrangement to work. We have two arrangements exactly of this kind. So \(|Fix(r^2)| = 2\)
<br />
<br />
What about \(j\)? Let \(j\) be the flip across the \(x\)-axis. Here, for the arrangement to be fixed, notice that the beads on the axis will not change. So we just need the top and bottom beads to have the same color. The number of arrangements with this condition is 2. In fact, \(r^2j\) (flip around the \(y\)-axis) will have the same number of arrangement fixed.
<br />
<br />
\(rj\) is a flip around the axis \(x=y\). So the beads on either side of the following axis must have the same color just like following picture</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/5.png" width="25%" class="center" /></p>

<p>In summary,</p>
<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>\(g\)</td>
    <td>\(|\text{Fix}(g)|\)</td>
  </tr>
  <tr>
	<td>\(e\)</td>
	<td>6</td>
  </tr>
  <tr>
	<td>\(r, r^3\)</td>
	<td>0</td>
  </tr>
  <tr>
    <td>\(j, r^2j\)</td>
	<td>2</td>
  </tr>
  <tr>
	<td>\(rj, r^3j\)</td>
	<td>2</td>
  </tr>
</table>
<p>So now let’s apply the burnside formula</p>
<div>
$$
\begin{align*}
\text{number of orbits } &amp;= \frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)| \\
                         &amp;= \frac{1}{8}(6+0+2+0+2+2+2+2) \\
						 &amp;= \frac{16}{8} = 2.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Here, we had \(G = D_9 = \{e, r, r^2, ..., r^8, j, rj, ..., r^8j\}\). Recall that \(|X| = 1260\). Doing the same analysis</p>
<ul>
	<li>\(e\): This is just the entire group so \(|\text{Fix}(e)| = 1260\).</li>
	<li>\(r\): Like before, we can't fix anything with a 90 degrees rotation. All the beads have to have the same color. This is the same for its inverse \(r^8\).</li>
	<li>\(r^2\): Note that \(r^2\) has order 9 as well. No possible arrangements here as well. In fact, the same thing happens with \(r^4\) and its inverse \(r^5\).</li>
	<li>\(r^3\): We have 9 beads. Here we need every third bead to match colors. So like in the following picture
	<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/6.png" width="45%" class="center" /></p>
	So vertices 1,4 and 7 have to have the same color. Vertices 2, 5 and 8 have to have the same color. Same for vertices 3, 6 and 9. But we recall that we have 4 red beads, 3 white beads, and 2 yellow beads.
	</li>
	<li>\(j\): A flip around the \(x\)-axis. So now there is a black bead fixed by the access itself and the remaining beads must match as follows
	<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/7.png" width="45%" class="center" /></p>
	We have 4 red beads, 3 white beads and 2 yellow. So the fixed bead must be a white bead. So now we have the remaining two whites. We need to put one white bead in one of the 4 slots (say in the bottom). We have 4 slots so 4 choices for the second white bead. Next, if we place the yellow bead, we'll have 3 slots available to choose from. Lastly, we have 2 red beads so they must go in the remaining slots. So 12 choices total.
	</li>
</ul>

<p><br /></p>
<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>\(g\)</td>
    <td>\(|\text{Fix}(g)|\)</td>
  </tr>
  <tr>
	<td>\(e\)</td>
	<td>1260</td>
  </tr>
  <tr>
	<td>\(r, r^8\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^2, r^7\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^3, r^6\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^4, r^5\)</td>
	<td>0</td>
  </tr>
  <!-------flips--------->
  <tr>
    <td>\(j, r^2j,...,r^8j\)</td>
	<td>12</td>
  </tr>
</table>
<p>So now let’s apply the burnside formula</p>
<div>
$$
\begin{align*}
\text{number of orbits } &amp;= \frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)| \\
                         &amp;= \frac{1}{18}(1260+9(12)) \\
						 &amp;= \frac{1374}{18} = 76.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Proof of Burnside Formula</b></h4>
<p>Let \(F = \{(g,x) \ | \ g \in G, x \in X, gx = x\}\). (So we’re writing it differently than before but it’s the same. We’re just counting any element that gets fixed by any \(g\). So all of them). We’re going to count this set in two different ways</p>
<ol>
	<li>\(|F| = \sum_{g \in G} | x \in X \ | \ gx = x \} = \sum_{g \in G} |\text{Fix}(g)| \)</li>
	<li>\(|F| = \sum_{x \in X} | g \in G \ | \ gx = X \} = \sum_{x \in X} |\text{Stab}(g)| \)</li>
</ol>
<p>From this, observe that</p>
<div>
$$
\begin{align*}
\frac{1}{|G|} \sum_{g \in G} |\text{Fix}(g)| &amp;= \frac{1}{|G|} \sum_{x \in X} |\text{Stab}(g)| \\
                                             &amp;= \sum_{x \in X} \frac{|\text{Stab}(g)|}{|G|}.
\end{align*}
$$
</div>
<p>By the Orbit-Stabilizer Theorem, \(|O(x)| = \frac{|G|}{\text{|Stab(x)}|}\). Therefore</p>
<div>
$$
\begin{align*}
\frac{1}{|G|} \sum_{g \in G} |\text{Fix}(g)| &amp;= \frac{1}{|G|} \sum_{x \in X} |\text{Stab}(g)| \\
                                             &amp;= \sum_{x \in X} \frac{|\text{Stab}(g)|}{|G|} \\
											 &amp;= \sum_{x \in X} \frac{1}{|O(x)|} \\
											 &amp;= \sum_{\text{Orbits}} \sum_{x \in O} \frac{1}{|O|} \\
											 &amp;= \text{number of orbits}
											 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 3 (Variant of Example 2)</b></h4>
<p>Consider example 2 again. We had a fixed number of beads (4 red, 3 white, 2 yellow). Now, suppose that we don’t have a constraint on the count for each color. We just want a necklace made of 3 colors and 9 beads. Then \(G = D_9\) acts on \(X\) which is the linear arrangements of beads of 3 colors. What is the size of \(X\)? The first bead can be any of the three colors, the second bead can be any of the three colors and so on. So</p>
<div>
$$
\begin{align*}
|X| = 3^9 = 19,683								 
\end{align*}
$$
</div>

<p>Here, we had \(G = D_9 = \{e, r, r^2, ..., r^8, j, rj, ..., r^8j\}\). Recall that \(|X| = 1260\). Doing the same analysis</p>
<ul>
	<li>\(e\): This is just the entire group so \(|\text{Fix}(e)| = 1260\).
	</li>
	<li>\(r\): Like before, we can't fix anything with a 90 degrees rotation. All the beads have to have the same color. This is the same for its inverse \(r^8\) and as well as \(r^2, r^4, r^5\).
	</li>
	<li>\(r^3\): We have 9 beads. Again we need every third bead to have the same color. We have three groups. Each group will need to be of the same color and it can be any color so 3 choices for every group. Therefore, we have \(3^3 = 27\) choices.
	</li>
	<li>\(j\): A flip around the \(x\)-axis. Like before, we had a bead fixed on the axis and 4 other groups of two beads each that needed to match colors. Each of these has a choice of 3 colors so \(3^5\) choices.
	</li>
</ul>

<p><br /></p>
<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>\(g\)</td>
    <td>\(|\text{Fix}(g)|\)</td>
  </tr>
  <tr>
	<td>\(e\)</td>
	<td>\(3^9\)</td>
  </tr>
  <tr>
	<td>\(r, r^8, r^2, r^4, r^5\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^3, r^6\)</td>
	<td>\(3^3 = 27\)</td>
  </tr>
  <!-------flips--------->
  <tr>
    <td>\(j, r^2j,...,r^8j\)</td>
	<td>\(3^5\)</td>
  </tr>
</table>
<p>So now let’s apply the burnside formula</p>
<div>
$$
\begin{align*}
\text{number of orbits } &amp;= \frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)| \\
                         &amp;= \frac{1}{18}(3^9 + 6*3 + 2*3^2 + 9*3^5) \\
						 &amp;= \frac{1374}{18} = 1219.
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Today we will discuss a formula for counting orbits calling the burnside formula. The book illustrates this with a necklace beads counting example. So how many necklaces are there with two orange, two blue beads? If we count naively, we have 4 slots. The two blue beads will go into two of the slots. So there are \(\binom{4}{2} = 6\) to do this. Then the orange beads will get the remaining two spots. So $$ \begin{align*} X = \{OOBB, OBOB, OBBO, BOBO, BOOB, BBOO\} \end{align*} $$ But that’s not right. There are only 2 distinct elements. This in fact is the same as counting orbits of a group action. 2 here is the number of orbits of a group action. The group action is $$ \begin{align*} G = \{ D_4 \}, |G| = 8 \quad \text{acts on } X = \text{ The set of linear arrangements where $|X|=6$} \end{align*} $$ To see this, imagine the beads moving by 90 degrees (which is the rotation \(r\)) to get the next shape in the picture. The symmetries of the beads are the same as the symmetries of the dihedral group. We can also flip the necklace on the \(x\)-axis which is the same as the flip \(j \in D_4\). Now, the orbits of this action are as follows]]></summary></entry><entry><title type="html">Lecture 28: Group Actions</title><link href="http://localhost:4000/jekyll/update/2025/02/20/math417-28-group-actions.html" rel="alternate" type="text/html" title="Lecture 28: Group Actions" /><published>2025-02-20T00:01:36-08:00</published><updated>2025-02-20T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/20/math417-28-group-actions</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/20/math417-28-group-actions.html"><![CDATA[<p>In the previous lecture, we introduced actions by a group \(G\) on a set \(X\) which we defined as a homomorphism from the group to a permutation group of the set \(X\).</p>
<div>
$$
\begin{align*}
\varphi: G &amp;\rightarrow Sym(X) \\
         gx &amp;= \phi(g)(x)
\end{align*}
$$
</div>
<p>The set \(X\) that gets acted on, gets partitioned into orbits. An orbit is</p>
<div>
$$
\begin{align*}
O(x) = \{ gx \ | \ g \in G \}
\end{align*}
$$
</div>
<p>We also saw that for each element in the set \(X\) that \(G\) acted on, we have a stabilizer. A stabilizer of an element \(x\) is the set of elements in \(G\) that fix \(x\) so</p>
<div>
$$
\begin{align*}
\text{Stab}(x) = \{ g \in G \ | \ gx = x \}
\end{align*}
$$
</div>
<p>Note here that \(\text{Stab}(x)\) is a subgroup of \(G\). Finally, there are a number of relationships between orbits and stabilizer but the most important one is the orbits/stabilizer theorem which gives a bijection between the orbit of an element and the index of its stabilizer group</p>
<div>
$$
\begin{align*}
|O(x)| = [G: \text{Stab}(x)]
\end{align*}
$$
</div>
<p>We’ve also discussed a few other things like calling an action transitive if there is only one orbit.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Conjugation by \(G\) on \(X = G\)</b></h4>
<p>The next thing we talked about was the conjugation action when the set \(G\) acted on was the set \(G\) itself. This was defined as</p>
<div>
	$$
	\begin{align*}
	c: G &amp;\rightarrow Aut(G) \leq Sym(G) \\
	c(g)(x) &amp;= gxg^{-1}
	\end{align*}
	$$
</div>
<p>We defined this action last lecture from \(G\) to \(Sym(G)\) but in fact, \(c\) is a homomorphism from \(G\) to \(Aut(G)\). \(Aut(G)\) is a subgroup of \(Sym(G)\). The conjugation action is the one that’s really useful for understanding things about a group \(G\). 
<br />
<br />
The <b>orbits</b> of the conjugation action are called <b>conjugacy classes.</b></p>
<div>
	$$
	\begin{align*}
	Cl(x) = \{gxg^{-1} \ | \ g \in G\}.
	\end{align*}
	$$
</div>
<p>We also studied <b>centralizers</b> and these are the <b>centralizers</b> of the action. So</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(x) &amp;= \{g \in G \ | \ gxg^{-1} = x\} \\
	               &amp;= \{g \in G \ | \ gx= xg\} .
	\end{align*}
	$$
</div>
<p>In other words, the centralizers are those elements that commute with the element \(x\). Using the orbit/stabilizer theorem</p>
<div>
	$$
	\begin{align*}
	|Cl(x)| &amp;= [G: \text{Cent}(x)]
	\end{align*}
	$$
</div>
<p>Additionally, the kernel of \(c\) is what we call the center \(c\). It’s the collection of elements that commute with every other element in the group. So</p>
<div>
	$$
	\begin{align*}
	Ker(c) = Z(G) &amp;= \{g \in G \ | \ gh = hg \ \forall h \in G \} \\
	              &amp;= \bigcap_{x \in G} \text{Cent}(x)
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>What is the conjugacy class of the identity? It’s always \(Cl(e) = \{e\}\). This means that the conjugation action is never transitive unless we’re acting on the trivial group. Since if \(G\) contains other elements, then they’ll be in a different orbit.
<br />
<br />
Now, suppose that \(b \in Z(G)\). So \(b\) commutes with every other element in \(G\). What is the conjugacy class of \(b\). It is \(Cl(b) = \{b\}\). Why? since \(b\) commutes with every element \(g\) in \(G\), then \(Cl(b) = \{gbg^{-1} \ | \ g \in G\}\) but</p>
<div>
	$$
	\begin{align*}
	gbg^{-1} = gg^{-1}b = b
	\end{align*}
	$$
</div>
<p>So \(Cl\) contains only \(b\). So if you’re in the center, then the conjugacy class contains only you. The other way direction is true. If \(Cl(g) = \{g\}\), \(g \in Z(G)\). 
<br />
<br />
Consequence: If \(G\) is abelian, then \(Z(G) = G\) and every conjugacy class in \(G\) in its own class. So \(Cl(g) = \{g\}\) for all \(g \in G\). What about the stabilizer or Center\((g)\)? It’s also all of \(G\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Conjugacy Classes in \(SO(3)\)</b></h4>
<p>Let \(G = SO(3) = \{Rot_u(\theta)\}\) where \(u\) is a unit vector. \(Rot_u(\theta)\) is a rotation by angle \(\theta\) around the unit vector \(u\). Recall</p>
<div>
	$$
	\begin{align*}
	 Rot_u(\theta + 2\pi n) &amp;= Rot_u(\theta), n \in \mathbf{Z} \\
	 Rot_{-u}(\theta) &amp;= Rot_{u}(-\theta) \\
	 Rot_{u}(0) &amp;= I
	\end{align*}
	$$
</div>

<p>How do we conjugate elements in this group? Recall from lecture 2? that if \(A \in SO(3)\), then</p>
<div>
	$$
	\begin{align*}
	ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)
	\end{align*}
	$$
</div>
<p>So if we conjugate the rotation by matrix \(A\), we get another rotation but now it’s about the axis \(Au\) instead of \(u\). This is the conjugation formula for elements in \(SO(3)\).
<br />
<br />
So now, what are the conjugacy classes of \(SO(3)\)</p>
<div>
	$$
	\begin{align*}
	Cl(Rot_u(\theta)) &amp;= \{A Rot_u(\theta)A^{-1} \ | \ A \in SO(3)\} \\
	                  &amp;= \{A Rot_{Au}(\theta) \ | \ A \in SO(3)\} \\
	\end{align*}
	$$
</div>
<p>For example. Take the identity mattress. What is the conjugacy class of the identity element?</p>
<div>
	$$
	\begin{align*}
	Cl(Rot_u(0)) = Cl(I) &amp;= I
	\end{align*}
	$$
</div>
<p>What about a non-identity matrix? so \(\theta\) is not zero. Fix a unit vector in space. What vectors do we get if we act on it by \(A\)? Suppose we limit \(\theta\) to \((0, 2\pi)\). Then</p>
<div>
	$$
	\begin{align*}
	Cl(Rot_u(0)) = \{Rot_v(\theta), v \in \mathbf{R}, \lVert v \rVert = 1 \}
	\end{align*}
	$$
</div>
<p>This means that we get all the rotations with the same \(\theta\) but the axis is changing to a different unit vector. 
<br />
<br />
Note: Both \(Rot_u(\theta)\) and \(Rot_u(-\theta)\) are in the same conjugacy class. For example, \(Rot_{e_3}(-120) = Rot_{-e_3}(120)\). The exception is 180 degrees because \(Rot_u(\pi) = Rot_u(-\pi)\). [Side note: Given two rotations \(Rot_{u_1}(\theta)\) and \(Rot_{u_2}(\theta)\) that have the same angle but different axes. They are in the same conjugacy class if \(u_1\) can be rotated into \(u_2\) by an element in \(SO(3)\)].
<br />
<br />
The centralizer of the identity is any element that commutes with the identity element. So it’s easy to see that \(\text{Cent}(x) = SO(3)\). What about the centralizer of \(Rot_u(\theta)\) where \(\theta \in (0,2\pi)\). We’re looking for elements such that \(ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)\).</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(Rot_u(\theta)) = \{A = Rot_u(\alpha) \ | \ \alpha \in \mathbf{R}\} 
	\end{align*}
	$$
</div>
<p>This is because rotations around the same axis commute with each other. There is exception for  \(\pi = 180\) because in addition to all the arbitrary rotations around the same axis, we also get another set which is the 180 rotations around any vector such that \(u \cdot v = 0\).</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(Rot_u(\theta) = \{A = Rot_u(\alpha) \ | \ \alpha \in \mathbf{R}\} \cup \{Rot_v(\pi) \ | \ \text{any } v \text{ such that } v \cdot u = 0\}.
	\end{align*}
	$$
</div>
<p>Study notes since I was confused here. The idea is that for any rotation in \(SO(3)\), we have a conjugation formula that states \(ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)\). This means that rotation about the axis \(u\) by \(\theta\) and then rotating by \(A\) is the same as rotating about the axis \(Au\) by the same angle. This tells us that two rotations in \(SO(3)\) are conjugate if it’s the same angle and if they can be related by some matrix \(A\). So now the questions is? given some unit vector \(u\)? and given a rotation around \(u\) by angle \(\theta\). Are we guaranteed to find another vector \(v\) such that \(ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)\)? The answer is yes for \(SO(3)\) specifically. For any two unit vectors \(u\) and \(v\), there exists an \(A\) such that \(Au = v\). Therefore, if we keep the same angle, then all the rotations by another other vector \(v\) around the same angle are in the same conjugacy class as the rotation around the original vector by \(\theta\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Symmetries of the Cube \(\leq SO(3)\)</b></h4>
<p>As a reminder, we did this in the previous lecture</p>
<div>
<table style="margin: 20px auto;">
  <tr>
    <td>Axis (Cube)</td>
	<td># of Axes</td>
    <td>Angle of Rotation</td>
	<td>Order of Symmetry</td>
	<td># of Symmetries of This Type</td>
    <td>Axis (Octahedron)</td>
	<td>Cycle Type</td>
  </tr>
  <tr>
    <td>Vertex/Vertex</td>
	<td>4</td>
    <td>\(\pm \frac{2\pi}{3}\)</td>
    <td>3</td>
	<td>8</td>
	<td>Faces/Face</td>
	<td>3-cycle</td>
  </tr>
  <tr>
    <td>Edge/Edge</td>
	<td>6</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>6</td>
	<td>Edge/Edge</td>
	<td>2-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>3</td>
    <td>\(\pm \frac{2\pi}{4}\)</td>
    <td>4</td>
	<td>6</td>
    <td>Vertex/Vertex</td>
	<td>4-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>3</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>3</td>
	<td></td>
	<td>2+2</td>
  </tr>
  <tr>
    <td>Identity Rotation</td>
    <td></td>
    <td></td>
	<td>1</td>
	<td>1</td>
	<td></td>
	<td>e</td>
  </tr>
</table>
</div>
<p>For example, for the diagonals between vertices. We have 4 diagonals but for each diagonal we could rotate by \(\frac{2\pi}{3}\) or \(-\frac{2\pi}{3}\). So we have 8 symmetries of that kind. We also have the axes that goes through opposite edges. Those have to be of 180 degrees rotations. We have 6 of them. Finally we have the ones that goes through the faces. We have two types of these. The quarter turns which can go \(\frac{\pi}{2}\) in either clockwise or counter clockwise and then we have the half turn rotations which are 180 degrees. Since we have 6 faces, then we have 3 axes. For the quarter turns, we have (2*3) symmetries and for the half turns, we have \(3\) symmetries.
<br />
<br />
In fact, these are exactly the conjugacy classes of the cube (5 classes). But we said earlier that rotations with the same angle but different axis are all going to be in the same conjugacy class in \(SO(3)\). So why do we have for example, two classes that have rotations by 180 degrees here (look at the edge/edge and face/face). While they are conjugate in \(SO(3)\), they are not conjugate in \(G\) (cube). Reminder, the conjugation formula is</p>
<div>
	$$
	\begin{align*}
	ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)
	\end{align*}
	$$
</div>
<p>What we want is an \(A\) such that it sends \(u\) to \(Au\). We don’t have that in the cube.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Conjugacy Classes of \(\S_n\)</b></h4>
<p>We have formula already for this</p>
<div>
	$$
	\begin{align*}
	g(a_1 \ a_2 \ ... \ g_k)g^{-1} = (g(a_1) \ g(a_2) \ ... \ g(a_k))
	\end{align*}
	$$
</div>
<p>So we get another \(k-\)cycle.
<br />
<br />
Example: suppose $\sigma = (1 \ 2 \ 3)(4 \ 5)\(and we want to conjugate by\)g = (1 \ 5 \ 2 \ 4 \ 6)$$. Then</p>
<div>
	$$
	\begin{align*}
	g(1 \ 2 \ 3)(4 \ 5)g^{-1} &amp;= g(1 \ 2 \ 3)g^{-1 } \circ g(4 \ 5)g^{-1} \\
	                          &amp;= (g(1) \ g(2) \ g(3)) \circ (g(4) \ g(5)) \\
							  &amp;= (5 \ 4 \ 3)(6 \ 2)
	\end{align*}
	$$
</div>
<p>Observation: If we start with a product of disjoint cycles and we conjugate by some element, then we get the same exact cycle type (a product of the same cycle type). Conjugation preserves the cycle type. Conversely if we have two products of the same cycle type, we can find some \(g\) takes us from one one element to the other. This leads to the following fact</p>
<div class="peachheaderdiv">
Fact
</div>
<div class="peachbodydiv">
The conjugacy class of an element \(g\) is all elements of the same cycle type of \(g\).
</div>
<p><br />
For example, consider \(S_4\). Take the cycle type \((a \ b)\). There are \(\binom{4}{2} = 6\) choices for the two elements. We can arrange them in \(2!\) but we need to divide by 2 since \((a \ b) = (b \ a)\). Therefore, we have \(6 * 2! / 2 = 6\) distinct permutations. We can consider \((a \ b \ c)\) next. We have 4 choices for the elements but we need to multiply by \(3!\) and divide by 3 to get 8. For \((a \ b)(c \ d)\). We have \(binom{4}{2}\) for the first cycle but then we need to multiply by \(2!\) and divide by 2. So we get 6 elements. For the second cycle, we have \(\binom{2}{2}*2!/2 = 1\) element. But then we also need to divide by \(2\) since \((a \ b)(c \ d) = (c \ d)(a \ b)\). So the total is \(6 * 1 / 2 = 3\).
<br /></p>
<div>
<table style="margin: 20px auto;">
  <tr>
    <td>Cycle</td>
	<td>Number</td>
    <td>Cycle Type</td>
  </tr>
  <tr>
    <td>\((a \ b \ c)\)</td>
	<td>8</td>
    <td>\(3+1\)</td>
  </tr>
  <tr>
    <td>\((a \ b)\)</td>
	<td>6</td>
    <td>\(2+1+1\)</td>
  </tr>
  <tr>
    <td>\((a \ b \ c \ d)\)</td>
	<td>6</td>
    <td>\(4\)</td>
  </tr>
  <tr>
    <td>\((a \ b)(c \ d)\)</td>
	<td>3</td>
    <td>\(2+2\)</td>
  </tr>
  <tr>
    <td>Identity</td>
    <td>1</td>
    <td>\(1 + 1 + 1 + 1\)</td>
  </tr>
</table>
</div>
<p>In fact this group is isomorphic to the symmetry group of the cube that we did earlier.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In the previous lecture, we introduced actions by a group \(G\) on a set \(X\) which we defined as a homomorphism from the group to a permutation group of the set \(X\). $$ \begin{align*} \varphi: G &amp;\rightarrow Sym(X) \\ gx &amp;= \phi(g)(x) \end{align*} $$ The set \(X\) that gets acted on, gets partitioned into orbits. An orbit is $$ \begin{align*} O(x) = \{ gx \ | \ g \in G \} \end{align*} $$ We also saw that for each element in the set \(X\) that \(G\) acted on, we have a stabilizer. A stabilizer of an element \(x\) is the set of elements in \(G\) that fix \(x\) so $$ \begin{align*} \text{Stab}(x) = \{ g \in G \ | \ gx = x \} \end{align*} $$ Note here that \(\text{Stab}(x)\) is a subgroup of \(G\). Finally, there are a number of relationships between orbits and stabilizer but the most important one is the orbits/stabilizer theorem which gives a bijection between the orbit of an element and the index of its stabilizer group $$ \begin{align*} |O(x)| = [G: \text{Stab}(x)] \end{align*} $$ We’ve also discussed a few other things like calling an action transitive if there is only one orbit. Conjugation by \(G\) on \(X = G\) The next thing we talked about was the conjugation action when the set \(G\) acted on was the set \(G\) itself. This was defined as $$ \begin{align*} c: G &amp;\rightarrow Aut(G) \leq Sym(G) \\ c(g)(x) &amp;= gxg^{-1} \end{align*} $$ We defined this action last lecture from \(G\) to \(Sym(G)\) but in fact, \(c\) is a homomorphism from \(G\) to \(Aut(G)\). \(Aut(G)\) is a subgroup of \(Sym(G)\). The conjugation action is the one that’s really useful for understanding things about a group \(G\). The orbits of the conjugation action are called conjugacy classes. $$ \begin{align*} Cl(x) = \{gxg^{-1} \ | \ g \in G\}. \end{align*} $$ We also studied centralizers and these are the centralizers of the action. So $$ \begin{align*} \text{Cent}(x) &amp;= \{g \in G \ | \ gxg^{-1} = x\} \\ &amp;= \{g \in G \ | \ gx= xg\} . \end{align*} $$ In other words, the centralizers are those elements that commute with the element \(x\). Using the orbit/stabilizer theorem $$ \begin{align*} |Cl(x)| &amp;= [G: \text{Cent}(x)] \end{align*} $$ Additionally, the kernel of \(c\) is what we call the center \(c\). It’s the collection of elements that commute with every other element in the group. So $$ \begin{align*} Ker(c) = Z(G) &amp;= \{g \in G \ | \ gh = hg \ \forall h \in G \} \\ &amp;= \bigcap_{x \in G} \text{Cent}(x) \end{align*} $$ Examples What is the conjugacy class of the identity? It’s always \(Cl(e) = \{e\}\). This means that the conjugation action is never transitive unless we’re acting on the trivial group. Since if \(G\) contains other elements, then they’ll be in a different orbit. Now, suppose that \(b \in Z(G)\). So \(b\) commutes with every other element in \(G\). What is the conjugacy class of \(b\). It is \(Cl(b) = \{b\}\). Why? since \(b\) commutes with every element \(g\) in \(G\), then \(Cl(b) = \{gbg^{-1} \ | \ g \in G\}\) but $$ \begin{align*} gbg^{-1} = gg^{-1}b = b \end{align*} $$ So \(Cl\) contains only \(b\). So if you’re in the center, then the conjugacy class contains only you. The other way direction is true. If \(Cl(g) = \{g\}\), \(g \in Z(G)\). Consequence: If \(G\) is abelian, then \(Z(G) = G\) and every conjugacy class in \(G\) in its own class. So \(Cl(g) = \{g\}\) for all \(g \in G\). What about the stabilizer or Center\((g)\)? It’s also all of \(G\). Conjugacy Classes in \(SO(3)\) Let \(G = SO(3) = \{Rot_u(\theta)\}\) where \(u\) is a unit vector. \(Rot_u(\theta)\) is a rotation by angle \(\theta\) around the unit vector \(u\). Recall $$ \begin{align*} Rot_u(\theta + 2\pi n) &amp;= Rot_u(\theta), n \in \mathbf{Z} \\ Rot_{-u}(\theta) &amp;= Rot_{u}(-\theta) \\ Rot_{u}(0) &amp;= I \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 26/27: Group Actions and Orbits</title><link href="http://localhost:4000/jekyll/update/2025/02/19/math417-26-orbits.html" rel="alternate" type="text/html" title="Lecture 26/27: Group Actions and Orbits" /><published>2025-02-19T00:01:36-08:00</published><updated>2025-02-19T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/19/math417-26-orbits</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/19/math417-26-orbits.html"><![CDATA[<div class="mintheaderdiv">
Group Actions
</div>
<div class="mintbodydiv">
A group action by a group \(G\) on a set \(X\) is a homomorphism
$$
\begin{align*}
\varphi: G \rightarrow Sym(X)
\end{align*}
$$
</div>
<p><br />
<!----------------------------------------------------------------------------->
Note here that for every element in the group \(G\) and every element in the set \(X\), we get a function \(\phi(g)\). This function is from \(x \rightarrow X\). Because it is a function, then we can plug in a value \(x\) such that \(\phi(g)(x) \in X\). Moreover,</p>
<div>
$$
\begin{align*}
\varphi(e)(x) &amp;= x \quad \text{ because $\varphi(e)$ is the identity function } \\
\varphi(g_1g_2)(x) &amp;= (\varphi(g_1)\varphi(g_2))(x) \quad \text{because $\phi$ is a homomorphism} \\ 
                &amp;= \varphi(g_1)\big(\varphi(g_2)(x)\big)
\end{align*}
$$
</div>
<p>Additionally, these identities force \(\varphi(g^{-1}) = \varphi(g)^{-1}\) which means that \(\varphi(g)\) is a bijection.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Alternate Notation</b></h4>
<p>Given \(g \in G\) and \(x \in X\), write \(gx\) for \(\varphi(g)(x)\). So now we get a function</p>
<div>
$$
\begin{align*}
G \times X &amp;\rightarrow X \\
(g,x) &amp;\rightarrow gx = \varphi(g)(x)
\end{align*}
$$
</div>
<p>Re-writing the previous identities with the new notation:</p>
<div>
$$
\begin{align*}
ex &amp;= x \quad \forall x \in X \\
(g_1g_2)x &amp;= g_1(g_2x) \quad \forall g_1,g_2 \in G, x \in X
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Tautological action by \(G = Sym(X)\) on X. What is this function?</p>
<div>
$$
\begin{align*}
\varphi: Sym(x) \xrightarrow{\text{identity function}} Sym(x)
\end{align*}
$$
</div>
<p>This just gives us \(\varphi(g)=g\). This is exactly what we use when we work with permutations. A permutation is a function from \(X\) to itself. So for \(g \in G\) and \(x \in X\), we apply it like \(g(x) = gx\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose \(G = SO(3)\) which acts on \(X = \mathbf{R}^3\). The action rule is given by</p>
<div>
$$
\begin{align*}
G \times X &amp;\rightarrow X \\
A,x &amp;\rightarrow Ax
\end{align*}
$$
</div>
<p>We’re defining a homomorphism</p>
<div>
$$
\begin{align*}
SO(3) &amp;\rightarrow Sym(\mathbf{R}^3) \\
\varphi(A)(x) &amp;= Ax
\end{align*}
$$
</div>
<p>\(Sym(\mathbf{R}^3)\) is the group of all bijection maps from \(\mathbf{R}^3\) to itself. The map \(\varphi\) takes a matrix \(A \in SO(3)\) and produces the following function</p>
<div>
$$
\begin{align*}
\varphi(A): \mathbf{R}^3 &amp;\rightarrow \mathbf{R}^3 \\
x &amp;\rightarrow Ax
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Orbits</b></h4>
<p>Next, we’ll define something called an orbit of action by \(G\) on \(X\).</p>
<div class="mintheaderdiv">
Orbit of Action by \(G\) on \(X\) 
</div>
<div class="mintbodydiv">
An Orbit is a subset of \(X\) of the form 
$$
\begin{align*}
O(x) = \{gx \ | \ g \in G\}, \quad \text{for some } x \in X
\end{align*}
$$
</div>
<p><br />
Fact: orbits are equivalence classes of an equivalence relation on \(X\). So \(x \sim y\) if and only if there is some \(g \in G\) such that \(gx = y\). So, \(O(x)O(y) = \emptyset\) or \(O(x) = O(y)\). Therefore, orbits partition \(X\) into pairwise disjoint and nonempty subsets. One more fact: We action is called transitive if there is only one orbit so \(O(x) = X\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Stabilizers</b></h4>
<p>One more defintion.</p>
<div class="mintheaderdiv">
Stabilizer
</div>
<div class="mintbodydiv">
For each \(x \in X\)
$$
\begin{align*}
\text{Stab}(x) = \text{Stab}_G(y) = \{g \in \ | \ gx = x\}
\end{align*}
$$
These are the set of elements of \(G\) that "fix" x.
</div>
<p><br />
Some facts:</p>
<ul>
	<li>\(\text{Stab}(x) \leq G\)</li>
	<li>If \(x, y \in X\) and these elements are related such that for some \(g \in G\), we have \(y = gx\), then \(\text{Stab}(y) = g\text{Stab}(x)g^{-1}\). So if \(x\) and \(y\) are in the same orbit, then their stabilizer subgroups are conjugate.</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Kernel of Action</b></h4>
<p>What is the kernel of a group action? By definition, the kernel contains all the elements in \(g\) such that \(\varphi(x)\) is the identity transformation</p>
<div>
$$
\begin{align*}
\ker(\varphi) = \{g \in G \ | \ gx = x \quad \text{for all } x \in X\}
\end{align*}
$$
</div>
<p>Of course this looks exactly like the definition of a stabilizer that we just did. So,</p>
<div>
$$
\begin{align*}
\ker(\varphi) = \bigcap_{x \in X} \text{Stab}(x) \unlhd G
\end{align*}
$$
</div>
<p>We say that the action is faithful if \(\ker(\varphi) = \{e\}\). This is is the same as saying the homomorphism \(G \rightarrow Sym(x)\) is injective.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Suppose \(G = D_n\) which acts on \(X = \{v_0,v_1,...,v_{n-1}\}\). The action rule is given by the homomorphism</p>
<div>
$$
\begin{align*}
\phi: D_n &amp;\rightarrow Sym(X) = S_n
\end{align*}
$$
</div>
<p>For example. Suppose \(G = D_3 = \{e, r, r^2, j, rj, r^2j\}\) with \(V = \{v_0, v_1, v_2\}\).</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec27/triangle.png" width="45%" class="center" /></p>
<p>Given this setup. What are the orbits of the action? Pick one vertex like \(v_0\). What is the orbit of \(v_0\)? If we start with \(v_0\), then we can reach every other vertex so</p>
<div>
$$
\begin{align*}
rv_0 &amp;= v_1 \\
rv_1 &amp;= r^2v_0 = v_2
\end{align*}
$$
</div>
<p>So the orbit of \(v_0\) is \(O(v_0) = \{v_0, v_1, v_2\}\). So there is only one orbit and in this case the action is transitive. What about the stabilizers? these are the set of elements of \(G\) that fix \(x\). For example, if we take \(v_0\), then we know that \(e\) fixes \(x\) so it’s in the set. Is there any other element such that if we hit \(x\) with it, we get \(x\) back? Notice that \(j\) does that since \(v_0\) is on the flip axis. We can do the same calculation for \(v_1\) and \(v_2\) to see that</p>
<div>
$$
\begin{align*}
\text{Stab}(v_0) &amp;= \{e,j\} \\
\text{Stab}(v_1) &amp;= \{e,r^2j\} \\
\text{Stab}(v_2) &amp;= \{e, rj\}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Let’s take another example. Suppose that \(G = SO(3)\) and \(X = \mathbf{R}^3\). Take the action given by the homomorphism</p>
<div>
$$
\begin{align*}
SO(3) &amp;\rightarrow Sym(\mathbf{R}^3) \\
A,x &amp;= Ax
\end{align*}
$$
</div>
<p>What is the orbit of \(v\)? Take a unit vector and compute its orbit under the action of \(SO(3)\). We elements do we get when we rotate this vector by any rotation in \(SO(3)\)? What vector do we get? We get another unit vector. In fact, given a vector with norm \(\lVert R \rVert = R\), we will get</p>
<div>
$$
\begin{align*}
O(v) = \{x \in \mathbf{R}^3 \ | \ \lVert x \rVert = R \}
\end{align*}
$$
</div>
<p>We get the set of vectors in \(\mathbf{R}^3\) with length \(R\). So a sphere of radius \(R\) centered at the origin. (Except for the zero vector where its orbit is 0 so \(O(0) = \{0\}\)). 
<br />
<br />
What about the stabilizers? If \(v\) is the zero vector, that’s easy because it’s all of \(SO(3)\). Any matrix \(A\) satisfies \(A0 = 0\). But for non-zero vectors, its the rotations of any angle around an axis through \(v\).</p>
<div>
$$
\begin{align*}
\text{Stab}(0) &amp;= SO(3) \\
\text{Stab}(v) &amp;= \{Rot_u(O), O \in \mathbf{R}\}, \quad u = v/\lVert v \rVert
\end{align*}
$$
</div>
<p>Is this a faithful? Is there any matrix \(A\) that fixes every vector \(v\) besides \(I\)? No, in fact, the kernel has only the identity matrix.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Orbit Stabilizer Theorem</b></h4>
<div class="yellowheaderdiv">
Orbit Stabilizer Theorem
</div>
<div class="yellowbodydiv">
Let \(G\) be a group that acts on \(X\). If \(x \in X\), then there is a bijection
$$
\begin{align*}
G/H &amp;\rightarrow O(x), \quad \text{where } H = \text{Stab}(x) \\
\end{align*}
$$
\(G/H\) is the set \(\{gH\}\) of left \(H\)-cosets.
</div>
<p><br />
<!----------------------------------------------------------------------------->
A consequence of this theorem is a formula for the size of the orbit</p>
<div>
$$
\begin{align*}
|O(x)| = [G: \text{Stab}(x)]
\end{align*}
$$
</div>
<p>This is useful when the group is finite. So when it’s finite, then \(|O(x)| = \frac{|G|}{\text{Stab}(x)}|\). So the size of any orbit has to divide the order of the group.
<br />
<br />
<!----------------------------------------------------------------------------->
<b>Proof</b>
<br />
We need to give a bijection. Consider the formula that takes a left coset \(gH\) to what you get when you apply the action on \(x\).</p>
<div>
$$
\begin{align*}
\alpha: G/H &amp;\rightarrow O(x) \\
        \alpha(gH) &amp;= gx
\end{align*}
$$
</div>
<p>Step 1 involves showing that this is well defined. So given that \(gH = g'H\), then \(gx = g'x\). The reason why they would be equal is that \(H\) is simply the stabilizer \(\text{Stab}(x)\). Step 2 is showing that this is indeed a bijection.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Actions by \(G\) on \(X = G\)</b></h4>
<p>Suppose you have a group \(G\). What will happen if it acts on its own set. So</p>
<div>
$$
\begin{align*}
\varphi: G &amp;\rightarrow Sym(G)
\end{align*}
$$
</div>
<p>There several of these. For example, we have the Left Regular Action. This is given by the homomorphism</p>
<div>
	$$
	\begin{align*}
	\lambda: G &amp;\rightarrow Sym(G) \\
	\lambda(g)(x) &amp;= gx, \quad g,x \in G
	\end{align*}
	$$
</div>
<p>What are the orbits of this action? For example, the orbit of \(e\) is everything</p>
<div>
	$$
	\begin{align*}
	O(e) = \{\lambda(g)(e), g \in G\} = \{ge, g \in G\} = G = X
	\end{align*}
	$$
</div>
<p>What about the stabilizers?</p>
<div>
$$
\begin{align*}
\text{Stab}(x) = \{g \in G \ | \ gx = x \} = \{e\}
\end{align*}
$$
</div>
<p>A consequence of this is that the kernel of the action is, \(\ker(\lambda) = \{e\}\). Therefore, we have a faithful action. This actually led to the next big theorem which is Cayley’s Theorem
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Cayley's Theorem
</div>
<div class="yellowbodydiv">
Every finite group \(G\) is isomorphic to a subgroup of some symmetric group \(S_m\)
</div>
<p><br />
<b>Proof</b>
<br />
Use the left regular action \(\lambda: G \rightarrow Sym(G) \cong S_n\). This is a faithful action which means that this action is injective. Therefore, \(G\) is isomorphic to the image subgroup which is \(Sym_n\)
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Right Regular and Conjugate Actions</b></h4>
<p>We also have a Right Regular Action (though we won’t use it)</p>
<div>
	$$
	\begin{align*}
	\rho: G &amp;\rightarrow Sym(G) \\
	\rho(g)(x) &amp;= xg^{-1}, \quad g,x \in G
	\end{align*}
	$$
</div>
<p>And the Conjugate Action</p>
<div>
	$$
	\begin{align*}
	c: G &amp;\rightarrow Sym(G) \\
	c(g)(x) &amp;= gxg^{-1}
	\end{align*}
	$$
</div>
<p>This is an action \(c(g_1) \circ c(g_2) = c(g_1g_2)\). The orbits of the conjugation action are called conjugacy classes. For example, if we have \(x \in X \in G\), then the conjugacy class of \(x\) is. It is a subset of the group \(G\)</p>
<div>
	$$
	\begin{align*}
	Cl(x) = \{gxg^{-1} \ | \ g \in G\}.
	\end{align*}
	$$
</div>
<p>For the stabilizers, if we have \(x \in X = G\), then</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(x) &amp;= \{g \in G \ | \ gxg^{-1} = x \} \\
	        &amp;= \{g \in G \ | \ gx = xg \}
	\end{align*}
	$$
</div>
<p>The kernel</p>
<div>
	$$
	\begin{align*}
	Ker(c) = Z(G) &amp;= \{g \in G \ | \ gx = xg \ \forall x \in G \} \\
	              &amp;= \bigcap_{x \in G} \text{Cent}(x)
	\end{align*}
	$$
</div>
<p>We actually do a relavent Theorem here
<br /></p>
<div class="yellowheaderdiv">
Orbits/Stabilizers
</div>
<div class="yellowbodydiv">
When \(G\) is finite, then \(|Cl(x)| = \frac{|G|}{|\text{Cent}(x)|}\)
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose \(G = D_3 = \{e, r, r^2, j, rj, r^2j\}\). We want to compute the orbits which are now the conjugacy classes for each element. For example, the orbit/conjugacy class of the identity is just the identity, since conjugating the identity by any element in \(g\) gives us the identity back.</p>
<div>
$$
\begin{align*}
geg^{-1} &amp;= e \quad \forall g \in G, \quad Cl(e) = \{e\} 
\end{align*}
$$
</div>
<p>For \(r\) we we can try conjugating by every element of \(g\) but we will see that we will always get \(r\) and \(r^2\) back.</p>
<div>
$$
\begin{align*}
rrr^{-1} &amp;= r \\
jrj^{-1} &amp;= r^{-1} = r^2
\end{align*}
$$
</div>
<p>Therefore, \(Cl(r) = \{r, r^2\}\). Finally, let’s find the conjugacy class of \(j\)</p>
<div>
$$
\begin{align*}
rjr^{-1} &amp;= rrj = r^2j \\
jjj &amp;= j \\
rjj(rj)^{-1} &amp;= rjr^{-1} = rrj = r^2j \\
...
\end{align*}
$$
</div>
<p>We’ll get \(Cl(j) = \{j, rj, r^2j\}\). Now, we can do the stabilizers. The stabilizer of \(e\) is any element that \(e\) fixes so it’s all of \(G\). What about \(r\)?</p>
<ul>
	<li> By the previous theorem, \(|Cl(x)| = \frac{|G|}{|\text{Cent}(x)|}\). Since \(|Cl(r)|=2\), then we must have three elements in the centralizer. </li>
	<li> We also know that the centralizer is a subgroup. So we must have the identity and not just that but we must have the element itself since it commutes with itself. But the subgroup is closed so it must have any product</li>
</ul>
<p>Therefore, \(\text{Cent}(r) = \{e,r,r^2\}\). Similarly, \(\text{Cent}(j) = \{e,j\}\). What if we take another element that’s in the same orbit like \(rj\). We will see that \(\text{Cent}(rj) = \{e, rj\}\). 
<br />
<br />
In fact, all the dihedral odd groups will look similar to what we found above for \(D_3\). The even groups however are slightly different. \(D_4\) for example has an interesting element which is \(r^2\). It is interesting because it commutes with every other element in \(G\). Therefore, in \(D_4\), the center of the group is \(Z(D_4) = \{e, r^2\}\).  When \(n\) is odd, \(Z(D)\) contains the identity only.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Group Actions A group action by a group \(G\) on a set \(X\) is a homomorphism $$ \begin{align*} \varphi: G \rightarrow Sym(X) \end{align*} $$ Note here that for every element in the group \(G\) and every element in the set \(X\), we get a function \(\phi(g)\). This function is from \(x \rightarrow X\). Because it is a function, then we can plug in a value \(x\) such that \(\phi(g)(x) \in X\). Moreover, $$ \begin{align*} \varphi(e)(x) &amp;= x \quad \text{ because $\varphi(e)$ is the identity function } \\ \varphi(g_1g_2)(x) &amp;= (\varphi(g_1)\varphi(g_2))(x) \quad \text{because $\phi$ is a homomorphism} \\ &amp;= \varphi(g_1)\big(\varphi(g_2)(x)\big) \end{align*} $$ Additionally, these identities force \(\varphi(g^{-1}) = \varphi(g)^{-1}\) which means that \(\varphi(g)\) is a bijection. Alternate Notation Given \(g \in G\) and \(x \in X\), write \(gx\) for \(\varphi(g)(x)\). So now we get a function $$ \begin{align*} G \times X &amp;\rightarrow X \\ (g,x) &amp;\rightarrow gx = \varphi(g)(x) \end{align*} $$ Re-writing the previous identities with the new notation: $$ \begin{align*} ex &amp;= x \quad \forall x \in X \\ (g_1g_2)x &amp;= g_1(g_2x) \quad \forall g_1,g_2 \in G, x \in X \end{align*} $$ Example Tautological action by \(G = Sym(X)\) on X. What is this function? $$ \begin{align*} \varphi: Sym(x) \xrightarrow{\text{identity function}} Sym(x) \end{align*} $$ This just gives us \(\varphi(g)=g\). This is exactly what we use when we work with permutations. A permutation is a function from \(X\) to itself. So for \(g \in G\) and \(x \in X\), we apply it like \(g(x) = gx\). Example Suppose \(G = SO(3)\) which acts on \(X = \mathbf{R}^3\). The action rule is given by $$ \begin{align*} G \times X &amp;\rightarrow X \\ A,x &amp;\rightarrow Ax \end{align*} $$ We’re defining a homomorphism $$ \begin{align*} SO(3) &amp;\rightarrow Sym(\mathbf{R}^3) \\ \varphi(A)(x) &amp;= Ax \end{align*} $$ \(Sym(\mathbf{R}^3)\) is the group of all bijection maps from \(\mathbf{R}^3\) to itself. The map \(\varphi\) takes a matrix \(A \in SO(3)\) and produces the following function $$ \begin{align*} \varphi(A): \mathbf{R}^3 &amp;\rightarrow \mathbf{R}^3 \\ x &amp;\rightarrow Ax \end{align*} $$ Orbits Next, we’ll define something called an orbit of action by \(G\) on \(X\). Orbit of Action by \(G\) on \(X\) An Orbit is a subset of \(X\) of the form $$ \begin{align*} O(x) = \{gx \ | \ g \in G\}, \quad \text{for some } x \in X \end{align*} $$ Fact: orbits are equivalence classes of an equivalence relation on \(X\). So \(x \sim y\) if and only if there is some \(g \in G\) such that \(gx = y\). So, \(O(x)O(y) = \emptyset\) or \(O(x) = O(y)\). Therefore, orbits partition \(X\) into pairwise disjoint and nonempty subsets. One more fact: We action is called transitive if there is only one orbit so \(O(x) = X\). Stabilizers One more defintion. Stabilizer For each \(x \in X\) $$ \begin{align*} \text{Stab}(x) = \text{Stab}_G(y) = \{g \in \ | \ gx = x\} \end{align*} $$ These are the set of elements of \(G\) that "fix" x. Some facts: \(\text{Stab}(x) \leq G\) If \(x, y \in X\) and these elements are related such that for some \(g \in G\), we have \(y = gx\), then \(\text{Stab}(y) = g\text{Stab}(x)g^{-1}\). So if \(x\) and \(y\) are in the same orbit, then their stabilizer subgroups are conjugate. Kernel of Action What is the kernel of a group action? By definition, the kernel contains all the elements in \(g\) such that \(\varphi(x)\) is the identity transformation $$ \begin{align*} \ker(\varphi) = \{g \in G \ | \ gx = x \quad \text{for all } x \in X\} \end{align*} $$ Of course this looks exactly like the definition of a stabilizer that we just did. So, $$ \begin{align*} \ker(\varphi) = \bigcap_{x \in X} \text{Stab}(x) \unlhd G \end{align*} $$ We say that the action is faithful if \(\ker(\varphi) = \{e\}\). This is is the same as saying the homomorphism \(G \rightarrow Sym(x)\) is injective. Example 1 Suppose \(G = D_n\) which acts on \(X = \{v_0,v_1,...,v_{n-1}\}\). The action rule is given by the homomorphism $$ \begin{align*} \phi: D_n &amp;\rightarrow Sym(X) = S_n \end{align*} $$ For example. Suppose \(G = D_3 = \{e, r, r^2, j, rj, r^2j\}\) with \(V = \{v_0, v_1, v_2\}\). Given this setup. What are the orbits of the action? Pick one vertex like \(v_0\). What is the orbit of \(v_0\)? If we start with \(v_0\), then we can reach every other vertex so $$ \begin{align*} rv_0 &amp;= v_1 \\ rv_1 &amp;= r^2v_0 = v_2 \end{align*} $$ So the orbit of \(v_0\) is \(O(v_0) = \{v_0, v_1, v_2\}\). So there is only one orbit and in this case the action is transitive. What about the stabilizers? these are the set of elements of \(G\) that fix \(x\). For example, if we take \(v_0\), then we know that \(e\) fixes \(x\) so it’s in the set. Is there any other element such that if we hit \(x\) with it, we get \(x\) back? Notice that \(j\) does that since \(v_0\) is on the flip axis. We can do the same calculation for \(v_1\) and \(v_2\) to see that $$ \begin{align*} \text{Stab}(v_0) &amp;= \{e,j\} \\ \text{Stab}(v_1) &amp;= \{e,r^2j\} \\ \text{Stab}(v_2) &amp;= \{e, rj\} \end{align*} $$ Example 2 Let’s take another example. Suppose that \(G = SO(3)\) and \(X = \mathbf{R}^3\). Take the action given by the homomorphism $$ \begin{align*} SO(3) &amp;\rightarrow Sym(\mathbf{R}^3) \\ A,x &amp;= Ax \end{align*} $$ What is the orbit of \(v\)? Take a unit vector and compute its orbit under the action of \(SO(3)\). We elements do we get when we rotate this vector by any rotation in \(SO(3)\)? What vector do we get? We get another unit vector. In fact, given a vector with norm \(\lVert R \rVert = R\), we will get $$ \begin{align*} O(v) = \{x \in \mathbf{R}^3 \ | \ \lVert x \rVert = R \} \end{align*} $$ We get the set of vectors in \(\mathbf{R}^3\) with length \(R\). So a sphere of radius \(R\) centered at the origin. (Except for the zero vector where its orbit is 0 so \(O(0) = \{0\}\)). What about the stabilizers? If \(v\) is the zero vector, that’s easy because it’s all of \(SO(3)\). Any matrix \(A\) satisfies \(A0 = 0\). But for non-zero vectors, its the rotations of any angle around an axis through \(v\). $$ \begin{align*} \text{Stab}(0) &amp;= SO(3) \\ \text{Stab}(v) &amp;= \{Rot_u(O), O \in \mathbf{R}\}, \quad u = v/\lVert v \rVert \end{align*} $$ Is this a faithful? Is there any matrix \(A\) that fixes every vector \(v\) besides \(I\)? No, in fact, the kernel has only the identity matrix. Orbit Stabilizer Theorem Orbit Stabilizer Theorem Let \(G\) be a group that acts on \(X\). If \(x \in X\), then there is a bijection $$ \begin{align*} G/H &amp;\rightarrow O(x), \quad \text{where } H = \text{Stab}(x) \\ \end{align*} $$ \(G/H\) is the set \(\{gH\}\) of left \(H\)-cosets. A consequence of this theorem is a formula for the size of the orbit $$ \begin{align*} |O(x)| = [G: \text{Stab}(x)] \end{align*} $$ This is useful when the group is finite. So when it’s finite, then \(|O(x)| = \frac{|G|}{\text{Stab}(x)}|\). So the size of any orbit has to divide the order of the group. Proof We need to give a bijection. Consider the formula that takes a left coset \(gH\) to what you get when you apply the action on \(x\). $$ \begin{align*} \alpha: G/H &amp;\rightarrow O(x) \\ \alpha(gH) &amp;= gx \end{align*} $$ Step 1 involves showing that this is well defined. So given that \(gH = g'H\), then \(gx = g'x\). The reason why they would be equal is that \(H\) is simply the stabilizer \(\text{Stab}(x)\). Step 2 is showing that this is indeed a bijection. Actions by \(G\) on \(X = G\) Suppose you have a group \(G\). What will happen if it acts on its own set. So $$ \begin{align*} \varphi: G &amp;\rightarrow Sym(G) \end{align*} $$ There several of these. For example, we have the Left Regular Action. This is given by the homomorphism $$ \begin{align*} \lambda: G &amp;\rightarrow Sym(G) \\ \lambda(g)(x) &amp;= gx, \quad g,x \in G \end{align*} $$ What are the orbits of this action? For example, the orbit of \(e\) is everything $$ \begin{align*} O(e) = \{\lambda(g)(e), g \in G\} = \{ge, g \in G\} = G = X \end{align*} $$ What about the stabilizers? $$ \begin{align*} \text{Stab}(x) = \{g \in G \ | \ gx = x \} = \{e\} \end{align*} $$ A consequence of this is that the kernel of the action is, \(\ker(\lambda) = \{e\}\). Therefore, we have a faithful action. This actually led to the next big theorem which is Cayley’s Theorem Cayley's Theorem Every finite group \(G\) is isomorphic to a subgroup of some symmetric group \(S_m\) Proof Use the left regular action \(\lambda: G \rightarrow Sym(G) \cong S_n\). This is a faithful action which means that this action is injective. Therefore, \(G\) is isomorphic to the image subgroup which is \(Sym_n\) Right Regular and Conjugate Actions We also have a Right Regular Action (though we won’t use it) $$ \begin{align*} \rho: G &amp;\rightarrow Sym(G) \\ \rho(g)(x) &amp;= xg^{-1}, \quad g,x \in G \end{align*} $$ And the Conjugate Action $$ \begin{align*} c: G &amp;\rightarrow Sym(G) \\ c(g)(x) &amp;= gxg^{-1} \end{align*} $$ This is an action \(c(g_1) \circ c(g_2) = c(g_1g_2)\). The orbits of the conjugation action are called conjugacy classes. For example, if we have \(x \in X \in G\), then the conjugacy class of \(x\) is. It is a subset of the group \(G\) $$ \begin{align*} Cl(x) = \{gxg^{-1} \ | \ g \in G\}. \end{align*} $$ For the stabilizers, if we have \(x \in X = G\), then $$ \begin{align*} \text{Cent}(x) &amp;= \{g \in G \ | \ gxg^{-1} = x \} \\ &amp;= \{g \in G \ | \ gx = xg \} \end{align*} $$ The kernel $$ \begin{align*} Ker(c) = Z(G) &amp;= \{g \in G \ | \ gx = xg \ \forall x \in G \} \\ &amp;= \bigcap_{x \in G} \text{Cent}(x) \end{align*} $$ We actually do a relavent Theorem here Orbits/Stabilizers When \(G\) is finite, then \(|Cl(x)| = \frac{|G|}{|\text{Cent}(x)|}\) Example Suppose \(G = D_3 = \{e, r, r^2, j, rj, r^2j\}\). We want to compute the orbits which are now the conjugacy classes for each element. For example, the orbit/conjugacy class of the identity is just the identity, since conjugating the identity by any element in \(g\) gives us the identity back. $$ \begin{align*} geg^{-1} &amp;= e \quad \forall g \in G, \quad Cl(e) = \{e\} \end{align*} $$ For \(r\) we we can try conjugating by every element of \(g\) but we will see that we will always get \(r\) and \(r^2\) back. $$ \begin{align*} rrr^{-1} &amp;= r \\ jrj^{-1} &amp;= r^{-1} = r^2 \end{align*} $$ Therefore, \(Cl(r) = \{r, r^2\}\). Finally, let’s find the conjugacy class of \(j\) $$ \begin{align*} rjr^{-1} &amp;= rrj = r^2j \\ jjj &amp;= j \\ rjj(rj)^{-1} &amp;= rjr^{-1} = rrj = r^2j \\ ... \end{align*} $$ We’ll get \(Cl(j) = \{j, rj, r^2j\}\). Now, we can do the stabilizers. The stabilizer of \(e\) is any element that \(e\) fixes so it’s all of \(G\). What about \(r\)? By the previous theorem, \(|Cl(x)| = \frac{|G|}{|\text{Cent}(x)|}\). Since \(|Cl(r)|=2\), then we must have three elements in the centralizer. We also know that the centralizer is a subgroup. So we must have the identity and not just that but we must have the element itself since it commutes with itself. But the subgroup is closed so it must have any product Therefore, \(\text{Cent}(r) = \{e,r,r^2\}\). Similarly, \(\text{Cent}(j) = \{e,j\}\). What if we take another element that’s in the same orbit like \(rj\). We will see that \(\text{Cent}(rj) = \{e, rj\}\). In fact, all the dihedral odd groups will look similar to what we found above for \(D_3\). The even groups however are slightly different. \(D_4\) for example has an interesting element which is \(r^2\). It is interesting because it commutes with every other element in \(G\). Therefore, in \(D_4\), the center of the group is \(Z(D_4) = \{e, r^2\}\). When \(n\) is odd, \(Z(D)\) contains the identity only. References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 25: Symmetry Groups of the Regular Polyhedra</title><link href="http://localhost:4000/jekyll/update/2025/02/18/math417-25-symmetries-of-regular-polyhedra.html" rel="alternate" type="text/html" title="Lecture 25: Symmetry Groups of the Regular Polyhedra" /><published>2025-02-18T00:01:36-08:00</published><updated>2025-02-18T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/18/math417-25-symmetries-of-regular-polyhedra</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/18/math417-25-symmetries-of-regular-polyhedra.html"><![CDATA[<p>A regular polyhedra is a three dimensional solid where the faces are congruent regular polygons and such that the same number of faces meet at each vertex. There are exactly 5 regular polyhedra described below:</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec25/solids.png" width="55%" class="center" /></p>

<div>
<table style="max-width: 700px; margin: 20px auto;">
  <tr>
    <td></td>
    <td>Vertices</td>
    <td>Edges</td>
	<td>Faces</td>
	<td>Vertices on the Faces</td>
	<td>Edges at a Vertex</td>
	<td>\(|G|\)</td>
  </tr>
  <tr>
    <td>Tetrahedron</td>
    <td>4</td>
    <td>6</td>
	<td>4</td>
	<td>3</td>
	<td>3</td>
	<td>12</td>
  </tr>
  <tr>
    <td>Cube</td>
    <td>8</td>
    <td>12</td>
	<td>6</td>
	<td>4</td>
	<td>3</td>
	<td>24</td>
  </tr>
  <tr>
    <td>Octahedron</td>
    <td>6</td>
    <td>12</td>
	<td>8</td>
	<td>3</td>
	<td>4</td>
	<td>24</td>
  </tr>
  <tr>
    <td>Dodecahedron</td>
    <td>20</td>
    <td>30</td>
	<td>12</td>
	<td>5</td>
	<td>3</td>
	<td>60</td>
  </tr>
  <tr>
    <td>Icosahedron</td>
    <td>12</td>
    <td>30</td>
	<td>20</td>
	<td>3</td>
	<td>5</td>
	<td>60</td>
  </tr>
</table>
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Symmetry Groups</b></h4>
<p>The reason we introduced these regular polyhedra is because we want to study their symmetry groups. So now, given a Polyhedron, we have a set \(V\) of vertices (with the center of mass at 0). Define, the symmetry group \(G\) of the polyhedron as</p>
<div>
	$$
	\begin{align*}
	G = \{ A \in SO(3) \ | \ v \in V \Leftrightarrow Av \in V, v \in \mathbf{R}^3 \}
	\end{align*}
	$$
</div>
<p>So this a point is a vertex if and only if, rotating this point by \(A\) is also a vertex. It should be clear that \(G\) is a subgroup of \(SO(3)\). For example, how many rotational symmetries does the cube have? It is twice the number of edges for all of them. In fact, the symmetry group of the octahedron and the cube are isomorphic to each other because the two solids are duel of each other. Similarly, the symmetry group of dodecahedron is isomorphic to the symmetry group of the icosahedron. 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>The Symmetry Group of the Tetrahedron</b></h4>
<p>Let’s study the specific symmetry group of the Tetrahedron. Notice that if we draw a line from any vertex through the centroid of the opposite face, we get an axis where we can rotate around. We have two possible rotations here. Going one third clockwise and one third of full rotation counter clockwise. So \(2\pi / 3\) in either direction. Each of these have order 3. Moreover, there are 4 possible axes of this type since we have 4 different vertices.
<br />
<br />
The next possible rotational symmetry is the one that goes through a mid point of an edge to another midpoint of another edge. It’s a 180 degrees symmetry. So its order is 2 and we have 3 such possible axes.</p>
<div>
<table style="max-width: 700px; margin: 20px auto;">
  <tr>
    <td>Axis of Rotation</td>
    <td>Angle of Rotation</td>
	<td>Order of Symmetry</td>
	<td># of Axes</td>
	<td># of Symmetries of This Type</td>
	<td>Cycle Type</td>
  </tr>
  <tr>
    <td>Vertex/Centroid (1)</td>
    <td>\(\frac{+2\pi}{3}\)</td>
    <td>3</td>
	<td>4</td>
	<td>4</td>
	<td>3-cycle</td>
  </tr>
  <tr>
    <td>Vertex/Centroid (2)</td>
    <td>\(\frac{+2\pi}{3}\)</td>
    <td>3</td>
	<td>4</td>
	<td>4</td>
	<td>3-cycle</td>
  </tr>
  <tr>
    <td>Edge Midpoint to Midpoint</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>3</td>
	<td>3</td>
	<td>2+2 cycle type</td>
  </tr>
  <tr>
    <td>Identity Rotation</td>
    <td></td>
    <td></td>
	<td></td>
	<td></td>
	<td></td>
  </tr>
</table>
</div>
<p>So we have exactly 12 symmetries of the tetrahedron. This group is isomorphic to \(A_4\). The group of even permutations inside \(S_4\). How does this happen? Consider the set of vertices \(\{v_1,v_2,v_3,v_4\}\) of a tetrahedron. Now, any symmetry permutes the vertices. Since we have 4 vertices, then \(Sym(4) \cong S_4\). We can construct a homomorphism such that</p>
<div>
	$$
	\begin{align*}
	\phi: G \rightarrow Sym(V) \cong S_4 \\
	\end{align*}
	$$
</div>
<p>The kernel of this homomorphism is just the identity symmetry. If you think about it, what symmetry corresponds to the identity permutation? it’s just the identity symmetry so the kernel is the trivial kernel. How do we find the actual permutations that get mapped to the symmetries in the above table. Well, the first kind where the axis is from a vertex to the centroid of a face is equivalent to a permutation that leaves one vertex fixed. Thus, we get all the 3-cycle type permutations. For the other edge to edge axis kind of symmetry, notice here that each pair of vertices switch, therefore, we have to include the 2+2 cycle type permutations. Therefore, the the permutations we have are the 2+2 and the 3-cycle permutations. There are 12 of those and in fact, this is the subgroup \(A_4\). Since the kernel of \(\phi\) is the trivial subgroup, then \(G \cong A_4\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>The Symmetry Group of the Cube</b></h4>
<p>Next, we want to explore the symmetries of the cube. The first symmetry that we can construct is the symmetry around the axis that goes between two opposite vertices. We have 8 vertices total. Therefore, there are 4 pairs of those vertices. So 4 axes total. A rotation is one third of a full spin, so the angle is \(2\pi/3\). Since we have two choices for the angle, then the number of total rotations is \(2*4 = 8\) total rotations.
<br />
<br />
The next symmetry is the symmetry around the axis connecting the midpoints of two opposite edges. We have 12 edges. So we have 6 pairs of opposite edges. The angle is exactly 180 degrees. So we have exactly 6 symmetries
<br />
<br />
The next symmetry is the symmetry around the axis connecting the centroid of two faces. We can rotate by a 90 degrees either clockwise or counter clockwise. There are 6 faces so 3 opposite pairs of faces. There are 2 rotations for each pair so a total of 6 symmetries. But we can also rotate by 180 degrees. So therefore we have another 3 rotations here.</p>
<div>
<table style="margin: 20px auto;">
  <tr>
    <td>Axis (Cube)</td>
	<td># of Axes</td>
    <td>Angle of Rotation</td>
	<td>Order of Symmetry</td>
	<td># of Symmetries of This Type</td>
    <td>Axis (Octahedron)</td>
	<td>Cycle Type</td>
  </tr>
  <tr>
    <td>Vertex/Vertex</td>
	<td>4</td>
    <td>\(\pm \frac{2\pi}{3}\)</td>
    <td>3</td>
	<td>8</td>
	<td>Faces/Face</td>
	<td>3-cycle</td>
  </tr>
  <tr>
    <td>Edge/Edge</td>
	<td>6</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>6</td>
	<td>Edge/Edge</td>
	<td>2-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>3</td>
    <td>\(\pm \frac{2\pi}{4}\)</td>
    <td>4</td>
	<td>6</td>
    <td>Vertex/Vertex</td>
	<td>4-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>3</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>3</td>
	<td></td>
	<td>2+2</td>
  </tr>
  <tr>
    <td>Identity Rotation</td>
    <td></td>
    <td></td>
	<td>1</td>
	<td>1</td>
	<td></td>
	<td>e</td>
  </tr>
</table>
</div>
<p>So we have exactly 24 symmetries just like we expected (twice the number of edges). What group is this isomorphic to? It turns out to be isomorphic to \(S_4\). Previously we have 4 vertices so it was easy to see or construct the homomorphism. But now we have 8 vertices so it’s not exactly clear. We want to construct a homomorphism from \(G\) to \(S_4\). So we need to find function from \(G\) to \(Sym(\text{4 things in a cube})\). What will the 4 things be? The easiest choice is the diagonals. We have 4 different diagonals between every pair of opposite vertices. The symmetry of the cube permutes the 4 diagonals.
<br />
<br />
So next we want \(\phi\) to not just be a homomorphism. We want it to be an isomorphism. Each group has exactly 24 elements. So if we should that \(\phi\) is injective, then that suffies to conclude that it’s an isomorphism. In order to do that, we just need to figure out what each of these symmetries do. So</p>
<ul>
	<li>Vertex/Vertex Symmetry: Notice here that the diagonal between the axis we're rotating around is fixed. While the other three diagonals get permuted. The other three diagonals get permuted cyclicly. So this is 3-cycle permutation.</li>
	<!-------------------------------->
	<li>Edge/Edge Symmetry: Here the axis is from an edge midpoint to another edge point. When we rotate around this axis by 180 degrees, notice here that two of the diagonals stay fixed. The ones in the xy plane. The remaining diagonals get switched. So this is a two cycle. </li>
	<!-------------------------------->
	<li>Face/Face Symmetry (Quarter Turn): When we do a quarter turn here, all the diagonals get switched cyclicly so this is  a 4-cycle. </li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>The Symmetry Group of the Dodecahedron</b></h4>
<p>Next, we’ll do the symmetry group of the Dodecahedron</p>
<ul>
	<li>Vertex/Vertex Symmetry: We have a total of 20 vertices. So 20 pairs of axes. Each vertex is connected to three edges. So each turn is a third of a full turn like before. Since it's 3 turns total, then each symmetry is of order 3. The number of symmetries is then 10*2 for each angle, so 20 total symmetries. </li>
	<!-------------------------------->
	<li>Edge/Edge Symmetry: Here the axis is from an edge midpoint to another edge point. Since an edge needs to go to another edge, the only angle here is 180 degrees. So the order is 2. There are 30 edges so we have 15 pairs. </li>
	<!-------------------------------->
	<li>Face/Face Symmetry: We have 12 faces total. When we rotate around an axis connecting two faces, then there are 4 possible turns. So the angle is \(\pm \frac{2\pi}{3}\). Therefore, the order is 5.</li>
</ul>

<div>
<table style="margin: 20px auto;">
  <tr>
    <td>Axis (Dodecahedron)</td>
	<td># of Axes</td>
    <td>Angle</td>
	<td>Order of \(g\)</td>
	<td># of \(g\)</td>
    <td>Axis (Icosahedron)</td>
	<td>Cycle Type</td>
  </tr>
  <tr>
    <td>Vertex/Vertex</td>
	<td>10</td>
    <td>\(\pm \frac{2\pi}{3}\)</td>
    <td>3</td>
	<td>20</td>
	<td></td>
	<td>3-cycle</td>
  </tr>
  <tr>
    <td>Edge/Edge</td>
	<td>15</td>
    <td>\(\pm \frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>15</td>
	<td></td>
	<td>2+2</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>6</td>
    <td>\(\pm \frac{2\pi}{5}\)</td>
    <td>5</td>
	<td>12</td>
    <td></td>
	<td>5-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>6</td>
    <td>\(\pm \frac{4\pi}{5}\)</td>
    <td>5</td>
	<td>12</td>
	<td></td>
	<td>5-cycle</td>
  </tr>
  <tr>
    <td>Identity Rotation</td>
    <td></td>
    <td></td>
	<td>1</td>
	<td>1</td>
	<td></td>
	<td>e</td>
  </tr>
</table>
</div>
<p>So there are 60 total symmetries matching the rule (edges * 2). Note here that all the cycles are even cycles. In fact, \(G\) is isomorphic to \(A_5\). To show this we need to construct a homomorphism to a permutation group of a set of 5 things in the dodecahedron such that</p>
<div>
	$$
	\begin{align*}
	G \rightarrow Sym(\text{5 things in the dodecahedron}) \cong S_5
	\end{align*}
	$$
</div>
<p>Take the 30 edges and divide them into groups of 6 edges each. Each edge has 5 other friends. One friend is the opposite edge that’s parallel to it. And there are 4 other perpendicular edges to it. So everything in group of 6 is either parallel or perpendicular. 
<br />
<br />
The image of this homomorphism is exactly \(A_5\) and this homomorphism is also injective. From these two facts, we can conclude that \(G \rightarrow A_5\). To show this, we simply determine the cycle type like we did before in the above table (last column). This is really hard to visualize actually. But you want to see what happens to the group of edge friends when we rotate around each of the axes in the above table. For example, for the face to face axis, all the friends get cyclically rotated. So it’s a 5-cycle.
<br />
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[A regular polyhedra is a three dimensional solid where the faces are congruent regular polygons and such that the same number of faces meet at each vertex. There are exactly 5 regular polyhedra described below:]]></summary></entry></feed>