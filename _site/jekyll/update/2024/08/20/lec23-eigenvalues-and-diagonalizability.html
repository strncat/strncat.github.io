<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Lecture 23: Eigenvalues and Diagonalizability</title>
  <link rel="stylesheet" href="/assets/css/nemo-theme.css">
  <!-- MathJax if you want math -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="container">
    <nav class="sidebar">
      <!-- You can generate links here manually, or use site.pages or a _data file -->
      <h2>Contents</h2>
      <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/about/">About</a></li>
        <li><a href="/jekyll/update/2024/07/11/linear-algebra.html">Linear Algebra</a></li>
        <li><a href="/jekyll/update/2024/12/05/abstract-algebra.html">Abstract Algebra</a></li>
        <li><a href="/jekyll/update/2025/01/03/number-theory.html">Number Theory</a></li>
        <li><a href="/jekyll/update/2024/07/09/realanalysis.html">Real Analysis</a></li>
        <!-- Add more links as needed -->
      </ul>
    </nav>
    <main class="content">
		<header class="content-header">
		        nemo's notebook
		      </header>
      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 23: Eigenvalues and Diagonalizability</h1>
    <!--
    <p class="post-meta">
      <time class="dt-published" datetime="2024-08-20T01:01:36-07:00" itemprop="datePublished">Aug 20, 2024
      </time></p>
     -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!------------------------------------------------------------------------------------>
<h4><b>Motivation</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonal if all entries off diagonal are 0. \(A_{ij} = 0\) if \(i \neq j\).
</div>
<p><br />
All computations involving diagonal matrices are simple. If \(A, B\) are diagonal, then</p>
<div>
$$
\begin{align*}
\det(A) = A_{11}...A_{nn}
\end{align*}
$$
</div>
<p>Similarly, matrix multiplication of diagonal matrices is simple. The \(AB_{ij}\)th entry is</p>
<div>
$$
\begin{align*}
AB_{ij} &amp;= \begin{cases} A_{ij}B_{ij} \quad \text{if } i = j \\ 0\phantom{A_{ij}B} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This can be generalized to computing \((A)^k\) where the \(ij\) entry is</p>
<div>
$$
\begin{align*}
(A^k)_{ij} &amp;= \begin{cases} (A_{ij})^k \quad \text{if } i = j \\ 0\phantom{(A_{ij})} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This leads to the question of whether we can transform any matrix to a diagonal matrix so we can perform these computations easily. In the next definition we formalize this.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>When is \(A\) Diagonalizable?</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \in V \rightarrow V\) is diagonalizable if there is a basis \(\beta = \{v_1,...,v_n\}\) of \(V\) such that 
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
Two questions arises from this definition:
<br />
Questions 1: Does such a basis exist?
<br />
Question 2: If it exists, how can we compute it?
<br />
<br />
Suppose we have a basis \(\beta = \{v_1,...,v_n\}\) such that</p>
<div>
$$
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
$$
</div>
<p><br />
This is equivalent to</p>
<ul style="list-style: none;">
	<li> \(\leftrightarrow\) This matrix above by defintion is 
<div>
$$
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
[T(v_1)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta}
\end{pmatrix}
$$
</div>
	</li>
    <li>\(\leftrightarrow\) We can factor \(\lambda\) to see that \([T(v_j)]_{\beta} =  \lambda_j \begin{pmatrix} 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \end{pmatrix}^t \). But this is just the \(j\)th vector of the standard basis so we can write it as \(\lambda_j[v_j]_{\beta}\). [TODO: why?].
	</li>
    <li>\(\leftrightarrow\) We can take the constant \(\lambda_j\) inside since \([ \quad ]_{\beta}\) is a linear map to see that
		<div>
		$$
		 [T(v_j)]_{\beta} = \lambda_j[v_j]_{\beta} = [\lambda_jv_j]_{\beta}  \text{ for } j = 1,...,n.
		$$
		</div>
	</li>
	<li>\(\leftrightarrow \). Because both sides of the equation are written with respect to basis \(\beta\), we can take it out and write
		<div>
		$$
		 T(v_j) = \lambda_jv_j \text { for } j = 1,...,n \text { and } \lambda_1,...,\lambda_n \in \mathbf{R}
		$$
		</div>
	</li>
	<li>\(\leftrightarrow T(v) = \lambda v\).
	</li>
	<li>\(\leftrightarrow T(v) = \lambda I_V(v)\). (Since the identity matrix does nothing)
	</li>
	<li>\(\leftrightarrow T(v) - \lambda I_V(v) = \bar{0}_V\).
	</li>
	<li>\(\leftrightarrow (T - \lambda I_V)(v) = \bar{0}_V\).
	</li>
</ul>
<p>The left hand side is a family of linear maps parameterized by \(\lambda\). The solution to this is the set of all the non-zero vectors \(v\) of the nullspace. We don’t care about the zero solution since we want to build a basis.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of a Linear Transformation</b></h4>
<p>So now we’ve seen that finding such a basis boils down to finding all vectors such that \(T(v) = \lambda v\). These vectors are called eigenvectors. More formally,</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(\ T \in V \rightarrow V\) is a \(v \neq \bar{0}_V\) such that
$$
\begin{align*}
T(v) = \lambda v
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
And the \(\lambda\)’s are called,
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(\ T \in V \rightarrow V\) if \(\exists v \neq \bar{0}_V\) such that \(T(v) = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
We can now restate the previous theorem as the following,
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \in V \rightarrow V\) is diagonalizable if and only if there is a basis \(\beta = \{v_1,...,v_n\}\) consisting of eigenvectors.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
As we’ve seen before, finding a basis \(\beta\) where</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}, \lambda_1,...,\lambda_n \text{ eigenvalues}
\end{align*}
$$
</div>
<p>is equivalent to find the set of eigenvectors that satisfy \(T(v) = \lambda v\). This is all great. But now instead of looking at general linear maps that satisfy these conditions, let’s turn our focus on matrices.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of Matrices</b></h4>
<p>When is a matrix diagonalizable? and what are the eigenvectors and eigenvalues of a given matrix \(A\)?
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonalizable if \(L_A\) is diagonalizable.
</div>
<p><br />
This is equivalent to “There is a \(Q \in M_{n \times n}\) such that \(Q^{-1}AQ\) is diagonal”.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(A \in M_{n \times n}\) is a \(v \neq \bar{0} \in \mathbf{R}^n\) such that \(Av = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and finally,
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(A \in M_{n \times n}\) if \(\exists v \neq \bar{0}_V\) such that \(Av = \lambda v\).
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Eigenvectors</b></h4>
<p>Okay now that we’ve narrowed down the discussion to matrices, how do we actually find these eigenvectors of \(A\)? Set the nullspace of \(A\) to \(N(A) = N(L_A)\). Next we will need the following lemma
<br /></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
\(v \in \mathbf{R}^n\) is an eigenvector of \(A\) with eigenvalue \(\lambda\) if and only if \(v \in N(A - \lambda I_n)\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(v\) is an eigenvector of (A). By definition this means that \(Av = \lambda v\). We can re-write this as,</p>
<div>
$$
\begin{align*}
Av &amp;= \lambda I_n v \\
Av - \lambda I_n v &amp;= \bar{0}
\end{align*}
$$
</div>
<p>But this precisely means that \(v \in N(A - \lambda I_n). \ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find all the eigen values of</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix} 
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0 \\
-2 &amp; 0 &amp; 3
\end{pmatrix}
\end{align*}
$$
</div>
<p>with eigenvalue 1.
<br />
<br />
By the lemma, we want all vectors in the null space of \(A - \lambda I_n\).</p>
<div>
$$
\begin{align*}
A - (1)I_n = 
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>We’ll put this matrix in row echelon form.</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
R_3 \rightarrow -2R_1 + R_3, R_1 \leftrightarrow -1R_1
\begin{pmatrix} 
1 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>From this we see that the null space consists of vectors of the form</p>
<div>
$$
\begin{align*}
&amp;= \{(x_1, x_2, x_3) \ | \ x_3 = t, x_1 = t, x_2 = 0\}. \\
&amp;= \{(t, 0, t) \ | \ t \in \mathbf{R} \} \\
&amp;= span\{ (1,0,1) \}
\end{align*}
$$
</div>
<p>This is easy because we are given the eigenvalue. But typically, we also need to find the eigenvalues too!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenspace</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
If \(\lambda\) is an eigenvalue of \(A\), then the eigenspace of \(A\) corresponding to \(\lambda\) is
$$
\begin{align*}
E_{\lambda} &amp;= N(A - \lambda I_n) \\
&amp;= \{ \text{eigenvectors for } \lambda \} \cup \{\bar{0}\}
\end{align*}
$$
</div>
<p><br />
[TODO: What is the difference between the eigenspace and the nullspace?]
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Eigenvalues</b></h4>
<p>Again, if we’re given an eigenvalue, then finding the eigenspace or eigenvectors is easy and simple. We’re just solving a system of linear equations like we did for finding the nullspace. The question is how can we find the eigenvalues? for this we need the following theorem</p>
<div class="purdiv">
Theorem 5.2
</div>
<div class="purbdiv">
\(\lambda\) is an eigenvalue of \(A\) if and only if \(\det(A - \lambda I_n) = 0\).
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
\(\lambda\) is an eigenvalue is equivalent to</p>
<div>
$$
\begin{align*}
&amp;\leftrightarrow \exists v \neq \bar{0} \text{ such that } Av = \lambda v \\
&amp;\leftrightarrow (A - \lambda I_n)(v) = \bar{0} \text{ for } v \neq 0\\
&amp;\leftrightarrow N(A - \lambda I_n)(v) \neq \bar{0} \\
&amp;\leftrightarrow A - \lambda I_n \text{ is not 1-1} \\
&amp;\leftrightarrow A - \lambda I_n \text{ is not invertible} \\
&amp;\leftrightarrow \det(A - \lambda I_n) = 0. \\
\end{align*}
$$
</div>
<p><br />
So we see now that we have the necessary and sufficient conditions for \(\lambda\) to be an eigenvalue of \(A\). So what’s next? \(A\) is given to us in this equation but we need a \(\lambda\) that would make the equation \(\det(A - \lambda I_n)\) equal to zero. Let’s look at the following definition
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(f(t) = \det(A - tI_n)\) is the characteristic polynomial of \(A\).
</div>
<p><br />
What is this saying? we can interpret the right hand side as a function. We’re given \(A\). We know the identity matrix. So the unknown is \(t\). So inside the determinant, we’ll have a matrix with entries that depend on \(A\) and \(t\). We know the determinant is a map / inductive formula. So this expression when expanded as a whole is some number that depends on it. In fact it shouldn’t be surprising that \(f(t)\) is a polynomial of degree \(n\) (FACT). 
<br />
<br />
Based on this, we can rephrase the previous theorem as the following corollary,
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
\(\lambda\) is an eigenvalue of \(A\) if and only if \(f(t) = \det(A - \lambda I_n) = 0\).
</div>
<p><br />
So eigenvalues are the roots of this polynomial so it’s not always easy to do. How many roots? We know the degree of \(f(t)\) is at most \(n\). Therefore,
<br /></p>
<div class="purdiv">
Corollary 2 (Theorem 5.3(b))
</div>
<div class="purbdiv">
\(A\) has at most \(n\) eigenvalues.
</div>
<p><br />
So we know at least that there can only be \(n\) roots/eigenvalues at most.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find the eigenvalues of
\(\begin{align*}
A =
\begin{pmatrix} 
0 &amp; -1 \\
1 &amp; 0
\end{pmatrix}.
\end{align*}\)
<br />
<br />
Let’s write the characteristic polynomial and find its roots so</p>
<div>
$$
\begin{align*}
f(t) = \det(A - tI_n) &amp;= 0 \\
\det
\begin{pmatrix} 
-t &amp; -1 \\
1 &amp; -t
\end{pmatrix} &amp;= 0 \\
t^2 + 1 &amp;= 0 \\
t^2 &amp;= -1 \\
\end{align*}
$$
</div>
<p>This polynomial has no real roots! and so the matrix \(A\) has no eigenvalues.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>


  </div>
<!-- stupid ads<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier =  /jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://strncat-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
  <a class="u-url" href="/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html" hidden></a>
</article>

    </main>
  </div>

</body>
</html>