<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-07T09:01:48-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 21: Determinants and Row Operations</title><link href="http://localhost:4000/jekyll/update/2024/08/18/lec21-determinants-row-operations.html" rel="alternate" type="text/html" title="Lecture 21: Determinants and Row Operations" /><published>2024-08-18T01:01:36-07:00</published><updated>2024-08-18T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/18/lec21-determinants-row-operations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/18/lec21-determinants-row-operations.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}\). If we perform the following row operations
<ol style="list-style-type:upper-roman;">
	<li>\(A \xrightarrow{R_i \leftrightarrow R_j} B \), then \(\det(B) = -\det(A)\)</li>
	<li>\(A \xrightarrow{R_i \rightarrow cR_i} B \), then \(\det(B) = c\det(A)\)</li>
	<li>\(A \xrightarrow{R_i \rightarrow R_i + cR_j} B \), then \(\det(B) = -\det(A)\)</li>	
</ol>
</div>
<p><br />
We will only need to use Theorem 1 and Corollary 2 from last lecture to prove this result!
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
For (II) Let</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, 
	B = \begin{pmatrix} a_1 \\ \vdots \\ ca_i \\ \vdots \\ a_n \end{pmatrix}, 
	\end{align*}
	$$
</div>
<p>Using theorem 1 we know that \(\det(B) = c\det(A)\) as we wanted to show. For (III) Let</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i  \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	B = \begin{pmatrix} a_1 \\ \vdots \\ a_i+ca_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	C = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	\end{align*}
	$$
</div>
<p>Using theorem 1 we know that \(\det(B) = \det(A) + c\det(C)\). But Corollary 2 implies that \(\det(C) = 0\) since \(C\) has two identical rows. Therefore, \(\det(B) = \det(A)\) as we wanted to show. For (I)</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	B = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, 
	C = \begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix}, 
	\end{align*}
	$$
</div>
<p>One thing we know right away here is that \(\det(C)=0\) since \(C\) has two identical rows (by corollary 2) so</p>
<div>
	$$
	\begin{align*}
	0 = \det\begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix} &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} \\
	&amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} + 
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} +
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} +
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} \\
	&amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} +
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} \\
0	&amp;= \det(A) + \det(B) \\
det(A) &amp;= -\det(B)
	\end{align*}
	$$
</div>
<p>Which is what we wanted to show. \(\blacksquare\)
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n }\) is upper(lower) triangular if all entries below (above) diagonal are zero.
</div>
<p><br />
For example any \(n \times n\) matrix in REF is upper triangular. One reason why the Upper/Lower triangular matrices are interesting is the following theorem.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(A \in M_{2 \times 2}\) is upper or lower triangular then
$$
\begin{align*}
\det(A) = A_{11}A_{22}A_{nn}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
For upper triangular matrices, by induction on \(n\).
<br />
<br />
Base Case: \(n = 2\)</p>
<div>
$$
\begin{align*}
\det
\begin{pmatrix} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix}
= A_{11}A_{22}.
\end{align*}
$$
</div>

<p>Inductive Case: assume this is true for \(n-1 \geq 2\). Consider</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 
A_{11} &amp; A_{12} &amp; \cdots &amp; \cdots &amp; A_{1n} \\ 
0 &amp; A_{22} &amp; \cdots &amp; \cdots &amp; \vdots \\
\vdots &amp; 0 &amp; \ddots &amp; \cdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; A_{nn} \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>We can compute the determinant by choosing to cofactor along the \(n\)th row and so,</p>
<div>
$$
\begin{align*}
\det(A) = \sum^n_{j=1}(-1)^{n+j}A_{nj}\det(\tilde{A_{nj}}) \\
= (-1)^{n+n}A_{nn}\det(\tilde{A_{nn}}) \\
\end{align*}
$$
</div>
<p>But \(\tilde{A_{nn}}\) is an \(n-1 \times n-1\) upper triangular matrix. So by the inductive hypothesis its determinant is the product of the diagonal entries and so</p>
<div>
$$
\begin{align*}
\det(\tilde{A_{nn}} = A_{11}A_{22}...A_{n-1n-1}
\end{align*}
$$
</div>
<p>And therefore,</p>
<div>
$$
\begin{align*}
\det(A) = A_{11}A_{22}...A_{nn}
\end{align*}
$$
</div>
<p>As we wanted to show. \(\blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Compute the determinant for 
\(A = \begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	1 &amp; 0 &amp; 2 \\
	2 &amp; 1 &amp; 1
	\end{pmatrix}\).
<br />
<br />
To use the theorem, we’ll put \(A\) in REF.</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	1 &amp; 0 &amp; 2 \\
	2 &amp; 1 &amp; 1
	\end{pmatrix}
	R_2 \rightarrow R_2 - R_1
	R_3 \rightarrow R_3 - 2R_1
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; -2 &amp; -1 \\
	0 &amp; -3 &amp; -5
	\end{pmatrix} 
	\end{align*}
	$$
</div>
<p>Note here that these two operations will not change the value of the determinant per thoerem 1 from last lecture. Continuing with REF:</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; -2 &amp; -1 \\
	0 &amp; -3 &amp; -5
	\end{pmatrix}
	R_2 \rightarrow -\frac{1}{2}R_2
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; -3 &amp; -5
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Here, since we scaled by \(-\frac{1}{2}\) then we know that \(\det(A) = -2\det(RREF(A))\).</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; -3 &amp; -5
	\end{pmatrix}
	R_3 \rightarrow 3R_2 + R_3
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; 0 &amp; -\frac{7}{2} 
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>This operation will not further change the value of the determinant. So now we can apply the theorem to compute the determinant since \(A\) is an upper triangular matrix</p>
<div>
	$$
	\begin{align*}
	\det
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	1 &amp; 0 &amp; 2 \\
	2 &amp; 1 &amp; 1
	\end{pmatrix}
	= -2\det
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; 0 &amp; -\frac{7}{2} 
	\end{pmatrix}
	= -2(1 * 1 * -\frac{7}{2}) = 7
	\end{align*}
	$$
</div>
<p><br />
<!--------------------------------------------------------------------------></p>
<h4><b>Comparisons</b></h4>
<p>Computing \(\det(A)\) for \(A \in M_{n \times n}\) using the inductive formula is roughly \(n!\) operations. Using row operations, it is roughly \(n^3\) operations.
<br />
<br />
<br />
<!--------------------------------------------------------------------------></p>
<h4><b>Determinant, rank and invertibility</b></h4>
<p>It is clear that none of the row operations will lead the determinant to be 0. In fact \(det(A) \neq 0\) implies</p>
<ul style="list-style: none;">
<li>\(\leftrightarrow det(REF) \neq 0\)</li>
<li>\(\leftrightarrow REF\) of \(A\) has \(n\) leading entries</li>
<li>\(\leftrightarrow A\) is invertible</li>
</ul>
<p>det(AB) = det(A)(B)</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem Let \(A \in M_{n \times n}\). If we perform the following row operations \(A \xrightarrow{R_i \leftrightarrow R_j} B \), then \(\det(B) = -\det(A)\) \(A \xrightarrow{R_i \rightarrow cR_i} B \), then \(\det(B) = c\det(A)\) \(A \xrightarrow{R_i \rightarrow R_i + cR_j} B \), then \(\det(B) = -\det(A)\) We will only need to use Theorem 1 and Corollary 2 from last lecture to prove this result! Proof For (II) Let $$ \begin{align*} A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, B = \begin{pmatrix} a_1 \\ \vdots \\ ca_i \\ \vdots \\ a_n \end{pmatrix}, \end{align*} $$ Using theorem 1 we know that \(\det(B) = c\det(A)\) as we wanted to show. For (III) Let $$ \begin{align*} A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, B = \begin{pmatrix} a_1 \\ \vdots \\ a_i+ca_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, C = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, \end{align*} $$ Using theorem 1 we know that \(\det(B) = \det(A) + c\det(C)\). But Corollary 2 implies that \(\det(C) = 0\) since \(C\) has two identical rows. Therefore, \(\det(B) = \det(A)\) as we wanted to show. For (I) $$ \begin{align*} A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, B = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, C = \begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix}, \end{align*} $$ One thing we know right away here is that \(\det(C)=0\) since \(C\) has two identical rows (by corollary 2) so $$ \begin{align*} 0 = \det\begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix} &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} \\ &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} \\ &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} \\ 0 &amp;= \det(A) + \det(B) \\ det(A) &amp;= -\det(B) \end{align*} $$ Which is what we wanted to show. \(\blacksquare\) Definition \(A \in M_{n \times n }\) is upper(lower) triangular if all entries below (above) diagonal are zero. For example any \(n \times n\) matrix in REF is upper triangular. One reason why the Upper/Lower triangular matrices are interesting is the following theorem. Theorem If \(A \in M_{2 \times 2}\) is upper or lower triangular then $$ \begin{align*} \det(A) = A_{11}A_{22}A_{nn} \end{align*} $$ Proof For upper triangular matrices, by induction on \(n\). Base Case: \(n = 2\) $$ \begin{align*} \det \begin{pmatrix} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix} = A_{11}A_{22}. \end{align*} $$]]></summary></entry><entry><title type="html">Section 2.2: Exercise 12</title><link href="http://localhost:4000/jekyll/update/2024/08/17/lec13-exercise-12.html" rel="alternate" type="text/html" title="Section 2.2: Exercise 12" /><published>2024-08-17T01:01:36-07:00</published><updated>2024-08-17T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/17/lec13-exercise-12</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/17/lec13-exercise-12.html"><![CDATA[<div class="ydiv">
2.2 Exercise 12
</div>
<div class="ybdiv">
Let \(\beta=\{v_1,...,v_n\}\) be a basis for a vector space \(V\) and \(T: V \rightarrow V\) be a linear transformation. Prove that \([T]_{\beta}\) is upper triangular if and only if \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n.\)
</div>
<p><br />
Proof: Let \(A = [T]_{\beta}\). We’ll prove both directions as follows:<br />
\(\Rightarrow:\) Suppose that \(A\) is upper triangular. This means that the entries below the diagonal are all zero. In other words, \(A_{ij} = 0\) for \(j = 1,2,...,n\) and  \(i &gt; j\). 
<br />
<br />
Now, let \(v_j \in \beta\). By the definition of matrix multiplication,</p>
<div>
$$
\begin{align*}
T(v_j) &amp;= \sum_{k}^n A_{jk}v_j = A_{j1}v_j + A_{j2}v_j + ... + A_{jn}v_j
\end{align*}
$$
</div>
<p>But we know whenever \(j &gt; k\), then \(A_{jk = 0}\) so 
<br />
<br />
\(\Leftarrow:\) Suppose that \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n\). This means that \(T(v_j)\) can be written as a linear combination of the elements \(\{v_1,...,v_j\}\) so</p>
<div>
$$
\begin{align*}
T(v_j) &amp;= a_1v_1 + a_2v_2 + ... + a_jv_j. \\
\end{align*}
$$
</div>
<p>for some scalars \(a_1,...,a_j\). We also know that</p>
<div>
$$
\begin{align*}
T(v_j) &amp;= [T]_{\beta}[v_j]_{\beta} = [T]_{\beta}v_j. \\
\end{align*}
$$
</div>
<p>\(\ \blacksquare\)</p>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[2.2 Exercise 12 Let \(\beta=\{v_1,...,v_n\}\) be a basis for a vector space \(V\) and \(T: V \rightarrow V\) be a linear transformation. Prove that \([T]_{\beta}\) is upper triangular if and only if \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n.\) Proof: Let \(A = [T]_{\beta}\). We’ll prove both directions as follows: \(\Rightarrow:\) Suppose that \(A\) is upper triangular. This means that the entries below the diagonal are all zero. In other words, \(A_{ij} = 0\) for \(j = 1,2,...,n\) and \(i &gt; j\). Now, let \(v_j \in \beta\). By the definition of matrix multiplication, $$ \begin{align*} T(v_j) &amp;= \sum_{k}^n A_{jk}v_j = A_{j1}v_j + A_{j2}v_j + ... + A_{jn}v_j \end{align*} $$ But we know whenever \(j &gt; k\), then \(A_{jk = 0}\) so \(\Leftarrow:\) Suppose that \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n\). This means that \(T(v_j)\) can be written as a linear combination of the elements \(\{v_1,...,v_j\}\) so $$ \begin{align*} T(v_j) &amp;= a_1v_1 + a_2v_2 + ... + a_jv_j. \\ \end{align*} $$ for some scalars \(a_1,...,a_j\). We also know that $$ \begin{align*} T(v_j) &amp;= [T]_{\beta}[v_j]_{\beta} = [T]_{\beta}v_j. \\ \end{align*} $$ \(\ \blacksquare\)]]></summary></entry><entry><title type="html">Section 1.3: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/16/lec05-exercise-20.html" rel="alternate" type="text/html" title="Section 1.3: Exercise 20" /><published>2024-08-16T01:01:36-07:00</published><updated>2024-08-16T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/16/lec05-exercise-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/16/lec05-exercise-20.html"><![CDATA[<div class="ydiv">
1.3 Exercise 20
</div>
<div class="ybdiv">
Prove that if \(W\) is a subspace of a vector space \(V\) and \(w_1,w_2,...,w_n\) in \(W\), then \(a_1w_1 + a_2w_2 + ... + a_nw_n \in W\) for any scalars \(a_1, a_2,...,a_n\).
</div>
<p><br />
Proof:
By induction on \(n\):
<br />
<br />
Base Case (\(n=1\)): If \(w_1 \in W\), then \(a_1w_1 \in W\) since \(W\) is a subspace and so it is closed under scalar multiplication.
<br />
<br />
Inductive Case: Suppose this is true for \(k &gt; 1\) so if \(w_1,...,w_k \in W\), then \(a_1w_1+...+a_kw_k \in W\) for any scalars \(a_1,...,a_k\). We will show that this is true for \(k+1\). So suppose that \(w_1,...,w_k, w_{k+1} \in W\), and let \(a_1,...,a_{k+1}\) be any scalars. We know that \(a_{k+1}w_{k+1} \in W\) since \(W\) is closed under scalar multiplication. We also know that \(a_1w_1+...+a_kw_k\) is in \(W\) by the inductive hypothesis. Furthermore, \(a_1w_1+...+a_kw_k + a_{k+1}w_{k+1}\) is also in \(W\) since \(W\) is closed under addition.
<br />
<br />
Therefore, the statmeent is true for all \(n \geq 1. \ \blacksquare\).</p>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.3 Exercise 20 Prove that if \(W\) is a subspace of a vector space \(V\) and \(w_1,w_2,...,w_n\) in \(W\), then \(a_1w_1 + a_2w_2 + ... + a_nw_n \in W\) for any scalars \(a_1, a_2,...,a_n\). Proof: By induction on \(n\): Base Case (\(n=1\)): If \(w_1 \in W\), then \(a_1w_1 \in W\) since \(W\) is a subspace and so it is closed under scalar multiplication. Inductive Case: Suppose this is true for \(k &gt; 1\) so if \(w_1,...,w_k \in W\), then \(a_1w_1+...+a_kw_k \in W\) for any scalars \(a_1,...,a_k\). We will show that this is true for \(k+1\). So suppose that \(w_1,...,w_k, w_{k+1} \in W\), and let \(a_1,...,a_{k+1}\) be any scalars. We know that \(a_{k+1}w_{k+1} \in W\) since \(W\) is closed under scalar multiplication. We also know that \(a_1w_1+...+a_kw_k\) is in \(W\) by the inductive hypothesis. Furthermore, \(a_1w_1+...+a_kw_k + a_{k+1}w_{k+1}\) is also in \(W\) since \(W\) is closed under addition. Therefore, the statmeent is true for all \(n \geq 1. \ \blacksquare\).]]></summary></entry><entry><title type="html">Section 1.4: Theorem 1.5</title><link href="http://localhost:4000/jekyll/update/2024/08/15/lec06-theorem-1.5.html" rel="alternate" type="text/html" title="Section 1.4: Theorem 1.5" /><published>2024-08-15T01:01:36-07:00</published><updated>2024-08-15T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/15/lec06-theorem-1.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/15/lec06-theorem-1.5.html"><![CDATA[<div class="purdiv">
Theorem 1.5
</div>
<div class="purbdiv">
The span of any subset \(S\) of a vector space \(V\) is a subspace of \(V\) that contains \(S\). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\).
</div>
<p><br />
Proof:
<br />
<br />
If \(S = \emptyset\), then \(span(S)=\{\bar{0}\}\). \(\{\bar{0}\}\) is a subspace that contains \(S\) and is also contained in any any subspace of \(V\).
<br />
<br />
If \(S \neq \emptyset\), then \(span(S)\) is a subspace because</p>
<ul style="list-style-type:lower-alpha">
	<li>\(\bar{0} \in span(S)\) since \(0v = \bar{0}\) for some vector \(v \in S\).</li>
	<li>\(span(S)\) is closed under addition. For any two vectors \(x, y \in span(S)\), there exists vectors \(v_1,v_2...,v_n,u_1,u_2...,u_m\) in \(S\) and scalars \(a_1,a_2,..,a_n,b_1,b_2,...,b_m\) such that
	<div>
		$$
		\begin{align*}
		x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n \\
		y &amp;= b_1u_1 + b_2u_2 + ... + b_nu_m.
		\end{align*}
		$$
	</div>
The addition of \(x\) and \(y\) is also in \(span(S)\) because it's a linear combination of some elements in \(S\)  as follows:
	<div>
		$$
		\begin{align*}
		x+y &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n + b_1u_1 + b_2u_2 + ... + b_nu_m.
		\end{align*}
		$$
	</div>
</li>
	<li>\(span(S)\) is closed under scalar multiplication. For any vector \(x \in span(S)\), there exists vectors \(v_1,v_2...,v_n\) in \(S\) and scalars \(a_1,a_2,..,a_n\) such that 
	<div>
		$$
		\begin{align*}
		x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n
		\end{align*}
		$$
	</div>
For any scalar \(c\)
	<div>
		$$
		\begin{align*}
		cx &amp;= (ca_1)v_1 + (ca_2)v_2 + ... + (ca_n)v_n
		\end{align*}
		$$
	</div>
which is a linear combination of some elements in \(S\) and so it is in \(span(S)\).
	</li>
</ul>
<p>So \(span(S)\) is a subspace of \(V\). Furthermore, for any element \(v \in S\), \(1.v \in span(S)\) and so \(v \in span(S)\). So the span of \(S\) contains \(S\).
<br />
<br />
For the second part of the statement, suppose that there exists a subspace \(W\) of \(V\) that contains \(S\). We claim that \(W\) must contain \(span(S)\). Let \(w \in span(S)\), we will show that \(w \in W\). Since \(w \in span(S)\) then \(w\) can be expressed a linear combination of some vectors \(w_1,...,w_k \in S\) and some scalars \(c_1,...,c_k\) such that</p>
<div>
	$$
	\begin{align*}
	w = c_1w_1 + c_2w_2 + ... + c_kw_k,
	\end{align*}
	$$
</div>
<p>But we know that \(W\) contains \(S\) and therefore \(w_1,...,w_k \in W\). We previously proved (Lecture 05, Exercise 20 of section 1.3) that if \(W\) is a subspace, then any linear combination of the elements of \(W\) is also in \(W\). Therefore, \(w \in W\) and \(span(S) \subseteq W\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 1.5 The span of any subset \(S\) of a vector space \(V\) is a subspace of \(V\) that contains \(S\). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\). Proof: If \(S = \emptyset\), then \(span(S)=\{\bar{0}\}\). \(\{\bar{0}\}\) is a subspace that contains \(S\) and is also contained in any any subspace of \(V\). If \(S \neq \emptyset\), then \(span(S)\) is a subspace because \(\bar{0} \in span(S)\) since \(0v = \bar{0}\) for some vector \(v \in S\). \(span(S)\) is closed under addition. For any two vectors \(x, y \in span(S)\), there exists vectors \(v_1,v_2...,v_n,u_1,u_2...,u_m\) in \(S\) and scalars \(a_1,a_2,..,a_n,b_1,b_2,...,b_m\) such that $$ \begin{align*} x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n \\ y &amp;= b_1u_1 + b_2u_2 + ... + b_nu_m. \end{align*} $$ The addition of \(x\) and \(y\) is also in \(span(S)\) because it's a linear combination of some elements in \(S\) as follows: $$ \begin{align*} x+y &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n + b_1u_1 + b_2u_2 + ... + b_nu_m. \end{align*} $$ \(span(S)\) is closed under scalar multiplication. For any vector \(x \in span(S)\), there exists vectors \(v_1,v_2...,v_n\) in \(S\) and scalars \(a_1,a_2,..,a_n\) such that $$ \begin{align*} x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n \end{align*} $$ For any scalar \(c\) $$ \begin{align*} cx &amp;= (ca_1)v_1 + (ca_2)v_2 + ... + (ca_n)v_n \end{align*} $$ which is a linear combination of some elements in \(S\) and so it is in \(span(S)\). So \(span(S)\) is a subspace of \(V\). Furthermore, for any element \(v \in S\), \(1.v \in span(S)\) and so \(v \in span(S)\). So the span of \(S\) contains \(S\). For the second part of the statement, suppose that there exists a subspace \(W\) of \(V\) that contains \(S\). We claim that \(W\) must contain \(span(S)\). Let \(w \in span(S)\), we will show that \(w \in W\). Since \(w \in span(S)\) then \(w\) can be expressed a linear combination of some vectors \(w_1,...,w_k \in S\) and some scalars \(c_1,...,c_k\) such that $$ \begin{align*} w = c_1w_1 + c_2w_2 + ... + c_kw_k, \end{align*} $$ But we know that \(W\) contains \(S\) and therefore \(w_1,...,w_k \in W\). We previously proved (Lecture 05, Exercise 20 of section 1.3) that if \(W\) is a subspace, then any linear combination of the elements of \(W\) is also in \(W\). Therefore, \(w \in W\) and \(span(S) \subseteq W\) as we wanted to show. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 2.1: Theorem 2.2</title><link href="http://localhost:4000/jekyll/update/2024/08/14/lec11-theorem-2.2.html" rel="alternate" type="text/html" title="Section 2.1: Theorem 2.2" /><published>2024-08-14T01:01:36-07:00</published><updated>2024-08-14T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/14/lec11-theorem-2.2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/14/lec11-theorem-2.2.html"><![CDATA[<div class="purdiv">
Theorem 2.2
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is linear and \(\beta=\{v_1,...,v_n\}\) is basis for \(V\), then
$$
\begin{align*}
R(T) = span(T(\beta)) = span(\{T(v_1),...,T(v_n)\} 
\end{align*}
$$
</div>
<p><br />
Proof:
<br />
<br />
To show that \(R(T) = span(T(\beta))\) we will show that \(span(T(\beta)) \subseteq R(T)\) and \(R(T) \subseteq span(T(\beta))\).
<br />
<br />
\(span(T(\beta)) \subseteq R(T)\): By definition, we know that for any \(v \in \beta\), \(T(v) \in R(T)\) so \(T(\beta) \subseteq R(T)\). Since \(R(T)\) is a subspace and \(R(T)\) contains the set \(T(\beta)\), then by <a href="https://strncat.github.io/jekyll/update/2024/08/15/lec06-theorem-1.5.html">theorem 1.5</a>, it must contain the span of this set as well and so</p>
<div>
	$$
	\begin{align*}
	span(T(\beta)) = span(\{T(v_1),...,T(v_n)\}) \subseteq R(T)
	\end{align*}
	$$
</div>
<p><br />
\(R(T) \subseteq span(T(\beta))\): Let \(w \in R(T)\). We know that \(w = T(v)\) for some \(v \in V\). Since \(\beta\) is a basis, then we can express \(v\) as a linear combination of the elements in \(\beta\) such that
<br /></p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n,
	\end{align*}
	$$
</div>
<p>for some scalars \(a_1, ..., a_n\). Since \(T\) is linear then we can see that</p>
<div>
	$$
	\begin{align*}
	w = T(v) &amp;= T(a_1v_1 + ... + a_nv_n) \\
	   &amp;= a_1T(v_1) + ... + a_nT(v_n).
	\end{align*}
	$$
</div>
<p>So \(w\) is a linear combination of the elements of \(T(\beta)\). This means that \(w\) is also in \(span(T(\beta))\) by the definition of a span and so \(R(T) \subseteq span(T(\beta))\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 2.2 If \(T: V \rightarrow W\) is linear and \(\beta=\{v_1,...,v_n\}\) is basis for \(V\), then $$ \begin{align*} R(T) = span(T(\beta)) = span(\{T(v_1),...,T(v_n)\} \end{align*} $$ Proof: To show that \(R(T) = span(T(\beta))\) we will show that \(span(T(\beta)) \subseteq R(T)\) and \(R(T) \subseteq span(T(\beta))\). \(span(T(\beta)) \subseteq R(T)\): By definition, we know that for any \(v \in \beta\), \(T(v) \in R(T)\) so \(T(\beta) \subseteq R(T)\). Since \(R(T)\) is a subspace and \(R(T)\) contains the set \(T(\beta)\), then by theorem 1.5, it must contain the span of this set as well and so $$ \begin{align*} span(T(\beta)) = span(\{T(v_1),...,T(v_n)\}) \subseteq R(T) \end{align*} $$ \(R(T) \subseteq span(T(\beta))\): Let \(w \in R(T)\). We know that \(w = T(v)\) for some \(v \in V\). Since \(\beta\) is a basis, then we can express \(v\) as a linear combination of the elements in \(\beta\) such that $$ \begin{align*} v = a_1v_1 + ... + a_nv_n, \end{align*} $$ for some scalars \(a_1, ..., a_n\). Since \(T\) is linear then we can see that $$ \begin{align*} w = T(v) &amp;= T(a_1v_1 + ... + a_nv_n) \\ &amp;= a_1T(v_1) + ... + a_nT(v_n). \end{align*} $$ So \(w\) is a linear combination of the elements of \(T(\beta)\). This means that \(w\) is also in \(span(T(\beta))\) by the definition of a span and so \(R(T) \subseteq span(T(\beta))\) as we wanted to show. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 2.4: Theorem 2.17 (Corollary)</title><link href="http://localhost:4000/jekyll/update/2024/08/13/lec15-corollary-2.17.html" rel="alternate" type="text/html" title="Section 2.4: Theorem 2.17 (Corollary)" /><published>2024-08-13T01:01:36-07:00</published><updated>2024-08-13T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/13/lec15-corollary-2.17</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/13/lec15-corollary-2.17.html"><![CDATA[<p>This is the proof from the book for the Corollary following theorem 2.17.</p>
<div class="purdiv">
Theorem 2.17 (Corollary)
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is invertible, then \(V\) is finite dimensional if and only if \(W\) is finite dimensional. In this case \(\dim V = \dim W\)
</div>
<p><br />
Proof:
<br />
<br />
\(\Rightarrow:\) Suppose that \(V\) is finite dimensional. If \(T\) is invertible, then \(T\) is onto. By the definition of onto, this means that \(R(T)=W\) (or that for any \(w \in W\), there exists a \(v \in V\) such that \(T(v)=w\)). Since \(V\) is finite dimensional, then let \(\beta\) be a finite basis for \(V\). By <a href="https://strncat.github.io/jekyll/update/2024/08/14/lec11-theorem-2.2.html">Theorem 2.2</a>, \(span(T(\beta)) = R(T)\). But \(R(T) = W\). Therefore, \(W\) must be finite dimensional.
<br />
<br />
\(\Leftarrow:\) Suppose that \(W\) is finite dimensional. If \(T\) is invertible, then \(T^{-1}\) is linear and invertible. We can now apply the same argument from before. \(T^{-1}\) is invertible and so \(T^{-1}\) is onto. Since \(W\) is finite dimensional, let \(\gamma\) be a basis for \(W\). Therefore, \(R(T^{-1})=V\) and since \(span(T^{-1}(\gamma)) = R(T^{-1}) = V\), then \(V\) is finite dimensional.
<br />
<br />
So now suppose that \(V\) and \(W\) are finite dimensional. Because \(T\) is one-to-one and onto, then \(nullity(T)=0\) and \(rank(T) = \dim(R(T)) = \dim(W)\). By the dimension theorem we know,</p>
<div>
	$$
	\begin{align*}
	\dim(V) &amp;= \dim(N(T)) + \dim(R(T)) \\
	        &amp;= 0 + \dim(W) \\
	\end{align*}
	$$
</div>
<p>Therefore, \(\dim(V)=\dim(W)\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof from the book for the Corollary following theorem 2.17. Theorem 2.17 (Corollary) If \(T: V \rightarrow W\) is invertible, then \(V\) is finite dimensional if and only if \(W\) is finite dimensional. In this case \(\dim V = \dim W\) Proof: \(\Rightarrow:\) Suppose that \(V\) is finite dimensional. If \(T\) is invertible, then \(T\) is onto. By the definition of onto, this means that \(R(T)=W\) (or that for any \(w \in W\), there exists a \(v \in V\) such that \(T(v)=w\)). Since \(V\) is finite dimensional, then let \(\beta\) be a finite basis for \(V\). By Theorem 2.2, \(span(T(\beta)) = R(T)\). But \(R(T) = W\). Therefore, \(W\) must be finite dimensional. \(\Leftarrow:\) Suppose that \(W\) is finite dimensional. If \(T\) is invertible, then \(T^{-1}\) is linear and invertible. We can now apply the same argument from before. \(T^{-1}\) is invertible and so \(T^{-1}\) is onto. Since \(W\) is finite dimensional, let \(\gamma\) be a basis for \(W\). Therefore, \(R(T^{-1})=V\) and since \(span(T^{-1}(\gamma)) = R(T^{-1}) = V\), then \(V\) is finite dimensional. So now suppose that \(V\) and \(W\) are finite dimensional. Because \(T\) is one-to-one and onto, then \(nullity(T)=0\) and \(rank(T) = \dim(R(T)) = \dim(W)\). By the dimension theorem we know, $$ \begin{align*} \dim(V) &amp;= \dim(N(T)) + \dim(R(T)) \\ &amp;= 0 + \dim(W) \\ \end{align*} $$ Therefore, \(\dim(V)=\dim(W)\) as we wanted to show. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 11: Exercise 0</title><link href="http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0.html" rel="alternate" type="text/html" title="Lecture 11: Exercise 0" /><published>2024-08-12T01:01:36-07:00</published><updated>2024-08-12T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0.html"><![CDATA[<div class="ydiv">
Problem 4
</div>
<div class="ybdiv">
Let \(V\) and \(W\) be finite dimensional vector spaces and \(T: V \rightarrow W\) be linear.
<ol style="list-style-type:lower-alpha">
	<li>Prove that if \(\dim(V) &lt; \dim (W)\), then \(T\) cannot be onto.</li>
	<li>Prove that if \(\dim(W) &lt; \dim(V)\), then \(T\) cannot be one-to-one.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Suppose for the sake of contradiction that \(T\) was onto. This means that \(Rank(T) = \dim(W)\). The dimension theorem states that
	<div>
		$$
		\begin{align*}
		\dim(V) &amp;= Nullity(T) + Rank(T) \\
		       &amp;= Nullity(T) + \dim(W)
		\end{align*}
		$$
	</div>
This implies that \(Nullity(T) = \dim(V) - \dim(W) &lt; 0\) since \(\dim(V) &lt; \dim(W)\). This is a contradiction and so \(T\) cannot be onto. \(\blacksquare\)  
	</li>
	<li>Similarly, suppose for the sake of contradiction that \(T\) is one-to-one. This means that \(Nullity(T) = 0\) and so by the dimension theorem, it must be that \(\dim(V) = \dim(W)\). But this is a contradiction since \(\dim(W) &lt; \dim(V)\).</li> </ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.geneseo.edu/~heap/courses/333/exam2_F2007_practice_sol.pdf">Practice Midterm</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Problem 4 Let \(V\) and \(W\) be finite dimensional vector spaces and \(T: V \rightarrow W\) be linear. Prove that if \(\dim(V) &lt; \dim (W)\), then \(T\) cannot be onto. Prove that if \(\dim(W) &lt; \dim(V)\), then \(T\) cannot be one-to-one. Proof: Suppose for the sake of contradiction that \(T\) was onto. This means that \(Rank(T) = \dim(W)\). The dimension theorem states that $$ \begin{align*} \dim(V) &amp;= Nullity(T) + Rank(T) \\ &amp;= Nullity(T) + \dim(W) \end{align*} $$ This implies that \(Nullity(T) = \dim(V) - \dim(W) &lt; 0\) since \(\dim(V) &lt; \dim(W)\). This is a contradiction and so \(T\) cannot be onto. \(\blacksquare\) Similarly, suppose for the sake of contradiction that \(T\) is one-to-one. This means that \(Nullity(T) = 0\) and so by the dimension theorem, it must be that \(\dim(V) = \dim(W)\). But this is a contradiction since \(\dim(W) &lt; \dim(V)\). References: Practice Midterm]]></summary></entry><entry><title type="html">Lecture 19/20: Determinants</title><link href="http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants.html" rel="alternate" type="text/html" title="Lecture 19/20: Determinants" /><published>2024-08-11T01:01:36-07:00</published><updated>2024-08-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The determinant is a map
$$
\begin{align*}
\det: &amp;M_{n \times n} \rightarrow \mathbf{R} \\
     &amp;A \rightarrow \det(A)
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Properties of the determinant</b></h4>
<ol>
	<li>\(A\) is invertible iff \(\det(A) \neq 0\)</li>
	<li>\(\det(A)\) has geometric meaning.</li>
	To see this, let 
	<div>
	$$
	\begin{align*}
	[0,1]^n = \{(x_1,...,x_n) \in \mathbf{R}^n \ | \ x_i \in [0,1]\}
	\end{align*}
	$$
	</div>
	What does this represent? In \(\mathbf{R}^2\), this is a unit square. In general, it's a cube determined by the vectors \(\{e_1, e_2,...,e_n\}\). 
	<br />
	What does this have anything to do with the determinant? 
	<br />
	Consider, the matrix \(A \in M_{n \times n}\). When we apply \(A\) on each vector in the standard basis, we get \(Ae_1, Ae_2, ...\).
	<br />
	\(L_A([0,1]^n\) is the parallelepiped determined by \(\{Ae_1, Ae_2, ...,Ae_n\}\) and \(volume(L_A([0,1]^n)) = |det(A)|\).
	<br />
	<br />
	<li>The determinant map is not linear except for \(n = 1\). (It is linear in the rows of \(A\))</li>
	<li>\(\det(AB) = \det(A)\det(B)\).</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Definition of the Determinant</b></h4>
<p>The definition of the \(\det: M_{n \times n} \rightarrow \mathbf{R}\) is inductive on \(n\).
<br />
<br />
<!-------------------n=1------------------->
For \(n = 1\):</p>
<div>
$$
\begin{align*}
&amp;M_{1\times 1} = \{(a)\} \leftrightarrow \mathbf{R} \\
       &amp;\det((a)) = a
\end{align*}
$$
</div>
<p>Checking the four properties:</p>
<ol>
	<li>Since we only have one entry in the matrix, then the inverse exists and is \((a)^{-1} = (\frac{1}{a})\) if and only if \(a = \det((a)) \neq 0\).</li>
	<li>Checking the second property we see that for \(L_{(a)}: \mathbf{R}^1 \rightarrow \mathbf{R}^1\)
<div>
$$
\begin{align*}
L_{(a)} : &amp;\mathbf{R}^1 \rightarrow \mathbf{R}^1 \\
&amp;x \rightarrow ax \\
L_{(a)}([0,1]) &amp;= \begin{cases} [0,a] \quad \text{if } a \geq 0 \\ [a,0] \quad \text{if } a \lt 0\end{cases}\\
volume(L_{(a)}([0,1])) &amp;= |a| = |det((a))|.
\end{align*}
$$
</div>
	</li>
	<li>The determinant is linear for \(n = 1\)</li>
	<li>\( det((a)(b)) = det((ab)) = ab = det((a))det((b))  \) </li>
</ol>
<p><br />
<!-------------------n=2------------------->
For \(n = 2\):</p>
<div>
$$
\begin{align*}
\det: \ &amp;M_{2 \times 2} \rightarrow \mathbf{R} \\
       &amp;\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix} \rightarrow ad - bc
\end{align*}
$$
</div>
<p>Checking the four properties:</p>
<ol>
	<li>We previously proved that \(\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\) is invertible if and only if \(ad - bc \neq 0\).</li>
	<li> This property takes some work to see. Check the book for a nice description of it.</li>
	<li>The determinant is not linear for \(n = 2\)</li>
	<li>We want to check that \(\det(AB) = \det(A)\det(B)\). To see this:
<div>
$$
\begin{align*}
\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\begin{pmatrix}\alpha &amp; \beta \\ \gamma &amp; \delta \end{pmatrix} &amp;= \begin{pmatrix}a\alpha + b\gamma &amp; a\beta + b\delta \\ c\alpha + d\gamma &amp; c\beta + d\delta \end{pmatrix} \\
det(AB) &amp;= ((a\alpha + b\gamma)(c\beta + d\delta) - (a\beta + b\delta)(c\beta + d\delta)) \\
&amp;= a\alpha d\delta + b\gamma c\beta - a\beta d\gamma - b\delta c\alpha \\
&amp;= (ad - bc)(a\delta - \beta \gamma)\\
&amp;= \det(A) \det(B).
 \\
\end{align*}
$$
</div>
	</li>
</ol>
<p><br />
<!-------------------n------------------->
So now what about the general case?</p>
<div>
$$
\begin{align*}
\det: \ &amp;M_{n \times n} \rightarrow \mathbf{R}
\end{align*}
$$
</div>
<p>Define \(\tilde{A_{ij}}\) as the \((n-1)\times(n-1)\) matrix obtained from \(A\) by deleting its \(i\)th row and \(j\)th column.
<br />
<br />
For example</p>
<div>
$$
\begin{align*}
A &amp;= \begin{pmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{pmatrix} \\
\tilde{A_{23}} &amp;= \begin{pmatrix}1 &amp; 2 \\ 7 &amp; 8 \end{pmatrix}
\tilde{A_{31}} = \begin{pmatrix}2 &amp; 3 \\ 5 &amp; 6 \end{pmatrix}  
\end{align*}
$$
</div>
<p>So now for \(n \geq 2\) and \(A \in M_{n \times n}\)</p>
<div>
$$
\begin{align*}
det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) 
\end{align*}
$$
</div>
<p>Remark 1:</p>
<div>
$$
\begin{align*}
(-1)^k &amp;= \begin{cases} 1 \quad \phantom{-} k \text{ even} \\ -1 \quad \text{k odd } \end{cases}
\end{align*}
$$
</div>
<p>Remark 2:</p>
<div>
$$
\begin{align*}
\det\left( \begin{pmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix}\right)  
&amp;= (-1)^{1+1}A_{11} det((A_{22})) + (-1)^{1+2}A_{12}det((A_{21})) \\ 
&amp;= A_{11}A_{22} - A_{12}A_{21}.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Compute the determinant for</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 1 &amp; 1 \end{pmatrix} 
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
det(\tilde{A_{11}}) = \det \left(
\begin{pmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{pmatrix}\right) &amp;= -2 \\
det(\tilde{A_{12}}) = \det \left(
\begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= -3 \\
det(\tilde{A_{13}}) = \det \left(
\begin{pmatrix} 1 &amp; 0 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= 1 \\
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
\det(A) &amp;= (-1)^{1+1}(1)(-2) + (-1)^{1+2}(2)(-3) + (-1)^{1+3}(3)(1) \\
       &amp;= -2 + 6 + 3 = 7.
\end{align*}
$$
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(\det(A)\) is linear in rows of \(A\).
</div>
<p><br />
What does this mean? Suppose we have three matrices \(A, B\) and \(C\) in \(M_{n \times n}\) which are equal in all rows but the \(r\)th row. And suppose that for the \(r\)th row that \(a_r = b_r + kc_r\).</p>
<div>
$$
\begin{align*}
B = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}, C = \begin{pmatrix} \pi &amp; \pi \\ 1 &amp; 1 \end{pmatrix}, A = \begin{pmatrix} 1+k\pi &amp; 1+k\pi \\ 1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>Here, they’re all equal except for the \(r\)th row which is the first row here so \(r = 1\). The \(r\)th row of \(A\) is \(a_r = b_r + kc_r\). In this case,</p>
<div>
$$
\begin{align*}
\det(A) = \det(B) + k\det(C)
\end{align*}
$$
</div>
<p><b>Proof:</b>
<br />
We have two cases. The \(r\)th row is 1 or the \(r\)th row is some other row other than 1. Why? because the current definition of the determinant that we have right now is “favoring” the first row. So we want to split the cases around this.
<br />
<br />
Case 1 (\(r = 1\)): 
<br />
Suppose the matrices differ in the first row where the first row of \(A\) is some linear combination of the first row in \(B\) and the first row in \(C\). We know that</p>
<div>
$$
\begin{align*}
\det(A) =  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}})
\end{align*}
$$
</div>
<p>However we know that every entry in the first row of \(A\) can be written as a linear combination of the entries in \(B\) and \(C\) and</p>
<div>
$$
\begin{align*}
A_{1j} = B_{1j} + kC_{1j}.
\end{align*}
$$
</div>
<p>Additionally, by the definition that we have of computing the determinant,</p>
<div>
$$
\begin{align*}
\tilde{A_{1j}} = \tilde{B_{1j}} = \tilde{C_{1j}}.
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\det(A) &amp;=  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} (B_{1j} + kC_{1j}) \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{A_{1j}}) +  \sum_{j=1}^{n} (-1)^{1+j} kC_{1j} \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) +  k\sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\
&amp;= \det(B) + k\det(C).
\end{align*}
$$
</div>
<p>Case 2 (\(r &gt; 1\)): By Induction on \(n\)
<br />
Base Case: When \(n = 1\), the determinat is linear.
<br />
<br />
Inductive Step: Suppose it is true for \(n - 1\). Then,</p>
<div>
$$
\begin{align*}
\det(A) &amp;=  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}})
\end{align*}
$$
</div>
<p>In this case, we know the matrices differ in the \(r\)‘th row where \(r &gt; 1\). and so the first row of each matrix is the same. \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). What about the determinants of \(\tilde{A_{1j}}, \tilde{B_{1j}}\) and \(\tilde{C_{1j}}\)? Well, since they are now of size \(n-1 \times n-1\), then we can use the inductive hypothesis to conclude that,</p>
<div>
$$
\begin{align*}
\det(\tilde{A_{1j}}) &amp;=  \det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}})
\end{align*}
$$
</div>
<p>So now we can use all of these facts to compute the determinant,</p>
<div>
$$
\begin{align*}
\det(A) &amp;=  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} (\det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}})) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{C_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\
&amp;= \det(B) + k\det(C). \ \blacksquare
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For any \(r = 1,2,...,n\), we have 
$$
\begin{align*}
\det(A) =  \sum_{j=1}^{n} (-1)^{r+j} A_{rj} \det(\tilde{A_{rj}})
\end{align*}
$$
</div>
<p><br />
The proof for this theorem requires the following lemma
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
Given \(A\). Let \(B\) be the matrix equal to \(A\) with the \(r\)th row placed by \(e_j\) so 
$$
\begin{align*}
B &amp;= \begin{pmatrix} \bar{b_1} \\ \vdots \\ e_j \\ \vdots \\ \bar{b_n} \end{pmatrix}
\end{align*}
$$
Then,
$$
\begin{align*}
\det(B) &amp;=  (-1)^{r+j} \det(\tilde{B_{rj}})
\end{align*}
$$
</div>
<p><br />
The proof of this lemma is in the textbook (page 214). (TODO: check). So now we’ll do the proof for the theorem.
<!------------------------------------------------------------------------------------>
<br />
<br />
<b>Proof (Using the Lemma)</b>
<br />
Given \(A\). Let \(B_j\) be the matrix equal to \(A\) with the \(r\)th row replaced by \(e_j\). By the technical lemma,</p>
<div>
$$
\begin{align*}
\det(B_j) &amp;=  (-1)^{r+j} \det(\tilde{(B_{j})_{rj}})
\end{align*}
$$
</div>
<p>(<i>Note this is this the same expression as the technical lemma. It’s just that the matrix here is called \(B_j\) and in the technical lemma it is \(B\). Moreover, \(\det(\tilde{(B_{j})_{rj}})\) says take the matrix \(B_j\) and remove the row \(r\) and column \(j\) to compute that determinant)</i> 
<br />
<br />
So now, since we’re computing the determinant by removing the row \(r\) and column \(j\), this determinant should the same exact determinant as \(\det(\tilde{A_{rj}})\) since they only differ in row \(r\) and column \(j\). Therefore, we replace it with \(\det(\tilde{A_{rj}})\) in</p>
<div>
$$
\begin{align*}
\det(B_j) &amp;=  (-1)^{r+j} \det(\tilde{(B_{j})_{rj}}) \\
&amp;=  (-1)^{r+j} \det(\tilde{A_{rj}})
\end{align*}
$$
</div>
<p>Next, notice that \(e_1,...e_j\) .. are vectors of the standard basis for \(\mathbf{R}^n\). So we know that the \(r\)th row of \(A\) can be expressed as a linear combination of these vectors. Moreover, the coefficients of this linear combination are just the entries in the \(r\)th row of \(A\). So by Theorem 1,</p>
<div>
$$
\begin{align*}
\det(A) &amp;= \sum A_{rj} \det(B_j) \\
        &amp;= \sum A_{rj}  (-1)^{r+j} \det(\tilde{A_{rj}}).
\end{align*}
$$
</div>
<p>as we wanted to show. \(\blacksquare\)
<!------------------------------------------------------------------------------------>
<br />
<br />
This gives the following corollary
<br /></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
If \(A\) has a row of all zeros, then \(\det A = 0\).
</div>
<p><br />
This is all fine but unfortunately, it doesn’t still solve the problem of the computation of the determinant taking too long to compute. Since a matrix at random will likely not have a row of all zeros. We still have another corollary
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
If \(A\) has two identical rows, then \(\det A = 0\).
</div>
<p><br />
<b>Proof</b>
<br />
By induction on \(n\). 
<br />
<br />
Base Case: \(n = 2\).</p>
<div>
$$
\begin{align*}
\det\begin{pmatrix} a &amp; b \\ a &amp; b \end{pmatrix} = ab - ab = 0.
\end{align*}
$$
</div>
<p>Inductive Step: <br />
Assume this is true for \(n - 1\).</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} \bar{a_1} \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ \bar{b_n} \end{pmatrix}, \text{ where $a_i = a_j$}
	\end{align*}
	$$
</div>
<p>The goal is to compute the determinant of \(A\) using the inductive hypothesis. Well if we choose \(r\) such that it is not \(i\) or \(j\), then by the previous theorem we know that</p>
<div>
	$$
	\begin{align*}
	\det(A) &amp;= \sum A_{rj}  (-1)^{r+j} \det(\tilde{A_{rj}}).
	\end{align*}
	$$
</div>
<p>But \(\det(\tilde{A_{rj}})\) is 0 since \(\tilde{A_{rj}}\) has two identical rows and its size is \(n-1 \times n-1\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\det(A) &amp;= 0.
	\end{align*}
	$$
</div>
<p>As we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The determinant is a map $$ \begin{align*} \det: &amp;M_{n \times n} \rightarrow \mathbf{R} \\ &amp;A \rightarrow \det(A) \end{align*} $$ Properties of the determinant \(A\) is invertible iff \(\det(A) \neq 0\) \(\det(A)\) has geometric meaning. To see this, let $$ \begin{align*} [0,1]^n = \{(x_1,...,x_n) \in \mathbf{R}^n \ | \ x_i \in [0,1]\} \end{align*} $$ What does this represent? In \(\mathbf{R}^2\), this is a unit square. In general, it's a cube determined by the vectors \(\{e_1, e_2,...,e_n\}\). What does this have anything to do with the determinant? Consider, the matrix \(A \in M_{n \times n}\). When we apply \(A\) on each vector in the standard basis, we get \(Ae_1, Ae_2, ...\). \(L_A([0,1]^n\) is the parallelepiped determined by \(\{Ae_1, Ae_2, ...,Ae_n\}\) and \(volume(L_A([0,1]^n)) = |det(A)|\). The determinant map is not linear except for \(n = 1\). (It is linear in the rows of \(A\)) \(\det(AB) = \det(A)\det(B)\). Definition of the Determinant The definition of the \(\det: M_{n \times n} \rightarrow \mathbf{R}\) is inductive on \(n\). For \(n = 1\): $$ \begin{align*} &amp;M_{1\times 1} = \{(a)\} \leftrightarrow \mathbf{R} \\ &amp;\det((a)) = a \end{align*} $$ Checking the four properties: Since we only have one entry in the matrix, then the inverse exists and is \((a)^{-1} = (\frac{1}{a})\) if and only if \(a = \det((a)) \neq 0\). Checking the second property we see that for \(L_{(a)}: \mathbf{R}^1 \rightarrow \mathbf{R}^1\) $$ \begin{align*} L_{(a)} : &amp;\mathbf{R}^1 \rightarrow \mathbf{R}^1 \\ &amp;x \rightarrow ax \\ L_{(a)}([0,1]) &amp;= \begin{cases} [0,a] \quad \text{if } a \geq 0 \\ [a,0] \quad \text{if } a \lt 0\end{cases}\\ volume(L_{(a)}([0,1])) &amp;= |a| = |det((a))|. \end{align*} $$ The determinant is linear for \(n = 1\) \( det((a)(b)) = det((ab)) = ab = det((a))det((b)) \) For \(n = 2\): $$ \begin{align*} \det: \ &amp;M_{2 \times 2} \rightarrow \mathbf{R} \\ &amp;\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix} \rightarrow ad - bc \end{align*} $$ Checking the four properties: We previously proved that \(\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\) is invertible if and only if \(ad - bc \neq 0\). This property takes some work to see. Check the book for a nice description of it. The determinant is not linear for \(n = 2\) We want to check that \(\det(AB) = \det(A)\det(B)\). To see this: $$ \begin{align*} \begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\begin{pmatrix}\alpha &amp; \beta \\ \gamma &amp; \delta \end{pmatrix} &amp;= \begin{pmatrix}a\alpha + b\gamma &amp; a\beta + b\delta \\ c\alpha + d\gamma &amp; c\beta + d\delta \end{pmatrix} \\ det(AB) &amp;= ((a\alpha + b\gamma)(c\beta + d\delta) - (a\beta + b\delta)(c\beta + d\delta)) \\ &amp;= a\alpha d\delta + b\gamma c\beta - a\beta d\gamma - b\delta c\alpha \\ &amp;= (ad - bc)(a\delta - \beta \gamma)\\ &amp;= \det(A) \det(B). \\ \end{align*} $$ So now what about the general case? $$ \begin{align*} \det: \ &amp;M_{n \times n} \rightarrow \mathbf{R} \end{align*} $$ Define \(\tilde{A_{ij}}\) as the \((n-1)\times(n-1)\) matrix obtained from \(A\) by deleting its \(i\)th row and \(j\)th column. For example $$ \begin{align*} A &amp;= \begin{pmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{pmatrix} \\ \tilde{A_{23}} &amp;= \begin{pmatrix}1 &amp; 2 \\ 7 &amp; 8 \end{pmatrix} \tilde{A_{31}} = \begin{pmatrix}2 &amp; 3 \\ 5 &amp; 6 \end{pmatrix} \end{align*} $$ So now for \(n \geq 2\) and \(A \in M_{n \times n}\) $$ \begin{align*} det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ Remark 1: $$ \begin{align*} (-1)^k &amp;= \begin{cases} 1 \quad \phantom{-} k \text{ even} \\ -1 \quad \text{k odd } \end{cases} \end{align*} $$ Remark 2: $$ \begin{align*} \det\left( \begin{pmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix}\right) &amp;= (-1)^{1+1}A_{11} det((A_{22})) + (-1)^{1+2}A_{12}det((A_{21})) \\ &amp;= A_{11}A_{22} - A_{12}A_{21}. \end{align*} $$ Example Compute the determinant for $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 1 &amp; 1 \end{pmatrix} \end{align*} $$ $$ \begin{align*} det(\tilde{A_{11}}) = \det \left( \begin{pmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{pmatrix}\right) &amp;= -2 \\ det(\tilde{A_{12}}) = \det \left( \begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= -3 \\ det(\tilde{A_{13}}) = \det \left( \begin{pmatrix} 1 &amp; 0 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= 1 \\ \end{align*} $$ $$ \begin{align*} \det(A) &amp;= (-1)^{1+1}(1)(-2) + (-1)^{1+2}(2)(-3) + (-1)^{1+3}(3)(1) \\ &amp;= -2 + 6 + 3 = 7. \end{align*} $$ Theorem \(\det(A)\) is linear in rows of \(A\). What does this mean? Suppose we have three matrices \(A, B\) and \(C\) in \(M_{n \times n}\) which are equal in all rows but the \(r\)th row. And suppose that for the \(r\)th row that \(a_r = b_r + kc_r\). $$ \begin{align*} B = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}, C = \begin{pmatrix} \pi &amp; \pi \\ 1 &amp; 1 \end{pmatrix}, A = \begin{pmatrix} 1+k\pi &amp; 1+k\pi \\ 1 &amp; 1 \end{pmatrix} \end{align*} $$ Here, they’re all equal except for the \(r\)th row which is the first row here so \(r = 1\). The \(r\)th row of \(A\) is \(a_r = b_r + kc_r\). In this case, $$ \begin{align*} \det(A) = \det(B) + k\det(C) \end{align*} $$ Proof: We have two cases. The \(r\)th row is 1 or the \(r\)th row is some other row other than 1. Why? because the current definition of the determinant that we have right now is “favoring” the first row. So we want to split the cases around this. Case 1 (\(r = 1\)): Suppose the matrices differ in the first row where the first row of \(A\) is some linear combination of the first row in \(B\) and the first row in \(C\). We know that $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ However we know that every entry in the first row of \(A\) can be written as a linear combination of the entries in \(B\) and \(C\) and $$ \begin{align*} A_{1j} = B_{1j} + kC_{1j}. \end{align*} $$ Additionally, by the definition that we have of computing the determinant, $$ \begin{align*} \tilde{A_{1j}} = \tilde{B_{1j}} = \tilde{C_{1j}}. \end{align*} $$ Therefore, $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} (B_{1j} + kC_{1j}) \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{A_{1j}}) + \sum_{j=1}^{n} (-1)^{1+j} kC_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) + k\sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\ &amp;= \det(B) + k\det(C). \end{align*} $$ Case 2 (\(r &gt; 1\)): By Induction on \(n\) Base Case: When \(n = 1\), the determinat is linear. Inductive Step: Suppose it is true for \(n - 1\). Then, $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ In this case, we know the matrices differ in the \(r\)‘th row where \(r &gt; 1\). and so the first row of each matrix is the same. \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). What about the determinants of \(\tilde{A_{1j}}, \tilde{B_{1j}}\) and \(\tilde{C_{1j}}\)? Well, since they are now of size \(n-1 \times n-1\), then we can use the inductive hypothesis to conclude that, $$ \begin{align*} \det(\tilde{A_{1j}}) &amp;= \det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}}) \end{align*} $$ So now we can use all of these facts to compute the determinant, $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} (\det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}})) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{C_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\ &amp;= \det(B) + k\det(C). \ \blacksquare \end{align*} $$ Theorem For any \(r = 1,2,...,n\), we have $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{r+j} A_{rj} \det(\tilde{A_{rj}}) \end{align*} $$ The proof for this theorem requires the following lemma Lemma Given \(A\). Let \(B\) be the matrix equal to \(A\) with the \(r\)th row placed by \(e_j\) so $$ \begin{align*} B &amp;= \begin{pmatrix} \bar{b_1} \\ \vdots \\ e_j \\ \vdots \\ \bar{b_n} \end{pmatrix} \end{align*} $$ Then, $$ \begin{align*} \det(B) &amp;= (-1)^{r+j} \det(\tilde{B_{rj}}) \end{align*} $$ The proof of this lemma is in the textbook (page 214). (TODO: check). So now we’ll do the proof for the theorem. Proof (Using the Lemma) Given \(A\). Let \(B_j\) be the matrix equal to \(A\) with the \(r\)th row replaced by \(e_j\). By the technical lemma, $$ \begin{align*} \det(B_j) &amp;= (-1)^{r+j} \det(\tilde{(B_{j})_{rj}}) \end{align*} $$ (Note this is this the same expression as the technical lemma. It’s just that the matrix here is called \(B_j\) and in the technical lemma it is \(B\). Moreover, \(\det(\tilde{(B_{j})_{rj}})\) says take the matrix \(B_j\) and remove the row \(r\) and column \(j\) to compute that determinant) So now, since we’re computing the determinant by removing the row \(r\) and column \(j\), this determinant should the same exact determinant as \(\det(\tilde{A_{rj}})\) since they only differ in row \(r\) and column \(j\). Therefore, we replace it with \(\det(\tilde{A_{rj}})\) in $$ \begin{align*} \det(B_j) &amp;= (-1)^{r+j} \det(\tilde{(B_{j})_{rj}}) \\ &amp;= (-1)^{r+j} \det(\tilde{A_{rj}}) \end{align*} $$ Next, notice that \(e_1,...e_j\) .. are vectors of the standard basis for \(\mathbf{R}^n\). So we know that the \(r\)th row of \(A\) can be expressed as a linear combination of these vectors. Moreover, the coefficients of this linear combination are just the entries in the \(r\)th row of \(A\). So by Theorem 1, $$ \begin{align*} \det(A) &amp;= \sum A_{rj} \det(B_j) \\ &amp;= \sum A_{rj} (-1)^{r+j} \det(\tilde{A_{rj}}). \end{align*} $$ as we wanted to show. \(\blacksquare\) This gives the following corollary Corollary 1 If \(A\) has a row of all zeros, then \(\det A = 0\). This is all fine but unfortunately, it doesn’t still solve the problem of the computation of the determinant taking too long to compute. Since a matrix at random will likely not have a row of all zeros. We still have another corollary Corollary 2 If \(A\) has two identical rows, then \(\det A = 0\). Proof By induction on \(n\). Base Case: \(n = 2\). $$ \begin{align*} \det\begin{pmatrix} a &amp; b \\ a &amp; b \end{pmatrix} = ab - ab = 0. \end{align*} $$ Inductive Step: Assume this is true for \(n - 1\). $$ \begin{align*} A &amp;= \begin{pmatrix} \bar{a_1} \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ \bar{b_n} \end{pmatrix}, \text{ where $a_i = a_j$} \end{align*} $$ The goal is to compute the determinant of \(A\) using the inductive hypothesis. Well if we choose \(r\) such that it is not \(i\) or \(j\), then by the previous theorem we know that $$ \begin{align*} \det(A) &amp;= \sum A_{rj} (-1)^{r+j} \det(\tilde{A_{rj}}). \end{align*} $$ But \(\det(\tilde{A_{rj}})\) is 0 since \(\tilde{A_{rj}}\) has two identical rows and its size is \(n-1 \times n-1\). Therefore, $$ \begin{align*} \det(A) &amp;= 0. \end{align*} $$ As we wanted to show. \(\blacksquare\) References Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 18: Elementary Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/10/lec18-elementary-matrices.html" rel="alternate" type="text/html" title="Lecture 18: Elementary Matrices" /><published>2024-08-10T01:01:36-07:00</published><updated>2024-08-10T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/10/lec18-elementary-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/10/lec18-elementary-matrices.html"><![CDATA[<p>Recall three types of elementary row operations</p>
<ul style="list-style-type:lower-alpha">
	<li>\(R_i \leftrightarrow R_j\)</li>
	<li>\(R_i \rightarrow \lambda R_i\) where \(\lambda \neq 0\)</li>
	<li>\(R_i \rightarrow R_i + \lambda R_j\)</li>
</ul>
<p>Now that we’ve studied matrix multiplication we can state the fact that performing an elementary row operation on \(A \in M_{2 \times 2}\) can actually be described using matrix multiplication.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Elementary Matrices</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Applying the three types of elementary row operations results in the following matrices</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix} \rightarrow E_1 &amp;= 
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix} \\
E_2 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; \lambda
\end{pmatrix} \\
E_3 &amp;= 
\begin{pmatrix}
1 &amp; 0 \\
\lambda &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>This leads to the next theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	Let \(E\) be the elementary matrix obtained from \(I_n\) by performing row operation \(\mathcal{R}(E = E(\mathcal{R})))\) for any \(A \in M_{m \times n}\), the product
	$$
	\begin{align*}
	E(\mathcal{R}) \cdot A_{m \times n}
    \end{align*}
	$$
	is equal to the matrix obtained from \(A\) by performing \(\mathcal{R}\).
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let’s apply the elementary matrices on the following given matrix</p>
<div>
$$
\begin{align*}
E_1  \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
\\
E_2
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} 
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ \lambda c &amp; \lambda d \end{pmatrix} 
\\
E_3
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
&amp;=\begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix}
\begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
= \begin{pmatrix} a &amp; b \\ c + \lambda a &amp; d + \lambda b \end{pmatrix} 
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>RREF by Matrix Multiplication</b></h4>
<p>Since we can perform elementary row operations by matrix multiplication, then we can possibly see how we can put a matrix in reduced row echelon by multiplication. But first, there is an observation</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	Each elementary matrix is invertible.
	$$
	\begin{align*}
	&amp;\mathcal{R}: R_i \leftrightarrow R_j \quad \quad \quad \quad \ \ \mathcal{R}^{-1}: R_j \leftrightarrow R_i \\
	&amp;\mathcal{R}: R_i \rightarrow \lambda R_i \quad \quad \quad \quad \mathcal{R}^{-1}: R_i \rightarrow \frac{1}{\lambda} R_i \\
	&amp;\mathcal{R}: R_i \rightarrow R_i + \lambda R_j \quad \quad \mathcal{R}^{-1}: R_i \rightarrow R_i - \lambda R_j
    \end{align*}
	$$
</div>
<p><br />
Proof: apply the elementary row operation by multiplying by \(E(\mathcal{R})\) and then apply the inverse again by multiplying by \(E(\mathcal{R}^{-1})\). The result is the identity matrix. In other words, \(E(\mathcal{R})E(\mathcal{R}^{-1}) = I_n\). 
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For every \(A \in M_{m \times n}\), there is a finite set of elementary matrices \(E_1,...,E_k \in M_{m \times n}\) such that \(E_k ... E_2E_1\) is in RREF.  
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	\(A \in M_{m \times n}\) is invertible if and only iff there is a finite set of elementary matrices \(E_1,...,E_k \in M_{n \times n}\) such that \(E_k...E_2E_1A = I_n\).  \((A \ | \ I_n)\).
</div>
<p><br />
Note here that from the expression above we can see that \(A^{-1} = E_k...E_1\) and \(A = E_1^{-1}E_2^{-1}...E^{-1}_{k-1}E^{-1}_{k}\)
<br />
<br /></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	\(A\) is invertible if and only iff it can be written as a product of elementary matrices.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Rank of a Matrix</b></h4>
<p>Recall that the rank of \(T: V \rightarrow W\) is \(\dim(R(T))\).</p>

<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	The rank of \(A \in M_{n \times n}\), \(rank(A)\) is the rank of \(L_A: \mathbf{R}^m \rightarrow \mathbf{R}^n\).
</div>
<p><br />
This definition is kind of awkward and instead we want to find an expression for rank(\(A\)) in terms of \(A\) itself and not have to rely on \(L_A\). To figure this out, we need the following result</p>
<div class="purdiv">
Proposition
</div>
<div class="purbdiv">
If \(B \in M_{n \times n}\) is invertible, then rank(\(BA\)) = rank(\(A\)).
</div>
<p><br />
So multiplication by \(B\) doesn’t change the rank.
<br />
<br />
Proof: <br />
By definition \(rank(BA)\) is the rank of the linear map \(L_{BA}\). But the rank of a linear map is the dimension of its range and so</p>
<div>
$$
\begin{align*}
rank(BA) &amp;= rank(L_{BA}) \\
         &amp;= \dim(R(L_{BA})).
\end{align*}
$$
</div>
<p>B definition, to get the range, we just apply the linear map on \(v \in \mathbf{R}^n\). This range is a subset of \(\mathbf{R}^m\). Applying the linear map is just multiplying \(v\) by the matrices \(A\) and \(B\). So we can see this below:</p>
<div>
$$
\begin{align*}
\dim(R(L_{BA})) &amp;= \{L_{BA}(v) \ | \ v \in \mathbf{R}^n \} \subset \mathbf{R}^m \\
                &amp;= \{BA(v) \ | \ v \mathbf{R}^n\} \\
				&amp;= \{B(A(v)) \ | \ v \mathbf{R}^n\} \\
				&amp;= \{B(A(v)) \ | \ v \mathbf{R}^n\}
\end{align*}
$$
</div>
<p>We can also re-arrange this by applying the linear map \(L_B\) on the set that we get from applying \(A\) as follows</p>
<div>
$$
\begin{align*}
\dim(R(L_{BA})) &amp;= L_B(\{A(v) \ | \ v \mathbf{R}^n\}) \\
\end{align*}
$$
</div>
<p>But this internal set is the range of the linear map \(L_A\), \(R(L_A)\). So define the map</p>
<div>
$$
\begin{align*}
L_B&amp;: \mathbf{R}^m \rightarrow \mathbf{R}^m \\
\tilde{L_B}&amp;: R(L_A) \rightarrow L_B(R(L_A))
\end{align*}
$$
</div>
<p>We claim this new map is invertible. It’s onto because we restricted the target to the image \(R(L_A)\). It’s one-to-one because \(B\) is invertible. Therefore, the dimension of the domain and the target are the same. So</p>
<div>
$$
\begin{align*}
\dim(R(L_A)) = \dim(L_B(R(L_A)))
\end{align*}
$$
</div>
<p>But we know that \(\dim(R(L_A))\) is the rank of \(A\). and \(\dim(L_B(R(L_A))\) is the rank of \(L_{BA}\) (why?) so \(rank(A) = rank(BA)\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Rank of a Matrix</b></h4>
<p>So now we can go back to our original goal of finding an expression for finding the rank of a matrix \(A\) without going back to the rank of the linear map \(L_A\). We just proved that \(rank(BA) = rank(A)\). for any invertible matrix \(B\). We further studied earlier that elementary matrices are invertible. So multiplying \(A\) by a set of elementary matrices will not change its rank. So we can get \(A\) in RREF without its rank changing. Based on this we have the following corollaries:</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	Elementary row operations don't change rank.
</div>
<p><br />
and</p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
	$$
	\begin{align*}
	rank(A) = rank(RREF(A))
    \end{align*}
	$$
</div>
<p><br />
Why do we want RREF? because it’s easy to read off and we can easily figure out the dimension easily from seeing a matrix in its RREF.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>What is the range of the following matrix?</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>We know the rank of \(A\). It is the dimension of the range of \(L_A\) or \(\dim(R(L_A))\). Notice for the above matrix, if we multiply \(A\) by a vector \(v\), the result is a linear combination of the columns of \(A\). Recall</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}
\begin{pmatrix} x \\ y \\ z \\ w \end{pmatrix}
=
x
\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}
+
y
\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
+
z
\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}
+
w
\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>In other words, the columns span the range. Here, we see that we have 3 non-zero columns and they are linearly independent. so \(rank(A) = \dim(R(L_A)) = 3\). This works here but it’s not generally true!! we’re missing something … so let’s clarify with another example
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>What is the range of the following matrix?</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>It’s still true that the range of this is still spanned by the columns (always true). But the rank of \(A\) in general is the number of columns with leading entries in \(REF\) of \(A\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Nullity and The Dimension Theorem</b></h4>
<p>So we know that \(\text{nullity}(A) = dim(N(A))\). We found a basis for the null space of \(A\) by solving \(Ax = 0\) and finding all the solutions and then writing a set that spans that solution set. From the basis we knew the dimension of the null space. Specifically when we solved \(Ax = 0\), it was the number of columns without leading entries. So we can write \(\text{nullity}(A) = dim(N(A)) =\) # of columns without leading entries.
<br />
<br />
So now if we put together the number of columns without leading entries (nullity of \(A\)) and the number of columns with leading entries (rank of \(A\)), then we get \(n\). This is basically the dimension theorem.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recall three types of elementary row operations \(R_i \leftrightarrow R_j\) \(R_i \rightarrow \lambda R_i\) where \(\lambda \neq 0\) \(R_i \rightarrow R_i + \lambda R_j\) Now that we’ve studied matrix multiplication we can state the fact that performing an elementary row operation on \(A \in M_{2 \times 2}\) can actually be described using matrix multiplication. Elementary Matrices Definition An \(m \times n\) elementary matrix obtained from \(I_n\) by performing an elementary row operation of type I, II or III. Example Applying the three types of elementary row operations results in the following matrices $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} \rightarrow E_1 &amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \\ E_2 &amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} \\ E_3 &amp;= \begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix} \end{align*} $$ This leads to the next theorem Theorem Let \(E\) be the elementary matrix obtained from \(I_n\) by performing row operation \(\mathcal{R}(E = E(\mathcal{R})))\) for any \(A \in M_{m \times n}\), the product $$ \begin{align*} E(\mathcal{R}) \cdot A_{m \times n} \end{align*} $$ is equal to the matrix obtained from \(A\) by performing \(\mathcal{R}\). Example Let’s apply the elementary matrices on the following given matrix $$ \begin{align*} E_1 \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} &amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} \\ E_2 \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} &amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \lambda \end{pmatrix} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = \begin{pmatrix} a &amp; b \\ \lambda c &amp; \lambda d \end{pmatrix} \\ E_3 \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} &amp;=\begin{pmatrix} 1 &amp; 0 \\ \lambda &amp; 1 \end{pmatrix} \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} = \begin{pmatrix} a &amp; b \\ c + \lambda a &amp; d + \lambda b \end{pmatrix} \end{align*} $$ RREF by Matrix Multiplication Since we can perform elementary row operations by matrix multiplication, then we can possibly see how we can put a matrix in reduced row echelon by multiplication. But first, there is an observation Corollary Each elementary matrix is invertible. $$ \begin{align*} &amp;\mathcal{R}: R_i \leftrightarrow R_j \quad \quad \quad \quad \ \ \mathcal{R}^{-1}: R_j \leftrightarrow R_i \\ &amp;\mathcal{R}: R_i \rightarrow \lambda R_i \quad \quad \quad \quad \mathcal{R}^{-1}: R_i \rightarrow \frac{1}{\lambda} R_i \\ &amp;\mathcal{R}: R_i \rightarrow R_i + \lambda R_j \quad \quad \mathcal{R}^{-1}: R_i \rightarrow R_i - \lambda R_j \end{align*} $$ Proof: apply the elementary row operation by multiplying by \(E(\mathcal{R})\) and then apply the inverse again by multiplying by \(E(\mathcal{R}^{-1})\). The result is the identity matrix. In other words, \(E(\mathcal{R})E(\mathcal{R}^{-1}) = I_n\). Theorem For every \(A \in M_{m \times n}\), there is a finite set of elementary matrices \(E_1,...,E_k \in M_{m \times n}\) such that \(E_k ... E_2E_1\) is in RREF. Theorem \(A \in M_{m \times n}\) is invertible if and only iff there is a finite set of elementary matrices \(E_1,...,E_k \in M_{n \times n}\) such that \(E_k...E_2E_1A = I_n\). \((A \ | \ I_n)\). Note here that from the expression above we can see that \(A^{-1} = E_k...E_1\) and \(A = E_1^{-1}E_2^{-1}...E^{-1}_{k-1}E^{-1}_{k}\) Corollary \(A\) is invertible if and only iff it can be written as a product of elementary matrices. The Rank of a Matrix Recall that the rank of \(T: V \rightarrow W\) is \(\dim(R(T))\).]]></summary></entry><entry><title type="html">Lecture 17: Change of Coordinates and Matrix Representations</title><link href="http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates.html" rel="alternate" type="text/html" title="Lecture 17: Change of Coordinates and Matrix Representations" /><published>2024-08-09T01:01:36-07:00</published><updated>2024-08-09T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/09/lec17-change-of-coordinates.html"><![CDATA[<p>Given a vector space \(V\) with two finite basis \(\beta\) and \(\beta'\). For \(v \in V\), what is the relationship between \([v]_{\beta}\) and \([v]_{\beta'}\)?
<br />
<br />
Starting with \([v]_{\beta'}\), we want to derive an expression for \([v]_{\beta}\). We can write</p>
<div>
$$
\begin{align*}
[v]_{\beta'} &amp;= [I_V(v)]_{\beta'} \\
           &amp;= [I_V]_{\beta}^{\beta'}[v]_{\beta} \\
\end{align*}
$$
</div>
<p>In the first line we just re-wrote the vector so now the identity map is applied on \(v\) which doesn’t change \(v\). 
<br />
<br />
By theorem 2.14, we can re-write this line by first computing the matrix representative of the identity transformation that changes coordinates from \(\beta\) to \(\beta'\). We get this matrix by taking the vectors in \(\beta\), applying the identity transformation which does nothing and then re-writing these vectors with respect to \(\beta'\). These vectors will be the columns of the matrix. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = P_1\), \(\beta = \{1, x\}\) and \(\beta' = \{1+x, 1-x\}\). Then to compute the matrix \([I_V]_{\beta}^{\beta'}\), we want to apply the map on the basis \(\beta\) and then write these relative to the basis \(\beta'\) (meaning for each vector, find its coordinates when written as a linear combination of the vectors of the basis \(\beta'\)).</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'} &amp;= 
\begin{pmatrix}
[I_V(1)]_{\beta'} &amp; [I_V(x)]_{\beta'}
\end{pmatrix} \\
&amp;=
\begin{pmatrix}
[1]_{\beta'} &amp; [x]_{\beta'}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Now we need to find the coordinates when 1 is written as a linear combination of the vectors of basis \(\beta'\) so</p>
<div>
$$
\begin{align*}
1 &amp;= a(1 + x) + b(1 - x) \\
1 &amp;= a + ax + b - bx \\
1 &amp;= a + b + x(a - b) \\
\end{align*}
$$
</div>
<p>from this we see that \(a = b = 1/2\). Similarly we need to find the coordinates of \(x\)</p>
<div>
$$
\begin{align*}
x &amp;= a(1 + x) + b(1 - x) \\
0 &amp;= a + ax + b - bx - x \\
0 &amp;= a + b + x(a - b - 1) \\
\end{align*}
$$
</div>
<p>A solution to this is \(a = 1/2\) and \(b = -1/2\). So now we have</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'} &amp;= 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \\
\frac{1}{2} &amp; -\frac{1}{2}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We can now use this matrix to transform any vector in \(V\) written with respect to \(\beta\) to a vector written with respect to \(\beta'\). Given some polynomial in \(V\),</p>
<div>
$$
\begin{align*}
[a_0 + a_1x]_{\beta'} = 
\begin{pmatrix}
\frac{1}{2} &amp; \frac{1}{2} \\
\frac{1}{2} &amp; -\frac{1}{2}
\end{pmatrix}
\begin{pmatrix}
a_0 \\
a_1
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{2}(a_0 + a_1) \\
\frac{1}{2}(a_0 - a_1)
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Inverse of the Change of Coordinates Matrix</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	$$
	\begin{align*}
([I_V]_{\beta}^{\beta'})^{-1} = [I_V]^{\beta}_{\beta'}
    \end{align*}
	$$
</div>
<p><br />
Proof: To prove that a matrix is an inverse of another matrix, we need to show that their product is the identity matrix so</p>
<div>
$$
\begin{align*}
[I_V]_{\beta}^{\beta'}[I_V]^{\beta}_{\beta'} &amp;= [I_V \circ I_V]^{\beta'}_{\beta'} \text{ (Theorem 2.11)}\\
 &amp;= [I_V]^{\beta'}_{\beta'} \\
 &amp;= I_n. \ \blacksquare
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Change of Bases</b></h4>
<p>Question: Given \(T: V \rightarrow W\) and finite bases \(\beta, \beta'\) for \(V\) and \(\gamma, \gamma'\) for \(W\), how are \([T]^{\gamma}_{\beta}\) and \([T]^{\gamma'}_{\beta'}\) related? The only tool we have is composition and using Theorem 2.11</p>
<div>
$$
\begin{align*}
[T]^{\gamma'}_{\beta'} &amp;= [I_W \circ T \circ I_V]^{\gamma'}_{\beta'} \\
	&amp;= [I_W]^{\gamma'}_{\gamma}  [T \circ I_V]^{\gamma}_{\beta'} \\
	&amp;= [I_W]^{\gamma'}_{\gamma}  [T]^{\gamma}_{\beta} [I_V]^{\beta}_{\beta'} \\
\end{align*}
$$
</div>
<p>If \(W = V\) and \(\gamma, \gamma' = \beta, \beta'\), then</p>
<div>
$$
\begin{align*}
[T]^{\beta'}_{\beta'} &amp;= [I_W]^{\beta'}_{\beta}  [T]^{\beta}_{\beta} [I_V]^{\beta}_{\beta'} \\
[T]_{\beta'} &amp;= Q^{-1}[T]_{\beta}Q
\end{align*}
$$
</div>
<p>Where \(Q = [I_V]^{\beta}_{\beta'}\). If we find such a matrix \(Q\) to relate \([T]_{\beta}\) and \([T]_{\beta'}\) then we call these matrices similar. 
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	\(A, B \in M_{2 \times 2}\) are similar if \(\exists Q \in M_{2 \times 2}\) such that \(B = Q^{-1}AQ\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(A =  \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix}\) and consider the map \(L_A: \mathbf{R}^2 \rightarrow \mathbf{R}^2\). Suppose we want to compute \(L_A^k = L_A \circ L_A \circ ... \circ L_A\) (\(k\) times)? If we try this the naive way</p>
<div>
$$
\begin{align*}
A^k = \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} ...
\end{align*}
$$
</div>
<p>This is going to be really hard to multiply. Can we simplify this computation?
<br />
<br />
So what is \(A^2\) really? It’s the matrix representative of the composition \([L_A \circ L_A]\). In fact if we go back to just \([L_A]_{\beta}\). This is the matrix representative of this map with respect to the standard basis so \([L_A]_{\beta} = A\). The idea then is to find another basis such that the matrix representation of the map with respect to that basis is easier to multiply. So the plan is to find \(\beta'\) such that \([L_A]_{\beta'}^{\beta'}\) is easier to multiply.
<br />
<br />
Note that for \(v_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, v_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\), we’ll find that</p>
<div>
$$
\begin{align*}
L_A(v_1) &amp;= Av_1 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 8 \\ 8 \end{pmatrix} = 8v_1 + 0v_2 \\
L_A(v_2) &amp;= Av_2 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 4 \\ -4 \end{pmatrix} = 0v_1 + 4v_2.
\end{align*}
$$
</div>
<p>Notice that \(\beta' = \{v_1, v_2\}\) is a basis for \(\mathbf{R}^2\). To find \([L_A]_{\beta'}^{\beta'} = [L_A]_{\beta'}\), we need to apply \(L_A\) on the basis vectors of \(\beta'\) and then write them with respect to \(\beta'\).</p>
<div>
$$
\begin{align*}
[L_A]_{\beta'} &amp;= ([L_A(v_1)]_{\beta'}, [L_A]_{\beta'}) \\
              &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix}
\end{align*}
$$
</div>
<p>This leads to</p>
<div>
$$
\begin{align*}
[L_A \circ L_A ... \circ L_A]_{\beta'}^{\beta'} &amp;= [L_A]_{\beta'}^{\beta'} ... [L_A]_{\beta'}^{\beta'} \\
              &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} ... \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} \\
			  &amp;= \begin{pmatrix} 8^k &amp; 0 \\0 &amp; 4^k \end{pmatrix}.
\end{align*}
$$
</div>
<p>The trick here was to find \(v_1\) and \(v_2\) which isn’t yet obvious yet but we will develop a procedure later to find these specific vectors via diagonalization.
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example: Matrix Representation of Orthogonal Projection</b></h4>
<p>Let \(W = Span\{(1,-1,0), (0,1,-1)\}\). \(W\) is a subspace of \(\mathbf{R}^3\) (It is a plane in \(\mathbf{R}^3\)). 
<br />
<br />
Let \(T: \mathbf{R}^3 \rightarrow \mathbf{R}^3\) be the orthogonal projection onto \(W\). Because we’re projecting onto \(W\) and both \(w_1\) and \(w_2\) are part of this plane, then we know that \(T(w_1) = w_1\) and \(T(w_2) = w_2\). 
<br />
<br />
What we want is to find a matrix expression for this project \([T]_{\beta}^{\beta}\) where \(\beta\) is the standard basis so \(\beta = \{(1,0,0),(0,1,0),(0,0,1)\}\).
<br />
<br />
To find this matrix, it is easier to find \(\beta'\) so that \([T]_{\beta'}^{\beta'}\) is easily understood and use with the tools that we have. We can then use a change of basis formula to convert between the \([T]_{\beta'}\) and \([T]_{\beta}\).
<br />
<br />
What is a candidate for \(\beta'\), something that makes the matrix expression simple? well when \(T\) acts on \(w_1\) and \(w_2\), it doesn’t change them. But we still need one more vector. What should we choose for \(w_3\)? One candidate is the orthogonal vector to the place \(W\) which will mean that its projection is going to be zero. How do we find it? We can take the cross product of \(w_1\) and \(w_2\). \(w_3 = w_1 \times w_2 = (1,1,1)\) and \(T(w_3) = (0,0,0)\).</p>
<div>
$$
\begin{align*}
[T]_{\beta'}^{\beta'} &amp;= \begin{pmatrix} [T(w_1)]_{\beta'} &amp; [T(w_2)]_{\beta'} &amp; [T(w_3)]_{\beta'} \end{pmatrix} \\
               &amp;= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}
\end{align*}
$$
</div>
<p>Again we derived these by writing \(T(w_1)\) as a linear combination of the basis vectors \(\beta'=\{w_1,w_2,w_3\}\). So \([T(w_1)]_{\beta'} = 1(w_1) + 0(w_2) + 0(w_3)\). and so the coordinate vector is \((1,0,0)\). Similarly \([T(w_2)]_{\beta'} = 0(w_1) + 1(w_2) + 0(w_3)\) and so on.
<br />
<br />
But this is \(T\) with respect to \(\beta'\) and we want \(T\) with respect to \(\beta\). So now we need to use the change of basis formula to get what we want. So</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} [I_{\mathbf{R}^3}]_{\beta}^{\beta'}
\end{align*}
$$
</div>
<p>So now we need to compute \([I_{\mathbf{R}^3}]_{\beta'}^{\beta}\). This means for each vector in \(\beta'\), we will apply the identity map which does nothing and then we want to write the vector in terms of the standard basis.</p>
<div>
$$
\begin{align*}
[I(w_1)]_{\beta} = [w_1]_{\beta} = (1, -1, 0) = 1e_1 -1e_2 + 0e_3 \\
[I(w_2)]_{\beta} = [w_2]_{\beta} = (0, 1, -1) = 0e_1 + 1e_2 - 1e_3 \\
[I(w_3)]_{\beta} = [w_3]_{\beta} = (1, 1, 1) = 1e_1 + 1e_2 - 1e_3.
\end{align*}
$$
</div>

<div>
$$
\begin{align*}
[I_{\mathbf{R}^3}]_{\beta'}^{\beta} = \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 1 \\ 0 &amp; -1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>But \([I_{\mathbf{R}^3}]_{\beta}^{\beta'}\) is a little tricker since we want to write the vectors in \(\beta\) (the standard basis) as linear combinations of the basis \(\beta'\) which we could do. Instead it is easier to just compute the inverse in this case</p>
<div>
$$
\begin{align*}
[I_{\mathbf{R}^3}]_{\beta}^{\beta'} = ([I_{\mathbf{R}^3}]_{\beta'}^{\beta})^{-1} = \frac{1}{3}\begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} 
 [I_{\mathbf{R}^3}]_{\beta}^{\beta'} \\
  &amp;= 
  \frac{1}{3} \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ -1 &amp; 1 &amp; 1 \\ 0 &amp; -1 &amp; 1 \end{pmatrix} 
  \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}
  \begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix} \\
  &amp;= 
  \frac{1}{3}\begin{pmatrix} 2 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 2 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>To verify this is correct, we know how \(T\) acts on \(w_1\) and \(w_2\) so we can check if multiplying by \(T\) gives us the expected result in each so and so we want to verify that</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta}[w_1]_{\beta} &amp;= [w_1]_{\beta}.
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Given a vector space \(V\) with two finite basis \(\beta\) and \(\beta'\). For \(v \in V\), what is the relationship between \([v]_{\beta}\) and \([v]_{\beta'}\)? Starting with \([v]_{\beta'}\), we want to derive an expression for \([v]_{\beta}\). We can write $$ \begin{align*} [v]_{\beta'} &amp;= [I_V(v)]_{\beta'} \\ &amp;= [I_V]_{\beta}^{\beta'}[v]_{\beta} \\ \end{align*} $$ In the first line we just re-wrote the vector so now the identity map is applied on \(v\) which doesn’t change \(v\). By theorem 2.14, we can re-write this line by first computing the matrix representative of the identity transformation that changes coordinates from \(\beta\) to \(\beta'\). We get this matrix by taking the vectors in \(\beta\), applying the identity transformation which does nothing and then re-writing these vectors with respect to \(\beta'\). These vectors will be the columns of the matrix. Example Let \(V = P_1\), \(\beta = \{1, x\}\) and \(\beta' = \{1+x, 1-x\}\). Then to compute the matrix \([I_V]_{\beta}^{\beta'}\), we want to apply the map on the basis \(\beta\) and then write these relative to the basis \(\beta'\) (meaning for each vector, find its coordinates when written as a linear combination of the vectors of the basis \(\beta'\)). $$ \begin{align*} [I_V]_{\beta}^{\beta'} &amp;= \begin{pmatrix} [I_V(1)]_{\beta'} &amp; [I_V(x)]_{\beta'} \end{pmatrix} \\ &amp;= \begin{pmatrix} [1]_{\beta'} &amp; [x]_{\beta'} \end{pmatrix}. \end{align*} $$ Now we need to find the coordinates when 1 is written as a linear combination of the vectors of basis \(\beta'\) so $$ \begin{align*} 1 &amp;= a(1 + x) + b(1 - x) \\ 1 &amp;= a + ax + b - bx \\ 1 &amp;= a + b + x(a - b) \\ \end{align*} $$ from this we see that \(a = b = 1/2\). Similarly we need to find the coordinates of \(x\) $$ \begin{align*} x &amp;= a(1 + x) + b(1 - x) \\ 0 &amp;= a + ax + b - bx - x \\ 0 &amp;= a + b + x(a - b - 1) \\ \end{align*} $$ A solution to this is \(a = 1/2\) and \(b = -1/2\). So now we have $$ \begin{align*} [I_V]_{\beta}^{\beta'} &amp;= \begin{pmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; -\frac{1}{2} \end{pmatrix}. \end{align*} $$ We can now use this matrix to transform any vector in \(V\) written with respect to \(\beta\) to a vector written with respect to \(\beta'\). Given some polynomial in \(V\), $$ \begin{align*} [a_0 + a_1x]_{\beta'} = \begin{pmatrix} \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; -\frac{1}{2} \end{pmatrix} \begin{pmatrix} a_0 \\ a_1 \end{pmatrix} = \begin{pmatrix} \frac{1}{2}(a_0 + a_1) \\ \frac{1}{2}(a_0 - a_1) \end{pmatrix} \end{align*} $$ The Inverse of the Change of Coordinates Matrix Theorem $$ \begin{align*} ([I_V]_{\beta}^{\beta'})^{-1} = [I_V]^{\beta}_{\beta'} \end{align*} $$ Proof: To prove that a matrix is an inverse of another matrix, we need to show that their product is the identity matrix so $$ \begin{align*} [I_V]_{\beta}^{\beta'}[I_V]^{\beta}_{\beta'} &amp;= [I_V \circ I_V]^{\beta'}_{\beta'} \text{ (Theorem 2.11)}\\ &amp;= [I_V]^{\beta'}_{\beta'} \\ &amp;= I_n. \ \blacksquare \end{align*} $$ Change of Bases Question: Given \(T: V \rightarrow W\) and finite bases \(\beta, \beta'\) for \(V\) and \(\gamma, \gamma'\) for \(W\), how are \([T]^{\gamma}_{\beta}\) and \([T]^{\gamma'}_{\beta'}\) related? The only tool we have is composition and using Theorem 2.11 $$ \begin{align*} [T]^{\gamma'}_{\beta'} &amp;= [I_W \circ T \circ I_V]^{\gamma'}_{\beta'} \\ &amp;= [I_W]^{\gamma'}_{\gamma} [T \circ I_V]^{\gamma}_{\beta'} \\ &amp;= [I_W]^{\gamma'}_{\gamma} [T]^{\gamma}_{\beta} [I_V]^{\beta}_{\beta'} \\ \end{align*} $$ If \(W = V\) and \(\gamma, \gamma' = \beta, \beta'\), then $$ \begin{align*} [T]^{\beta'}_{\beta'} &amp;= [I_W]^{\beta'}_{\beta} [T]^{\beta}_{\beta} [I_V]^{\beta}_{\beta'} \\ [T]_{\beta'} &amp;= Q^{-1}[T]_{\beta}Q \end{align*} $$ Where \(Q = [I_V]^{\beta}_{\beta'}\). If we find such a matrix \(Q\) to relate \([T]_{\beta}\) and \([T]_{\beta'}\) then we call these matrices similar. Definition \(A, B \in M_{2 \times 2}\) are similar if \(\exists Q \in M_{2 \times 2}\) such that \(B = Q^{-1}AQ\) Example Let \(A = \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix}\) and consider the map \(L_A: \mathbf{R}^2 \rightarrow \mathbf{R}^2\). Suppose we want to compute \(L_A^k = L_A \circ L_A \circ ... \circ L_A\) (\(k\) times)? If we try this the naive way $$ \begin{align*} A^k = \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} ... \end{align*} $$ This is going to be really hard to multiply. Can we simplify this computation? So what is \(A^2\) really? It’s the matrix representative of the composition \([L_A \circ L_A]\). In fact if we go back to just \([L_A]_{\beta}\). This is the matrix representative of this map with respect to the standard basis so \([L_A]_{\beta} = A\). The idea then is to find another basis such that the matrix representation of the map with respect to that basis is easier to multiply. So the plan is to find \(\beta'\) such that \([L_A]_{\beta'}^{\beta'}\) is easier to multiply. Note that for \(v_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, v_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\), we’ll find that $$ \begin{align*} L_A(v_1) &amp;= Av_1 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 8 \\ 8 \end{pmatrix} = 8v_1 + 0v_2 \\ L_A(v_2) &amp;= Av_2 \begin{pmatrix} 6 &amp; 2 \\2 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} 4 \\ -4 \end{pmatrix} = 0v_1 + 4v_2. \end{align*} $$ Notice that \(\beta' = \{v_1, v_2\}\) is a basis for \(\mathbf{R}^2\). To find \([L_A]_{\beta'}^{\beta'} = [L_A]_{\beta'}\), we need to apply \(L_A\) on the basis vectors of \(\beta'\) and then write them with respect to \(\beta'\). $$ \begin{align*} [L_A]_{\beta'} &amp;= ([L_A(v_1)]_{\beta'}, [L_A]_{\beta'}) \\ &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} \end{align*} $$ This leads to $$ \begin{align*} [L_A \circ L_A ... \circ L_A]_{\beta'}^{\beta'} &amp;= [L_A]_{\beta'}^{\beta'} ... [L_A]_{\beta'}^{\beta'} \\ &amp;= \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} ... \begin{pmatrix} 8 &amp; 0 \\0 &amp; 4 \end{pmatrix} \\ &amp;= \begin{pmatrix} 8^k &amp; 0 \\0 &amp; 4^k \end{pmatrix}. \end{align*} $$ The trick here was to find \(v_1\) and \(v_2\) which isn’t yet obvious yet but we will develop a procedure later to find these specific vectors via diagonalization. Example: Matrix Representation of Orthogonal Projection Let \(W = Span\{(1,-1,0), (0,1,-1)\}\). \(W\) is a subspace of \(\mathbf{R}^3\) (It is a plane in \(\mathbf{R}^3\)). Let \(T: \mathbf{R}^3 \rightarrow \mathbf{R}^3\) be the orthogonal projection onto \(W\). Because we’re projecting onto \(W\) and both \(w_1\) and \(w_2\) are part of this plane, then we know that \(T(w_1) = w_1\) and \(T(w_2) = w_2\). What we want is to find a matrix expression for this project \([T]_{\beta}^{\beta}\) where \(\beta\) is the standard basis so \(\beta = \{(1,0,0),(0,1,0),(0,0,1)\}\). To find this matrix, it is easier to find \(\beta'\) so that \([T]_{\beta'}^{\beta'}\) is easily understood and use with the tools that we have. We can then use a change of basis formula to convert between the \([T]_{\beta'}\) and \([T]_{\beta}\). What is a candidate for \(\beta'\), something that makes the matrix expression simple? well when \(T\) acts on \(w_1\) and \(w_2\), it doesn’t change them. But we still need one more vector. What should we choose for \(w_3\)? One candidate is the orthogonal vector to the place \(W\) which will mean that its projection is going to be zero. How do we find it? We can take the cross product of \(w_1\) and \(w_2\). \(w_3 = w_1 \times w_2 = (1,1,1)\) and \(T(w_3) = (0,0,0)\). $$ \begin{align*} [T]_{\beta'}^{\beta'} &amp;= \begin{pmatrix} [T(w_1)]_{\beta'} &amp; [T(w_2)]_{\beta'} &amp; [T(w_3)]_{\beta'} \end{pmatrix} \\ &amp;= \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ Again we derived these by writing \(T(w_1)\) as a linear combination of the basis vectors \(\beta'=\{w_1,w_2,w_3\}\). So \([T(w_1)]_{\beta'} = 1(w_1) + 0(w_2) + 0(w_3)\). and so the coordinate vector is \((1,0,0)\). Similarly \([T(w_2)]_{\beta'} = 0(w_1) + 1(w_2) + 0(w_3)\) and so on. But this is \(T\) with respect to \(\beta'\) and we want \(T\) with respect to \(\beta\). So now we need to use the change of basis formula to get what we want. So $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= [I_{\mathbf{R}^3}]_{\beta'}^{\beta} [T]_{\beta'}^{\beta'} [I_{\mathbf{R}^3}]_{\beta}^{\beta'} \end{align*} $$ So now we need to compute \([I_{\mathbf{R}^3}]_{\beta'}^{\beta}\). This means for each vector in \(\beta'\), we will apply the identity map which does nothing and then we want to write the vector in terms of the standard basis. $$ \begin{align*} [I(w_1)]_{\beta} = [w_1]_{\beta} = (1, -1, 0) = 1e_1 -1e_2 + 0e_3 \\ [I(w_2)]_{\beta} = [w_2]_{\beta} = (0, 1, -1) = 0e_1 + 1e_2 - 1e_3 \\ [I(w_3)]_{\beta} = [w_3]_{\beta} = (1, 1, 1) = 1e_1 + 1e_2 - 1e_3. \end{align*} $$]]></summary></entry></feed>