<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-20T12:01:19-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">4.1-4.2: Study Notes</title><link href="http://localhost:4000/jekyll/update/2024/09/19/4.1-4.2.html" rel="alternate" type="text/html" title="4.1-4.2: Study Notes" /><published>2024-09-19T01:01:36-07:00</published><updated>2024-09-19T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/19/4.1-4.2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/19/4.1-4.2.html"><![CDATA[<!------------------------------------4.1------------------------------------------->
<p><br /></p>
<div class="purdiv">
Theorem 4.1
</div>
<div class="purbdiv">
The function \(\det: M_{2 \times 2}(\mathbf{F}) \rightarrow \mathbf{F}\) is a linear function of each of a \(2 \times 2\) matrix when the other row is held fixed. That is if \(u, v\) and \(w\) are in \(\mathbf{F}^2\) and \(k\) is a scalar, then
$$
\begin{align*}
\det \begin{pmatrix} u + kv \\ w \end{pmatrix}
= \det \begin{pmatrix} u \\ w \end{pmatrix} + k \det \begin{pmatrix} v \\ w \end{pmatrix}
\end{align*}
$$
and
$$
\begin{align*}
\det \begin{pmatrix} w \\ u + kv \end{pmatrix}
= \det \begin{pmatrix} w \\ u \end{pmatrix} + k \det \begin{pmatrix} w \\ v \end{pmatrix}
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
Let \(u = (a_1, a_2), v = (b_1, b_2)\) and \(w = (c_1, c_2)\) be in \(\mathbf{F}^2\) and let \(k\) be a scalar. Then</p>
<div>
$$
\begin{align*}
\det \begin{pmatrix} u \\ v \end{pmatrix} + \det k\begin{pmatrix} w \\ v \end{pmatrix}
&amp;= \det \begin{pmatrix} a_1 &amp; a_2 \\ c_1 &amp; c_2 \end{pmatrix} 
+ k \det \begin{pmatrix} b_1 &amp; b_2 \\ c_1 &amp; c_2 \end{pmatrix} 
\\
&amp;= (a_1c_2 - a_2c_1) + k(b_1c_2 - b_2c_1) \\
&amp;=  a_1c_2 - a_2c_1 + kb_1c_2 + kb_2c_1 \\
&amp;= c_2(a_1 + kb_1) - c_1(a_2 +kb_2) \\
&amp;= \det \begin{pmatrix} a_1+kb_1 &amp; a_2+kb_2 \\ c_1 &amp; c_2 \end{pmatrix} 
\end{align*}
$$
</div>
<p>The other direction is similarly calculated.
<br />
<!------------------------------------4.2------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.2
</div>
<div class="purbdiv">
Let \(A \in M_{2 \times 2}(\mathbf{F})\). Then the determinant of \(A\) is nonzero if and only if \(A\) is invertible. Moreover, if \(A\) is invertible, then
$$
\begin{align*}
A^{-1} = \frac{1}{\det(A)}
= \det \begin{pmatrix} A_{22} &amp; -A_{12} \\ -A_{21} &amp; A_{11} \end{pmatrix}
\end{align*}
$$
</div>
<!---------------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
<br />
We can construct the matrix such that we can show that \(AM = MA = I\) so this proves that \(A\) is invertible. Conversely, then \(A\) must have rank 2 and therefore we musteither have \(A_{21} \neq 0\) or \(A_{11} \neq 0\). If \(A_{11} \neq 0\), we can add \(-A_{21}/A_{11}\) times row 1 to row 2 to obtain</p>
<div>
$$
\begin{align*}
\begin{pmatrix} A_{11} &amp; -A_{12} \\ 0 &amp; A_{22} - \frac{A_{12}A_{21}}{A_{11}}\end{pmatrix}
\end{align*}
$$
</div>
<p>Because this is an elementary row operation, then we know the rank is preserved. Notice now that \(A_{21} = 0\). This means that \(A_{22} - \frac{A_{12}A_{21}}{A_{11}}\) can’t be zero. From this, we will see that the determinant is not zero. For the other case when \(A_{21} \neq 0\), we can instead add \(-A_{11}/A_{21}\) to achieve the same conclusion.å
<br />
<!---------------------------------------------------------------------------------->
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A \in M_{n \times n}(\mathbf{F})\). If \(n = 1\), so that \(A = (A_{11}\), we define \(\det(A) = A_{11}\). For \(n \geq 2\), we define \(\det(A)\) recursively as
$$
\begin{align*}
\det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) 
\end{align*}
$$
The scalar \(\det(A)\) is called the <b>determinant</b> of \(A\) and is also denoted by \(|A|\). The scalar
$$
\begin{align*}
(-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) 
\end{align*}
$$
is called the <b>cofactor</b> of the entry \(A\) in row \(i\), column \(j\).
</div>
<p><br />
<!------------------------------------4.3------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.3
</div>
<div class="purbdiv">
The determinant of \(n \times n\) matrix is a linear function of each row when the remaining rows are held fixed. That is for \(1 \leq r \leq n\). we have
$$
\begin{align*}
\det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u + kv \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix}
=
\det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix}
+
k
\det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ v \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix}
\end{align*}
$$
where \(k\) is a scalar and \(u,v\) are each \(a_i\) are row vectors in \(\mathbf{F}^n\)
</div>
<!---------------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
<br />
By induction on \(n\). If \(n = 1\), the result is immediate. <br />
Now suppose that \(n \geq 2\) and assume the hypothesis is true for \(n-1\), that is the determinant of any \((n - 1) \times (n-1)\) is a linear function of each row when the remaining rows are held fixed. 
<br />
<br />
Let \(A\) be an \(n \times n\) matrix with rows \(a_1, a_2,...,a_n\), respectively and suppose that for some \(r\) where \((1 \leq r \leq n)\), we have \(a_r = u + kv\) for some \(u, v \in \mathbf{F}^n\) and scalar \(k\). Let \(u = (b_1,...,b_n)\) and let \(v = (c_1, ...,c_n)\). Now, let \(B\) and \(C\) be the matrices obtained from \(A\) by replacing row \(r\) of \(A\) by \(u\) and \(v\) respectively. We want to show that \(\det(A) = \det(B) + k\det(C)\).
<br />
<br />
Case \(r = 1\): TODO (it’s in lecture 18)
<br />
<br />
Case \(r &gt; 1\): For the matrices \(\tilde{A_{1j}}\), \(\tilde{B_{1j}}\), \(\tilde{C_{1j}}\) which are constructed by removing the first row and the \(j\)th column, we know that they are the same except for row \(r-1\) (\(r-1\) since the first row is removed, so the \(r\)‘th row is now the \(r-1\)th row). The row \(r-1\) specifically looks like</p>
<div>
$$
\begin{align*}
(b_1 + kc_1,...,b_{j-1} + kc_{j-1},b_{j+1} + kc_{j+1},...,b_n + kc_n)
\end{align*}
$$
</div>
<p>Moreover, these matrices are now of size \((n-1) \times (n-1)\) so we can invoke the inductive hypothesis to conclude that</p>
<div>
$$
\begin{align*}
\tilde{A_{1j}} = \tilde{B_{1j}} + k\tilde{C_{1j}}
\end{align*}
$$
</div>
<p>We also know that the first row is equal among all three matrices so \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). Let’s now compute the determinant using the definition above</p>
<div>
$$
\begin{align*}
\det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\
        &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} [\tilde{B_{1j}} + k\tilde{C_{1j}}] \\
        &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \tilde{B_{1j}} + k \sum_{j=1}^{n} (-1)^{1+j}  \tilde{C_{1j}}] \\
		&amp;= \det(B) + k\det(C).
\end{align*}
$$
</div>
<p>Therefore the inductive hypothesis is true for any \(n \times n\) matrix. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------4.3(c)---------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.3 (Corollary)
</div>
<div class="purbdiv">
If \(A \in M_{n \times n}(\mathbf{F})\) has a row consisting entirely of zeros, then \(\det(A)=0\).
</div>
<p><br />
<b>Proof</b>
TODO (Exercise 4.2, 24)
<br />
<br />
<!------------------------------------Lemma---------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.3 (Corollary)
</div>
<div class="purbdiv">
Let \(B \in M_{n \times n}(\mathbf{F})\) where \(n \geq 2\). If row \(i\) of \(B\) equals \(e_k\) for some \(k \ (1 \leq k \leq n)\), then \(\det(B) = (-1)^{i+k}\det(\tilde{B_{ik}})\)
</div>
<p><br />
<!---------------------------------------------------------------------------------->
<b>Proof</b>
<br />
By Induction on \(n\). 
<br />
<br />
Base Case \(n = 2\): TODO
<br />
<br />
Inductive Case: Assume this is true for \((n - 1) \times (n - 1)\). We will show this is true for an \(n \times n\) matrix. Let \(B\) be an \(n \times\) matrix where the \(i\)th row is \(e_k\). If \(i = 1\), then we’re done by the definition of the determinant. Otherwise suppose that \(i \neq 1\). 
<br />
<br />
Now, for column \(j \neq k\), let \(C_{ij}\) be the \((n-2) \times (n-2)\) matrix obtained from \(B\) by deleting rows 1 and \(i\) and columns \(j\) and \(k\). Then, \(\tilde{B_{1j}}\) is the following vector in \(\mathbf{F}^{n-1}\)</p>
<div>
$$
\begin{align*}
\begin{cases} e_{k-1} \quad \text{ if }j &lt; k \\ 
              0 \quad \quad \ \text{ if } j = k \\
			  e_{k} \quad \quad \text{ if } j &gt; k
\end{cases}
\end{align*}
$$
</div>
<p>TODO ….
<br />
<!------------------------------------4.4------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 4.4
</div>
<div class="purbdiv">
The determinant of a square matrix can be evaluated by any cofactor expansion along any row. That is if \(A \in M_{n \times n}(\mathbf{F})\). Then for any integer \(i \ (1 \leq i \leq n)\)
	$$
	\begin{align*}
	\det(A) = \sum_{j=1}^{n} (-1)^{i+j} A_{ij} \det(\tilde{A_{ij}}) 
	\end{align*}
	$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
<br />
<br /></p>
<hr />

<p><br /></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 4.1 The function \(\det: M_{2 \times 2}(\mathbf{F}) \rightarrow \mathbf{F}\) is a linear function of each of a \(2 \times 2\) matrix when the other row is held fixed. That is if \(u, v\) and \(w\) are in \(\mathbf{F}^2\) and \(k\) is a scalar, then $$ \begin{align*} \det \begin{pmatrix} u + kv \\ w \end{pmatrix} = \det \begin{pmatrix} u \\ w \end{pmatrix} + k \det \begin{pmatrix} v \\ w \end{pmatrix} \end{align*} $$ and $$ \begin{align*} \det \begin{pmatrix} w \\ u + kv \end{pmatrix} = \det \begin{pmatrix} w \\ u \end{pmatrix} + k \det \begin{pmatrix} w \\ v \end{pmatrix} \end{align*} $$ Proof Let \(u = (a_1, a_2), v = (b_1, b_2)\) and \(w = (c_1, c_2)\) be in \(\mathbf{F}^2\) and let \(k\) be a scalar. Then $$ \begin{align*} \det \begin{pmatrix} u \\ v \end{pmatrix} + \det k\begin{pmatrix} w \\ v \end{pmatrix} &amp;= \det \begin{pmatrix} a_1 &amp; a_2 \\ c_1 &amp; c_2 \end{pmatrix} + k \det \begin{pmatrix} b_1 &amp; b_2 \\ c_1 &amp; c_2 \end{pmatrix} \\ &amp;= (a_1c_2 - a_2c_1) + k(b_1c_2 - b_2c_1) \\ &amp;= a_1c_2 - a_2c_1 + kb_1c_2 + kb_2c_1 \\ &amp;= c_2(a_1 + kb_1) - c_1(a_2 +kb_2) \\ &amp;= \det \begin{pmatrix} a_1+kb_1 &amp; a_2+kb_2 \\ c_1 &amp; c_2 \end{pmatrix} \end{align*} $$ The other direction is similarly calculated. Theorem 4.2 Let \(A \in M_{2 \times 2}(\mathbf{F})\). Then the determinant of \(A\) is nonzero if and only if \(A\) is invertible. Moreover, if \(A\) is invertible, then $$ \begin{align*} A^{-1} = \frac{1}{\det(A)} = \det \begin{pmatrix} A_{22} &amp; -A_{12} \\ -A_{21} &amp; A_{11} \end{pmatrix} \end{align*} $$ Proof We can construct the matrix such that we can show that \(AM = MA = I\) so this proves that \(A\) is invertible. Conversely, then \(A\) must have rank 2 and therefore we musteither have \(A_{21} \neq 0\) or \(A_{11} \neq 0\). If \(A_{11} \neq 0\), we can add \(-A_{21}/A_{11}\) times row 1 to row 2 to obtain $$ \begin{align*} \begin{pmatrix} A_{11} &amp; -A_{12} \\ 0 &amp; A_{22} - \frac{A_{12}A_{21}}{A_{11}}\end{pmatrix} \end{align*} $$ Because this is an elementary row operation, then we know the rank is preserved. Notice now that \(A_{21} = 0\). This means that \(A_{22} - \frac{A_{12}A_{21}}{A_{11}}\) can’t be zero. From this, we will see that the determinant is not zero. For the other case when \(A_{21} \neq 0\), we can instead add \(-A_{11}/A_{21}\) to achieve the same conclusion.å Definition Let \(A \in M_{n \times n}(\mathbf{F})\). If \(n = 1\), so that \(A = (A_{11}\), we define \(\det(A) = A_{11}\). For \(n \geq 2\), we define \(\det(A)\) recursively as $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ The scalar \(\det(A)\) is called the determinant of \(A\) and is also denoted by \(|A|\). The scalar $$ \begin{align*} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ is called the cofactor of the entry \(A\) in row \(i\), column \(j\). Theorem 4.3 The determinant of \(n \times n\) matrix is a linear function of each row when the remaining rows are held fixed. That is for \(1 \leq r \leq n\). we have $$ \begin{align*} \det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u + kv \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix} = \det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ u \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix} + k \det \begin{pmatrix} a_1 \\ \vdots \\ a_{r-1} \\ v \\ a_{r+1} \\ \vdots \\ a_n \end{pmatrix} \end{align*} $$ where \(k\) is a scalar and \(u,v\) are each \(a_i\) are row vectors in \(\mathbf{F}^n\) Proof By induction on \(n\). If \(n = 1\), the result is immediate. Now suppose that \(n \geq 2\) and assume the hypothesis is true for \(n-1\), that is the determinant of any \((n - 1) \times (n-1)\) is a linear function of each row when the remaining rows are held fixed. Let \(A\) be an \(n \times n\) matrix with rows \(a_1, a_2,...,a_n\), respectively and suppose that for some \(r\) where \((1 \leq r \leq n)\), we have \(a_r = u + kv\) for some \(u, v \in \mathbf{F}^n\) and scalar \(k\). Let \(u = (b_1,...,b_n)\) and let \(v = (c_1, ...,c_n)\). Now, let \(B\) and \(C\) be the matrices obtained from \(A\) by replacing row \(r\) of \(A\) by \(u\) and \(v\) respectively. We want to show that \(\det(A) = \det(B) + k\det(C)\). Case \(r = 1\): TODO (it’s in lecture 18) Case \(r &gt; 1\): For the matrices \(\tilde{A_{1j}}\), \(\tilde{B_{1j}}\), \(\tilde{C_{1j}}\) which are constructed by removing the first row and the \(j\)th column, we know that they are the same except for row \(r-1\) (\(r-1\) since the first row is removed, so the \(r\)‘th row is now the \(r-1\)th row). The row \(r-1\) specifically looks like $$ \begin{align*} (b_1 + kc_1,...,b_{j-1} + kc_{j-1},b_{j+1} + kc_{j+1},...,b_n + kc_n) \end{align*} $$ Moreover, these matrices are now of size \((n-1) \times (n-1)\) so we can invoke the inductive hypothesis to conclude that $$ \begin{align*} \tilde{A_{1j}} = \tilde{B_{1j}} + k\tilde{C_{1j}} \end{align*} $$ We also know that the first row is equal among all three matrices so \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). Let’s now compute the determinant using the definition above $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} [\tilde{B_{1j}} + k\tilde{C_{1j}}] \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \tilde{B_{1j}} + k \sum_{j=1}^{n} (-1)^{1+j} \tilde{C_{1j}}] \\ &amp;= \det(B) + k\det(C). \end{align*} $$ Therefore the inductive hypothesis is true for any \(n \times n\) matrix. \(\ \blacksquare\) Theorem 4.3 (Corollary) If \(A \in M_{n \times n}(\mathbf{F})\) has a row consisting entirely of zeros, then \(\det(A)=0\). Proof TODO (Exercise 4.2, 24) Theorem 4.3 (Corollary) Let \(B \in M_{n \times n}(\mathbf{F})\) where \(n \geq 2\). If row \(i\) of \(B\) equals \(e_k\) for some \(k \ (1 \leq k \leq n)\), then \(\det(B) = (-1)^{i+k}\det(\tilde{B_{ik}})\) Proof By Induction on \(n\). Base Case \(n = 2\): TODO Inductive Case: Assume this is true for \((n - 1) \times (n - 1)\). We will show this is true for an \(n \times n\) matrix. Let \(B\) be an \(n \times\) matrix where the \(i\)th row is \(e_k\). If \(i = 1\), then we’re done by the definition of the determinant. Otherwise suppose that \(i \neq 1\). Now, for column \(j \neq k\), let \(C_{ij}\) be the \((n-2) \times (n-2)\) matrix obtained from \(B\) by deleting rows 1 and \(i\) and columns \(j\) and \(k\). Then, \(\tilde{B_{1j}}\) is the following vector in \(\mathbf{F}^{n-1}\) $$ \begin{align*} \begin{cases} e_{k-1} \quad \text{ if }j &lt; k \\ 0 \quad \quad \ \text{ if } j = k \\ e_{k} \quad \quad \text{ if } j &gt; k \end{cases} \end{align*} $$ TODO …. Theorem 4.4 The determinant of a square matrix can be evaluated by any cofactor expansion along any row. That is if \(A \in M_{n \times n}(\mathbf{F})\). Then for any integer \(i \ (1 \leq i \leq n)\) $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{i+j} A_{ij} \det(\tilde{A_{ij}}) \end{align*} $$ Proof References Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">3.1-3.2: Study Notes</title><link href="http://localhost:4000/jekyll/update/2024/09/18/3.1-3.2.html" rel="alternate" type="text/html" title="3.1-3.2: Study Notes" /><published>2024-09-18T01:01:36-07:00</published><updated>2024-09-18T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/18/3.1-3.2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/18/3.1-3.2.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A\) be an \(m \times n\). Any one of the following three operations on the rows [columns] of \(A\) is called an <b>elementary row [column] operation</b>
<ol type="1">
	<li>interchanging any two rows [columns] of \(A\);</li>
	<li>multiplying any row [column] of \(A\) by a non-zero scalar;</li>
	<li>adding any scalar multiple of a row [column] of \(A\) to another row [column].</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of <b>type 1, 2 or 3</b> according to whether the elementary operation performed on \(I_n\) is of type 1, 2 or 3 operation, respectively.
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.1
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{F})\) and suppose that \(B\) is obtained from \(A\) by performing an elementary row [column] operation. Then, there exists an \(m \times m [n \times n]\) elementary matrix \(E\) such that \(B = EA [B = AE]\). In fact, \(E\) is obtained from \(I_m [I_n]\) by performing the same elementary row [column] operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m [n \times n]\) matrix, then \(EA [AE]\) is the matrix obtained from \(A\) by performing the same elementary row operation.
</div>
<p><br />
<!----------------------------------------3.2------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.2
</div>
<div class="purbdiv">
Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type.
</div>
<p><br />
<b>Proof</b>
<br />
<br />
Let \(E\) be an elementary \(n \times n\) matrix. Then \(E\) can be obtained by an elementary row operation on \(I_n\) by definition. Reverse the steps used to transform \(I_n\) into \(E\) to get \(I_n\) back. The result is that \(I_n\) can be obtained from \(E\) by an elementary row operation of the same type. By Theorem 3.1, there is an elementary matrix \(\overline{E}\) such that \(\overline{E}E = I_n\). But since \(\overline{E}E = I_n\), then by Exercise 10 from 2.4, \(\overline{E}\) and \(E\) are invertible and we have \(E^{-1} = \overline{E}\). \(\ \blacksquare\)
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
If \(A \in M_{n \times n}(\mathbf{F})\) we define the <b>rank</b> of \(A\), denoted \(\text{rank}(A)\), to be the rank of the linear transformation \(L_A: \mathbf{F}^n \rightarrow \mathbf{F}^m\)
</div>
<p><br />
One important implication here is that an \(n \times n\) matrix is invertible if and only if its rank is \(n\).
<br />
<br />
<!----------------------------------------3.3------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.3
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear transformation between finite-dimensional vector spaces, and let \(\beta\) and \(\gamma\) be ordered bases for \(V\) and \(W\), respectively. Then \(rank(T) = rank([T]_{\beta}^{\gamma})\)
</div>
<p><br />
The rank of a linear transformation is the same as the rank of one its matrix representations.
<br />
<br />
<!--------------------------------------3.4-------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.4
</div>
<div class="purbdiv">
Let \(A\) be an \(m \times n\) matrix. If \(P\) and \(Q\) are invertible \(m \times m\) and \(n \times n\) matrices, respectively, then 
<ol type="a">
	<li>\(\text{rank}(AQ) = \text{rank}(A)\),</li>
	<li>\(\text{rank}(PA) = \text{rank}(A)\) and therefore,</li>
	<li>\(\text{rank}(PAQ) = \text{rank}(A)\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
<br />
(a) Observe that</p>
<div>
$$
\begin{align*}
R(L_{AQ}) &amp;= R(L_AL_Q) \\
          &amp;= L_AL_Q(\mathbf{F}) \quad \text{(to get the range, we apply the linear map)}\\
		  &amp;= L_A(L_Q(\mathbf{F})) \quad \text{(we apply $L_Q$ first)}\\
		  &amp;= L_A(\mathbf{F}) \quad \text{(because $L_Q$ is onto)}\\
		  &amp;= R(L_A)
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\text{rank}(AQ) &amp;= \dim(R(L_{AQ})) \quad \text{(the rank is the dimension of the range)} \\
                &amp;= \dim(R(L_A)) \quad \text{(by the previous result)}  \\
				&amp;= rank(A).
\end{align*}
$$
</div>
<p><br />
<!--------------------------------------3.4(c)------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 3.4 (Corollary)
</div>
<div class="purbdiv">
Elementary row and column operations on a matrix are rank-preserving.
</div>
<p><br />
<!--------------------------------------3.5-------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.5
</div>
<div class="purbdiv">
The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns.
</div>
<p><br />
<!----------------------->
<b>Proof</b>
<br />
<br />
For any \(A \in M_{m \times n}(\mathbf{F})\),</p>
<div>
$$
\begin{align*}
\text{rank}(A) = \text{rank}(L_A) = \dim(R(L_A))
\end{align*}
$$
</div>
<p>Let \(\beta\) be the standard ordered basis for \(\mathbf{F}^n\). Then \(\beta\) spans \(\mathbf{F}^n\). Theorem 2.2 assets that \(R(T) = \text(T(\beta))\) so</p>
<div>
$$
\begin{align*}
R(L_A) &amp;= \text{span}(L_A(\beta)) \quad \text{(by Theorem 2.2)} \\
       &amp;= \text{span}(\{L_A(e_1),...,L_A(e_n)\})
\end{align*}
$$
</div>
<p>By theorem 2.13(b) \(L_A(e_j) = Ae_j = a_j\) where \(a_j\) is the \(j\)th column of \(A\). So</p>
<div>
$$
\begin{align*}
R(L_A) &amp;= \text{span}(\{a_1,...,a_n\})
\end{align*}
$$
</div>
<p>Therefore</p>
<div>
$$
\begin{align*}
\text{rank}(A) &amp;= \dim(R(L_A)) = \dim(\text{span}(\{a_1,...,a_n\})) \ \blacksquare
\end{align*}
$$
</div>
<p>Note: If you’ve forgotten 2.13 which you just did. Remember that by definition, matrix-vector multiplication \(Av\) is defined as \(a_1v_1 + a_2v_2 +...+ a_nv_n\) where \(a_1,...,a_n\) are the columns of \(A\) and \(v_1,...,v_n\) are the entries of the vector \(v\). Therefore, if you multiply \(Ae_j\) where \(e_j\) is from the standard basis, then we know that vector is all zeros except for the \(j\)th entry. So this means that \(Ae_j = a_j\). 
<br />
<br />
<!--------------------------------------3.6-------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6
</div>
<div class="purbdiv">
Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then \(r \leq m, r \leq n\), and by means of a finite number of elementary row and column operations, \(A\) can be transformed into the matrix
$$
\begin{align*}
D = \begin{pmatrix}
I_r &amp; O_1 \\
O_2 &amp; O_3
\end{pmatrix}
\end{align*}
$$
where \(O_1\), \(O_2\) and \(O_3\) are zero matrices. Thus \(D_{ii} = 1\) for \(i \leq r\) and \(D_{ij} = 0\) otherwise.
</div>
<p><br />
<!------------------------------------3.6(c)--------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6 (Corollary 1)
</div>
<div class="purbdiv">
Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then there exists invertible matrices \(B\) and \(C\) of sizes \(m \times m \) and \(n \times n\) respectively, such that \(D = BAC\) where
$$
\begin{align*}
D = \begin{pmatrix}
I_r &amp; O_1 \\
O_2 &amp; O_3
\end{pmatrix}
\end{align*}
$$
is the \(m \times n\) in which \(O_1\), \(O_2\) and \(O_3\) are zero matrices.
</div>
<p><br />
<b>Proof</b>
TODO
<br />
<!------------------------------------3.6(c)--------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6 (Corollary 2)
</div>
<div class="purbdiv">
Let \(A\) be \(m \times n\) matrices. Then
<ol type="i">
</ol>
</div>
<p><br />
<b>Proof</b>
TODO
<br /></p>

<p><br />
<!------------------------------------3.6(c)--------------------------------------------->
<br /></p>
<div class="purdiv">
Theorem 3.6 (Corollary 3)
</div>
<div class="purbdiv">
Every invertible matrix is a product of elementary matrices
</div>
<p><br />
<b>Proof</b>
TODO
<br /></p>

<p><br />
<br /></p>
<hr />

<p><br /></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition Let \(A\) be an \(m \times n\). Any one of the following three operations on the rows [columns] of \(A\) is called an elementary row [column] operation interchanging any two rows [columns] of \(A\); multiplying any row [column] of \(A\) by a non-zero scalar; adding any scalar multiple of a row [column] of \(A\) to another row [column]. Definition An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of type 1, 2 or 3 according to whether the elementary operation performed on \(I_n\) is of type 1, 2 or 3 operation, respectively. Theorem 3.1 Let \(A \in M_{n \times n}(\mathbf{F})\) and suppose that \(B\) is obtained from \(A\) by performing an elementary row [column] operation. Then, there exists an \(m \times m [n \times n]\) elementary matrix \(E\) such that \(B = EA [B = AE]\). In fact, \(E\) is obtained from \(I_m [I_n]\) by performing the same elementary row [column] operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m [n \times n]\) matrix, then \(EA [AE]\) is the matrix obtained from \(A\) by performing the same elementary row operation. Theorem 3.2 Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type. Proof Let \(E\) be an elementary \(n \times n\) matrix. Then \(E\) can be obtained by an elementary row operation on \(I_n\) by definition. Reverse the steps used to transform \(I_n\) into \(E\) to get \(I_n\) back. The result is that \(I_n\) can be obtained from \(E\) by an elementary row operation of the same type. By Theorem 3.1, there is an elementary matrix \(\overline{E}\) such that \(\overline{E}E = I_n\). But since \(\overline{E}E = I_n\), then by Exercise 10 from 2.4, \(\overline{E}\) and \(E\) are invertible and we have \(E^{-1} = \overline{E}\). \(\ \blacksquare\) Definition If \(A \in M_{n \times n}(\mathbf{F})\) we define the rank of \(A\), denoted \(\text{rank}(A)\), to be the rank of the linear transformation \(L_A: \mathbf{F}^n \rightarrow \mathbf{F}^m\) One important implication here is that an \(n \times n\) matrix is invertible if and only if its rank is \(n\). Theorem 3.3 Let \(T: V \rightarrow W\) be a linear transformation between finite-dimensional vector spaces, and let \(\beta\) and \(\gamma\) be ordered bases for \(V\) and \(W\), respectively. Then \(rank(T) = rank([T]_{\beta}^{\gamma})\) The rank of a linear transformation is the same as the rank of one its matrix representations. Theorem 3.4 Let \(A\) be an \(m \times n\) matrix. If \(P\) and \(Q\) are invertible \(m \times m\) and \(n \times n\) matrices, respectively, then \(\text{rank}(AQ) = \text{rank}(A)\), \(\text{rank}(PA) = \text{rank}(A)\) and therefore, \(\text{rank}(PAQ) = \text{rank}(A)\) Proof (a) Observe that $$ \begin{align*} R(L_{AQ}) &amp;= R(L_AL_Q) \\ &amp;= L_AL_Q(\mathbf{F}) \quad \text{(to get the range, we apply the linear map)}\\ &amp;= L_A(L_Q(\mathbf{F})) \quad \text{(we apply $L_Q$ first)}\\ &amp;= L_A(\mathbf{F}) \quad \text{(because $L_Q$ is onto)}\\ &amp;= R(L_A) \end{align*} $$ Therefore, $$ \begin{align*} \text{rank}(AQ) &amp;= \dim(R(L_{AQ})) \quad \text{(the rank is the dimension of the range)} \\ &amp;= \dim(R(L_A)) \quad \text{(by the previous result)} \\ &amp;= rank(A). \end{align*} $$ Theorem 3.4 (Corollary) Elementary row and column operations on a matrix are rank-preserving. Theorem 3.5 The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns. Proof For any \(A \in M_{m \times n}(\mathbf{F})\), $$ \begin{align*} \text{rank}(A) = \text{rank}(L_A) = \dim(R(L_A)) \end{align*} $$ Let \(\beta\) be the standard ordered basis for \(\mathbf{F}^n\). Then \(\beta\) spans \(\mathbf{F}^n\). Theorem 2.2 assets that \(R(T) = \text(T(\beta))\) so $$ \begin{align*} R(L_A) &amp;= \text{span}(L_A(\beta)) \quad \text{(by Theorem 2.2)} \\ &amp;= \text{span}(\{L_A(e_1),...,L_A(e_n)\}) \end{align*} $$ By theorem 2.13(b) \(L_A(e_j) = Ae_j = a_j\) where \(a_j\) is the \(j\)th column of \(A\). So $$ \begin{align*} R(L_A) &amp;= \text{span}(\{a_1,...,a_n\}) \end{align*} $$ Therefore $$ \begin{align*} \text{rank}(A) &amp;= \dim(R(L_A)) = \dim(\text{span}(\{a_1,...,a_n\})) \ \blacksquare \end{align*} $$ Note: If you’ve forgotten 2.13 which you just did. Remember that by definition, matrix-vector multiplication \(Av\) is defined as \(a_1v_1 + a_2v_2 +...+ a_nv_n\) where \(a_1,...,a_n\) are the columns of \(A\) and \(v_1,...,v_n\) are the entries of the vector \(v\). Therefore, if you multiply \(Ae_j\) where \(e_j\) is from the standard basis, then we know that vector is all zeros except for the \(j\)th entry. So this means that \(Ae_j = a_j\). Theorem 3.6 Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then \(r \leq m, r \leq n\), and by means of a finite number of elementary row and column operations, \(A\) can be transformed into the matrix $$ \begin{align*} D = \begin{pmatrix} I_r &amp; O_1 \\ O_2 &amp; O_3 \end{pmatrix} \end{align*} $$ where \(O_1\), \(O_2\) and \(O_3\) are zero matrices. Thus \(D_{ii} = 1\) for \(i \leq r\) and \(D_{ij} = 0\) otherwise. Theorem 3.6 (Corollary 1) Let \(A\) be an \(m \times n \) matrix of rank \(r\). Then there exists invertible matrices \(B\) and \(C\) of sizes \(m \times m \) and \(n \times n\) respectively, such that \(D = BAC\) where $$ \begin{align*} D = \begin{pmatrix} I_r &amp; O_1 \\ O_2 &amp; O_3 \end{pmatrix} \end{align*} $$ is the \(m \times n\) in which \(O_1\), \(O_2\) and \(O_3\) are zero matrices. Proof TODO Theorem 3.6 (Corollary 2) Let \(A\) be \(m \times n\) matrices. Then Proof TODO]]></summary></entry><entry><title type="html">7.1: Study Notes</title><link href="http://localhost:4000/jekyll/update/2024/09/17/7.1.html" rel="alternate" type="text/html" title="7.1: Study Notes" /><published>2024-09-17T01:01:36-07:00</published><updated>2024-09-17T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/17/7.1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/17/7.1.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be a scalar. A non-zero vector \(x\) in \(V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \((T - \lambda I)^p(x) = 0\) for some positive integer \(p\).
</div>
<p><br />
Note here that if \(x\) is a generalized eigenvector of \(T\) corresponding to \(\lambda\) and \(p\) is the smallest positive integer for which \((T - \lambda I)^{p}(x) = 0\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I)^{p-1}(x) &amp;= y \neq 0 \\
(T - \lambda I)(T - \lambda I)^{p-1}(x) &amp;= (T - \lambda I) y \\
(T - \lambda I)^{p}(x) &amp;= T(y) - \lambda y \\
\bar{0} &amp;= T(y) - \lambda y \\
T(y) &amp;= \lambda y
\end{align*}
$$
</div>
<p>So \(y\) is an eigenvector of \(T\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). The generalized eigenspace of \(T\) corresponding to \(\lambda\) denoted \(K_{\lambda}\), is the subset of \(V\) defined by
$$
\begin{align*}
K_{\lambda} = \{ x \in V: (T - \lambda I)^p(x) = 0 \quad \text{for some positive integer $p$} \}
\end{align*}
$$
</div>
<p><br />
Note that \(K_{\lambda}\) consists of the zero vector and all generalized eigenvectors corresponding to \(\lambda\).
<br />
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.1
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then
<ol type="a">
	<li>\(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\) (the eigenspace of \(T\) corresponding to \(\lambda\))</li>
	<li>For any eigenvalue \(\mu\) of \(T\) such that \(\mu \neq \lambda\), \(K_{\lambda} \cap K_{\mu} = \{0\}\)</li>
	<li>For any scalar \(\mu \neq \lambda\), the restriction of \(T - \mu I\) to \(K_{\lambda}\) is one-to-one and onto.</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
(a) Showing that \(K_{\lambda}\) is a subspace is straightforward. We need to show that \(\bar{0} \in K_{\lambda}\) and that \(K_{\lambda}\) is closed under scalar multiplication and addition.
<br />
<br />
To show that \(K_{\lambda}\) is \(T\)-invariant. We need to show for any \(x \in K_{\lambda}\), that \(T(x) \in K_{\lambda}\). By definition, let be \(p\) be a positive integer, then \((T - \lambda I)^p(x) = \bar{0}\). We know to show that \((T - \lambda I)^p(T(x)) = \bar{0}\). Observe that</p>
<div>
$$
\begin{align*}
(T - \lambda I)^p(T(x)) = T(T - \lambda I)^p(x) = T(\bar{0}) = \bar{0}
\end{align*}
$$
</div>
<p>(b) TODO
<br />
(c) Let \(\mu\) be a scalar such that \(\mu \neq \lambda\). Let \(x \in K_{\lambda}\).  Let \(p\) be the smallest integer such that \((T - \lambda I)^p x = 0\) and</p>
<div>
	$$
	\begin{align*}
	W = \text{span}\{x, (T - \lambda I)(x),...,(T - \lambda I)^{p-1}(x)\}
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.2
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that the characteristic polynomial splits. Suppose that \(\lambda\) is an eigenvalue of \(T\) with multiplicity \(m\). Then
<ol type="a">
	<li>\(\dim(K_{\lambda}) \leq m\)</li>
	<li>\(K_{\lambda} = N((T - \lambda I)^m)\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
(a) Let \(W = K_{\lambda}\). Let the characteristic polynomial of \(W\) be \(h(t)\) We know by Theorem 7.1, that \(W\) is a \(T\)-invariant subspace of \(V\). Therefore, by Theorem 5.20, \(h(t)\) divides the characteristic polynomial of \(T\). From Theorem 7.1(b), we know that \(\lambda\) is the only eigenvalue of \(T_W\). Therefore \(h(t) = (-1)^d(t - \lambda)^d\) where \(d = \dim(W)\) and \(d \leq m\). 
<br />
<br />
(b) By definition, we know that \(N(T)=\{x \in V \ | \ T(x) = 0\}\). We also know that \(K_{\lambda} =\{ x \in V \ | \ (T - \lambda I)^p = 0 \ \text{ for }p &gt; 0 \}\). So we can see that for any \(x \in N((T - \lambda I)^m)\)</p>
<div>
	$$
	\begin{align*}
	(T - \lambda I)^m(x) = 0
	\end{align*}
	$$
</div>
<p>Since \(m\) is the multiplicity of an eigenvalue, then it’s positive and so \(x \in K_{\lambda}\) by definition. So \(N((T - \lambda I)^m) \in K_{\lambda}\).
<br />
<br />
Now, suppose that \(x \in K_{\lambda}\). We want to show that \(x \in N((T - \lambda I)^m)\) where \(m\) is the multiplicity of \(\lambda\). Since the characteristic polynomial of \(T\) splits, then let it be of the form \(f(t) = (t - \lambda)^m g(t)\) where \(g(t)\) is the product of powers of the form \(t - \mu\) for eigenvalues \(\mu \neq \lambda\). By Theorem 7.1, \(g(T)\) is one-to-one on \(K_{\lambda}\)[TODO: WHY?]. It is also onto since \(K_{\lambda}\) is finite dimensional. Since  \(x \in K_{\lambda}\), then there exists some \(y\) such that \(g(T)(x) = y\). [WHY?]. Hence</p>
<div>
	$$
	\begin{align*}
	(T - \lambda I)^m(x) = (T - \lambda I)^m g(T)(y) = f(T)(y) = \bar{0}
	\end{align*}
	$$
</div>
<p>Why? ….. [TODO]
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.3
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that characteristic polynomial of \(T\) splits, and let \(\lambda_1,\lambda_2,..., \lambda_k\) be the distinct eigenvalues of \(T\). Then, for every \(x \in V\), there are exist unique vectors \(v_i \in K_{\lambda}\), for \(i = 1,2,...,k\) such that
$$
\begin{align*}
x = v_1 + v_2 + ... + v_k
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.4
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial \((t - \lambda_1)^{m_1}(t - \lambda_2)^{m_2}...(t - \lambda_k)^{m_k}\) splits. For \(i = 1,2,...,k\), let \(\beta_i\) be an ordered basis for \(K_{\lambda_i}\). Then
<ol type="a">
	<li>\(\beta_i \cap \beta_j = \emptyset \text{ for } i \neq j\)</li>
	<li>\(\beta = \beta_1 \cup \beta_2 \cup ... \cup \beta_k \) is an ordered basis for \(V\)</li>
	<li>\(\dim(K_{\lambda_j}) = m_i\) for all \(i\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
(a) Consequence of Theorem 7.1(b)
<br />
<br />
(b) We need to show that \(\beta\) is linearly independent and that \(\beta\) spans \(V\). To see that \(\beta\) is linearly independent. 
<br />
<br />
(c) 
<br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.6
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Suppose that \(\gamma_1, \gamma_2,...,\gamma_q\) are cycles of generalized eigenvectors of \(T\) corresponding to \(\lambda\) such that the initial vectors of the \(\gamma_i\)'s are distinct and form a linearly independent set. Then, the \(\gamma_i\)'s are disjoint and their union \(\gamma = \bigcup\limits_{i=1}^q \gamma_i\) is linearly independent.
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary (7.6)
</div>
<div class="purbdiv">
Every cycle of generalized eigenvectors of a linear operator is linearly independent
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Theorem 7.7
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then \(K_{\lambda}\) has an ordered basis consisting of a union of disjoint cycles of generalized eigenvectors corresponding to \(\lambda\).
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary 1 (7.7)
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial splits. Then \(T\) has a Jordan canonical form.
</div>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be a scalar. A non-zero vector \(x\) in \(V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \((T - \lambda I)^p(x) = 0\) for some positive integer \(p\). Note here that if \(x\) is a generalized eigenvector of \(T\) corresponding to \(\lambda\) and \(p\) is the smallest positive integer for which \((T - \lambda I)^{p}(x) = 0\), then $$ \begin{align*} (T - \lambda I)^{p-1}(x) &amp;= y \neq 0 \\ (T - \lambda I)(T - \lambda I)^{p-1}(x) &amp;= (T - \lambda I) y \\ (T - \lambda I)^{p}(x) &amp;= T(y) - \lambda y \\ \bar{0} &amp;= T(y) - \lambda y \\ T(y) &amp;= \lambda y \end{align*} $$ So \(y\) is an eigenvector of \(T\). Definition Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). The generalized eigenspace of \(T\) corresponding to \(\lambda\) denoted \(K_{\lambda}\), is the subset of \(V\) defined by $$ \begin{align*} K_{\lambda} = \{ x \in V: (T - \lambda I)^p(x) = 0 \quad \text{for some positive integer $p$} \} \end{align*} $$ Note that \(K_{\lambda}\) consists of the zero vector and all generalized eigenvectors corresponding to \(\lambda\). Theorem 7.1 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then \(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\) (the eigenspace of \(T\) corresponding to \(\lambda\)) For any eigenvalue \(\mu\) of \(T\) such that \(\mu \neq \lambda\), \(K_{\lambda} \cap K_{\mu} = \{0\}\) For any scalar \(\mu \neq \lambda\), the restriction of \(T - \mu I\) to \(K_{\lambda}\) is one-to-one and onto. Proof (a) Showing that \(K_{\lambda}\) is a subspace is straightforward. We need to show that \(\bar{0} \in K_{\lambda}\) and that \(K_{\lambda}\) is closed under scalar multiplication and addition. To show that \(K_{\lambda}\) is \(T\)-invariant. We need to show for any \(x \in K_{\lambda}\), that \(T(x) \in K_{\lambda}\). By definition, let be \(p\) be a positive integer, then \((T - \lambda I)^p(x) = \bar{0}\). We know to show that \((T - \lambda I)^p(T(x)) = \bar{0}\). Observe that $$ \begin{align*} (T - \lambda I)^p(T(x)) = T(T - \lambda I)^p(x) = T(\bar{0}) = \bar{0} \end{align*} $$ (b) TODO (c) Let \(\mu\) be a scalar such that \(\mu \neq \lambda\). Let \(x \in K_{\lambda}\). Let \(p\) be the smallest integer such that \((T - \lambda I)^p x = 0\) and $$ \begin{align*} W = \text{span}\{x, (T - \lambda I)(x),...,(T - \lambda I)^{p-1}(x)\} \end{align*} $$ Theorem 7.2 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that the characteristic polynomial splits. Suppose that \(\lambda\) is an eigenvalue of \(T\) with multiplicity \(m\). Then \(\dim(K_{\lambda}) \leq m\) \(K_{\lambda} = N((T - \lambda I)^m)\) Proof (a) Let \(W = K_{\lambda}\). Let the characteristic polynomial of \(W\) be \(h(t)\) We know by Theorem 7.1, that \(W\) is a \(T\)-invariant subspace of \(V\). Therefore, by Theorem 5.20, \(h(t)\) divides the characteristic polynomial of \(T\). From Theorem 7.1(b), we know that \(\lambda\) is the only eigenvalue of \(T_W\). Therefore \(h(t) = (-1)^d(t - \lambda)^d\) where \(d = \dim(W)\) and \(d \leq m\). (b) By definition, we know that \(N(T)=\{x \in V \ | \ T(x) = 0\}\). We also know that \(K_{\lambda} =\{ x \in V \ | \ (T - \lambda I)^p = 0 \ \text{ for }p &gt; 0 \}\). So we can see that for any \(x \in N((T - \lambda I)^m)\) $$ \begin{align*} (T - \lambda I)^m(x) = 0 \end{align*} $$ Since \(m\) is the multiplicity of an eigenvalue, then it’s positive and so \(x \in K_{\lambda}\) by definition. So \(N((T - \lambda I)^m) \in K_{\lambda}\). Now, suppose that \(x \in K_{\lambda}\). We want to show that \(x \in N((T - \lambda I)^m)\) where \(m\) is the multiplicity of \(\lambda\). Since the characteristic polynomial of \(T\) splits, then let it be of the form \(f(t) = (t - \lambda)^m g(t)\) where \(g(t)\) is the product of powers of the form \(t - \mu\) for eigenvalues \(\mu \neq \lambda\). By Theorem 7.1, \(g(T)\) is one-to-one on \(K_{\lambda}\)[TODO: WHY?]. It is also onto since \(K_{\lambda}\) is finite dimensional. Since \(x \in K_{\lambda}\), then there exists some \(y\) such that \(g(T)(x) = y\). [WHY?]. Hence $$ \begin{align*} (T - \lambda I)^m(x) = (T - \lambda I)^m g(T)(y) = f(T)(y) = \bar{0} \end{align*} $$ Why? ….. [TODO] Theorem 7.3 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) such that characteristic polynomial of \(T\) splits, and let \(\lambda_1,\lambda_2,..., \lambda_k\) be the distinct eigenvalues of \(T\). Then, for every \(x \in V\), there are exist unique vectors \(v_i \in K_{\lambda}\), for \(i = 1,2,...,k\) such that $$ \begin{align*} x = v_1 + v_2 + ... + v_k \end{align*} $$ Proof TODO Theorem 7.4 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial \((t - \lambda_1)^{m_1}(t - \lambda_2)^{m_2}...(t - \lambda_k)^{m_k}\) splits. For \(i = 1,2,...,k\), let \(\beta_i\) be an ordered basis for \(K_{\lambda_i}\). Then \(\beta_i \cap \beta_j = \emptyset \text{ for } i \neq j\) \(\beta = \beta_1 \cup \beta_2 \cup ... \cup \beta_k \) is an ordered basis for \(V\) \(\dim(K_{\lambda_j}) = m_i\) for all \(i\) Proof (a) Consequence of Theorem 7.1(b) (b) We need to show that \(\beta\) is linearly independent and that \(\beta\) spans \(V\). To see that \(\beta\) is linearly independent. (c) Theorem 7.6 Let \(T\) be a linear operator on a vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Suppose that \(\gamma_1, \gamma_2,...,\gamma_q\) are cycles of generalized eigenvectors of \(T\) corresponding to \(\lambda\) such that the initial vectors of the \(\gamma_i\)'s are distinct and form a linearly independent set. Then, the \(\gamma_i\)'s are disjoint and their union \(\gamma = \bigcup\limits_{i=1}^q \gamma_i\) is linearly independent. Proof TODO Corollary (7.6) Every cycle of generalized eigenvectors of a linear operator is linearly independent Theorem 7.7 Let \(T\) be a linear operator on a finite-dimensional vector space \(V\), and let \(\lambda\) be an eigenvalue of \(T\). Then \(K_{\lambda}\) has an ordered basis consisting of a union of disjoint cycles of generalized eigenvectors corresponding to \(\lambda\). Proof TODO Corollary 1 (7.7) Let \(T\) be a linear operator on a finite-dimensional vector space \(V\) whose characteristic polynomial splits. Then \(T\) has a Jordan canonical form. Proof TODO References Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 39: Jordan Blocks and Generalized Eigenvectors</title><link href="http://localhost:4000/jekyll/update/2024/09/16/lec39-jordan-blocks-generalized-eigenvectors.html" rel="alternate" type="text/html" title="Lecture 39: Jordan Blocks and Generalized Eigenvectors" /><published>2024-09-16T01:01:36-07:00</published><updated>2024-09-16T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/16/lec39-jordan-blocks-generalized-eigenvectors</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/16/lec39-jordan-blocks-generalized-eigenvectors.html"><![CDATA[<p>Last lecture, we saw why matrices in Jordan Canonical form are useful and we spent the entire lecture  understanding the following major theorem
<br /></p>
<div class="purdiv">
Theorem (JCF)
</div>
<div class="purbdiv">
Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Specifically, we looked at the first Jordan block (\(A_1\)) in \([T]_{\beta}^{\beta}\) and analyzed what these basis elements need to be in order for \([T]_{\beta}^{\beta}\) to be in Jordan Canonical form. 
<br />
<br />
FACT 1 was that a Jordan Canonical Basis must consists of generalized eigenvectors.</p>
<div>
$$
\begin{align*}
\{v_1,...v_{n_1-1},v_{n_1}\} = \{(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}),...,(T - \lambda_1 I_V)^{2}(v_{n_1}), (T - \lambda_1 I_V)(v_{n_1}), v_{n_1}\}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>And then FACT 2 was that A Jordan Canonical Basis is made of “cyclic pieces”. The basis is not only made of generalized eigenvectors but they also appear in this cyclic pattern. This leads us to the following definition
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(x \in K_{\lambda}\) and \(p\) be the smallest integer such that \((T - \lambda_i I_V)^p(x) = \bar{0}_V\). Let 
$$
\begin{align*}
\gamma = \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\}
\end{align*}
$$
\(\gamma\) is the cycle of generalized eigenvectors generated by \(x\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Some terminology:</p>
<ul>
	<li>The length of \(\gamma\) is \(p\)</li> 
	<li>The initial vector of \(\gamma\) is \((T - \lambda_i I_V)^{p-1}\) </li>
	<li>\(\gamma = \gamma(x)\)</li>
</ul>
<p>What can we say about these objects?
<br /></p>
<div class="purdiv">
Theorem 2.1
</div>
<div class="purbdiv">
\(T\) is diagonalizable if and only if
<ol type="a">
	<li>\(\gamma\) is linearly independent</li>
	<li>\(W = \text{span}(\gamma)\) is \(T\)-invariant</li>
	<li>\([T_W]_{\gamma}^{\gamma}\) is a Jordan block</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
<br />
Re-write \(\gamma\) as</p>
<div>
$$
\begin{align*}
\gamma &amp;= \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\} \\
       &amp;= \{v_1, ..., v_n\}
\end{align*}
$$
</div>
<p>These satisfy the following relations</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)(v_1) &amp;= (T - \lambda I_V)^p(x) = \bar{0}_V\\
(T - \lambda I_V)(v_i) &amp;= (T - \lambda I_V)^p(v_{i-1})
\end{align*}
$$
</div>
<p>We can re-write these equations as</p>
<div>
$$
\begin{align*}
(T(v_1) &amp;= \lambda v_1\\
(T(v_i) &amp;= \lambda v_i + v_{i-1}
\end{align*}
$$
</div>
<p>This implies that \(T(v_j) \in \text{span}(\gamma) \quad \forall j=1,...,p\). This means that \(T(W) \subseteq W\). Therefore (b) holds.
<br />
<br />
We can also write the matrix representative of \(T\) with respect to \(W\)</p>
<div>
$$
\begin{align*}
[T_W]_{\gamma}^{\gamma} = 
\begin{pmatrix}
\lambda &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \lambda &amp; \ddots &amp; \vdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<p>From this we see that \((c)\) holds. 
<br />
<br />
So now we only need to prove \((a)\). We need to build a basis of each \(K_{\lambda}\) out of cycles. To show this we need the next two results
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2.2
</div>
<div class="purbdiv">
Let \(\gamma_1,...,\gamma_m\) be cycles for \(\lambda\) with linearly independent initial vectors. Then
$$
\begin{align*}
\gamma = \cup_{j=1}^m \gamma_j
\end{align*}
$$
is linearly independent.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and
<br /></p>
<div class="purdiv">
Theorem 2.3
</div>
<div class="purbdiv">
Each \(K_{\lambda}\) has a basis consisting of disjoint cycles.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The JCF Proof</b></h4>
<p>Theorems 1.1 to 2.3 combined prove the JCF Theorem! To see how. Let \(\lambda_1,...,\lambda_k\) be the disjoint eigenvalues of \(T\). For \(\lambda_j\) find a basis \(\beta_j\) of \(K_{\lambda_j}\) consisting of disjoint cycles (Theorem 2.3).
<br />
By Theorem 1.4, \(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis of \(V\). And by Theorem 2.1, \([T]_{\beta}^{\beta}\) is in JCF.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(A = \begin{pmatrix}
3 &amp; 1 &amp; -2 \\
-1 &amp; 0 &amp; 5 \\
-1 &amp; -1 &amp; 4
\end{pmatrix}\). Put \(A\) in JCF if possible. Here \(T\) is \(L_A: \mathbf{R}^3 \rightarrow \mathbf{R}^3\). 
<br />
<br />
We first need to check if the characteristic polynomial splits.</p>
<div>
$$
\begin{align*}
\det(A - tI_3) = (3 - t)(2 - t)^2
\end{align*}
$$
</div>
<p>So the characteristic polynomial splits. (In fact \(\lambda_1\) has algebraic multiplicity 1 and \(\lambda_2\) has algebraic multiplicity 2).
<br />
<br />
We know the general form of the Jordan Canonical Basis where \(\beta = \beta_1 \cup ... \cup \beta_m\) where \(\beta_j = \gamma_1^j \cup \gamma_2^j \cup ... \cup \gamma_1^{k_j}\), a collection of disjoint cycles. So let’s build these pieces starting with the first eigenvalue as follows
<br />
<br />
\(\lambda_1 = 3\): The algebraic multiplicity of \(\lambda_1\) is 1. This implies that the generalized eigenspace \(K_{\lambda_1} = E_{\lambda_1}\) why is that? The dimension of the generalized eigenspace is equal to the algebraic multiplicity so its dimension is 1. But we know that \(E_{\lambda_1}\) has a non-zero dimension and that it sits inside \(K_{\lambda_1}\). Therefore \(K_{\lambda_1} = E_{\lambda_1}\). What about the cycles of this generalized eigenspace? the length of the cycle is (p=1). So all we need to do is find the nullspace of this eigenspace.</p>
<div>
$$
\begin{align*}
A - 3I_3 = 
A = \begin{pmatrix}
0 &amp; 1 &amp; -2 \\
-1 &amp; -3 &amp; 5 \\
-1 &amp; -1 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>Putting this in echelon form, we see</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -2 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>The solution will this lead to</p>
<div>
$$
\begin{align*}
\beta_1 = \left\{
\begin{pmatrix}
-1 \\
2 \\
1
\end{pmatrix}
\right\}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>\(\lambda_2 = 2\): By Theorem 1.2, \(K_{\lambda_2} = N((A - 2I_3)^2)\) so</p>
<div>
$$
(A - 2I_3)^2 = 
\begin{align*}
A - 2I_3 = 
\begin{pmatrix}
1 &amp; 1 &amp; -2 \\
-1 &amp; -2 &amp; 5 \\
-1 &amp; -1 &amp; 2
\end{pmatrix}^2 
= 
\begin{pmatrix}
2 &amp; 1 &amp; -1 \\
-4 &amp; -2 &amp; 2 \\
-2 &amp; -1 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>Putting this in echelon form, we see</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; \frac{1}{2} &amp; -\frac{1}{2} \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>The solution set is then</p>
<div>
$$
\begin{align*}
K_{\lambda_2} = \text{span}\left\{
\begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}
\begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}
\right\}
\end{align*}
$$
</div>
<p>So we have a basis but we don’t have cycles yet. So we need to start generating cycles. Let’s choose the second vector above.</p>
<div>
$$
\begin{align*}
(A - 2I_3)(x) = 
\begin{pmatrix}
1 &amp; 1 &amp; -2 \\
-1 &amp; -2 &amp; 5 \\
-1 &amp; -1 &amp; 2
\end{pmatrix}
\begin{pmatrix}
1 \\
0 \\
2
\end{pmatrix}
=
\begin{pmatrix}
-3 \\
9 \\
3
\end{pmatrix}
\end{align*}
$$
</div>
<p>So given \(x \in K_{\lambda} \implies \gamma=\{(T - \lambda I_V)^{p-1}(x),...,x\}\). The term above is the second term from last. So the term remaining is just when \(p = 0\) which means it’s \(x\) itself. Therefore</p>
<div>
$$
\begin{align*}
\gamma = \left\{
\begin{pmatrix}
-3 \\
9 \\
3
\end{pmatrix},
\begin{pmatrix}
1 \\
0 \\
2
\end{pmatrix}
\right\} = \beta_2
\end{align*}
$$
</div>
<p>Therefore, the Jordan Canonical Basis is</p>
<div>
$$
\begin{align*}
\beta = \beta_1 \cup \beta_2 
= \left\{
\begin{pmatrix}
	-1 \\
	2 \\
	1
\end{pmatrix},
\begin{pmatrix}
-3 \\
9 \\
3
\end{pmatrix},
\begin{pmatrix}
1 \\
0 \\
2
\end{pmatrix}
\right\}
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
[L_A]_{\beta}^{\beta} =
\begin{pmatrix}
3 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>The diagonal elements are the eigenvalues. What about other non-zero elements? The second block is generated by a cycle of length 2 So there is a 1 in that block.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Determining the Jordan Block for an Eigenvalue</b></h4>
<p>But can we get to the end JCF form without having to compute all of these cycles?
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(\lambda\) be an eigenvalue of \(T\) with algebraic multiplicity \(m\). 
$$
\begin{align*}
r_1 &amp;= \dim(V) - rank(T - \lambda I_V) \\
r_i &amp;= rank((T - \lambda I_V)^{i-1}) - rank((T - \lambda I_V)^i)
\end{align*}
$$
Using \(r_1, r_2, ...\) form the diagram
$$
\begin{align*}
\begin{matrix}
\circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_1\text{ dots} \\
\circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_2\text{ dots} \\
\vdots &amp; \vdots \\
\circ &amp; \circ &amp;        &amp;       &amp; \quad &amp; r_j\text{ dots} \\
\end{matrix}
\end{align*}
$$
Then each column corresponds to Jordan block of \(\lambda\) of size the number of the dots in each column 
</div>
<p><br />
Observation:</p>
<ul>
	<li>\(r_1 \geq r_2 \geq r_3 \geq ...\)</li>
	<li>\(r_1 + r_2 + r_3 + ... = m\)</li>
	<li>\(r_1 =\) the number of Jordan Blocks for \(\lambda\)</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last lecture, we saw why matrices in Jordan Canonical form are useful and we spent the entire lecture understanding the following major theorem Theorem (JCF) Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form. Specifically, we looked at the first Jordan block (\(A_1\)) in \([T]_{\beta}^{\beta}\) and analyzed what these basis elements need to be in order for \([T]_{\beta}^{\beta}\) to be in Jordan Canonical form. FACT 1 was that a Jordan Canonical Basis must consists of generalized eigenvectors. $$ \begin{align*} \{v_1,...v_{n_1-1},v_{n_1}\} = \{(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}),...,(T - \lambda_1 I_V)^{2}(v_{n_1}), (T - \lambda_1 I_V)(v_{n_1}), v_{n_1}\} \end{align*} $$ And then FACT 2 was that A Jordan Canonical Basis is made of “cyclic pieces”. The basis is not only made of generalized eigenvectors but they also appear in this cyclic pattern. This leads us to the following definition Definition Let \(x \in K_{\lambda}\) and \(p\) be the smallest integer such that \((T - \lambda_i I_V)^p(x) = \bar{0}_V\). Let $$ \begin{align*} \gamma = \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\} \end{align*} $$ \(\gamma\) is the cycle of generalized eigenvectors generated by \(x\). Some terminology: The length of \(\gamma\) is \(p\) The initial vector of \(\gamma\) is \((T - \lambda_i I_V)^{p-1}\) \(\gamma = \gamma(x)\) What can we say about these objects? Theorem 2.1 \(T\) is diagonalizable if and only if \(\gamma\) is linearly independent \(W = \text{span}(\gamma)\) is \(T\)-invariant \([T_W]_{\gamma}^{\gamma}\) is a Jordan block Proof Re-write \(\gamma\) as $$ \begin{align*} \gamma &amp;= \{(T - \lambda_i I_V)^{p-1}(x),..., (T - \lambda_i I_V)(x),x\} \\ &amp;= \{v_1, ..., v_n\} \end{align*} $$ These satisfy the following relations $$ \begin{align*} (T - \lambda I_V)(v_1) &amp;= (T - \lambda I_V)^p(x) = \bar{0}_V\\ (T - \lambda I_V)(v_i) &amp;= (T - \lambda I_V)^p(v_{i-1}) \end{align*} $$ We can re-write these equations as $$ \begin{align*} (T(v_1) &amp;= \lambda v_1\\ (T(v_i) &amp;= \lambda v_i + v_{i-1} \end{align*} $$ This implies that \(T(v_j) \in \text{span}(\gamma) \quad \forall j=1,...,p\). This means that \(T(W) \subseteq W\). Therefore (b) holds. We can also write the matrix representative of \(T\) with respect to \(W\) $$ \begin{align*} [T_W]_{\gamma}^{\gamma} = \begin{pmatrix} \lambda &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; \lambda &amp; \ddots &amp; \vdots \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \lambda \end{pmatrix} \end{align*} $$ From this we see that \((c)\) holds. So now we only need to prove \((a)\). We need to build a basis of each \(K_{\lambda}\) out of cycles. To show this we need the next two results Theorem 2.2 Let \(\gamma_1,...,\gamma_m\) be cycles for \(\lambda\) with linearly independent initial vectors. Then $$ \begin{align*} \gamma = \cup_{j=1}^m \gamma_j \end{align*} $$ is linearly independent. and Theorem 2.3 Each \(K_{\lambda}\) has a basis consisting of disjoint cycles. The JCF Proof Theorems 1.1 to 2.3 combined prove the JCF Theorem! To see how. Let \(\lambda_1,...,\lambda_k\) be the disjoint eigenvalues of \(T\). For \(\lambda_j\) find a basis \(\beta_j\) of \(K_{\lambda_j}\) consisting of disjoint cycles (Theorem 2.3). By Theorem 1.4, \(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis of \(V\). And by Theorem 2.1, \([T]_{\beta}^{\beta}\) is in JCF. Example Let \(A = \begin{pmatrix} 3 &amp; 1 &amp; -2 \\ -1 &amp; 0 &amp; 5 \\ -1 &amp; -1 &amp; 4 \end{pmatrix}\). Put \(A\) in JCF if possible. Here \(T\) is \(L_A: \mathbf{R}^3 \rightarrow \mathbf{R}^3\). We first need to check if the characteristic polynomial splits. $$ \begin{align*} \det(A - tI_3) = (3 - t)(2 - t)^2 \end{align*} $$ So the characteristic polynomial splits. (In fact \(\lambda_1\) has algebraic multiplicity 1 and \(\lambda_2\) has algebraic multiplicity 2). We know the general form of the Jordan Canonical Basis where \(\beta = \beta_1 \cup ... \cup \beta_m\) where \(\beta_j = \gamma_1^j \cup \gamma_2^j \cup ... \cup \gamma_1^{k_j}\), a collection of disjoint cycles. So let’s build these pieces starting with the first eigenvalue as follows \(\lambda_1 = 3\): The algebraic multiplicity of \(\lambda_1\) is 1. This implies that the generalized eigenspace \(K_{\lambda_1} = E_{\lambda_1}\) why is that? The dimension of the generalized eigenspace is equal to the algebraic multiplicity so its dimension is 1. But we know that \(E_{\lambda_1}\) has a non-zero dimension and that it sits inside \(K_{\lambda_1}\). Therefore \(K_{\lambda_1} = E_{\lambda_1}\). What about the cycles of this generalized eigenspace? the length of the cycle is (p=1). So all we need to do is find the nullspace of this eigenspace. $$ \begin{align*} A - 3I_3 = A = \begin{pmatrix} 0 &amp; 1 &amp; -2 \\ -1 &amp; -3 &amp; 5 \\ -1 &amp; -1 &amp; 1 \end{pmatrix} \end{align*} $$ Putting this in echelon form, we see $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The solution will this lead to $$ \begin{align*} \beta_1 = \left\{ \begin{pmatrix} -1 \\ 2 \\ 1 \end{pmatrix} \right\} \end{align*} $$ \(\lambda_2 = 2\): By Theorem 1.2, \(K_{\lambda_2} = N((A - 2I_3)^2)\) so $$ (A - 2I_3)^2 = \begin{align*} A - 2I_3 = \begin{pmatrix} 1 &amp; 1 &amp; -2 \\ -1 &amp; -2 &amp; 5 \\ -1 &amp; -1 &amp; 2 \end{pmatrix}^2 = \begin{pmatrix} 2 &amp; 1 &amp; -1 \\ -4 &amp; -2 &amp; 2 \\ -2 &amp; -1 &amp; 1 \end{pmatrix} \end{align*} $$ Putting this in echelon form, we see $$ \begin{align*} \begin{pmatrix} 1 &amp; \frac{1}{2} &amp; -\frac{1}{2} \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The solution set is then $$ \begin{align*} K_{\lambda_2} = \text{span}\left\{ \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \right\} \end{align*} $$ So we have a basis but we don’t have cycles yet. So we need to start generating cycles. Let’s choose the second vector above. $$ \begin{align*} (A - 2I_3)(x) = \begin{pmatrix} 1 &amp; 1 &amp; -2 \\ -1 &amp; -2 &amp; 5 \\ -1 &amp; -1 &amp; 2 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} = \begin{pmatrix} -3 \\ 9 \\ 3 \end{pmatrix} \end{align*} $$ So given \(x \in K_{\lambda} \implies \gamma=\{(T - \lambda I_V)^{p-1}(x),...,x\}\). The term above is the second term from last. So the term remaining is just when \(p = 0\) which means it’s \(x\) itself. Therefore $$ \begin{align*} \gamma = \left\{ \begin{pmatrix} -3 \\ 9 \\ 3 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \right\} = \beta_2 \end{align*} $$ Therefore, the Jordan Canonical Basis is $$ \begin{align*} \beta = \beta_1 \cup \beta_2 = \left\{ \begin{pmatrix} -1 \\ 2 \\ 1 \end{pmatrix}, \begin{pmatrix} -3 \\ 9 \\ 3 \end{pmatrix}, \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \right\} \end{align*} $$ and $$ \begin{align*} [L_A]_{\beta}^{\beta} = \begin{pmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ The diagonal elements are the eigenvalues. What about other non-zero elements? The second block is generated by a cycle of length 2 So there is a 1 in that block. Determining the Jordan Block for an Eigenvalue But can we get to the end JCF form without having to compute all of these cycles? Theorem Let \(\lambda\) be an eigenvalue of \(T\) with algebraic multiplicity \(m\). $$ \begin{align*} r_1 &amp;= \dim(V) - rank(T - \lambda I_V) \\ r_i &amp;= rank((T - \lambda I_V)^{i-1}) - rank((T - \lambda I_V)^i) \end{align*} $$ Using \(r_1, r_2, ...\) form the diagram $$ \begin{align*} \begin{matrix} \circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_1\text{ dots} \\ \circ &amp; \circ &amp; \cdots &amp; \circ &amp; \quad &amp; r_2\text{ dots} \\ \vdots &amp; \vdots \\ \circ &amp; \circ &amp; &amp; &amp; \quad &amp; r_j\text{ dots} \\ \end{matrix} \end{align*} $$ Then each column corresponds to Jordan block of \(\lambda\) of size the number of the dots in each column Observation: \(r_1 \geq r_2 \geq r_3 \geq ...\) \(r_1 + r_2 + r_3 + ... = m\) \(r_1 =\) the number of Jordan Blocks for \(\lambda\) References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 37/38: The Jordan Canonical Form and Generalized Eigenvectors</title><link href="http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form.html" rel="alternate" type="text/html" title="Lecture 37/38: The Jordan Canonical Form and Generalized Eigenvectors" /><published>2024-09-15T01:01:36-07:00</published><updated>2024-09-15T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/15/lec37-jordan-canonical-form.html"><![CDATA[<p>Last time we proved that If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\). This therefore lead to the conclusion that \(T\) is diagonalizable.
<!------------------------------------------------------------------------------------></p>
<h4><b>A Test For Diagonalizability</b></h4>
<p>We studied previously a few ways a linear operator can be tested for diagonalizability. From lecture 25,</p>
<div class="purdiv">
Theorem (5.8(a))
</div>
<div class="purbdiv">
\(T\) is diagonalizable if and only if
<ul style="list-style-type:lower-alpha">
	<li>The characteristic polynomial splits over its field \(\mathbf{R}\) or \(\mathbf{C}\)</li>
	<li>For each eigenvalue of \(T\), its geometric multiplicity (\(\dim(E_{\lambda})\)) = algebraic multiplicity</li>
</ul>
</div>
<p><br />
We mentioned last time too that if \(V\) is over \(\mathbf{C}\), the the characteristic polynomial always splits. Note also that you can have \((a)\) but not \((b)\). For example</p>
<div>
$$
\begin{align*}
A =
\begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>splits but doesn’t satisfy \((b)\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Jordan Canonical Form</b></h4>
<p>Based on the previous observation. It turns out there is a nice form that we can put \(T\) into in order to achieve \((a)\) and \((b)\)
<br /></p>
<div class="purdiv">
Theorem (JCF)
</div>
<div class="purbdiv">
Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form.
</div>
<p><br />
But what is a Jordan Canonical form? We first define a Jordan Block as follows
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A Jordan Block is a square matrix of the form
$$
\begin{align*}
\begin{pmatrix}
\lambda
\end{pmatrix}
\quad \text{ or } \quad 
\begin{pmatrix}
\lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
So they’re almost diagonal but not really. For example, for a \(2 \times 2\) and \(3 \times 3\) matrices, a Jordan block looks like</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
\lambda &amp; 1 \\
0 &amp; \lambda
\end{pmatrix}
,
\begin{pmatrix}
\lambda &amp; 1 &amp; 0 \\
0 &amp; \lambda &amp; 1 \\
0 &amp; 0 &amp; \lambda
\end{pmatrix}
\end{align*}
$$
</div>
<p>Using these Jordan blocks, we can now define what the Jordan Canonical form is
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A square matrix \(A\) is in Jordan Canonical Form if
$$
\begin{align*}
A =
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_{l}
\end{pmatrix},
\ \text{ where $A_j$ is a Jordan block}
\end{align*}
$$
</div>
<p><br />
You can think of this matrix as more of a generalization of a diagonal matrix.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>The following are examples of matrix in Jordan Canonical Form</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix},
B = 
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that the characteristic polynomial of both \(A\) and \(B\) is \((1-t)^3(2-t)^2\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing Powers of Matrices in JFC</b></h4>
<p>It turns out that we can write a formula for powers of matrices in JFC. It is not as easy as taking the power of a diagonal matrix but at least have a formula.
<br />
<br />
Fact 1:</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
\lambda &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda
\end{pmatrix}
\implies
\begin{pmatrix}
\lambda^k &amp; k\lambda^{k-1} &amp; \cdots &amp; \frac{k(k-1)...(k-n+2)}{n!}\lambda^{k-n+1} \\
0 &amp; \ddots &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; k\lambda^{k-1} \\
0 &amp; \cdots &amp; 0 &amp; \lambda^k
\end{pmatrix}
\end{align*}
$$
</div>
<p>Note here that the third entry in the first row for example is \(\frac{k(k-1)}{2!}\lambda^{k-1}\)
<br />
<br /> 
Fact 2: If \(A\) is in Jordan Canonical form, then the power of the matrix is as follows</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_l
\end{pmatrix}^k
\implies
\begin{pmatrix}
A_1^k &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2^k &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_l^k
\end{pmatrix}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Proof of JCF Theorem</b></h4>
<p>So now we see that matrices in JCF are useful and we have enough motivation to prove the theorem above which again states that if the characteristic polynomial splits, then there is a basis such that \([T]^{\beta}_{\beta}\) is in JFC! To prove it, let \(\beta = \{v_1, ..., v_n\}\) of \(V\). We this basis to be such that</p>
<div>
$$
\begin{align*}
[T]^{\beta}_{\beta} = 
\begin{pmatrix}
[T(v)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta}
\end{pmatrix}
=
\begin{pmatrix}
A_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; A_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_l
\end{pmatrix}
\end{align*}
$$
</div>
<p>These vectors \(v_1,...,v_j\) are not necessarily eigenvectors. What are they? Let’s focus on \(A_1\) above and let \(A_1\) be of size \(n_1 \times n_1\).</p>
<div>
$$
\begin{align*}
A_1 =
\begin{pmatrix}
\lambda_1 &amp; 1 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; 1 \\
0 &amp; \cdots &amp; 0 &amp; \lambda_l
\end{pmatrix}
\end{align*}
$$
</div>
<p>Since \(A_1\) has the form above (Jordan block), then we know at least for \(\lambda_1\), \(T(v_1) = \lambda_1 v_1\). What about \(T(v_2)\)? We see that column 2 has 1 in the first row and then \(\lambda_2\) in the second row. So \(T(v_2) = v_1 + \lambda_2 v_2\). Therefore, \(v_2\) is not an eigenvector but we can observe that</p>
<div>
$$
\begin{align*}
T(v_2) &amp;= v_1 + \lambda_2 v_2 \\
T(v_2) - \lambda_2 v_2 &amp;= v_1 \\
(T - \lambda_1 I_V)(v_2) &amp;= v_1 \\
(T - \lambda_1 I_V)^2(v_2) &amp;= \bar{0}_V \quad \text{ (because $v_1$ is an eigenvector)}
\end{align*}
$$
</div>
<p>What about the remaining vectors?</p>
<div>
$$
\begin{align*}
T(v_3) &amp;= v_2 + \lambda_1 v_3 \\
(T - \lambda_1 I_V)(v_3) &amp;= v_2 \\
(T - \lambda_1 I_V)^3(v_3) &amp;= (T - \lambda_1 I_V)^2(v_2) \\
(T - \lambda_1 I_V)^3(v_2) &amp;= \bar{0}_V
\end{align*}
$$
</div>
<p>So they’re not eigenvectors but they satisfy these equations. Based on this observation, we’re going to define the following
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(x \in V\) is a generalized eigenvector of \(T: V \rightarrow V\) corresponding to \(\lambda\) if 
$$
\begin{align*}
(T - \lambda I_V)^p(x) = \bar{0}_V
\end{align*}
$$
for some integer \(p &gt; 0\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
So now we know that the basis we want to build will consists of generalized eigenvectors.<br />
FACT 1: A Jordan Canonical Basis consists of generalized eigenvectors (with special properties). <br />
If we start with the equations above from the bottom to the top, then</p>
<div>
$$
\begin{align*}
\{v_1,...v_{n_1-1},v_{n_1}\} = \{(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}),...,(T - \lambda_1 I_V)^{2}(v_{n_1}), (T - \lambda_1 I_V)(v_{n_1}), v_{n_1}\}
\end{align*}
$$
</div>
<p>So we can see here that the first \(n_1\) basis vectors coming from the \(A_1\) block are all obtained from the last vector and applying the maps over and over again. This leads to fact 2 below. 
<br />
<br />
FACT 2: A Jordan Canonical Basis is made of “cyclic pieces”. The basis is not only made of generalized eigenvectors but they also appear in this cyclic pattern.
<br />
<br />
Here is an observation. Let \(x\) be a generalized eigenvector of \(T\) and Let \(p\) be the smallest positive integer such that \((T - \lambda I_V)^p(x) = \bar{0}_V\). Then</p>
<div>
$$
\begin{align*}
y = (T - \lambda I_V)^{p-1}(x) \neq \bar{0}_V
\end{align*}
$$
</div>
<p>But if we apply \((T - \lambda I_V)\) to both sides, then</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)(y) = (T - \lambda I_V)^{p}(x) = 0
\end{align*}
$$
</div>
<p>Then \(y\) is an eigenvector of \(T\).
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(\lambda\) be an eigenvalue of \(T: V \rightarrow V\). The generalized eigenspace of \(\lambda\) is
$$
\begin{align*}
K_{\lambda} = \{x \in V : (T - \lambda I_V)^p(x) = \bar{0}_V \text{ for some } p &gt; 0 \}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="purdiv">
Theorem 1.1
</div>
<div class="purbdiv">
<ol type="a">
	<li>\(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\).</li>
	<li>For \(\mu = \lambda\), the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one.</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br />
<br />
(a) We need to show three things. \(K_{\lambda}\) contains \(E_{\lambda}\). This is clearly true by definition and \(E_{\lambda} \subseteq K_{\lambda}\). 
<br />
<br />
Next we need to show that \(K_{\lambda}\) is a subspace. This means we need to show that it contains the zero vector and it is closed under scalar multiplication and addition. \((T - \lambda I_V)(\bar{0}_V) = \bar{0}_V\) so \(\bar{0}_V \in K_{\lambda}\). Now consider \(x, y \in K_{\lambda}\) and \(c \in \mathbf{F}\), we need to show that \(x + cy \in K_{\lambda}\). Since \(x\) and \(y\) are in \(K_{\lambda}\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)(x) &amp;= \bar{0}_V \text{ for } p &gt; 0 \\
(T - \lambda I_V)(y) &amp;= \bar{0}_V \text { for } q &gt; 0
\end{align*}
$$
</div>
<p>Therefore</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p+q}(x + cy) &amp;= (T - \lambda I_V)^{p+q}(x) + c(T - \lambda I_V)^{p+q}(y) \\
                 &amp;= (T - \lambda I_V)^{p}(x)(T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y)(T - \lambda I_V)^{q}(y) \\
				&amp;= (T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y) \\
				&amp; = \bar{0}_V
\end{align*}
$$
</div>
<p>Next, we need to show that \(K_{\lambda}\) is \(T\)-invariant. This means that we want to show that \(T(K_{\lambda}) \subseteq K_{\lambda}\). Therefore, Let \(x \in K_{\lambda}\) so that \((T - \lambda I_V)^{p}(x) = \bar{0}_V\) by definition. We want to show that \(T(x) \in K_{\lambda}\)</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p}(T(x)) &amp;= T(T - \lambda I_V)^{p}(x)
\end{align*}
$$
</div>
<p>Taking \((T - \lambda I_V)\) to a power expands to some form of \(\lambda^k T^l\). But then we can reverse the order of the terms and write the following instead</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p}(T(x)) &amp;= T(T - \lambda I_V)^{p}(x) \\
                            &amp;= (T - \lambda I_V)^{p}T(x) \\1
                            &amp;= \bar{0}_V
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
(b) Next, we want to prove that the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one. So we want to think of this map \(T - \mu I_V\) as map on \(K_{\lambda}\) but what is the right target? We know it maps to \(V\) but should we consider another target like \(K_{\lambda}\)? Let’s look at the image of this map when it acts on a vector in \(K_{\lambda}\)</p>
<div>
$$
\begin{align*}
(T - \mu I_V)(x) &amp;= T(x) - \mu x \\                       
\end{align*}
$$
</div>
<p>We know that \(K_{\lambda}\) is \(T\)-invariant so \(T(x) \in K_{\lambda}\). What about \(\mu x\)? This is just a multiply of \(x\) and since \(K_{\lambda}\) is a subspace then we know that \(\mu x \in K_{\lambda}\). Therefore the addition of the two terms is also in \(K_{\lambda}\) since \(K_{\lambda}\) is a subspace. So this tells us that the target we want to consider is \(K_{\lambda}\).
<br />
<br />
So now we want to prove that this map \(T - \mu I_V: K_{\lambda} \rightarrow K_{\lambda}\) is one to one. One way to show this is to prove that the nullspace of this map is trivial. This means that the solution to</p>
<div>
$$
\begin{align*}
(T - \mu I_V)(x) &amp;= \bar{0}_V \\                       
\end{align*}
$$
</div>
<p>is the trivial solution where \(x = \bar{0}_V\). Suppose for the sake of contradiction that this isn’t true and \(x \neq \bar{0}_V\) but \((T - \mu I_V)(x) = \bar{0}_V\). We know that \(x \in K_{\lambda}\) so it must be killed by some power of the operator so let \(p\) be the smallest integer such that</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^p(x) &amp;= \bar{0}_V                     
\end{align*}
$$
</div>
<p>But if we take the power just below \(p\), then</p>
<div>
$$
\begin{align*}
(T - \lambda I_V)^{p-1}(x) = y \neq \bar{0}_V                    
\end{align*}
$$
</div>
<p>This implies that \(y\) is an eigenvector and \(y \in E_{\lambda}\). So</p>
<div>
$$
\begin{align*}
(T - \mu I_V)(y) &amp;= (T - \mu I_V)(T - \lambda I_V)^{p-1}(x) \\
                &amp;= (T - \lambda I_V)^{p-1}(T - \mu I_V)(x) \quad \text{ same as before, the sum is powers of $\lambda$ and $T$}\\ 
				&amp;= \bar{0}_V             
\end{align*}
$$
</div>
<p>So we’ve shown that \(y\) is an eigenvector for an eigenvalue \(\mu\). So \(y \neq \bar{0}_V\) and \(y \in E_{\mu}\). But we also see that \(y \in E_{\lambda}\). However \(\lambda \neq \mu\). So \(y \in E_{\mu} \cap E_{\lambda}\). So this is a contradiction. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding the Generalized Eigenvectors</b></h4>
<p>So to remind ourselves, the goal of this whole process is to find a basis consisting of generalized eigenvectors. The next theorem makes it practically easier to find them.
<br /></p>
<div class="purdiv">
Theorem 1.2
</div>
<div class="purbdiv">
If \(\lambda\) has algebraic multiplicity \(m\), then the generalized eigenspace
$$
\begin{align*}
K_{\lambda} = N((T - \lambda I_V)^m)          
\end{align*}
$$
</div>
<p><br />
This makes finding a basis for \(K_{\lambda}\) simple because it’s just a matter of finding the nullspace like we did before by putting the matrix in echelon form. What’s next? We want these generalized eigenvectors to span \(V\) since we want a basis. The following theorem confirms it.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.3
</div>
<div class="purbdiv">
Let \(\lambda_1, ..., \lambda_k\) be the distinct eigenvalues of \(T:V \rightarrow V\). For any \(x \in V\), there are \(v_j \in K_{\lambda_j}\) such that 
$$
\begin{align*}
x = v_1 + ... + v_k        
\end{align*}
$$
In other words, \(\text{span}(K_{\lambda_1} \cup ... \cup K_{\lambda_k}) = V\)
</div>
<p><br />
<!------------------------------------------------------------------------------------>
The next thing that we need is obviously knowing that these generalized eigenvectors are linearly independent. Once we get that we can construct the basis that we want.
<br /></p>
<div class="purdiv">
Theorem 1.4
</div>
<div class="purbdiv">
Let \(\beta_j\) be a basis for \(K_{\lambda_j}\). Then
<ol type="a">
	<li>\(\beta_i \cap \beta_j = \emptyset\) for \(i \neq j\) </li>
	<li>\(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis for \(V\) </li>
	<li>\(\dim(K_{\lambda_j}) = \) algebraic multiplicity of \(\lambda_j \) </li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
<br />
(a): Assume for the sake of contradiction that \(\beta_i \cap \beta_j \neq \emptyset\). Then there exists \(x \in \beta_i \cap \beta_j\). We know that \(\beta_i \cap \beta_j \subseteq K_{\lambda_i} \cap K_{\lambda_j}\) (Why?). Since \(i \neq j\), then \(\lambda_i \neq \lambda_j\). Therefore by Theorem 1.1(b)</p>
<div>
$$
\begin{align*}
&amp;\implies (T - \lambda_i I_V)\Big|_{K_{\lambda_i}} \quad \text{ is one-to-one}    \\
&amp;\implies  (T - \lambda_i I_V)(x) \neq \bar{0}_V \\ 
&amp;\implies  (T - \lambda_i I_V)^2(x) \neq \bar{0}_V \\ 
&amp;\implies  (T - \lambda_i I_V)^p(x) \neq \bar{0}_V \quad \text{ for any $p &gt; 0$}\\
&amp;\implies x \not\in K_{\lambda_i} 
\end{align*}
$$
</div>
<p>This is a contradiction.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we proved that If \(T: V \rightarrow V\) is self-adjoint, then there is an orthonormal basis \(\beta\) of \(V\) consisting of eigenvectors of \(T\). This therefore lead to the conclusion that \(T\) is diagonalizable. A Test For Diagonalizability We studied previously a few ways a linear operator can be tested for diagonalizability. From lecture 25, Theorem (5.8(a)) \(T\) is diagonalizable if and only if The characteristic polynomial splits over its field \(\mathbf{R}\) or \(\mathbf{C}\) For each eigenvalue of \(T\), its geometric multiplicity (\(\dim(E_{\lambda})\)) = algebraic multiplicity We mentioned last time too that if \(V\) is over \(\mathbf{C}\), the the characteristic polynomial always splits. Note also that you can have \((a)\) but not \((b)\). For example $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix} \end{align*} $$ splits but doesn’t satisfy \((b)\) Jordan Canonical Form Based on the previous observation. It turns out there is a nice form that we can put \(T\) into in order to achieve \((a)\) and \((b)\) Theorem (JCF) Suppose \(V\) is finite dimensional. If the characteristic polynomial of \(T: V \rightarrow V\) splits, then there is a basis \(\beta\) of \(V\) such that \([T]_{\beta}^{\beta}\) is in Jordan Canonical form. But what is a Jordan Canonical form? We first define a Jordan Block as follows Definition A Jordan Block is a square matrix of the form $$ \begin{align*} \begin{pmatrix} \lambda \end{pmatrix} \quad \text{ or } \quad \begin{pmatrix} \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda \end{pmatrix} \end{align*} $$ So they’re almost diagonal but not really. For example, for a \(2 \times 2\) and \(3 \times 3\) matrices, a Jordan block looks like $$ \begin{align*} \begin{pmatrix} \lambda &amp; 1 \\ 0 &amp; \lambda \end{pmatrix} , \begin{pmatrix} \lambda &amp; 1 &amp; 0 \\ 0 &amp; \lambda &amp; 1 \\ 0 &amp; 0 &amp; \lambda \end{pmatrix} \end{align*} $$ Using these Jordan blocks, we can now define what the Jordan Canonical form is Definition A square matrix \(A\) is in Jordan Canonical Form if $$ \begin{align*} A = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_{l} \end{pmatrix}, \ \text{ where $A_j$ is a Jordan block} \end{align*} $$ You can think of this matrix as more of a generalization of a diagonal matrix. Examples The following are examples of matrix in Jordan Canonical Form $$ \begin{align*} A = \begin{pmatrix} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}, B = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ Note here that the characteristic polynomial of both \(A\) and \(B\) is \((1-t)^3(2-t)^2\). Computing Powers of Matrices in JFC It turns out that we can write a formula for powers of matrices in JFC. It is not as easy as taking the power of a diagonal matrix but at least have a formula. Fact 1: $$ \begin{align*} A = \begin{pmatrix} \lambda &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda \end{pmatrix} \implies \begin{pmatrix} \lambda^k &amp; k\lambda^{k-1} &amp; \cdots &amp; \frac{k(k-1)...(k-n+2)}{n!}\lambda^{k-n+1} \\ 0 &amp; \ddots &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; k\lambda^{k-1} \\ 0 &amp; \cdots &amp; 0 &amp; \lambda^k \end{pmatrix} \end{align*} $$ Note here that the third entry in the first row for example is \(\frac{k(k-1)}{2!}\lambda^{k-1}\) Fact 2: If \(A\) is in Jordan Canonical form, then the power of the matrix is as follows $$ \begin{align*} A = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_l \end{pmatrix}^k \implies \begin{pmatrix} A_1^k &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2^k &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_l^k \end{pmatrix} \end{align*} $$ Proof of JCF Theorem So now we see that matrices in JCF are useful and we have enough motivation to prove the theorem above which again states that if the characteristic polynomial splits, then there is a basis such that \([T]^{\beta}_{\beta}\) is in JFC! To prove it, let \(\beta = \{v_1, ..., v_n\}\) of \(V\). We this basis to be such that $$ \begin{align*} [T]^{\beta}_{\beta} = \begin{pmatrix} [T(v)]_{\beta} &amp; \cdots &amp; [T(v_n)]_{\beta} \end{pmatrix} = \begin{pmatrix} A_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; A_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_l \end{pmatrix} \end{align*} $$ These vectors \(v_1,...,v_j\) are not necessarily eigenvectors. What are they? Let’s focus on \(A_1\) above and let \(A_1\) be of size \(n_1 \times n_1\). $$ \begin{align*} A_1 = \begin{pmatrix} \lambda_1 &amp; 1 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 1 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_l \end{pmatrix} \end{align*} $$ Since \(A_1\) has the form above (Jordan block), then we know at least for \(\lambda_1\), \(T(v_1) = \lambda_1 v_1\). What about \(T(v_2)\)? We see that column 2 has 1 in the first row and then \(\lambda_2\) in the second row. So \(T(v_2) = v_1 + \lambda_2 v_2\). Therefore, \(v_2\) is not an eigenvector but we can observe that $$ \begin{align*} T(v_2) &amp;= v_1 + \lambda_2 v_2 \\ T(v_2) - \lambda_2 v_2 &amp;= v_1 \\ (T - \lambda_1 I_V)(v_2) &amp;= v_1 \\ (T - \lambda_1 I_V)^2(v_2) &amp;= \bar{0}_V \quad \text{ (because $v_1$ is an eigenvector)} \end{align*} $$ What about the remaining vectors? $$ \begin{align*} T(v_3) &amp;= v_2 + \lambda_1 v_3 \\ (T - \lambda_1 I_V)(v_3) &amp;= v_2 \\ (T - \lambda_1 I_V)^3(v_3) &amp;= (T - \lambda_1 I_V)^2(v_2) \\ (T - \lambda_1 I_V)^3(v_2) &amp;= \bar{0}_V \end{align*} $$ So they’re not eigenvectors but they satisfy these equations. Based on this observation, we’re going to define the following Definition \(x \in V\) is a generalized eigenvector of \(T: V \rightarrow V\) corresponding to \(\lambda\) if $$ \begin{align*} (T - \lambda I_V)^p(x) = \bar{0}_V \end{align*} $$ for some integer \(p &gt; 0\) So now we know that the basis we want to build will consists of generalized eigenvectors. FACT 1: A Jordan Canonical Basis consists of generalized eigenvectors (with special properties). If we start with the equations above from the bottom to the top, then $$ \begin{align*} \{v_1,...v_{n_1-1},v_{n_1}\} = \{(T - \lambda_1 I_V)^{n_1 - 1}(v_{n_1}),...,(T - \lambda_1 I_V)^{2}(v_{n_1}), (T - \lambda_1 I_V)(v_{n_1}), v_{n_1}\} \end{align*} $$ So we can see here that the first \(n_1\) basis vectors coming from the \(A_1\) block are all obtained from the last vector and applying the maps over and over again. This leads to fact 2 below. FACT 2: A Jordan Canonical Basis is made of “cyclic pieces”. The basis is not only made of generalized eigenvectors but they also appear in this cyclic pattern. Here is an observation. Let \(x\) be a generalized eigenvector of \(T\) and Let \(p\) be the smallest positive integer such that \((T - \lambda I_V)^p(x) = \bar{0}_V\). Then $$ \begin{align*} y = (T - \lambda I_V)^{p-1}(x) \neq \bar{0}_V \end{align*} $$ But if we apply \((T - \lambda I_V)\) to both sides, then $$ \begin{align*} (T - \lambda I_V)(y) = (T - \lambda I_V)^{p}(x) = 0 \end{align*} $$ Then \(y\) is an eigenvector of \(T\). Definition Let \(\lambda\) be an eigenvalue of \(T: V \rightarrow V\). The generalized eigenspace of \(\lambda\) is $$ \begin{align*} K_{\lambda} = \{x \in V : (T - \lambda I_V)^p(x) = \bar{0}_V \text{ for some } p &gt; 0 \} \end{align*} $$ Theorem 1.1 \(K_{\lambda}\) is a \(T\)-invariant subspace of \(V\) containing \(E_{\lambda}\). For \(\mu = \lambda\), the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one. Proof (a) We need to show three things. \(K_{\lambda}\) contains \(E_{\lambda}\). This is clearly true by definition and \(E_{\lambda} \subseteq K_{\lambda}\). Next we need to show that \(K_{\lambda}\) is a subspace. This means we need to show that it contains the zero vector and it is closed under scalar multiplication and addition. \((T - \lambda I_V)(\bar{0}_V) = \bar{0}_V\) so \(\bar{0}_V \in K_{\lambda}\). Now consider \(x, y \in K_{\lambda}\) and \(c \in \mathbf{F}\), we need to show that \(x + cy \in K_{\lambda}\). Since \(x\) and \(y\) are in \(K_{\lambda}\), then $$ \begin{align*} (T - \lambda I_V)(x) &amp;= \bar{0}_V \text{ for } p &gt; 0 \\ (T - \lambda I_V)(y) &amp;= \bar{0}_V \text { for } q &gt; 0 \end{align*} $$ Therefore $$ \begin{align*} (T - \lambda I_V)^{p+q}(x + cy) &amp;= (T - \lambda I_V)^{p+q}(x) + c(T - \lambda I_V)^{p+q}(y) \\ &amp;= (T - \lambda I_V)^{p}(x)(T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y)(T - \lambda I_V)^{q}(y) \\ &amp;= (T - \lambda I_V)^{q}(x) + c(T - \lambda I_V)^{p}(y) \\ &amp; = \bar{0}_V \end{align*} $$ Next, we need to show that \(K_{\lambda}\) is \(T\)-invariant. This means that we want to show that \(T(K_{\lambda}) \subseteq K_{\lambda}\). Therefore, Let \(x \in K_{\lambda}\) so that \((T - \lambda I_V)^{p}(x) = \bar{0}_V\) by definition. We want to show that \(T(x) \in K_{\lambda}\) $$ \begin{align*} (T - \lambda I_V)^{p}(T(x)) &amp;= T(T - \lambda I_V)^{p}(x) \end{align*} $$ Taking \((T - \lambda I_V)\) to a power expands to some form of \(\lambda^k T^l\). But then we can reverse the order of the terms and write the following instead $$ \begin{align*} (T - \lambda I_V)^{p}(T(x)) &amp;= T(T - \lambda I_V)^{p}(x) \\ &amp;= (T - \lambda I_V)^{p}T(x) \\1 &amp;= \bar{0}_V \end{align*} $$ (b) Next, we want to prove that the restriction of \(T - \mu I_V\) to \(K_{\lambda}\) is one-to-one. So we want to think of this map \(T - \mu I_V\) as map on \(K_{\lambda}\) but what is the right target? We know it maps to \(V\) but should we consider another target like \(K_{\lambda}\)? Let’s look at the image of this map when it acts on a vector in \(K_{\lambda}\) $$ \begin{align*} (T - \mu I_V)(x) &amp;= T(x) - \mu x \\ \end{align*} $$ We know that \(K_{\lambda}\) is \(T\)-invariant so \(T(x) \in K_{\lambda}\). What about \(\mu x\)? This is just a multiply of \(x\) and since \(K_{\lambda}\) is a subspace then we know that \(\mu x \in K_{\lambda}\). Therefore the addition of the two terms is also in \(K_{\lambda}\) since \(K_{\lambda}\) is a subspace. So this tells us that the target we want to consider is \(K_{\lambda}\). So now we want to prove that this map \(T - \mu I_V: K_{\lambda} \rightarrow K_{\lambda}\) is one to one. One way to show this is to prove that the nullspace of this map is trivial. This means that the solution to $$ \begin{align*} (T - \mu I_V)(x) &amp;= \bar{0}_V \\ \end{align*} $$ is the trivial solution where \(x = \bar{0}_V\). Suppose for the sake of contradiction that this isn’t true and \(x \neq \bar{0}_V\) but \((T - \mu I_V)(x) = \bar{0}_V\). We know that \(x \in K_{\lambda}\) so it must be killed by some power of the operator so let \(p\) be the smallest integer such that $$ \begin{align*} (T - \lambda I_V)^p(x) &amp;= \bar{0}_V \end{align*} $$ But if we take the power just below \(p\), then $$ \begin{align*} (T - \lambda I_V)^{p-1}(x) = y \neq \bar{0}_V \end{align*} $$ This implies that \(y\) is an eigenvector and \(y \in E_{\lambda}\). So $$ \begin{align*} (T - \mu I_V)(y) &amp;= (T - \mu I_V)(T - \lambda I_V)^{p-1}(x) \\ &amp;= (T - \lambda I_V)^{p-1}(T - \mu I_V)(x) \quad \text{ same as before, the sum is powers of $\lambda$ and $T$}\\ &amp;= \bar{0}_V \end{align*} $$ So we’ve shown that \(y\) is an eigenvector for an eigenvalue \(\mu\). So \(y \neq \bar{0}_V\) and \(y \in E_{\mu}\). But we also see that \(y \in E_{\lambda}\). However \(\lambda \neq \mu\). So \(y \in E_{\mu} \cap E_{\lambda}\). So this is a contradiction. Finding the Generalized Eigenvectors So to remind ourselves, the goal of this whole process is to find a basis consisting of generalized eigenvectors. The next theorem makes it practically easier to find them. Theorem 1.2 If \(\lambda\) has algebraic multiplicity \(m\), then the generalized eigenspace $$ \begin{align*} K_{\lambda} = N((T - \lambda I_V)^m) \end{align*} $$ This makes finding a basis for \(K_{\lambda}\) simple because it’s just a matter of finding the nullspace like we did before by putting the matrix in echelon form. What’s next? We want these generalized eigenvectors to span \(V\) since we want a basis. The following theorem confirms it. Theorem 1.3 Let \(\lambda_1, ..., \lambda_k\) be the distinct eigenvalues of \(T:V \rightarrow V\). For any \(x \in V\), there are \(v_j \in K_{\lambda_j}\) such that $$ \begin{align*} x = v_1 + ... + v_k \end{align*} $$ In other words, \(\text{span}(K_{\lambda_1} \cup ... \cup K_{\lambda_k}) = V\) The next thing that we need is obviously knowing that these generalized eigenvectors are linearly independent. Once we get that we can construct the basis that we want. Theorem 1.4 Let \(\beta_j\) be a basis for \(K_{\lambda_j}\). Then \(\beta_i \cap \beta_j = \emptyset\) for \(i \neq j\) \(\beta = \beta_1 \cup ... \cup \beta_k\) is a basis for \(V\) \(\dim(K_{\lambda_j}) = \) algebraic multiplicity of \(\lambda_j \) Proof (a): Assume for the sake of contradiction that \(\beta_i \cap \beta_j \neq \emptyset\). Then there exists \(x \in \beta_i \cap \beta_j\). We know that \(\beta_i \cap \beta_j \subseteq K_{\lambda_i} \cap K_{\lambda_j}\) (Why?). Since \(i \neq j\), then \(\lambda_i \neq \lambda_j\). Therefore by Theorem 1.1(b) $$ \begin{align*} &amp;\implies (T - \lambda_i I_V)\Big|_{K_{\lambda_i}} \quad \text{ is one-to-one} \\ &amp;\implies (T - \lambda_i I_V)(x) \neq \bar{0}_V \\ &amp;\implies (T - \lambda_i I_V)^2(x) \neq \bar{0}_V \\ &amp;\implies (T - \lambda_i I_V)^p(x) \neq \bar{0}_V \quad \text{ for any $p &gt; 0$}\\ &amp;\implies x \not\in K_{\lambda_i} \end{align*} $$ This is a contradiction. References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 36: Isometries</title><link href="http://localhost:4000/jekyll/update/2024/09/14/lec36-isometries.html" rel="alternate" type="text/html" title="Lecture 36: Isometries" /><published>2024-09-14T01:01:36-07:00</published><updated>2024-09-14T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/14/lec36-isometries</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/14/lec36-isometries.html"><![CDATA[<p>We’ve studied special classes of linear maps over an inner product space to itself like normal maps and self-adjoint maps. Today we will study a new special class
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T:V \rightarrow V\) is an isometry if
$$
\begin{align*}
\langle T(x), T(y) \rangle = \langle x, y \rangle
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
For example, in \(\mathbf{R}^3\), this means that isometries preserve lengths and angles. <br />
When \(V\) is over \(\mathbf{C}\), an isometry is sometimes called unitary while if \(V\) is over \(\mathbf{R}\), \(T\) is called orthogonal.
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let \(T = L_A\) where</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix}
\cos\theta &amp; - \sin\theta \\
\sin\theta &amp; \cos\theta 
\end{pmatrix}
\end{align*}
$$
</div>
<p>Remember from the previous lecture, \(A\) was normal but not self-adjoint. We claim that \(A\) is an isometry. We can verify this by evaluating</p>
<div>
$$
\begin{align*}
L_A 
\begin{pmatrix}
x \\
y
\end{pmatrix}
&amp;=
\begin{pmatrix}
x\cos\theta - y\sin\theta \\
x\sin\theta + y\cos\theta 
\end{pmatrix}
\end{align*}
$$
</div>
<p>while,</p>
<div>
$$
\begin{align*}
\left\langle
L_A 
\begin{pmatrix}
x_1 \\
y_1
\end{pmatrix},
L_A 
\begin{pmatrix}
x_2 \\
y_2
\end{pmatrix}
\right\rangle
&amp;=
(x_1\cos\theta - y_1\sin\theta)(x_2\sin\theta + y_2\cos\theta) \\
&amp; =  x_1y_1 + x_2y_2
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Let</p>
<div>
$$
\begin{align*}
V &amp;= C^0([0,1]), \mathbf{C}) \\
  &amp;= \{ F(t) = f(t) + ig(t) \ | \ f, g \in C^0([0,1)]\} \\
\langle F, G \rangle &amp;= \int_0^1 F(t)\overline{G(t)} dt
\end{align*}
$$
</div>
<p>Suppose that \(H \in V\) such that \(|H(t)| = 1\). For real numbers, we have only two functions 1 and -1 but for complex numbers, we have many of these functions. For example \(H(t) = e^{ih(t)}\).
<br />
<br />
Now we’re going to use this function to define an isometry. Specifically,</p>
<div>
$$
\begin{align*}
T \ : \ &amp;V \rightarrow V 
   &amp;F \rightarrow FH
\end{align*}
$$
</div>
<p>This is an isometry. \(H\) is kind of rotating by some angle depending on \(t\). To verify this,</p>
<div>
$$
\begin{align*}
\langle T(F), T(G) \rangle &amp;= \int_0^1 (F(t)H(t))(\overline{G}(t)\overline{H}(t)) \ dt \\
                           &amp;= \int_0^1 F(t)\overline{G}(t)  \ dt \\
						   &amp;= \langle F, G \rangle
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Equivalent Conditions for Isometry</b></h4>
<p>What can we say about Isometries? What does being an Isometry imply?</p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
The following are equivalent
<ol type="a">
	<li>\(T: V \rightarrow V\) is an isometry</li>
	<li>\(TT^* = T^*T = I_V \) (an isometry is a normal map) </li>
	<li>If \(\beta\) is an orthonormal basis, then so is \(T(\beta)\). [In other words, \(T\) is an Isometry if it takes one orthonormal basis to another orthonormal basis].</li>
	<li>\(T(\beta)\) is orthonormal for some orthonormal \(\beta\). [This is a weaker statement. \(T\) needs to take one orthonormal basis to another at a minimum once (not every)].</li>
	<li>\(\Vert T(x) \Vert = \Vert x \Vert \ \forall x \in V \). So \(T\) preserves lengths.</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
Why is \((e)\) true? to see why, we need the following lemma
<br /></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
Suppose 
\(S:V \rightarrow V\) is self-adjoint. If \(\langle S(x), x \rangle = 0\) for all \(x \in V\), then \(S = T_{0_V}\) (the zero map)
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis consisting of eigenvectors of \(S\) (this is possible because \(S\) is self adjoint). So \(S(v_j) = \lambda_j v_j\) for \(j = 1,...,n\). Then</p>
<div>
$$
\begin{align*}
0 &amp;= \langle S(v_j), v_j \rangle \quad \text{(by the given assumption)} \\
0 &amp;= \langle \lambda_j v_j, v_j \rangle \\
0 &amp;= \lambda_j \langle v_j, v_j \rangle \\
0 &amp;= \lambda_j \Vert v_j \Vert^2 \\
\end{align*}
$$
</div>
<p>\(v_j\) is a basis vector so it’s not zero. Therefore, we must have \(\lambda_j = 0\) for \(j = 1,...,n\). This implies that \(S\) is the zero map because all the eigenvalues are zero but we have a basis of eigenvectors. So every vector \(x\) can be written as a linear combination of the basis vectors and when we apply \(S\), we get a linear combination of \(\lambda_j v_j\) where all the \(\lambda_j\)’s are zero so \(S\) must be the zero vector.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Proof of Theorem</b></h4>
<p>We’re ready to prove the theorem. 
<br />
<br />
\((a) \implies (b):\)<br />
We’re given that \(T\) is an isometry so we know by definition that \(T\) preserves the inner product so \(\langle T(x), T(y) \rangle = \langle x, y \rangle\). We want to show that \(TT^* = T^*T\). We first observe that \(T^*T\) is also self-adjoint (which means it’s diagonalizable). To see this, see that the adjoint of \((T^*T)\) is \((T^*T)^* = T^*(T^*)^* = T^*T\). Since \(T^*T\) is self-adjoint, then we can add to it any multiple of the identity map and the new map is still self adjoint (proof is in last lecture?). So \(T^*T - I_V\) is self adjoint. Now observe</p>
<div>
$$
\begin{align*}
\langle (T^*T - I_V)(x), x \rangle &amp;= \langle T^*T(x) - x, x \rangle \\
                                 &amp;= \langle T^*T(x), x \rangle - \langle x, x \rangle \\
								 &amp;= \langle T(x), T(x) \rangle - \langle x, x \rangle \quad \text{(by definition of adjoint)}\\
								 &amp;= 0 \text{(Since $T$ is an Isometry)}\\
                                     
\end{align*}
$$
</div>
<p>So we showed that \(T^*T - I_V\) is a self adjoint map where for any vector \(x\), we have \(\langle (T^*T - I_V)(x), x \rangle - 0\). So this satisfies the lemma we proved previously. This implies that \(T^*T - I_V\) is the zero map. So</p>
<div>
$$
\begin{align*}
T^*T - I_V = T_0 \\
T^*T = I_V \\     
T^* = T^{-1}          
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
<!------------------------------------------------------------------------------------>
\((b) \implies (c):\)<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij}\). We know \(T(\beta) = \{T(v_1),...,T(v_n)\}\). We need to show that \(T(\beta)\) is an orthonormal basis which means that \(\langle T(v_i), T(v_j) \rangle = \delta_{ij}\). Now observe</p>
<div>
$$
\begin{align*}
\langle (T(v_i), T(v_j) \rangle &amp;= \langle v_i, T^*(T(v_j)) \rangle \\
                                 &amp;=  \langle v_i, v_j \rangle \quad \text{(by (b), $T^*T = I_V$)} \\
								 &amp;= \delta_{ij}
                                     
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>\((c) \implies (d):\) This is trivial since \(c\) is a stronger statement
<br />
<br />
<!------------------------------------------------------------------------------------>
\((d) \implies (e):\) 
<br />
We need to show that if there is some orthonormal basis that can be taken to another orthonormal basis, then this means that \(T\) preserves all lengths.
<br />
<br />
Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis with \(T(\beta) = \{T(v_1),...,T(v_n)\} = \{w_1, ..., w_n\}\) also an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij} = \langle w_i, w_j \rangle\). All these assumptions are by (d). Now we know to prove that \(\Vert T(x) \Vert = \Vert x \Vert\) for any \(x \in V\). Observe that</p>
<div>
$$
\begin{align*}
x &amp;= \sum_j a_jv_j \\
T(x) &amp;= \sum_j a_j T(v_j) = \sum_j  a_j w_j                               
\end{align*}
$$
</div>
<p>So now we use this to evaluate</p>
<div>
$$
\begin{align*}
\Vert T(x) \Vert^2 &amp;= \langle T(x), T(x) \rangle \\
                  &amp;= \langle \sum_i  a_i w_i, \sum_j  a_j w_j \rangle \\ 
				  &amp;= \sum_{i,j} a_i \bar{a_j} \langle w_i, w_j \rangle  \\
				  &amp;= \sum_{i} |a_i|^2 = \langle x,x \rangle                    
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>\((e) \implies (a):\) <br />
We want to show that if we preserve lengths, then we actually preserve all inner products so \(\Vert T(x) \Vert = \Vert x \Vert \implies \langle T(x), T(x) \rangle = \langle x, x \rangle\). To show this, we can use the following trick</p>
<div>
$$
\begin{align*}
\Vert x + y \Vert^2 &amp;= \langle x+y, x+y \rangle \\
                    &amp;= \Vert x\Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2   \\
					&amp;= \Vert x\Vert^2 + \langle x, y \rangle + \overline{\langle x, y \rangle} + \Vert y \Vert^2
\end{align*}
$$
</div>
<p>What is \(\langle x, y \rangle + \overline{\langle x, y \rangle}\)? It’s 2 times the real part of \(\langle x, y \rangle + \overline{\langle x, y \rangle}\). I don’t understand why? [TODO]</p>
<div>
$$
\begin{align*}
2Re\langle x, y \rangle &amp;= \frac{1}{2}(\Vert x + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2) \\
Im\langle x, y \rangle &amp;= -\frac{1}{2}(\Vert ix + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2)
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Isometries Are Rare</b></h4>
<p>From research, we know that Isometries are actually rare. One fact is that \(T\) is an isometry of \(\mathbf{R}^2\) if and only if</p>
<div>
$$
\begin{align*}
T = L_A \text{ for } 
A = 
\begin{pmatrix}
\cos\theta &amp; - \sin\theta \\
\sin\theta &amp; \cos\theta 
\end{pmatrix}
\text{ or }
A = 
\begin{pmatrix}
\cos\theta &amp;  \sin\theta \\
\sin\theta &amp; -\cos\theta 
\end{pmatrix}
\end{align*}
$$
</div>
<p>So these are the only possibilities for isometries for \(\mathbf{R}^2\).</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’ve studied special classes of linear maps over an inner product space to itself like normal maps and self-adjoint maps. Today we will study a new special class Definition \(T:V \rightarrow V\) is an isometry if $$ \begin{align*} \langle T(x), T(y) \rangle = \langle x, y \rangle \end{align*} $$ For example, in \(\mathbf{R}^3\), this means that isometries preserve lengths and angles. When \(V\) is over \(\mathbf{C}\), an isometry is sometimes called unitary while if \(V\) is over \(\mathbf{R}\), \(T\) is called orthogonal. Example 1 Let \(T = L_A\) where $$ \begin{align*} A = \begin{pmatrix} \cos\theta &amp; - \sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix} \end{align*} $$ Remember from the previous lecture, \(A\) was normal but not self-adjoint. We claim that \(A\) is an isometry. We can verify this by evaluating $$ \begin{align*} L_A \begin{pmatrix} x \\ y \end{pmatrix} &amp;= \begin{pmatrix} x\cos\theta - y\sin\theta \\ x\sin\theta + y\cos\theta \end{pmatrix} \end{align*} $$ while, $$ \begin{align*} \left\langle L_A \begin{pmatrix} x_1 \\ y_1 \end{pmatrix}, L_A \begin{pmatrix} x_2 \\ y_2 \end{pmatrix} \right\rangle &amp;= (x_1\cos\theta - y_1\sin\theta)(x_2\sin\theta + y_2\cos\theta) \\ &amp; = x_1y_1 + x_2y_2 \end{align*} $$ Example 2 Let $$ \begin{align*} V &amp;= C^0([0,1]), \mathbf{C}) \\ &amp;= \{ F(t) = f(t) + ig(t) \ | \ f, g \in C^0([0,1)]\} \\ \langle F, G \rangle &amp;= \int_0^1 F(t)\overline{G(t)} dt \end{align*} $$ Suppose that \(H \in V\) such that \(|H(t)| = 1\). For real numbers, we have only two functions 1 and -1 but for complex numbers, we have many of these functions. For example \(H(t) = e^{ih(t)}\). Now we’re going to use this function to define an isometry. Specifically, $$ \begin{align*} T \ : \ &amp;V \rightarrow V &amp;F \rightarrow FH \end{align*} $$ This is an isometry. \(H\) is kind of rotating by some angle depending on \(t\). To verify this, $$ \begin{align*} \langle T(F), T(G) \rangle &amp;= \int_0^1 (F(t)H(t))(\overline{G}(t)\overline{H}(t)) \ dt \\ &amp;= \int_0^1 F(t)\overline{G}(t) \ dt \\ &amp;= \langle F, G \rangle \end{align*} $$ Equivalent Conditions for Isometry What can we say about Isometries? What does being an Isometry imply? Theorem The following are equivalent \(T: V \rightarrow V\) is an isometry \(TT^* = T^*T = I_V \) (an isometry is a normal map) If \(\beta\) is an orthonormal basis, then so is \(T(\beta)\). [In other words, \(T\) is an Isometry if it takes one orthonormal basis to another orthonormal basis]. \(T(\beta)\) is orthonormal for some orthonormal \(\beta\). [This is a weaker statement. \(T\) needs to take one orthonormal basis to another at a minimum once (not every)]. \(\Vert T(x) \Vert = \Vert x \Vert \ \forall x \in V \). So \(T\) preserves lengths. Why is \((e)\) true? to see why, we need the following lemma Lemma Suppose \(S:V \rightarrow V\) is self-adjoint. If \(\langle S(x), x \rangle = 0\) for all \(x \in V\), then \(S = T_{0_V}\) (the zero map) Proof Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis consisting of eigenvectors of \(S\) (this is possible because \(S\) is self adjoint). So \(S(v_j) = \lambda_j v_j\) for \(j = 1,...,n\). Then $$ \begin{align*} 0 &amp;= \langle S(v_j), v_j \rangle \quad \text{(by the given assumption)} \\ 0 &amp;= \langle \lambda_j v_j, v_j \rangle \\ 0 &amp;= \lambda_j \langle v_j, v_j \rangle \\ 0 &amp;= \lambda_j \Vert v_j \Vert^2 \\ \end{align*} $$ \(v_j\) is a basis vector so it’s not zero. Therefore, we must have \(\lambda_j = 0\) for \(j = 1,...,n\). This implies that \(S\) is the zero map because all the eigenvalues are zero but we have a basis of eigenvectors. So every vector \(x\) can be written as a linear combination of the basis vectors and when we apply \(S\), we get a linear combination of \(\lambda_j v_j\) where all the \(\lambda_j\)’s are zero so \(S\) must be the zero vector. Proof of Theorem We’re ready to prove the theorem. \((a) \implies (b):\) We’re given that \(T\) is an isometry so we know by definition that \(T\) preserves the inner product so \(\langle T(x), T(y) \rangle = \langle x, y \rangle\). We want to show that \(TT^* = T^*T\). We first observe that \(T^*T\) is also self-adjoint (which means it’s diagonalizable). To see this, see that the adjoint of \((T^*T)\) is \((T^*T)^* = T^*(T^*)^* = T^*T\). Since \(T^*T\) is self-adjoint, then we can add to it any multiple of the identity map and the new map is still self adjoint (proof is in last lecture?). So \(T^*T - I_V\) is self adjoint. Now observe $$ \begin{align*} \langle (T^*T - I_V)(x), x \rangle &amp;= \langle T^*T(x) - x, x \rangle \\ &amp;= \langle T^*T(x), x \rangle - \langle x, x \rangle \\ &amp;= \langle T(x), T(x) \rangle - \langle x, x \rangle \quad \text{(by definition of adjoint)}\\ &amp;= 0 \text{(Since $T$ is an Isometry)}\\ \end{align*} $$ So we showed that \(T^*T - I_V\) is a self adjoint map where for any vector \(x\), we have \(\langle (T^*T - I_V)(x), x \rangle - 0\). So this satisfies the lemma we proved previously. This implies that \(T^*T - I_V\) is the zero map. So $$ \begin{align*} T^*T - I_V = T_0 \\ T^*T = I_V \\ T^* = T^{-1} \end{align*} $$ And we are done. \((b) \implies (c):\) Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij}\). We know \(T(\beta) = \{T(v_1),...,T(v_n)\}\). We need to show that \(T(\beta)\) is an orthonormal basis which means that \(\langle T(v_i), T(v_j) \rangle = \delta_{ij}\). Now observe $$ \begin{align*} \langle (T(v_i), T(v_j) \rangle &amp;= \langle v_i, T^*(T(v_j)) \rangle \\ &amp;= \langle v_i, v_j \rangle \quad \text{(by (b), $T^*T = I_V$)} \\ &amp;= \delta_{ij} \end{align*} $$ \((c) \implies (d):\) This is trivial since \(c\) is a stronger statement \((d) \implies (e):\) We need to show that if there is some orthonormal basis that can be taken to another orthonormal basis, then this means that \(T\) preserves all lengths. Let \(\beta = \{v_1,...,v_n\}\) be an orthonormal basis with \(T(\beta) = \{T(v_1),...,T(v_n)\} = \{w_1, ..., w_n\}\) also an orthonormal basis where \(\langle v_i, v_j \rangle = \delta_{ij} = \langle w_i, w_j \rangle\). All these assumptions are by (d). Now we know to prove that \(\Vert T(x) \Vert = \Vert x \Vert\) for any \(x \in V\). Observe that $$ \begin{align*} x &amp;= \sum_j a_jv_j \\ T(x) &amp;= \sum_j a_j T(v_j) = \sum_j a_j w_j \end{align*} $$ So now we use this to evaluate $$ \begin{align*} \Vert T(x) \Vert^2 &amp;= \langle T(x), T(x) \rangle \\ &amp;= \langle \sum_i a_i w_i, \sum_j a_j w_j \rangle \\ &amp;= \sum_{i,j} a_i \bar{a_j} \langle w_i, w_j \rangle \\ &amp;= \sum_{i} |a_i|^2 = \langle x,x \rangle \end{align*} $$ \((e) \implies (a):\) We want to show that if we preserve lengths, then we actually preserve all inner products so \(\Vert T(x) \Vert = \Vert x \Vert \implies \langle T(x), T(x) \rangle = \langle x, x \rangle\). To show this, we can use the following trick $$ \begin{align*} \Vert x + y \Vert^2 &amp;= \langle x+y, x+y \rangle \\ &amp;= \Vert x\Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2 \\ &amp;= \Vert x\Vert^2 + \langle x, y \rangle + \overline{\langle x, y \rangle} + \Vert y \Vert^2 \end{align*} $$ What is \(\langle x, y \rangle + \overline{\langle x, y \rangle}\)? It’s 2 times the real part of \(\langle x, y \rangle + \overline{\langle x, y \rangle}\). I don’t understand why? [TODO] $$ \begin{align*} 2Re\langle x, y \rangle &amp;= \frac{1}{2}(\Vert x + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2) \\ Im\langle x, y \rangle &amp;= -\frac{1}{2}(\Vert ix + y\Vert^2 - \Vert x\Vert^2 - \Vert y\Vert^2) \end{align*} $$ Isometries Are Rare From research, we know that Isometries are actually rare. One fact is that \(T\) is an isometry of \(\mathbf{R}^2\) if and only if $$ \begin{align*} T = L_A \text{ for } A = \begin{pmatrix} \cos\theta &amp; - \sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix} \text{ or } A = \begin{pmatrix} \cos\theta &amp; \sin\theta \\ \sin\theta &amp; -\cos\theta \end{pmatrix} \end{align*} $$ So these are the only possibilities for isometries for \(\mathbf{R}^2\).]]></summary></entry><entry><title type="html">Section 6.3: Theorem 6.11</title><link href="http://localhost:4000/jekyll/update/2024/09/13/6.3-theorem-6.11.html" rel="alternate" type="text/html" title="Section 6.3: Theorem 6.11" /><published>2024-09-13T01:01:36-07:00</published><updated>2024-09-13T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/13/6.3-theorem-6.11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/13/6.3-theorem-6.11.html"><![CDATA[<div class="purdiv">
Theorem 6.11
</div>
<div class="purbdiv">
Let \(V\) be an inner product space, and let \(T\) and \(U\) be linear operators on \(V\) whose adjoint exist. Then
<ol type="a">
	<li>\(T + U\) has an adjoint, and \((T+U)^{*} = T^* + U^*\).</li>
	<li>\(cT\) has an adjoint, and \((cT)^* = \bar{c}T^*\) for any \(c \in \mathbf{F}\).</li>
	<li>\(TU\) has an adjoint, and \((TU)^* = U^* T^*\).</li>
	<li>\(T^*\) has an adjoint, and \(T^{**} = T\).</li>
	<li>\(I\) has an adjoint, and \(I^* = I\).</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
For (a)</p>
<div>
$$
\begin{align*}
\langle (T + U)x, y \rangle &amp;= \langle T(x) + U(x), y \rangle \\
                        &amp;= \langle T(x), y \rangle + \langle U(x), y \rangle \\
                        &amp;= \langle x, T^*(y) \rangle + \langle x, U^*(y) \rangle \\
                        &amp;= \langle x, T^*(y) + U^*(y) \rangle \\                        
                        &amp;= \langle x, (T^* + U^*)(y) \rangle \\                        				
\end{align*}
$$
</div>
<p>Therefore, \((T+U)^*\) exists and it equals to \(T^*+U^*\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 6.11 (Corollary)
</div>
<div class="purbdiv">
Let \(A\) and \(B\) be \(n \times n\) matrices. Then
<ol type="a">
	<li>\((A + B)^* = A^* + B^*\).</li>
	<li>\((cA)^* = \bar{c}A^*\) for all \(c \in \mathbf{F}\).</li>
	<li>\((AB)^* = B^*A^*\).</li>
	<li>\(A^{**} = A\).</li>
	<li>\(I^* = I\).</li>
</ol>
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
[TODO]
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.11 Let \(V\) be an inner product space, and let \(T\) and \(U\) be linear operators on \(V\) whose adjoint exist. Then \(T + U\) has an adjoint, and \((T+U)^{*} = T^* + U^*\). \(cT\) has an adjoint, and \((cT)^* = \bar{c}T^*\) for any \(c \in \mathbf{F}\). \(TU\) has an adjoint, and \((TU)^* = U^* T^*\). \(T^*\) has an adjoint, and \(T^{**} = T\). \(I\) has an adjoint, and \(I^* = I\). Proof: For (a) $$ \begin{align*} \langle (T + U)x, y \rangle &amp;= \langle T(x) + U(x), y \rangle \\ &amp;= \langle T(x), y \rangle + \langle U(x), y \rangle \\ &amp;= \langle x, T^*(y) \rangle + \langle x, U^*(y) \rangle \\ &amp;= \langle x, T^*(y) + U^*(y) \rangle \\ &amp;= \langle x, (T^* + U^*)(y) \rangle \\ \end{align*} $$ Therefore, \((T+U)^*\) exists and it equals to \(T^*+U^*\). Theorem 6.11 (Corollary) Let \(A\) and \(B\) be \(n \times n\) matrices. Then \((A + B)^* = A^* + B^*\). \((cA)^* = \bar{c}A^*\) for all \(c \in \mathbf{F}\). \((AB)^* = B^*A^*\). \(A^{**} = A\). \(I^* = I\). Proof: [TODO] References Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.1: Theorem 6.1</title><link href="http://localhost:4000/jekyll/update/2024/09/11/6.1-theorem-6.1.html" rel="alternate" type="text/html" title="Section 6.1: Theorem 6.1" /><published>2024-09-11T01:01:36-07:00</published><updated>2024-09-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/11/6.1-theorem-6.1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/11/6.1-theorem-6.1.html"><![CDATA[<!------------------------------------------------------------------------------------>
<div class="purdiv">
Theorem 6.1
</div>
<div class="purbdiv">
Let \(V\) be an inner product space. Then for \(x, y, z \in V\) and \(c \in \mathbf{F}\), the following statements are true.
<ol type="a">
	<li>\(\langle x, y+z \rangle = \langle x, y \rangle + \langle x, z \rangle\)</li>
	<li>\(\langle x, cy \rangle = \bar{c} \langle x , y \rangle\)</li>
	<li>\(\langle x, 0 \rangle = \langle 0,x \rangle = 0 \) </li>
	<li>\(\langle x, x \rangle = 0 \text{ if and only if } x = 0\)</li>
	<li>If \(\langle x, y \rangle = \langle x, z \rangle\) for all \(x \in V\), then \(y = z\)</li>
</ol>
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof:</b>
<br />
<br />
For (a)</p>
<div>
$$
\begin{align*}
\langle x, y+z \rangle &amp;= \overline{\langle y+z, x \rangle} \\
                       &amp;=  \overline{\langle y, x \rangle + \langle z, x \rangle} \\
					   &amp;=  \overline{\langle y, x \rangle} + \overline{\langle z, x \rangle} \\
					   &amp;=  \langle x, y \rangle + \langle x, z \rangle \\
\end{align*}
$$
</div>
<p>For (b)</p>
<div>
$$
\begin{align*}
\langle x, cy \rangle &amp;= \overline{\langle cy, x \rangle} \\
                       &amp;= \overline{c\langle y, x \rangle} \\
					   &amp;= \overline{c} \overline{\langle y, x \rangle} \\
					   &amp;= \overline{c} \langle x, y \rangle \\
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 6.1 Let \(V\) be an inner product space. Then for \(x, y, z \in V\) and \(c \in \mathbf{F}\), the following statements are true. \(\langle x, y+z \rangle = \langle x, y \rangle + \langle x, z \rangle\) \(\langle x, cy \rangle = \bar{c} \langle x , y \rangle\) \(\langle x, 0 \rangle = \langle 0,x \rangle = 0 \) \(\langle x, x \rangle = 0 \text{ if and only if } x = 0\) If \(\langle x, y \rangle = \langle x, z \rangle\) for all \(x \in V\), then \(y = z\) Proof: For (a) $$ \begin{align*} \langle x, y+z \rangle &amp;= \overline{\langle y+z, x \rangle} \\ &amp;= \overline{\langle y, x \rangle + \langle z, x \rangle} \\ &amp;= \overline{\langle y, x \rangle} + \overline{\langle z, x \rangle} \\ &amp;= \langle x, y \rangle + \langle x, z \rangle \\ \end{align*} $$ For (b) $$ \begin{align*} \langle x, cy \rangle &amp;= \overline{\langle cy, x \rangle} \\ &amp;= \overline{c\langle y, x \rangle} \\ &amp;= \overline{c} \overline{\langle y, x \rangle} \\ &amp;= \overline{c} \langle x, y \rangle \\ \end{align*} $$ References Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.3: Exercise 18</title><link href="http://localhost:4000/jekyll/update/2024/09/11/6.3-exercise-18.html" rel="alternate" type="text/html" title="Section 6.3: Exercise 18" /><published>2024-09-11T01:01:36-07:00</published><updated>2024-09-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/11/6.3-exercise-18</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/11/6.3-exercise-18.html"><![CDATA[<div class="ydiv">
Exercise 18
</div>
<div class="ybdiv">
Let \(A\) be an \(n \times n\). Prove that \(\det(A^*) = \overline{A}\).
</div>
<p><br />
Proof:
<br />
<br />
Let \(A\) be a matrix of size \(n \times n\). We first will prove that \(\det(\overline{A}) = \overline{\det{A}}\) by induction on \(n\).
<br />
<br />
Base Case: \(n = 1\). \(\det(\overline{A}) = \overline{a_{11}} = \overline{\det{(A)}}\).
<br />
<br />
Inductive Case: Assume \(\det(\overline{A}) = \overline{\det{A}}\) for \(n-1\). We will prove this for \(n\). Computing the determinant of \(\overline{A}\) by cofactor expansion along the first row and applying the inductive hypothesis</p>
<div>
	$$
	\begin{align*}
	\det(\overline{A}) &amp;= (-1)^{1+1}\overline{a_{11}}\det{\widetilde{\overline{A_{11}}}} 
	                    + (-1)^{1+2}\overline{a_{12}}\det{\widetilde{\overline{A_{12}}}} + ... + \\
						&amp;+ (-1)^{1+n}\overline{a_{1n}}\det{\widetilde{\overline{a_{1n}}}} \\
	&amp;= (-1)^{1+1}\overline{a_{11}}\overline{\det{\widetilde{A_{11}}}} +          (-1)^{1+2}\overline{a_{12}}\overline{\det{\widetilde{A_{11}}}} + ... + \\
		&amp;+ (-1)^{1+n}\overline{a_{1n}}\overline{\det{\widetilde{A_{11}}}} \\
	&amp;= \overline{\det{A}}
	\end{align*}
	$$
</div>
<p>So now we can apply this result to show that</p>
<div>
	$$
	\begin{align*}
	\det(A^*) &amp;= \det(\overline{A^t}) \\
	          &amp;= \overline{\det(A^t)} \\
			  &amp;= \overline{\det{A}} \quad \text{(we prove previously that $\det{A}=\det{A^t}$)}
	\end{align*}
	$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_6_2.html">Reference Solution</a></li>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 18 Let \(A\) be an \(n \times n\). Prove that \(\det(A^*) = \overline{A}\). Proof: Let \(A\) be a matrix of size \(n \times n\). We first will prove that \(\det(\overline{A}) = \overline{\det{A}}\) by induction on \(n\). Base Case: \(n = 1\). \(\det(\overline{A}) = \overline{a_{11}} = \overline{\det{(A)}}\). Inductive Case: Assume \(\det(\overline{A}) = \overline{\det{A}}\) for \(n-1\). We will prove this for \(n\). Computing the determinant of \(\overline{A}\) by cofactor expansion along the first row and applying the inductive hypothesis $$ \begin{align*} \det(\overline{A}) &amp;= (-1)^{1+1}\overline{a_{11}}\det{\widetilde{\overline{A_{11}}}} + (-1)^{1+2}\overline{a_{12}}\det{\widetilde{\overline{A_{12}}}} + ... + \\ &amp;+ (-1)^{1+n}\overline{a_{1n}}\det{\widetilde{\overline{a_{1n}}}} \\ &amp;= (-1)^{1+1}\overline{a_{11}}\overline{\det{\widetilde{A_{11}}}} + (-1)^{1+2}\overline{a_{12}}\overline{\det{\widetilde{A_{11}}}} + ... + \\ &amp;+ (-1)^{1+n}\overline{a_{1n}}\overline{\det{\widetilde{A_{11}}}} \\ &amp;= \overline{\det{A}} \end{align*} $$ So now we can apply this result to show that $$ \begin{align*} \det(A^*) &amp;= \det(\overline{A^t}) \\ &amp;= \overline{\det(A^t)} \\ &amp;= \overline{\det{A}} \quad \text{(we prove previously that $\det{A}=\det{A^t}$)} \end{align*} $$ References Reference Solution Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 6.2: Exercise 11</title><link href="http://localhost:4000/jekyll/update/2024/09/10/6.2-exercise-11.html" rel="alternate" type="text/html" title="Section 6.2: Exercise 11" /><published>2024-09-10T01:01:36-07:00</published><updated>2024-09-10T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/10/6.2-exercise-11</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/10/6.2-exercise-11.html"><![CDATA[<div class="ydiv">
Exercise 11
</div>
<div class="ybdiv">
Let \(A\) be an \(n \times n\) matrix with complex entries. Prove that \(AA^* = I\) if and only if the rows of \(A\) form an orthonormal basis for \(\mathbf{C}^n\).
</div>
<p><br />
Proof:
<br />
<br />
By definition we know that \(A^*\) is defined as \((A^*)_{ij} = \overline{A_{ji}}\) for all \(i\) and \(j\). Therefore the row \(i\), column \(j\) entry of \(AA^*\) is</p>
<div>
	$$
	\begin{align*}
	(AA^*)_{ij} &amp;= \sum_{k=1}^n A_{ik}(A^*)_{kj} \\
	            &amp;= \sum_{k=1}^n A_{ik}\overline{A_{jk}} \\
				&amp;= A_{i1}\overline{A_{j1}} + A_{i2}\overline{A_{j2}} + ... + A_{in}\overline{A_{jn}} \\
				&amp;= \langle A_i, A_j \rangle
	\end{align*}
	$$
</div>
<p>where \(A_i\) and \(A_j\) are the \(i\)th and \(j\)th rows of \(A\). What does having \(AA^{*} = I\) then mean? The \(i\)th row and \(j\)th column entry of \(I\) is \(I_{ij} = \delta_{ij}\) and this corresponds to the inner product of the rows \(A_i\) and \(A_j\).</p>
<div> 
$$
\begin{align*}
\begin{pmatrix}
\cdots &amp; \cdots &amp; \cdots \\
\cdots &amp; A_{i} &amp; \cdots \\
\cdots &amp; \cdots &amp; \cdots \\
\end{pmatrix}
\begin{pmatrix}
\cdots &amp; \cdots &amp; \cdots \\
\cdots &amp; A_{j} &amp; \cdots \\
\cdots &amp; \cdots &amp; \cdots \\
\end{pmatrix}
=
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 = \delta_{22} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{pmatrix}
\end{align*}
$$
</div>
<p>So \(AA^{*} = I\) is equivalent to having \(\langle A_i, A_j \rangle = \delta_{ij}\). But this implies that the rows of \(A\) form an orthonormal set of \(n\) vectors in \(C^{n}\) by definition of an orthonormal set. Since there are \(n\) of them and they are linearly independent, then they form a basis for \(C^{n}\). \(\ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li><a href="https://media.pearsoncmg.com/aw/aw_friedberg_linearalgebra_5e/solutions/sec_6_2.html">Reference Solution </a>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
&lt;/ul&gt;





















</li></ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 11 Let \(A\) be an \(n \times n\) matrix with complex entries. Prove that \(AA^* = I\) if and only if the rows of \(A\) form an orthonormal basis for \(\mathbf{C}^n\). Proof: By definition we know that \(A^*\) is defined as \((A^*)_{ij} = \overline{A_{ji}}\) for all \(i\) and \(j\). Therefore the row \(i\), column \(j\) entry of \(AA^*\) is $$ \begin{align*} (AA^*)_{ij} &amp;= \sum_{k=1}^n A_{ik}(A^*)_{kj} \\ &amp;= \sum_{k=1}^n A_{ik}\overline{A_{jk}} \\ &amp;= A_{i1}\overline{A_{j1}} + A_{i2}\overline{A_{j2}} + ... + A_{in}\overline{A_{jn}} \\ &amp;= \langle A_i, A_j \rangle \end{align*} $$ where \(A_i\) and \(A_j\) are the \(i\)th and \(j\)th rows of \(A\). What does having \(AA^{*} = I\) then mean? The \(i\)th row and \(j\)th column entry of \(I\) is \(I_{ij} = \delta_{ij}\) and this corresponds to the inner product of the rows \(A_i\) and \(A_j\). $$ \begin{align*} \begin{pmatrix} \cdots &amp; \cdots &amp; \cdots \\ \cdots &amp; A_{i} &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots \\ \end{pmatrix} \begin{pmatrix} \cdots &amp; \cdots &amp; \cdots \\ \cdots &amp; A_{j} &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots \\ \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 = \delta_{22} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 \end{pmatrix} \end{align*} $$ So \(AA^{*} = I\) is equivalent to having \(\langle A_i, A_j \rangle = \delta_{ij}\). But this implies that the rows of \(A\) form an orthonormal set of \(n\) vectors in \(C^{n}\) by definition of an orthonormal set. Since there are \(n\) of them and they are linearly independent, then they form a basis for \(C^{n}\). \(\ \blacksquare\). References Reference Solution Linear Algebra 5th Edition &lt;/ul&gt;]]></summary></entry></feed>