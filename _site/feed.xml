<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-25T20:41:02-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 32: Fourier Coefficients and Decomposition of a Vector</title><link href="http://localhost:4000/jekyll/update/2024/09/02/lec32-fourier-coefficients.html" rel="alternate" type="text/html" title="Lecture 32: Fourier Coefficients and Decomposition of a Vector" /><published>2024-09-02T01:01:36-07:00</published><updated>2024-09-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/02/lec32-fourier-coefficients</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/02/lec32-fourier-coefficients.html"><![CDATA[<p>The coefficients with respect to an orthonormal spanning set that we studied last time have a special name:
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(S \subseteq V\) be an (possibly infinite) orthonormal subset. The scalars \(\langle x, u\rangle\) for \(u \in S\) are called the Fourier coefficients of \(x\) with respect to \(S\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div> 
$$
\begin{align*}
V &amp;= C^0([-1,1]), \langle f,g \rangle = \int_{-1}^1 f(t)g(t)dt \\
S &amp;= \big\{\frac{1}{\sqrt{2}}\big\} \cup \{\sin n \pi t\}_{n=1}^{\infty} \cup \{\cos n \pi t \}_{n=1}^{\infty}
\end{align*}
$$
</div>
<p>We can easily check that \(S\) is an orthonormal set. Each of the two vectors is orthogonal to each other and each vector is of unit length. Find the Fourier coefficients of \(f = |t| \in C^0([-1,1])\)</p>
<div> 
$$
\begin{align*}
\langle f, \frac{1}{\sqrt{2}} \rangle &amp;= \int_{-1}^1 \frac{1}{\sqrt{2}} |t| dt = \frac{1}{\sqrt{2}} \\
\langle f, \sin n \pi t \rangle &amp;= \int_{-1}^1 \sin n \pi t |t| dt = 0 \\
\langle f, \cos n \pi t \rangle &amp;= \int_{-1}^1 \cos n \pi t |t| dt = 
\begin{cases} 0 \quad \ \ \quad \text{if $n$ even } \\ \frac{-4}{(n\pi)^2} \quad \text{if $n$ odd } \end{cases}
\end{align*}
$$
</div>
<p>When \(S = \{u_1,...,u_k\}\) is finite, then we can write</p>
<div> 
$$
\begin{align*}
x = \sum_{j=1}^{k} \langle x, u_j \rangle u_j
\end{align*}
$$
</div>
<p>But now in the infinite case, Is</p>
<div> 
$$
\begin{align*}
|t| = \frac{1}{2} = \sum_{n\text{ odd}} \frac{-4}{(n\pi)^2} \cos n \pi t
\end{align*}
$$
</div>
<p>Yes it is true but this is an infinite sum that converges to a number that is the absolute value of \(t\). This is basically the beginning of studying Fourier Analysis where any sufficiently nice function can be written as an infinite sum of sines and cosines. 
<br />
<br />
But one thing we know here is that \(S\) is not a basis for \(C^0[-1,1]\). \(|t| \neq\) finite set of elements of \(S\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Orthogonal Complement</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The orthogonal complement to \(S\) is 
$$
\begin{align*}
S^{\perp} = \{x \in V \ | \ \langle x, y \rangle = 0 \forall y \in S\}
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose \(S = \{(0,0,1)\} \subset \mathbf{R}^3\). The orthogonal complement to \(S\) is any element in \(\mathbf{R}^3\) such that when we take the inner product between any element in \(S\) and any element \(\mathbf{R}^3\), the product must be zero.</p>
<div>
$$
\begin{align*}
S^{\perp} &amp;= \{(x,y,z) \in V \ | \ \langle (x,y,z), (0,0,1) \rangle = 0\} \\
          &amp;= \{(x,y,z) \in V \ | \ z = 0\} \\
          &amp;= \{(x,y,0)\} \\
\end{align*}
$$
</div>
<p>Note here if we chose \(S' = span(S)\), then the orthogonal complement will be the same! 
<br />
Exercise 1: \(S^{\perp}\) is a subspace of \(V\).
<br />
Exercise 2: If \(S\) is a subspace, then \(S \cap S^{\perp} = \{0\}\).
<br />
<br />
How to use these orthogonal complements? We have the following theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
Suppose \(W\) is finite dimensional subspace of inner produce space \(V\). For each \(x \in V\) there is a unique \(w \in W\) and \(z \in W^{\perp}\) such that \(x = w + z\)
<br />
If \(\{u_1,...,u_k\}\) is an orthonormal basis for \(W\), then \(w = \langle x, u_1 \rangle + ... + \langle x, u_k \rangle u_k\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
(TODO: Add pic)
<br />
<b>Proof</b>
<br />
We can always find \(\{u_1, ..., u_k\}\) using Gram-Schmidt. So we know \(w\) and we’re given \(x\). All we need is to verify that \(z = x - w\) and show that \(w\) and \(z\) are unique. In other words, it suffices to show for</p>
<div>
$$
\begin{align*}
w = \sum_{j=1}^{k} \langle x, u_j \rangle u_j
\end{align*}
$$
</div>
<p>that \(z = x - w\) is in \(W^{\perp}\) plus the uniqueness of this decomposition.</p>
<div>
$$
\begin{align*}
&amp;z = x - w \in W^{\perp} \\
\Leftrightarrow \ &amp;\langle z, y \rangle = 0 \ \forall y \in W
\end{align*}
$$
</div>
<p>It isn’t easy to verify this for every vector \(y \in W\). But we do have a basis for \(W\) and any \(y\) can be written as a linear combination of the basis elements. So we can instead check this for every basis element</p>
<div>
$$
\begin{align*}
\Leftrightarrow \ &amp;\langle z, u_j \rangle = 0 \ \forall j =1,...,k
\end{align*}
$$
</div>
<p>So let’s check for every basis element that it’s orthogonal to \(x - w\).</p>
<div>
$$
\begin{align*}
\Leftrightarrow \ \langle z, u_j \rangle &amp;= \langle x-w, u_j \rangle \\ 
 &amp;= \langle x - \sum_{j=1}^k \langle x, u_j \rangle u_j  , u_j \rangle \\
 &amp;= \langle x, u_j \rangle - \langle \sum_{i=1}^k \langle x, u_i \rangle u_i  , u_j \rangle  \\
 &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \langle u_i  , u_j \rangle  \\
 &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \delta_{ij} \\
 &amp;= \langle x, u_j \rangle - \langle x, u_j \rangle \\
 &amp;= 0
\end{align*}
$$
</div>
<p>So now we see that \(x = w + z\) where \(w \in W\) and \(z \in W^{\perp}\). We still need to show that this decomposition is unique. Suppose it’s not and that \(x = w + z = \tilde{w} + \tilde{z}\). This implies</p>
<div>
$$
\begin{align*}
w - \tilde{w} = z - \tilde{z} = \bar{0}_V
\end{align*}
$$
</div>
<p>But \(w,\tilde{w} \in W\) and \(z, \tilde{z} \in Z\) and since \(W \cap W^{\perp} = \bar{0}_V\), then \(w = \tilde{w}\) and \(z = \tilde{z}\). \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Here \(w\) is called the orthogonal projection of \(x\) onto \(W\) and is denoted as
$$
\begin{align*}
proj_W(x) = w
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br /></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
\(w = proj_W(x)\) is the vector in \(W\) closest to \(x\) in the following sense
$$
\begin{align*}
\Vert x - w \Vert \leq \Vert x - y \Vert \ \forall y \in W
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
It’s easier to square things since we don’t want to deal with squareroots so</p>
<div>
$$
\begin{align*}
\Vert x - y \Vert^2 &amp;= \Vert (w + z) - y \Vert^2 \text{ (by theorem 1)} \\
                 &amp;= \Vert (w - y) + z \Vert^2 \text{ (w and y are both in $W$)} \\
                 &amp;= \langle (w - y) + z, (w - y) + z \rangle \\
                 &amp;= \langle (w - y), (w - y) \rangle 
				 + \langle (w - y), z \rangle 
				 + \langle z, z \rangle 
				 + \langle z, (w - y) \rangle \\
				 &amp;= \Vert w - y \Vert^2 + \langle (w - y), z \rangle  + \langle z, (w - y) \rangle + \Vert z \Vert^2 \\
				 &amp;= \Vert w - y \Vert^2 + \Vert z \Vert^2 \text{ ($z \in W^{\perp}$ and $w, y \in W$)} \\
				 &amp;\geq \Vert z \Vert^2 \\
				 &amp;=  \Vert x - w \Vert^2. \ \blacksquare
				
				 
\end{align*}
$$
</div>
<p>In general we can think of the projection onto \(W\) as a map.</p>
<div>
$$
\begin{align*}
proj_W: \ &amp; V \rightarrow W \\
		&amp;x \rightarrow proj_W(x)		 
\end{align*}
$$
</div>
<p>where the formula is</p>
<div>
$$
\begin{align*}
w = \sum_{j = 1}^{k} \langle x, u_j \rangle u_j		 
\end{align*}
$$
</div>
<p>This formula tells us that the projection is linear in \(x\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The coefficients with respect to an orthonormal spanning set that we studied last time have a special name: Definition Let \(S \subseteq V\) be an (possibly infinite) orthonormal subset. The scalars \(\langle x, u\rangle\) for \(u \in S\) are called the Fourier coefficients of \(x\) with respect to \(S\). Example 1 $$ \begin{align*} V &amp;= C^0([-1,1]), \langle f,g \rangle = \int_{-1}^1 f(t)g(t)dt \\ S &amp;= \big\{\frac{1}{\sqrt{2}}\big\} \cup \{\sin n \pi t\}_{n=1}^{\infty} \cup \{\cos n \pi t \}_{n=1}^{\infty} \end{align*} $$ We can easily check that \(S\) is an orthonormal set. Each of the two vectors is orthogonal to each other and each vector is of unit length. Find the Fourier coefficients of \(f = |t| \in C^0([-1,1])\) $$ \begin{align*} \langle f, \frac{1}{\sqrt{2}} \rangle &amp;= \int_{-1}^1 \frac{1}{\sqrt{2}} |t| dt = \frac{1}{\sqrt{2}} \\ \langle f, \sin n \pi t \rangle &amp;= \int_{-1}^1 \sin n \pi t |t| dt = 0 \\ \langle f, \cos n \pi t \rangle &amp;= \int_{-1}^1 \cos n \pi t |t| dt = \begin{cases} 0 \quad \ \ \quad \text{if $n$ even } \\ \frac{-4}{(n\pi)^2} \quad \text{if $n$ odd } \end{cases} \end{align*} $$ When \(S = \{u_1,...,u_k\}\) is finite, then we can write $$ \begin{align*} x = \sum_{j=1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ But now in the infinite case, Is $$ \begin{align*} |t| = \frac{1}{2} = \sum_{n\text{ odd}} \frac{-4}{(n\pi)^2} \cos n \pi t \end{align*} $$ Yes it is true but this is an infinite sum that converges to a number that is the absolute value of \(t\). This is basically the beginning of studying Fourier Analysis where any sufficiently nice function can be written as an infinite sum of sines and cosines. But one thing we know here is that \(S\) is not a basis for \(C^0[-1,1]\). \(|t| \neq\) finite set of elements of \(S\). Orthogonal Complement Definition The orthogonal complement to \(S\) is $$ \begin{align*} S^{\perp} = \{x \in V \ | \ \langle x, y \rangle = 0 \forall y \in S\} \end{align*} $$ Example 2 Suppose \(S = \{(0,0,1)\} \subset \mathbf{R}^3\). The orthogonal complement to \(S\) is any element in \(\mathbf{R}^3\) such that when we take the inner product between any element in \(S\) and any element \(\mathbf{R}^3\), the product must be zero. $$ \begin{align*} S^{\perp} &amp;= \{(x,y,z) \in V \ | \ \langle (x,y,z), (0,0,1) \rangle = 0\} \\ &amp;= \{(x,y,z) \in V \ | \ z = 0\} \\ &amp;= \{(x,y,0)\} \\ \end{align*} $$ Note here if we chose \(S' = span(S)\), then the orthogonal complement will be the same! Exercise 1: \(S^{\perp}\) is a subspace of \(V\). Exercise 2: If \(S\) is a subspace, then \(S \cap S^{\perp} = \{0\}\). How to use these orthogonal complements? We have the following theorem Theorem 1 Suppose \(W\) is finite dimensional subspace of inner produce space \(V\). For each \(x \in V\) there is a unique \(w \in W\) and \(z \in W^{\perp}\) such that \(x = w + z\) If \(\{u_1,...,u_k\}\) is an orthonormal basis for \(W\), then \(w = \langle x, u_1 \rangle + ... + \langle x, u_k \rangle u_k\) (TODO: Add pic) Proof We can always find \(\{u_1, ..., u_k\}\) using Gram-Schmidt. So we know \(w\) and we’re given \(x\). All we need is to verify that \(z = x - w\) and show that \(w\) and \(z\) are unique. In other words, it suffices to show for $$ \begin{align*} w = \sum_{j=1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ that \(z = x - w\) is in \(W^{\perp}\) plus the uniqueness of this decomposition. $$ \begin{align*} &amp;z = x - w \in W^{\perp} \\ \Leftrightarrow \ &amp;\langle z, y \rangle = 0 \ \forall y \in W \end{align*} $$ It isn’t easy to verify this for every vector \(y \in W\). But we do have a basis for \(W\) and any \(y\) can be written as a linear combination of the basis elements. So we can instead check this for every basis element $$ \begin{align*} \Leftrightarrow \ &amp;\langle z, u_j \rangle = 0 \ \forall j =1,...,k \end{align*} $$ So let’s check for every basis element that it’s orthogonal to \(x - w\). $$ \begin{align*} \Leftrightarrow \ \langle z, u_j \rangle &amp;= \langle x-w, u_j \rangle \\ &amp;= \langle x - \sum_{j=1}^k \langle x, u_j \rangle u_j , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \langle \sum_{i=1}^k \langle x, u_i \rangle u_i , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \langle u_i , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \delta_{ij} \\ &amp;= \langle x, u_j \rangle - \langle x, u_j \rangle \\ &amp;= 0 \end{align*} $$ So now we see that \(x = w + z\) where \(w \in W\) and \(z \in W^{\perp}\). We still need to show that this decomposition is unique. Suppose it’s not and that \(x = w + z = \tilde{w} + \tilde{z}\). This implies $$ \begin{align*} w - \tilde{w} = z - \tilde{z} = \bar{0}_V \end{align*} $$ But \(w,\tilde{w} \in W\) and \(z, \tilde{z} \in Z\) and since \(W \cap W^{\perp} = \bar{0}_V\), then \(w = \tilde{w}\) and \(z = \tilde{z}\). \(\ \blacksquare\) Definition Here \(w\) is called the orthogonal projection of \(x\) onto \(W\) and is denoted as $$ \begin{align*} proj_W(x) = w \end{align*} $$ Theorem 2 \(w = proj_W(x)\) is the vector in \(W\) closest to \(x\) in the following sense $$ \begin{align*} \Vert x - w \Vert \leq \Vert x - y \Vert \ \forall y \in W \end{align*} $$ Proof It’s easier to square things since we don’t want to deal with squareroots so $$ \begin{align*} \Vert x - y \Vert^2 &amp;= \Vert (w + z) - y \Vert^2 \text{ (by theorem 1)} \\ &amp;= \Vert (w - y) + z \Vert^2 \text{ (w and y are both in $W$)} \\ &amp;= \langle (w - y) + z, (w - y) + z \rangle \\ &amp;= \langle (w - y), (w - y) \rangle + \langle (w - y), z \rangle + \langle z, z \rangle + \langle z, (w - y) \rangle \\ &amp;= \Vert w - y \Vert^2 + \langle (w - y), z \rangle + \langle z, (w - y) \rangle + \Vert z \Vert^2 \\ &amp;= \Vert w - y \Vert^2 + \Vert z \Vert^2 \text{ ($z \in W^{\perp}$ and $w, y \in W$)} \\ &amp;\geq \Vert z \Vert^2 \\ &amp;= \Vert x - w \Vert^2. \ \blacksquare \end{align*} $$ In general we can think of the projection onto \(W\) as a map. $$ \begin{align*} proj_W: \ &amp; V \rightarrow W \\ &amp;x \rightarrow proj_W(x) \end{align*} $$ where the formula is $$ \begin{align*} w = \sum_{j = 1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ This formula tells us that the projection is linear in \(x\). References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 31: Orthonormal and Orthogonal Sets</title><link href="http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets.html" rel="alternate" type="text/html" title="Lecture 31: Orthonormal and Orthogonal Sets" /><published>2024-09-01T01:01:36-07:00</published><updated>2024-09-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
<ul>
	<li>\(x, y \in V\) are orthogonal (perpendicular) if \(\langle x, y \rangle = 0\)</li>
	<li>\(S \in V\) is orthogonal if for any \(x, y\) distinct in \(S\), \(\langle x, y \rangle = 0\)</li>
	<li>\(S \subset V\) is orthonormal if it is orthogonal and for each \(x \in S, \Vert x \Vert = 1\)</li>
</ul>
</div>
<p><br />
Remark: \(x \in V\) is a unit vector if \(\Vert x \Vert = 1\). If \(x \neq \bar{0}_V\), then \(\frac{x}{\Vert x \Vert}\) is a unit vector. This process is normalization.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div> 
$$
\begin{align*}
V = C^0([0,1]), \langle f,g \rangle = \int_0^1 f(t)g(t)dt
\end{align*}
$$
</div>
<p>Show \(\sin(2\pi t)\) and \(\cos(2\pi t)\) are orthogonal. To do this, we’ll compute their inner product.</p>
<div> 
$$
\begin{align*}
\langle \sin(2\pi t), \cos(2\pi t) \rangle &amp;= \int_0^1 \sin(2\pi t) \cos(2\pi t) \\
                                           &amp;= \frac{\sin^2 2 \pi t}{4\pi} \Big|^1_0 = 0
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The standard basis \(\beta = \{e_1, e_2, ..., e_n \}\) of \(\mathbf{R}^n\) is an orthonormal subset. For every distinct two vectors in \(\beta\), their inner product is zero. Moreover, for any vector \(e_i \in \beta\), \(\langle e_i, e_i \rangle = 1\). In general,</p>
<div> 
$$
\begin{align*}
\langle e_i, e_j \rangle &amp;= \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j\end{cases}\\
\end{align*}
$$
</div>
<p>In general, if \(\{v_1,...,v_k\}\) is orthonormal, then \(\langle v_i, v_j \rangle = \delta_{ij}\). The next theorem tells us why orthonormal sets are useful.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(S = \{v_1,...,v_k\} \subseteq V\) is orthonormal. If \(y \in span(S)\), then
$$
\begin{align*}
y = \sum_{j=1}^k \langle y, v_j \rangle v_j
\end{align*}
$$
</div>
<p><br />
So we don’t need to solve a system of linear equations to figure out the coefficients if \(y\) written with respect to \(S\). 
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>:
<br />
<br />
We know that \(y \in span(S)\) Therefore, we can write \(y\) as</p>
<div>
$$
\begin{align*}
y = \sum_{j=1}^k a_j v_j
\end{align*}
$$
</div>
<p>for scalars \(a_1,...,a_k\). We also know that the \(S\) is an orthonormal set. Taking the inner product of both sides with respect to one vector from \(S\), we see that</p>
<div>
$$
\begin{align*}
\langle y, v_i \rangle &amp;= \langle \sum_{j=1}^k a_j v_j, v_i \rangle \text{ for all $i = 1,...,k$} \\
 &amp;= \sum_{j=1}^k a_j \langle v_j, v_i \rangle \text{ (the inner product is linear in its first argument)} \\
 &amp;= \sum_{j=1}^k a_j \delta_{ij} \\
 &amp;= a_i \text{ ($\delta_{ij}$ is 1 only for $i = j$)}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
y = \sum_{j=1}^k \langle y, v_j \rangle v_j. \quad \blacksquare
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
What about orthogonal subsets, can we say anything about them? Yes!
<br />
<br /></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\). If \(y \in span(S)\), then
$$
\begin{align*}
y = \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>:
<br />
<br />
If \(S = \{v_1,...,v_k\}\) is orthogonal. We can turn this set into an orthonormal set by normalizing the set so</p>
<div>
$$
\begin{align*}
\{\frac{v_1}{\Vert v_1 \Vert^2},...,\frac{v_k}{\Vert v_k \Vert^2}\}
\end{align*}
$$
</div>
<p>By the previous theorem, then</p>
<div>
$$
\begin{align*}
y &amp;= \sum_{j=1}^k \langle y, \frac{v_j}{\Vert v_j \Vert }\rangle \frac{v_j}{\Vert v_j \Vert}  \\
  &amp;= \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j. \quad \blacksquare
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------------->
<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\), then \(S\) is linearly independent. 
</div>
<p><br />
<br />
<b>Proof:</b>
To see that it’s linearly independent, then the only solution to the equation</p>
<div>
	$$
	\begin{align*}
	a_1v_1 + ... + a_kv_k = \bar{0}_V
	\end{align*}
	$$
</div>
<p>is the trivial solution. But by corollary 1, if \(\bar{0}_V \in span(S)\), then we know the coefficients when it’s written relative to (S). Specifically the \(j\)’s coefficient is</p>
<div>
	$$
	\begin{align*}
	a_j = \frac{\langle \bar{0}_V, v_j \rangle}{\Vert v_j \Vert^2} = 0. \quad \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<!---------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) is finite dimensional inner product space, then it has an orthonormal basis.
</div>
<p><br />
<br />
This will follow from the procedure we will study next …
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Gram-Schmidt Process</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(\{w_1,...,w_k\}\) be a linearly independent subset of \(V\). Set 
	$$
	\begin{align*}
	u_1 &amp;= \frac{w_1}{\Vert w_1 \Vert} \\
	u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle}{\Vert w_2 - \langle w_2, u_1 \rangle \Vert} \\
	&amp;\vdots \\
	u_k &amp;= \frac{w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j}{\Vert w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j \Vert}
	\end{align*}
	$$
Then \(\{u_1,...,u_k\}\) is orthonormal and has same span as \(\{w_1,...,w_k\}\).
</div>
<p><br />
<b>Proof</b>
<br />
<br />
The basic idea of the proof (by induction) is that given \(\{w_1,...,w_k\}\) is linearly independent and given than \(\{u_1, u_2\}\) is orthonormal with the same span as \(\{w_1, w_2\}\), we want \(u_3\) such that \(\{u_1, u_2, u_3\}\) is orthonormal and \(span(\{u_1, u_2, u_3\}) = span(\{w_1,w_2,w_3\})\)
<br />
<br />
To show that the two spans are the same, it suffices to show that \(w_3 \in span(\{u_1, u_2, u_3\})\). In this case, we know by the theorem above what the coefficients should be:</p>
<div>
	$$
	\begin{align*}
	w_3 = \langle w_3, u_1 \rangle u_1 + \langle w_3, u_2 \rangle u_2 + \langle w_3, u_3 \rangle u_3
	\end{align*}
	$$
</div>
<p>Therefore, we can use the above equation to solve for \(u_3\). But we don’t want to divide by \(\langle w_3, u_3 \rangle\) since we’re trying to solve for \(u_3\) so we can think of this term as a constant we’re multiplying with:</p>
<div>
	$$
	\begin{align*}
	u_3 = c( w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2  )
	\end{align*}
	$$
</div>
<p>But we know we want \(u_3\) to be a unit vector. So we can just divide by the length of it.</p>
<div>
	$$
	\begin{align*}
	u_3 = \frac{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2}{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 \Vert}
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Find an orthonormal basis for \(P_2 \in C^0([-1, 1])\). equipped with</p>
<div>
	$$
	\begin{align*}
	\langle f, g \rangle = \int_{-1}^{1} f(x)g(x) dx
	\end{align*}
	$$
</div>
<p>Choose $${1, x, x^2}. Apply Gram-Schmidt. So</p>
<div>
	$$
	\begin{align*}
	\Vert 1\Vert^2 &amp;= \langle 1, 1 \rangle = \int_{-1}^{1} 1dx = 2 \\
	u_1 &amp;= \frac{1}{\Vert 1\Vert} = \frac{1}{\sqrt{2}}
	\end{align*}
	$$
</div>
<p>Next, we’ll find \(u_2\)</p>
<div>
	$$
	\begin{align*}
	\Vert w_2 \Vert^2 &amp;= \langle x, x \rangle = \int_{-1}^{1} x^2 dx = \frac{x^2}{3} \Big|^1_{-1} = \frac{2}{3} \\
	\langle w_2, u_1 \rangle &amp;=  \int_{-1}^{1} x\frac{1}{\sqrt{2}} dx = 0 \\
	u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle u_1}{\Vert w_2 - \langle w_2, u_1 \rangle u_1 \Vert} = \frac{\sqrt{3}}{\sqrt{2}}x
	\end{align*}
	$$
</div>
<p>And finally \(u_3\)</p>
<div>
	$$
	\begin{align*}
	u_3 &amp;= \frac{w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1}{\Vert w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1 \Vert}
	\\
	&amp;= \sqrt{\frac{5}{8}}(3x^2 - 1)
	\end{align*}
	$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(x, y \in V\) are orthogonal (perpendicular) if \(\langle x, y \rangle = 0\) \(S \in V\) is orthogonal if for any \(x, y\) distinct in \(S\), \(\langle x, y \rangle = 0\) \(S \subset V\) is orthonormal if it is orthogonal and for each \(x \in S, \Vert x \Vert = 1\) Remark: \(x \in V\) is a unit vector if \(\Vert x \Vert = 1\). If \(x \neq \bar{0}_V\), then \(\frac{x}{\Vert x \Vert}\) is a unit vector. This process is normalization. Example 1 $$ \begin{align*} V = C^0([0,1]), \langle f,g \rangle = \int_0^1 f(t)g(t)dt \end{align*} $$ Show \(\sin(2\pi t)\) and \(\cos(2\pi t)\) are orthogonal. To do this, we’ll compute their inner product. $$ \begin{align*} \langle \sin(2\pi t), \cos(2\pi t) \rangle &amp;= \int_0^1 \sin(2\pi t) \cos(2\pi t) \\ &amp;= \frac{\sin^2 2 \pi t}{4\pi} \Big|^1_0 = 0 \end{align*} $$ Example 2 The standard basis \(\beta = \{e_1, e_2, ..., e_n \}\) of \(\mathbf{R}^n\) is an orthonormal subset. For every distinct two vectors in \(\beta\), their inner product is zero. Moreover, for any vector \(e_i \in \beta\), \(\langle e_i, e_i \rangle = 1\). In general, $$ \begin{align*} \langle e_i, e_j \rangle &amp;= \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j\end{cases}\\ \end{align*} $$ In general, if \(\{v_1,...,v_k\}\) is orthonormal, then \(\langle v_i, v_j \rangle = \delta_{ij}\). The next theorem tells us why orthonormal sets are useful. Theorem Suppose \(S = \{v_1,...,v_k\} \subseteq V\) is orthonormal. If \(y \in span(S)\), then $$ \begin{align*} y = \sum_{j=1}^k \langle y, v_j \rangle v_j \end{align*} $$ So we don’t need to solve a system of linear equations to figure out the coefficients if \(y\) written with respect to \(S\). Proof: We know that \(y \in span(S)\) Therefore, we can write \(y\) as $$ \begin{align*} y = \sum_{j=1}^k a_j v_j \end{align*} $$ for scalars \(a_1,...,a_k\). We also know that the \(S\) is an orthonormal set. Taking the inner product of both sides with respect to one vector from \(S\), we see that $$ \begin{align*} \langle y, v_i \rangle &amp;= \langle \sum_{j=1}^k a_j v_j, v_i \rangle \text{ for all $i = 1,...,k$} \\ &amp;= \sum_{j=1}^k a_j \langle v_j, v_i \rangle \text{ (the inner product is linear in its first argument)} \\ &amp;= \sum_{j=1}^k a_j \delta_{ij} \\ &amp;= a_i \text{ ($\delta_{ij}$ is 1 only for $i = j$)} \end{align*} $$ Therefore, $$ \begin{align*} y = \sum_{j=1}^k \langle y, v_j \rangle v_j. \quad \blacksquare \end{align*} $$ What about orthogonal subsets, can we say anything about them? Yes! Corollary 1 If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\). If \(y \in span(S)\), then $$ \begin{align*} y = \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j \end{align*} $$ Proof: If \(S = \{v_1,...,v_k\}\) is orthogonal. We can turn this set into an orthonormal set by normalizing the set so $$ \begin{align*} \{\frac{v_1}{\Vert v_1 \Vert^2},...,\frac{v_k}{\Vert v_k \Vert^2}\} \end{align*} $$ By the previous theorem, then $$ \begin{align*} y &amp;= \sum_{j=1}^k \langle y, \frac{v_j}{\Vert v_j \Vert }\rangle \frac{v_j}{\Vert v_j \Vert} \\ &amp;= \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j. \quad \blacksquare \end{align*} $$ Corollary 2 If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\), then \(S\) is linearly independent. Proof: To see that it’s linearly independent, then the only solution to the equation $$ \begin{align*} a_1v_1 + ... + a_kv_k = \bar{0}_V \end{align*} $$ is the trivial solution. But by corollary 1, if \(\bar{0}_V \in span(S)\), then we know the coefficients when it’s written relative to (S). Specifically the \(j\)’s coefficient is $$ \begin{align*} a_j = \frac{\langle \bar{0}_V, v_j \rangle}{\Vert v_j \Vert^2} = 0. \quad \blacksquare \end{align*} $$ Theorem If \(V\) is finite dimensional inner product space, then it has an orthonormal basis. This will follow from the procedure we will study next … Gram-Schmidt Process Theorem Let \(\{w_1,...,w_k\}\) be a linearly independent subset of \(V\). Set $$ \begin{align*} u_1 &amp;= \frac{w_1}{\Vert w_1 \Vert} \\ u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle}{\Vert w_2 - \langle w_2, u_1 \rangle \Vert} \\ &amp;\vdots \\ u_k &amp;= \frac{w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j}{\Vert w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j \Vert} \end{align*} $$ Then \(\{u_1,...,u_k\}\) is orthonormal and has same span as \(\{w_1,...,w_k\}\). Proof The basic idea of the proof (by induction) is that given \(\{w_1,...,w_k\}\) is linearly independent and given than \(\{u_1, u_2\}\) is orthonormal with the same span as \(\{w_1, w_2\}\), we want \(u_3\) such that \(\{u_1, u_2, u_3\}\) is orthonormal and \(span(\{u_1, u_2, u_3\}) = span(\{w_1,w_2,w_3\})\) To show that the two spans are the same, it suffices to show that \(w_3 \in span(\{u_1, u_2, u_3\})\). In this case, we know by the theorem above what the coefficients should be: $$ \begin{align*} w_3 = \langle w_3, u_1 \rangle u_1 + \langle w_3, u_2 \rangle u_2 + \langle w_3, u_3 \rangle u_3 \end{align*} $$ Therefore, we can use the above equation to solve for \(u_3\). But we don’t want to divide by \(\langle w_3, u_3 \rangle\) since we’re trying to solve for \(u_3\) so we can think of this term as a constant we’re multiplying with: $$ \begin{align*} u_3 = c( w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 ) \end{align*} $$ But we know we want \(u_3\) to be a unit vector. So we can just divide by the length of it. $$ \begin{align*} u_3 = \frac{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2}{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 \Vert} \end{align*} $$ Example 3 Find an orthonormal basis for \(P_2 \in C^0([-1, 1])\). equipped with $$ \begin{align*} \langle f, g \rangle = \int_{-1}^{1} f(x)g(x) dx \end{align*} $$ Choose $${1, x, x^2}. Apply Gram-Schmidt. So $$ \begin{align*} \Vert 1\Vert^2 &amp;= \langle 1, 1 \rangle = \int_{-1}^{1} 1dx = 2 \\ u_1 &amp;= \frac{1}{\Vert 1\Vert} = \frac{1}{\sqrt{2}} \end{align*} $$ Next, we’ll find \(u_2\) $$ \begin{align*} \Vert w_2 \Vert^2 &amp;= \langle x, x \rangle = \int_{-1}^{1} x^2 dx = \frac{x^2}{3} \Big|^1_{-1} = \frac{2}{3} \\ \langle w_2, u_1 \rangle &amp;= \int_{-1}^{1} x\frac{1}{\sqrt{2}} dx = 0 \\ u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle u_1}{\Vert w_2 - \langle w_2, u_1 \rangle u_1 \Vert} = \frac{\sqrt{3}}{\sqrt{2}}x \end{align*} $$ And finally \(u_3\) $$ \begin{align*} u_3 &amp;= \frac{w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1}{\Vert w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1 \Vert} \\ &amp;= \sqrt{\frac{5}{8}}(3x^2 - 1) \end{align*} $$ References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Section 4.3: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21.html" rel="alternate" type="text/html" title="Section 4.3: Exercise 21" /><published>2024-08-31T01:01:36-07:00</published><updated>2024-08-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21.html"><![CDATA[<div class="ydiv">
4.3: Exercise 21
</div>
<div class="ybdiv">
Prove that if \(M \in M_{n \times n}(\mathbf{F})\) can be written in the form
	$$
	\begin{align*}
	M = \begin{pmatrix}
	A &amp; B \\
	O &amp; C
	\end{pmatrix}
	\end{align*}
	$$
where \(A\) and \(C\) are square square matrices. Prove that \(\det(M) = \det(A)\det(C)\)
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(A\) is of size \(k \times k\) and \(C\) is of size \(m \times m\), then we know that \(k+m=n\) and \(O\) is of size \(m \times k\). We have two cases. If \(A\) is not invertible, then we know that \(\det(A) = 0\). Moreover, the columns of \(A\) are linearly dependent and the rank of \(A\) is less than \(k\). But \(M\) contains \(k+m\) columns and since the first \(k\) columns are linearly dependent, then the rank of \(M\) is less than \(k+m\). Therefore \(\det(M) = 0 = \det(A)\det(C)\) and we are done.
<br />
<br />
Suppose that \(A\) is invertible. Then define</p>
<div>
	$$
	\begin{align*}
     P &amp;= 
 	 \begin{pmatrix}
 	A &amp; O' \\
 	O &amp; I_m
 	\end{pmatrix},
	Q = 
	\begin{pmatrix}
	I_k &amp; A^{-1}B \\
	O &amp; C
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Where \(O'\) is of size \(k \times m\). Note here that \(PQ = M\). We claim that \(\det(P) = \det(A)\) and \(\det(Q)=C\). Once we show this claim is true, then we can conclude that \(\det(M) = \det(P)\det(Q) = \det(A)\det(Q)\).
<br />
<br />
To show that \(\det(P)=\det(O)\), We’ll prove this by induction on \(m\).<br />
Base Case \(m=1\): \(I_m\) is of size \(1 \times 1\). \(O\) is of size \(1 \times k\). The last row of \(P\) therefore contains a row of zeros followed by 1. Computing the determinant of \(P\) by cofactor expansion along this last row yields exactly \(\det(A)\) as required.
<br />
<br />
Inductive Case: Suppose this is true for \(m-1\). We will show that this is true for \(m\). In this case \(O\) is of size \(m \times k\) and \(I_m\) is of size \(m \times m\). The last row of this matrix is also a sequence of zeros followed by a 1 by the definition of \(O\) and \(I_m\). So compute the determinant by cofactor expansion along the last row to see that \(\det(P) = 1 \det(P')\). where \(P'\) is the matrix \(P\) but with the last column and the last row. We can now apply the inductive hypothesis to conclude that \(\det(P') = \det(A)\) and therefore \(\det(P) = 1\det(A) = \det(A)\) as we wanted to show.
<br />
<br />
We can use the same inductive proof to prove that \(\det(Q) = C\). Once we do so, we are done.</p>

<p>\(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[4.3: Exercise 21 Prove that if \(M \in M_{n \times n}(\mathbf{F})\) can be written in the form $$ \begin{align*} M = \begin{pmatrix} A &amp; B \\ O &amp; C \end{pmatrix} \end{align*} $$ where \(A\) and \(C\) are square square matrices. Prove that \(\det(M) = \det(A)\det(C)\) Proof: Suppose \(A\) is of size \(k \times k\) and \(C\) is of size \(m \times m\), then we know that \(k+m=n\) and \(O\) is of size \(m \times k\). We have two cases. If \(A\) is not invertible, then we know that \(\det(A) = 0\). Moreover, the columns of \(A\) are linearly dependent and the rank of \(A\) is less than \(k\). But \(M\) contains \(k+m\) columns and since the first \(k\) columns are linearly dependent, then the rank of \(M\) is less than \(k+m\). Therefore \(\det(M) = 0 = \det(A)\det(C)\) and we are done. Suppose that \(A\) is invertible. Then define $$ \begin{align*} P &amp;= \begin{pmatrix} A &amp; O' \\ O &amp; I_m \end{pmatrix}, Q = \begin{pmatrix} I_k &amp; A^{-1}B \\ O &amp; C \end{pmatrix} \end{align*} $$ Where \(O'\) is of size \(k \times m\). Note here that \(PQ = M\). We claim that \(\det(P) = \det(A)\) and \(\det(Q)=C\). Once we show this claim is true, then we can conclude that \(\det(M) = \det(P)\det(Q) = \det(A)\det(Q)\). To show that \(\det(P)=\det(O)\), We’ll prove this by induction on \(m\). Base Case \(m=1\): \(I_m\) is of size \(1 \times 1\). \(O\) is of size \(1 \times k\). The last row of \(P\) therefore contains a row of zeros followed by 1. Computing the determinant of \(P\) by cofactor expansion along this last row yields exactly \(\det(A)\) as required. Inductive Case: Suppose this is true for \(m-1\). We will show that this is true for \(m\). In this case \(O\) is of size \(m \times k\) and \(I_m\) is of size \(m \times m\). The last row of this matrix is also a sequence of zeros followed by a 1 by the definition of \(O\) and \(I_m\). So compute the determinant by cofactor expansion along the last row to see that \(\det(P) = 1 \det(P')\). where \(P'\) is the matrix \(P\) but with the last column and the last row. We can now apply the inductive hypothesis to conclude that \(\det(P') = \det(A)\) and therefore \(\det(P) = 1\det(A) = \det(A)\) as we wanted to show. We can use the same inductive proof to prove that \(\det(Q) = C\). Once we do so, we are done.]]></summary></entry><entry><title type="html">Section 5.3: Transition Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices.html" rel="alternate" type="text/html" title="Section 5.3: Transition Matrices" /><published>2024-08-30T01:01:36-07:00</published><updated>2024-08-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices.html"><![CDATA[<!---------------------------------------5.12--------------------------------------------->
<div class="purdiv">
Theorem 5.12
</div>
<div class="purbdiv">
Let \(A\) be a square matrix with complex entries. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists if and only if both of the following conditions hold:
<ol style="list-style-type:lower-alpha">
	<li>Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\)</li>
	<li>If 1 is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to 1 equals the multiplicity of 1 as an eigenvalue of 1. </li>
</ol>
</div>
<p><br />
<b>Proof:</b> [TODO]
<br />
<br />
<!----------------------------------------5.13--------------------------------------------></p>
<div class="purdiv">
Theorem 5.13
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) satisfy the following two conditions
<ol type="i"> 
	<li>Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\)</li>
	<li>\(A\) is diagonalizable. </li>
</ol>
Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Since \(A\) is diagonalizable, then we know that there exists an invertible matrix \(Q\) such that \(D = Q^{-1}AQ\). Suppose that</p>
<div> 
$$
\begin{align*}
D = 
\begin{pmatrix} 
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>\(\lambda_1,...,\lambda_n\) are the eigenvalues of \(A\). But condition (i) requires that for each \(i\), \(\lambda_i = 1\) or \(|\lambda_i| &lt; 1\). Thus</p>
<div>
$$
\begin{align*}
\lim\limits_{m \rightarrow \infty} {\lambda_i}^m &amp;= \begin{cases} 1 \quad \text{if } \lambda_i = 1 \\ 0 \quad \text{otherwise } \end{cases}
\end{align*}
$$
</div>
<p>so</p>
<div> 
$$
\begin{align*}
D^m = 
\begin{pmatrix} 
\lambda_1^m &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^m &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n^m
\end{pmatrix}
\end{align*}
$$
</div>
<p>and so the sequence \(D, D^2 ...\) converges to a limit \(L\). Therefore,</p>
<div> 
$$
\begin{align*}
\lim\limits_{m \rightarrow \infty} A^m = \lim\limits_{m \rightarrow \infty} (QDQ^{-1}) = QLQ^{-1}

\end{align*}
$$
</div>
<p><br />
<!----------------------------------------5.14--------------------------------------------></p>
<div class="purdiv">
Theorem 5.14
</div>
<div class="purbdiv">
Let \(M\) be an \(n \times n\) matrix having real nonnegative entries, let \(v\) be a column vector in \(\mathbf{R}^n\) having nonnegative coordinates, and let \(u \in \mathbf{R}^n\) be the column vector in which each coordinate equals 1. Then
<ol type="a"> 
	<li>\(M\) is a transition matrix if and only if \(u^tM = u^t\);</li>
	<li>\(v\) is a probability vector if and only if \(u^tv = (1)\). </li>
</ol>
</div>
<p><br />
<b>Proof (a):</b>
<br />
<br />
\(\Rightarrow\): Suppose \(M\) is a transition matrix. Then by the definition of matrix-vector multiplication,</p>
<div> 
$$
\begin{align*}
u^tM &amp;= 
\begin{pmatrix} 
1 &amp; 1 &amp; \cdots &amp; 1
\end{pmatrix}
\begin{pmatrix} 
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{m2} \\
\end{pmatrix}
=
\begin{pmatrix}
w_1 &amp; w_2 &amp; \cdots &amp; w_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>where \(w_i\) is the sum</p>
<div> 
$$
\begin{align*}
w_i &amp;= 1a_{1i} + 1a_{2i} + ... + 1a_{ni} \\
    &amp;= a_{1i} + a_{2i} + ... + a_{ni} \\
	&amp;= 1 \quad \text{(since $M$ is a transition matrix)}
\end{align*}
$$
</div>
<p><br />
\(\Leftarrow\): [TODO: same argument as \(\Rightarrow\)]
<br />
<br /><b>Proof (b):</b> [TODO: also very similar to \((a)\)]
<br />
<br />
<!-----------------------------------5.14 (Corollary)-----------------------------------------></p>
<div class="purdiv">
Theorem 5.14 (Corollary)
</div>
<div class="purbdiv">
<ol type="a"> 
	<li>The product of two \(n \times n \) transition matrices is an \(n \times n\) transition matrix. In particular, any power of a transition matrix is a transition matrix.</li>
	<li>The product of a transition matrix a probability vector is a probability vector.</li>
</ol>
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
[TODO]
<br />
<br />
<!-----------------------------------Definition-----------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A transition matrix is called regular if some power of the matrix contains only nonzero (i.e., positive) entries.
</div>
<p><br />
<br />
<!-----------------------------------Definition-----------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\). For \(1 \leq i,j \leq n\), define \(\rho_i(A)\) to be the sum of the absolute values of the entries of row \(i\) of \(A\), and define \(\nu_j(A)\) to be equal to the sum of the absolute values of the entries of column \(j\) of \(A\). Thus
$$
\begin{align*}
\rho_i(A) = \sum_{j=1}^n |A_{ij}| \quad \text{ for $i = 1,2,...,n$}
\end{align*}
$$
and
$$
\begin{align*}
\nu_j(A) = \sum_{i=1}^n |A_{ij}| \quad \text{ for $j = 1,2,...,n$}
\end{align*}
$$
The row sum of \(A\) denoted \(\rho(A)\), and the column sum of \(A\), denoted \(\nu(A)\), are defined as
$$
\begin{align*}
\rho(A) = \max\{\rho_i(A): 1 \leq i \leq n\}
\text{ and }
\nu(A) = \max\{\nu_j(A): 1 \leq j \leq n\}
\end{align*}
$$
</div>
<p><br />
<br />
TODO: definition of Gershgorin’s circle
<br />
<br />
<!-----------------------------------5.15-----------------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Gershgorin's Circle Theorem)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\). Then every eigenvalue of \(A\) is contained in a Greshgorin disk.
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 1)
</div>
<div class="purbdiv">
Let \(\lambda\) be any eigenvalue of \(A \in M_{n \times n}(\mathbf{C})\). Then \(|\lambda| \leq \rho(A)\).
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 2)
</div>
<div class="purbdiv">
Let \(\lambda\) be any eigenvalue of \(A \in M_{n \times n}(\mathbf{C})\). Then \(|\lambda| \leq 
min\{\rho(A),\nu(A)\}\).
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 3)
</div>
<div class="purbdiv">
If \(\lambda\) is an eigenvalue of a transition matrix, then \(|\lambda| \leq 1\).
</div>
<p><br />
<br />
<!--------------------------------------5.16-----------------------------------------></p>
<div class="purdiv">
Theorem 5.16
</div>
<div class="purbdiv">
Every transition matrix has 1 as an eigenvalue.
</div>
<p><br />
<br />
<!--------------------------------------5.17-----------------------------------------></p>
<div class="purdiv">
Theorem 5.17
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a matrix in which each entry is a positive real number, and let \(\lambda\) be a complex eigenvalue of \(A\) such that \(|\lambda| = \rho(A)\). Then \(\lambda = \rho(A)\) and \(\{u\}\) is a basis for \(E_{\lambda}\), where \(u \in \mathbf{C}^n\) is the column vector in which each coordinate equals 1.
</div>
<p><br />
<br />
<b>Proof</b>
[TODO: we did some version of this in class]
<br />
<br />
<!-----------------------------------5.17 (Corollary 1)--------------------------------------></p>
<div class="purdiv">
Theorem 5.17 (Corollary 1)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a matrix in which each entry is a positive, and let \(\lambda\) be an eigenvalue of \(A\) such that \(|\lambda| = \nu(A)\). Then \(\lambda = \nu(A)\) and the dimension of \(E_{\lambda}\) equals 1.
</div>
<p><br />
<br />
<!----------------------------------- 5.17 (Corollary 2) --------------------------------------></p>
<div class="purdiv">
Theorem 5.17 (Corollary 2)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a transition matrix in which each entry is positive, and let \(\lambda\) be an eigenvalue of \(A\) other than 1. Then \(|\lambda| &lt; 1\). Moreover, the eigenspace corresponding to the eigenvalue 1 has dimension 1.
</div>
<p><br />
<br />
Okay, so only if the transition matrix itself has positive entries (regular is not enough), then \(\lambda &lt; 1\) for eigenvalues that are not 1 AND the dimension of the eigenspace corresponding to eigenvalue 1 is 1.
<br />
<br />
<!-------------------------------------- 5.18 ----------------------------------------></p>
<div class="purdiv">
Theorem 5.18
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix, and let \(\lambda\) be an eigenvalue of \(A\). 
<ol type="a"> 
	<li>\(|\lambda| \leq 1\).</li>
	<li>If \(|\lambda| = 1\), then \(\lambda = 1\), and \(\dim(E_{\lambda}) = 1.\)</li>
</ol>
</div>
<p><br />
<br />
This is the other result I was looking for. If \(A\) is regular, then we’ll have the normal restriction of \(|\lambda| \leq 1\).
<br />
<br />
<!-------------------------------- 5.18 (Corollary) ---------------------------------></p>
<div class="purdiv">
Theorem 5.18 (Corollary)
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix that is diagonalizable. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists.
</div>
<p><br />
<br />
<!-------------------------------------- 5.19 ----------------------------------------></p>
<div class="purdiv">
Theorem 5.19
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix, and let \(\lambda\) be an eigenvalue of \(A\). 
<ol type="a"> 
	<!--------(a)------------>
	<li>The multiplicity of 1 as an eigenvalue of \(A\) is 1.</li>
	<!--------(b)------------>
	<li>\(\lim\limits_{m \rightarrow \infty} A^m\) exists.</li>
	<!--------(c)------------>
	<li>\(L = \lim\limits_{m \rightarrow \infty} A^m\) is a transition matrix.</li>
	<!--------(d)------------>
	<li>\(AL = LA = L\)</li>
	<!--------(e)------------>
	<li>The columns of \(L\) are identical. In fact, each column of \(L\) is equal to the unique probability vector \(v\) that is also an eigenvector of \(A\) corresponding to the eigenvalue 1.</li>
	<!--------(f)------------>
	<li>For any probability vector \(w\), \(\lim\limits_{m \rightarrow \infty} (A^m w) = v.\)</li>
</ol>
</div>
<p><br />
<br /></p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.12 Let \(A\) be a square matrix with complex entries. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists if and only if both of the following conditions hold: Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\) If 1 is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to 1 equals the multiplicity of 1 as an eigenvalue of 1. Proof: [TODO] Theorem 5.13 Let \(A \in M_{n \times n}(\mathbf{C})\) satisfy the following two conditions Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\) \(A\) is diagonalizable. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists Proof: Since \(A\) is diagonalizable, then we know that there exists an invertible matrix \(Q\) such that \(D = Q^{-1}AQ\). Suppose that $$ \begin{align*} D = \begin{pmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n \end{pmatrix} \end{align*} $$ \(\lambda_1,...,\lambda_n\) are the eigenvalues of \(A\). But condition (i) requires that for each \(i\), \(\lambda_i = 1\) or \(|\lambda_i| &lt; 1\). Thus $$ \begin{align*} \lim\limits_{m \rightarrow \infty} {\lambda_i}^m &amp;= \begin{cases} 1 \quad \text{if } \lambda_i = 1 \\ 0 \quad \text{otherwise } \end{cases} \end{align*} $$ so $$ \begin{align*} D^m = \begin{pmatrix} \lambda_1^m &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2^m &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n^m \end{pmatrix} \end{align*} $$ and so the sequence \(D, D^2 ...\) converges to a limit \(L\). Therefore, $$ \begin{align*} \lim\limits_{m \rightarrow \infty} A^m = \lim\limits_{m \rightarrow \infty} (QDQ^{-1}) = QLQ^{-1}]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5 (Corollary)</title><link href="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5 (Corollary)" /><published>2024-08-29T01:01:36-07:00</published><updated>2024-08-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html"><![CDATA[<div class="purdiv">
Theorem 5.5 Corollary
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable.
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\)
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Corollary Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable. Proof: Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5</title><link href="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5" /><published>2024-08-28T01:01:36-07:00</published><updated>2024-08-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html"><![CDATA[<div class="purdiv">
Theorem 5.5
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent.
</div>
<p><br />
Proof:
<br />
<br />
By induction on \(k\). 
<br />
<br />
Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done.
<br />
<br />
Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent.
<br />
<br /> 
Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0
	\end{align}
	$$
</div>
<p>We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that</p>
<div>
	$$
	\begin{align*}
	(T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\end{align*}
	$$
</div>
<p>But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means  that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0
	\end{align}
	$$
</div>
<p>The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). 
<br />
<br />
I really don’t like this proof!
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent. Proof: By induction on \(k\). Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done. Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent. Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0 \end{align} $$ We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that $$ \begin{align*} (T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \end{align*} $$ But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0 \end{align} $$ The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). I really don’t like this proof! References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.1: Exercise 15</title><link href="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html" rel="alternate" type="text/html" title="Section 5.1: Exercise 15" /><published>2024-08-27T01:01:36-07:00</published><updated>2024-08-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html"><![CDATA[<div class="ydiv">
Exercise 15
</div>
<div class="ybdiv">
For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues).
</div>
<p><br />
Proof:
<br />
<br />
Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det(A^t - (\lambda I_n)^t) \\
	                  &amp;= \det(A^t - \lambda I_n).
	\end{align*}
	$$
</div>
<p>From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 15 For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues). Proof: Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore, $$ \begin{align*} \det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det(A^t - (\lambda I_n)^t) \\ &amp;= \det(A^t - \lambda I_n). \end{align*} $$ From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 4.3: Theorem 4.8</title><link href="http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8.html" rel="alternate" type="text/html" title="Section 4.3: Theorem 4.8" /><published>2024-08-26T01:01:36-07:00</published><updated>2024-08-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8.html"><![CDATA[<div class="purdiv">
Theorem 4.8
</div>
<div class="purbdiv">
For any \(A \in M_{n \times n}(\mathbf{F})\), \(\det(A^t) = \det(A)\).
</div>
<p><br />
Proof:
<br />
<br />
If \(A\) is not invertible, then \(\text{rank}(A) &lt; n\). We know that \(\text{rank}(A) = \text{rank}(A^t)\) by theorem 3.6 (corollary 2). Therefore, \(A^t\) is not invertible and \(\det(A) = \det(A^t) = 0\).
<br />
<br />
Suppose now that \(A\) is invertible, then we can write \(A\) as a product of elementary matrices, \(A = E_k,E_{k-1}...,E_1\). Furthermore, we’ve proved that \(\det(E) = \det(E^t)\) for any elementary matrix. From this, notice that</p>

<div>
	$$
	\begin{align*}
	\det(A^t) &amp;= \det((E_k,E_{k-1}...,E_1)^t) \\
	          &amp;= \det(E_1^tE_2^t...E_k^t) \\
	          &amp;= \det(E_1^t)\det(E_2^t)...\det(E_k^t) \\
	          &amp;= \det(E_1)\det(E_2)...\det(E_k) \\
	          &amp;= \det(E_k)\det(E_{k-1})...\det(E_1) \\
	          &amp;= \det(E_k,E_{k-1}...,E_1) \\
			  &amp;= \det(A). \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 4.8 For any \(A \in M_{n \times n}(\mathbf{F})\), \(\det(A^t) = \det(A)\). Proof: If \(A\) is not invertible, then \(\text{rank}(A) &lt; n\). We know that \(\text{rank}(A) = \text{rank}(A^t)\) by theorem 3.6 (corollary 2). Therefore, \(A^t\) is not invertible and \(\det(A) = \det(A^t) = 0\). Suppose now that \(A\) is invertible, then we can write \(A\) as a product of elementary matrices, \(A = E_k,E_{k-1}...,E_1\). Furthermore, we’ve proved that \(\det(E) = \det(E^t)\) for any elementary matrix. From this, notice that]]></summary></entry><entry><title type="html">Lecture 29/30: Inner Product Spaces and Norms</title><link href="http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces.html" rel="alternate" type="text/html" title="Lecture 29/30: Inner Product Spaces and Norms" /><published>2024-08-25T01:01:36-07:00</published><updated>2024-08-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces.html"><![CDATA[<p>Let \(V\) be. vector space over \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\).</p>
<div> 
$$
\begin{align*}
\mathbf{C} = \{ z=a+ib \ | \ a, b \in \mathbf{R} \}
\end{align*}
$$
</div>
<p>The complex conjugate of \(z\) is \(\bar{z} = a - ib\).</p>
<div> 
$$
\begin{align*}
z\bar{z} = (a + ib)(a - ib) = a^2 + b^2 = |z|^2
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inner product \(\langle \ , \ \rangle\) on a vector \(V\) is a map
$$
\begin{align*}
\langle \ , \ \rangle : \ &amp;V \times V \rightarrow \mathbf{F} = \mathbf{R} \text{ or } \mathbf{C} \\
&amp;(x, y) \rightarrow \langle x , y \rangle
\end{align*}
$$
such that
<ol type="i">
	<li>\(\langle x+z, y \rangle = \langle x , y \rangle + \langle z , y \rangle\)</li>
	<li>\(\langle cx, y \rangle = c \langle x , y \rangle\)</li>
	<li>\(\langle x, y \rangle = \overline{\langle y, x \rangle}\). Note if \(\mathbf{F} = \mathbf{R}\), then \(\langle x, y \rangle = \langle y, x \rangle\) </li>
	<li>\(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\)</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>The simplest example is \(V = \mathbf{R}^1\) (vector space over \(\mathbf{R}\)) where</p>
<div> 
$$
\begin{align*}
\langle x, y \rangle &amp;= xy
\end{align*}
$$
</div>
<p>This map satisfies the inner product properties. Notice that</p>
<ol type="i">
	<li>\(\langle x+z, y \rangle = (x+z)y = xy + zx = \langle x , y \rangle + \langle z , y \rangle \)</li>
	<li>\(\langle cx, y \rangle = cxy = c\langle x, y \rangle\)</li>
	<li>\(\langle x, y \rangle = \overline{\langle y, x \rangle} = yx = xy\). </li>
	<li>\(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\)</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>We can also define this inner product</p>
<div> 
$$
\begin{align*}
\langle \langle x, y \rangle \rangle_c &amp;= c^2xy \quad c \neq 0
\end{align*}
$$
</div>
<p>which also satisfies the inner product properties (TODO)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3: Dot Product</b></h4>
<p>Another example is \(V = \mathbf{R}^n\) over \(\mathbf{R}\) where</p>
<div> 
$$
\begin{align*}
\langle x, y \rangle &amp;= x_1y_1 + ... + x_ny_n = \sum_j x_jy_j
\end{align*}
$$
</div>
<p>which is commonly known as the dot product.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Another example is \(V = \mathbf{R}^2\) over \(\mathbf{R}\) where</p>
<div> 
$$
\begin{align*}
\langle \langle x, y \rangle \rangle &amp;= 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2
\end{align*}
$$
</div>
<p>We claim that \(\langle \langle \ , \ \rangle \rangle\) is an inner product. To see this check each property in the definition. For example for property 4, we see that</p>
<div> 
$$
\begin{align*}
\langle \langle x, x \rangle \rangle &amp;= 2x_1x_1 + x_1x_2 + x_2x_1 + x_2x_2 \\
                                     &amp;= 2x_1^2 + 2x_1x_2 + x_2^2 \\
									 &amp;= x_1^2 + (x_1 + x_2)^2 &gt; 0
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>Define \(V = \mathbf{C}\) over \(\mathbf{C}\) where</p>
<div> 
$$
\begin{align*}
\langle z_1, z_2 \rangle &amp;= z_1\bar{z_2}
\end{align*}
$$
</div>
<p>Note here that if we defined the product as \(\langle z_1, z_2 \rangle = z_1z_2\). This will fail to satisfy the inner product conditions.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6: Frobenius Inner Product</b></h4>
<p>Define \(V = \mathbf{C}^0([0,1]) = \{f: [0,1] \rightarrow \mathbf{R} \ | \ f \text{ continuous}\}\)</p>
<div> 
$$
\begin{align*}
\langle f, g \rangle &amp;= \int_0^1 f(t)g(t)dt
\end{align*}
$$
</div>
<p>is an inner product. Checking property (4)</p>
<div> 
$$
\begin{align*}
\langle f, g \rangle &amp;= \int_0^1 f(t)f(t) dt \\
                    &amp;= \int_0^1 f^2(t) dt \\
					&amp;&gt; 0 \text{ unless $f(t)=0 \ \forall t \in [0,1]$ }				
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 7</b></h4>
<p>Define \(V = M_{n \times n}(\mathbf{R})\)</p>
<div> 
$$
\begin{align*}
\langle A, B \rangle &amp;= tr(B^tA)
\end{align*}
$$
</div>
<p>where</p>
<div> 
$$
\begin{align*}
tr: \ &amp;M_{n \times n}(\mathbf{R}) \rightarrow M_{n \times n}(\mathbf{R}) \\
&amp;C \rightarrow \sum_j C_{jj} = C_{11} + C_{22} + ... + C_{nn}
\end{align*}
$$
</div>
<p>Does this map satisfy the inner product conditions?</p>
<ol type="i">
	<!------------(i)---------------->
	<li> We want to show that \(\langle A + C, B \rangle = \langle A, B \rangle + \langle C, B \rangle\). Expand the inner product to see that
		<div> 
		$$
		\begin{align*}
		\langle A+C, B \rangle &amp;= tr(B^t(A+C)) \\
		                       &amp;= tr(B^tA + B^tC) \\
							   &amp;= tr(B^tA) + tr(B^tC) \\
							   &amp;= \langle A, B \rangle + \langle C, B \rangle 
		\end{align*}
		$$
		</div>
	</li>
	<!------------(ii)---------------->
	<li>We want to show that \(\langle cA, B \rangle = c\langle A, B \rangle\):
	<div> 
	$$
	\begin{align*}
	\langle cA, B \rangle &amp;= tr(B^t(cA)) \\
	                       &amp;= ctr(B^tA) \\
						   &amp;= c\langle A, B \rangle
	\end{align*}
	$$
	</div>
	</li>
	<!------------(iii)---------------->
	<li>We want to show that \(\langle A, B \rangle = \langle B, A \rangle\). (over \(\mathbf{R}\)). Note here that \(tr(C^t) = tr(C)\). Then
	<div> 
	$$
	\begin{align*}
	\langle A, B \rangle &amp;= tr(B^tA) \\
	                       &amp;= tr((B^tA)^t) \\
	                       &amp;= tr(A^tB) \\
						   &amp;= \langle B, A \rangle
	\end{align*}
	$$
	</div>
	</li>
	<!------------(iv)---------------->
	<li>We need to show that \(\langle A, A \rangle &gt; 0 \text{ if } A \neq \bar{0} \in M_{n \times n}(\mathbf{R})\)
		<div> 
		$$
		\begin{align*}
		\langle A, A \rangle &amp;= tr(A^tA) \\
		                       &amp;= \sum_j (A^tA)_{jj} \\
		                       &amp;= \sum_j \sum_k (A^t)_{jk} (A)_{kj} \\
		                       &amp;= \sum_j \sum_k (A)_{kj} (A)_{kj} \\
							   &amp;= \sum_j \sum_k A^2_{kj}						   
		\end{align*}
		$$
		</div>
		Note here that \(A^2_{kj} &gt; 0\) unless \(A_{1j},...A_{nj} = 0\). These are the entries of the \(j\)th column of \(A\). This means that this is zero unless \(A\) is the zero matrix.
	</li>
</ol>
<p>In fact, \(tr(B^tA) = \sum_{ij} A_{ij}B_{ij}\). the definition of \(tr(B^tA)\) is for square matrices while the definition \(\sum_{ij} A_{ij}B_{ij}\) works for any matrices. This works for an inner product on matrices.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Inner Product Spaces</b></h4>
<p>So far, we’ve seen that inner products are not unique. We can define many inner products on a given vector space. So we have the following definition to fix a specific inner product on a vector space
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inner product space is a vector space is a vector space \(V\) with a fixed inner product.
</div>
<p><br />
<br />
Example: (\(\mathbf{R}^2, \langle x,y \rangle = x_1y_1 + x_2y_2\)) is an inner product space different from \((\mathbf{R}^2, \langle x,y \rangle = 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2)\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Norm of a Vector</b></h4>
<p>For the rest of the lecture, we’re going to assume that we have a fixed inner product on \(V\). 
<br />
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(V, \langle \ , \ \rangle\) be an inner product space. The length (or norm) of \(v \in V\) is
		$$
		\begin{align*}
		\Vert v \Vert = \sqrt{\langle v, v \rangle}				   
		\end{align*}
		$$
</div>
<p><br />
<br />
Example \(V = \mathbf{C}^0([0,1])\), Let \(\Vert f \Vert: (\int_0^1 f(t)^2 dt)^{1/2}\). This is also called the \(L^2\)-norm.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The distance between \(x, y\) in \(V\) is \(\Vert x - y \Vert\)
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The sphere of radius \(r\) and center \(x \in V\) is \(\{y \in V \ | \ \Vert x - y \Vert = r \}\)
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
For any \(x, y \in V\) and \(c \in \mathbf{F}\)
<ol type="a">
	<li>\(\Vert cx \Vert = |c|\Vert x \Vert  \)</li>
	<li>\(\Vert x \Vert = 0 \leftrightarrow x = \bar{0}_V \)</li>
	<li>\(| \langle x , y \rangle | \leq \Vert x \Vert \Vert y \Vert \). Cauchy-Schwarz</li>
	<li>\(\Vert x + y \Vert \leq \Vert x \Vert \Vert y \Vert \). The Triangle Inequality</li>
	
</ol>
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------>
The proof for (a) and (b) follow easily. For (c). The motivation is from \(\mathbf{R}^2\) with the standard inner product.</p>
<div> 
$$
\begin{align*}
 \langle x , y \rangle  &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert  \cos(\theta) \\
| \langle x , y \rangle | &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert | \cos(\theta)|
\end{align*}
$$
</div>
<p>But we know that \(| \cos(\theta)| \leq 1\). Therefore</p>
<div> 
$$
\begin{align*}
| \langle x , y \rangle | &amp;\leq x \cdot y = \Vert x \Vert \Vert y \Vert 
\end{align*}
$$
</div>
<p>But we want to prove this in general so:
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof (c)</b>:
<br />
<br />
If \(y = \bar{0}_V\), then the inequality is true and we are done. So assume that \(y \neq \bar{0}_V\). If we multiply both sides by \(\frac{1}{\Vert y \Vert}\) which is a scalar, then this scalar can be factored out by property 1. Therefore, we can scale \(y\) by whatever factor we want and so let’s just assume that \(y\) has length 1 (\(\Vert y \Vert = 1\)). So it suffies to show that</p>
<div> 
$$
\begin{align*}
| \langle x , y \rangle | &amp;\leq \Vert x \Vert \\
| \langle x , y \rangle |^2 &amp;\leq \Vert x \Vert^2 \text{ (because it's easier than dealing with a squareroot)} \\
&amp;= \langle x, x \rangle 
\end{align*}
$$
</div>
<p>To show this, take the project of vector \(x\) onto vector \(y\) which has length 1. The projection is a vector \(x - \langle x , y \rangle y\). Moreover,</p>
<div> 
$$
\begin{align*}
0 &amp;\leq \Vert x - \langle x , y \rangle y \Vert^2 \text{ ( the length of any vector $\geq 0$)} \\
 &amp;= \langle x - \langle x , y \rangle y, x - \langle x , y \rangle y \rangle \\
 &amp;= \langle x, x \rangle - \langle\langle x , y \rangle y, x\rangle +  \langle x, -\langle x , y \rangle y\rangle + \langle - \langle x, y \rangle y, - \langle x, y \rangle y \rangle \text{( by property 1)} \\
 &amp;= \Vert x \Vert^2 - \langle x, y \rangle \langle y, x \rangle + \overline{- \langle x, y \rangle y, x \rangle} + \Vert -\langle x, y \rangle y \Vert^2 \\ 
 &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 -  \overline{\langle x, y \rangle} \overline{\langle y, x \rangle}  + |\langle x, y \rangle |^2 \Vert y \Vert^2 \\
 &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 - |\langle x, y \rangle |^2 + |\langle x, y \rangle |^2 \\
 &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof (d)</b>
<br /></p>
<div> 
$$
\begin{align*}
\Vert x + y \Vert^2 &amp;= \langle x + y, x + y \rangle \\
                   &amp;= \Vert x \Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2 \\
                   &amp;\leq \Vert x \Vert^2 + 2 |\langle x, y \rangle| + \Vert y \Vert^2 \text{ multiply a complex number by its conjecture to see}\\
                   &amp;\leq \Vert x \Vert^2 + 2 \Vert x \Vert \Vert y \Vert + \Vert y \Vert^2 \text { (By (c))}\\
                   &amp;= (\Vert x \Vert + \Vert y \Vert)^2 \\				   
\end{align*}
$$
</div>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(V\) be. vector space over \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\). $$ \begin{align*} \mathbf{C} = \{ z=a+ib \ | \ a, b \in \mathbf{R} \} \end{align*} $$ The complex conjugate of \(z\) is \(\bar{z} = a - ib\). $$ \begin{align*} z\bar{z} = (a + ib)(a - ib) = a^2 + b^2 = |z|^2 \end{align*} $$ Definition An inner product \(\langle \ , \ \rangle\) on a vector \(V\) is a map $$ \begin{align*} \langle \ , \ \rangle : \ &amp;V \times V \rightarrow \mathbf{F} = \mathbf{R} \text{ or } \mathbf{C} \\ &amp;(x, y) \rightarrow \langle x , y \rangle \end{align*} $$ such that \(\langle x+z, y \rangle = \langle x , y \rangle + \langle z , y \rangle\) \(\langle cx, y \rangle = c \langle x , y \rangle\) \(\langle x, y \rangle = \overline{\langle y, x \rangle}\). Note if \(\mathbf{F} = \mathbf{R}\), then \(\langle x, y \rangle = \langle y, x \rangle\) \(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\) Example 1 The simplest example is \(V = \mathbf{R}^1\) (vector space over \(\mathbf{R}\)) where $$ \begin{align*} \langle x, y \rangle &amp;= xy \end{align*} $$ This map satisfies the inner product properties. Notice that \(\langle x+z, y \rangle = (x+z)y = xy + zx = \langle x , y \rangle + \langle z , y \rangle \) \(\langle cx, y \rangle = cxy = c\langle x, y \rangle\) \(\langle x, y \rangle = \overline{\langle y, x \rangle} = yx = xy\). \(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\) Example 2 We can also define this inner product $$ \begin{align*} \langle \langle x, y \rangle \rangle_c &amp;= c^2xy \quad c \neq 0 \end{align*} $$ which also satisfies the inner product properties (TODO) Example 3: Dot Product Another example is \(V = \mathbf{R}^n\) over \(\mathbf{R}\) where $$ \begin{align*} \langle x, y \rangle &amp;= x_1y_1 + ... + x_ny_n = \sum_j x_jy_j \end{align*} $$ which is commonly known as the dot product. Example 4 Another example is \(V = \mathbf{R}^2\) over \(\mathbf{R}\) where $$ \begin{align*} \langle \langle x, y \rangle \rangle &amp;= 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2 \end{align*} $$ We claim that \(\langle \langle \ , \ \rangle \rangle\) is an inner product. To see this check each property in the definition. For example for property 4, we see that $$ \begin{align*} \langle \langle x, x \rangle \rangle &amp;= 2x_1x_1 + x_1x_2 + x_2x_1 + x_2x_2 \\ &amp;= 2x_1^2 + 2x_1x_2 + x_2^2 \\ &amp;= x_1^2 + (x_1 + x_2)^2 &gt; 0 \end{align*} $$ Example 5 Define \(V = \mathbf{C}\) over \(\mathbf{C}\) where $$ \begin{align*} \langle z_1, z_2 \rangle &amp;= z_1\bar{z_2} \end{align*} $$ Note here that if we defined the product as \(\langle z_1, z_2 \rangle = z_1z_2\). This will fail to satisfy the inner product conditions. Example 6: Frobenius Inner Product Define \(V = \mathbf{C}^0([0,1]) = \{f: [0,1] \rightarrow \mathbf{R} \ | \ f \text{ continuous}\}\) $$ \begin{align*} \langle f, g \rangle &amp;= \int_0^1 f(t)g(t)dt \end{align*} $$ is an inner product. Checking property (4) $$ \begin{align*} \langle f, g \rangle &amp;= \int_0^1 f(t)f(t) dt \\ &amp;= \int_0^1 f^2(t) dt \\ &amp;&gt; 0 \text{ unless $f(t)=0 \ \forall t \in [0,1]$ } \end{align*} $$ Example 7 Define \(V = M_{n \times n}(\mathbf{R})\) $$ \begin{align*} \langle A, B \rangle &amp;= tr(B^tA) \end{align*} $$ where $$ \begin{align*} tr: \ &amp;M_{n \times n}(\mathbf{R}) \rightarrow M_{n \times n}(\mathbf{R}) \\ &amp;C \rightarrow \sum_j C_{jj} = C_{11} + C_{22} + ... + C_{nn} \end{align*} $$ Does this map satisfy the inner product conditions? We want to show that \(\langle A + C, B \rangle = \langle A, B \rangle + \langle C, B \rangle\). Expand the inner product to see that $$ \begin{align*} \langle A+C, B \rangle &amp;= tr(B^t(A+C)) \\ &amp;= tr(B^tA + B^tC) \\ &amp;= tr(B^tA) + tr(B^tC) \\ &amp;= \langle A, B \rangle + \langle C, B \rangle \end{align*} $$ We want to show that \(\langle cA, B \rangle = c\langle A, B \rangle\): $$ \begin{align*} \langle cA, B \rangle &amp;= tr(B^t(cA)) \\ &amp;= ctr(B^tA) \\ &amp;= c\langle A, B \rangle \end{align*} $$ We want to show that \(\langle A, B \rangle = \langle B, A \rangle\). (over \(\mathbf{R}\)). Note here that \(tr(C^t) = tr(C)\). Then $$ \begin{align*} \langle A, B \rangle &amp;= tr(B^tA) \\ &amp;= tr((B^tA)^t) \\ &amp;= tr(A^tB) \\ &amp;= \langle B, A \rangle \end{align*} $$ We need to show that \(\langle A, A \rangle &gt; 0 \text{ if } A \neq \bar{0} \in M_{n \times n}(\mathbf{R})\) $$ \begin{align*} \langle A, A \rangle &amp;= tr(A^tA) \\ &amp;= \sum_j (A^tA)_{jj} \\ &amp;= \sum_j \sum_k (A^t)_{jk} (A)_{kj} \\ &amp;= \sum_j \sum_k (A)_{kj} (A)_{kj} \\ &amp;= \sum_j \sum_k A^2_{kj} \end{align*} $$ Note here that \(A^2_{kj} &gt; 0\) unless \(A_{1j},...A_{nj} = 0\). These are the entries of the \(j\)th column of \(A\). This means that this is zero unless \(A\) is the zero matrix. In fact, \(tr(B^tA) = \sum_{ij} A_{ij}B_{ij}\). the definition of \(tr(B^tA)\) is for square matrices while the definition \(\sum_{ij} A_{ij}B_{ij}\) works for any matrices. This works for an inner product on matrices. Inner Product Spaces So far, we’ve seen that inner products are not unique. We can define many inner products on a given vector space. So we have the following definition to fix a specific inner product on a vector space Definition An inner product space is a vector space is a vector space \(V\) with a fixed inner product. Example: (\(\mathbf{R}^2, \langle x,y \rangle = x_1y_1 + x_2y_2\)) is an inner product space different from \((\mathbf{R}^2, \langle x,y \rangle = 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2)\) The Norm of a Vector For the rest of the lecture, we’re going to assume that we have a fixed inner product on \(V\). Definition Let \(V, \langle \ , \ \rangle\) be an inner product space. The length (or norm) of \(v \in V\) is $$ \begin{align*} \Vert v \Vert = \sqrt{\langle v, v \rangle} \end{align*} $$ Example \(V = \mathbf{C}^0([0,1])\), Let \(\Vert f \Vert: (\int_0^1 f(t)^2 dt)^{1/2}\). This is also called the \(L^2\)-norm. Definition The distance between \(x, y\) in \(V\) is \(\Vert x - y \Vert\) Definition The sphere of radius \(r\) and center \(x \in V\) is \(\{y \in V \ | \ \Vert x - y \Vert = r \}\) Theorem For any \(x, y \in V\) and \(c \in \mathbf{F}\) \(\Vert cx \Vert = |c|\Vert x \Vert \) \(\Vert x \Vert = 0 \leftrightarrow x = \bar{0}_V \) \(| \langle x , y \rangle | \leq \Vert x \Vert \Vert y \Vert \). Cauchy-Schwarz \(\Vert x + y \Vert \leq \Vert x \Vert \Vert y \Vert \). The Triangle Inequality The proof for (a) and (b) follow easily. For (c). The motivation is from \(\mathbf{R}^2\) with the standard inner product. $$ \begin{align*} \langle x , y \rangle &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert \cos(\theta) \\ | \langle x , y \rangle | &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert | \cos(\theta)| \end{align*} $$ But we know that \(| \cos(\theta)| \leq 1\). Therefore $$ \begin{align*} | \langle x , y \rangle | &amp;\leq x \cdot y = \Vert x \Vert \Vert y \Vert \end{align*} $$ But we want to prove this in general so: Proof (c): If \(y = \bar{0}_V\), then the inequality is true and we are done. So assume that \(y \neq \bar{0}_V\). If we multiply both sides by \(\frac{1}{\Vert y \Vert}\) which is a scalar, then this scalar can be factored out by property 1. Therefore, we can scale \(y\) by whatever factor we want and so let’s just assume that \(y\) has length 1 (\(\Vert y \Vert = 1\)). So it suffies to show that $$ \begin{align*} | \langle x , y \rangle | &amp;\leq \Vert x \Vert \\ | \langle x , y \rangle |^2 &amp;\leq \Vert x \Vert^2 \text{ (because it's easier than dealing with a squareroot)} \\ &amp;= \langle x, x \rangle \end{align*} $$ To show this, take the project of vector \(x\) onto vector \(y\) which has length 1. The projection is a vector \(x - \langle x , y \rangle y\). Moreover, $$ \begin{align*} 0 &amp;\leq \Vert x - \langle x , y \rangle y \Vert^2 \text{ ( the length of any vector $\geq 0$)} \\ &amp;= \langle x - \langle x , y \rangle y, x - \langle x , y \rangle y \rangle \\ &amp;= \langle x, x \rangle - \langle\langle x , y \rangle y, x\rangle + \langle x, -\langle x , y \rangle y\rangle + \langle - \langle x, y \rangle y, - \langle x, y \rangle y \rangle \text{( by property 1)} \\ &amp;= \Vert x \Vert^2 - \langle x, y \rangle \langle y, x \rangle + \overline{- \langle x, y \rangle y, x \rangle} + \Vert -\langle x, y \rangle y \Vert^2 \\ &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 - \overline{\langle x, y \rangle} \overline{\langle y, x \rangle} + |\langle x, y \rangle |^2 \Vert y \Vert^2 \\ &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 - |\langle x, y \rangle |^2 + |\langle x, y \rangle |^2 \\ &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 \end{align*} $$ Proof (d) $$ \begin{align*} \Vert x + y \Vert^2 &amp;= \langle x + y, x + y \rangle \\ &amp;= \Vert x \Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2 \\ &amp;\leq \Vert x \Vert^2 + 2 |\langle x, y \rangle| + \Vert y \Vert^2 \text{ multiply a complex number by its conjecture to see}\\ &amp;\leq \Vert x \Vert^2 + 2 \Vert x \Vert \Vert y \Vert + \Vert y \Vert^2 \text { (By (c))}\\ &amp;= (\Vert x \Vert + \Vert y \Vert)^2 \\ \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 28: Invariant and T-cyclic Subspaces</title><link href="http://localhost:4000/jekyll/update/2024/08/24/lec28-invariant-subspaces.html" rel="alternate" type="text/html" title="Lecture 28: Invariant and T-cyclic Subspaces" /><published>2024-08-24T01:01:36-07:00</published><updated>2024-08-24T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/24/lec28-invariant-subspaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/24/lec28-invariant-subspaces.html"><![CDATA[<p>Consider a linear map \(T: V \rightarrow V\). Suppose \(v\) is an eigenvector of \(T\). We know by definition that \(Tv = \lambda v\) and \(v \neq 0\). We also know that \(span\{v\} \subset V\) is subspace. The observation here is that since \(v\) is an eigenvector, then when \(T\) acts on the subspace, then it stays inside the subspace meaning</p>
<div> 
$$
\begin{align*}
T(span\{v\}) \subset span\{v\}
\end{align*}
$$
</div>
<p>This is because \(T(cv) = cT(v) = (c\lambda) v\) which is in the span of \(v\). The span of an eigenvector is the simplest example an invariant subspace.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A subspace \(W \in V\) is \(T\)-invariant if \(T(W) \subseteq W\).
</div>
<p><br />
i.e. \(T(w) \in W \ \forall w \in W\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>Example 1: \(W = span\{v\}\) where \(v\) is an eigenvector of \(T\).
<br />
<br />
Example 2: \(I_V:V \rightarrow V\) Every subspace is \(I_V\)-invariant.
<br />
<br />
Example 3: \(0_V:V \rightarrow V\) Every subspace is \(0_V\)-invariant.
<br />
<br />
Example 4:</p>
<div> 
$$
\begin{align*}
T : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\
&amp;(x, y) \rightarrow (x,0)
\end{align*}
$$
</div>
<p>\(W = span\{(1,0)\}\) is \(T\)-invariant
<br />
\(W = span\{(1,1)\}\) is not \(T\)-invariant
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Characteristic Polynomial of Invariant Subspaces</b></h4>
<p>So what is the point of invariant subspaces? It helps us break off pieces of our map. What does that mean? If \(T: V \rightarrow V\) and \(W\) is \(T\)-invariant, then the restriction of \(T\) to \(W\), \(T_W\) satisfies \(T_W: W \rightarrow W\). So now have a smaller set instead of the entire vector space \(V\). The following theorem describes the relationship between the characteristic polynomial of \(T:V \rightarrow V\) and the characteristic polynomial of \(T: W \rightarrow W\). 
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\). If \(W\) is \(T\)-invariant, then the characteristic polynomial of \(T_W\) divides that of \(T\).
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
<br />
Let \(\beta_w = \{v_1,...,v_k\}\) be a basis of \(W\). Extend this to a basis for \(V\). so</p>
<div> 
$$
\begin{align*}
\beta = \{v_1,...,v_k,v_{k+1},...,v_n\}
\end{align*}
$$
</div>
<p>We can express \(T\) with respect \(\beta\) to get</p>
<div> 
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= \begin{pmatrix} [T(v_1)]_{\beta_1} &amp; \cdots &amp; [T(v_n)]_{\beta_1} \end{pmatrix}
\end{align*}
$$
</div>
<p>We know that \(T(v_i) \in W\) for \(i = 1,...,k\) since \(W\) is \(T\)-invariant. Therefore, we can express \(T(v_i)\) as a linear combination of just the first \(k\) vectors in \(\beta\). The rest of the coefficients will be zero for the remaining vectors in \(\beta\). So you can imagine below that for the first \(k\) columns of \([T]_{\beta}^{\beta}\), we’ll have zero entries for anything below the \(k\)th entry. Let \(B_1\) be the coefficients that we know will not be zero (at least some).</p>
<div> 
$$
\begin{align*}
[T]_{\beta}^{\beta} &amp;= 
\begin{pmatrix} 
B_1 &amp; | &amp; B_2 \\
---- &amp; - &amp; ---- \\
0 &amp; | &amp; B_3 \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>We claim that \(B_1 = [T]_{\beta_W}^{\beta_W}\). Now, we want to determine the characteristic polynomial of \([T]_{\beta}^{\beta}\).</p>
<div> 
$$
\begin{align*}
\det([T]_{\beta}^{\beta} - tI_n) &amp;= 
\begin{pmatrix} 
[T]_{\beta_W}^{\beta_W} - tI_k &amp; | &amp; B_2 \\
---- &amp; - &amp; ---- \\
0 &amp; | &amp; B_3 - tI_{n-k} \\
\end{pmatrix} \\
&amp;= \det([T]_{\beta_W}^{\beta_W} - tI_k)g(t)
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>T-cyclic Subspaces</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
For \(v \in V\) the \(T\)-cyclic subspace generated by \(v\) is \(W=span\{v, T(v), T^2(v),...\} \subset V\).
</div>
<p><br />
<br />
Observe here that \(W\) is \(T\)-invariant. Why? take any element \(w \in W\), \(T(w)\) is still in \(W\). To see this, notice that</p>
<div> 
$$
\begin{align*}
T(a_0v + a_1T(v) + ... + a_kT^k(v)) = a_0T(v) + a_1T^2(v)+...+a_kT^{k+1}(v) \in W
\end{align*}
$$
</div>
<p>Question: Are all \(T\)-invariant subspaces \(T\)-cyclic?
<br />
<br />
The answer is no. Suppose</p>
<div> 
$$
\begin{align*}
T \ &amp;: \mathbf{R}^3 \rightarrow \mathbf{R}^3  \\
 &amp;(x,y,z) \rightarrow (x,y,0)
\end{align*}
$$
</div>
<p>\(W = \{(x,y,0) \ | \ x, y \in \mathbf{R}\}\) is \(T\)-invariant. We just map it to itself. In fact \(T_W = I_W\).
<br />
<br />
So we see here that \(W\) is not \(T\)-cyclic. Take \((x, y, 0)\),</p>
<div> 
$$
\begin{align*}
span\{(x,y,0), T(x,y,0),...\} = span\{(x,y,0)\}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Theorem
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\).
Let \(W\) be a \(T\)-cyclic subspace generated by \(v\). Set \(\dim W = k \leq \dim V\). Then
<ul style="list-style-type:lower-alpha">
	<li>\(\{v,T(v),...,T^{k-1}(v)\}\) is a basis for \(W\)</li>
	<li>If \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\), then the characteristic polynomial of \(T_W\) \((-1)^{k+1}(a_0 + a_1t + ... + a_{k-1}t^{k-1} - t^k)\)</li>
</ul>
</div>
<p><br />
For (a). Since the dimension is not infinite anymore. Then it’s natural to ask if only \(k\) of the infinitely generated vectors is a span for \(W\) and the answer is yes.
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof:</b>
<br />
<br />
We’ll start with a proof of (b) given (a). To say anything about the characteristic polynomial, we need to find a basis and then compute the matrix with respect to the basis. A natural choice is the basis given to us in \(a\) so</p>
<div> 
$$
\begin{align*}
\beta_W = \{v,T(v),...,T^{k-1}(v)\}
\end{align*}
$$
</div>
<p>Next we need to compute \([T_W]_{\beta_W}^{\beta_W}\)</p>
<div> 
$$
\begin{align*}
[T_W]_{\beta_W}^{\beta_W} &amp;=
\begin{pmatrix} 
[T_W(v)]_{\beta_W} &amp; \cdots &amp; [T_W(T^{k-1}(v))]_{\beta_W}
\end{pmatrix}\\
&amp;= 
\begin{pmatrix} 
0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\
1 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1} \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>The first column is the coefficients of \(T_W(v)\) with respect to \(\beta_W\) so that’s just 1 for \(T(v)\) while the rest are 0. Similarly, we have the same thing for the rest of the \(k-1\) column vectors. But for the last column, we need to represent \(T_W(T^{k-1}(v)) = T^k(v)\). We’re given \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\) and so the rest of the coefficients are \(a_0,a_1 .... a_{k-1}\). 
<br />
<br />
Now, we can compute the determinant expanding across the first row</p>
<div> 
$$
\begin{align*}
&amp;\det([T_W]_{\beta_W}^{\beta_W} -tI_k)
= 
\begin{pmatrix} 
-t &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\
1 &amp; -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\
\end{pmatrix} \\
&amp;=
(-1)^{1+1}(-t)\det 
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
 \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\
\end{pmatrix}
+(-1)^{1+k}a_0 (1)
\end{align*}
$$
</div>
<p>The last determinant for the \(a_0\) component is 1 because that sub matrix is upper triangular so the determinant is the product of the entries on the diagonal which are all 1.
<br />
<br />
Next, we want to compute that new determinant but notice now that it has the same pattern so</p>
<div> 
$$
\begin{align*}
&amp;=
-t
\left(
\det 
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1  \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\
 \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\
\end{pmatrix}

\right)
+(-1)^{1+k}a_0 (1)
\\
&amp;=
(-t)
\left(
(-t)
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_2 \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_3 \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; \cdots &amp; \ddots &amp; -t &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\
\end{pmatrix}
+ (-1)^k a_1
\right)
+(-1)^{1+k}a_0 (1)
\\
&amp;=
(-1)^2
t^2
\begin{pmatrix}  
-t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_2 \\
1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_3 \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; \cdots &amp; \ddots &amp; -t &amp; a_{k-2} \\
 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\
\end{pmatrix}
+ (-1)^{k+1} (ta_1 + a_0)
\\
&amp;= (-1)^{k+1}(a_0 + ta_1 + ... + t^{k-1}a_{k-1} - t^k)
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Consider a linear map \(T: V \rightarrow V\). Suppose \(v\) is an eigenvector of \(T\). We know by definition that \(Tv = \lambda v\) and \(v \neq 0\). We also know that \(span\{v\} \subset V\) is subspace. The observation here is that since \(v\) is an eigenvector, then when \(T\) acts on the subspace, then it stays inside the subspace meaning $$ \begin{align*} T(span\{v\}) \subset span\{v\} \end{align*} $$ This is because \(T(cv) = cT(v) = (c\lambda) v\) which is in the span of \(v\). The span of an eigenvector is the simplest example an invariant subspace. Definition A subspace \(W \in V\) is \(T\)-invariant if \(T(W) \subseteq W\). i.e. \(T(w) \in W \ \forall w \in W\) Examples Example 1: \(W = span\{v\}\) where \(v\) is an eigenvector of \(T\). Example 2: \(I_V:V \rightarrow V\) Every subspace is \(I_V\)-invariant. Example 3: \(0_V:V \rightarrow V\) Every subspace is \(0_V\)-invariant. Example 4: $$ \begin{align*} T : \ &amp;\mathbf{R}^2 \rightarrow \mathbf{R}^2 \\ &amp;(x, y) \rightarrow (x,0) \end{align*} $$ \(W = span\{(1,0)\}\) is \(T\)-invariant \(W = span\{(1,1)\}\) is not \(T\)-invariant The Characteristic Polynomial of Invariant Subspaces So what is the point of invariant subspaces? It helps us break off pieces of our map. What does that mean? If \(T: V \rightarrow V\) and \(W\) is \(T\)-invariant, then the restriction of \(T\) to \(W\), \(T_W\) satisfies \(T_W: W \rightarrow W\). So now have a smaller set instead of the entire vector space \(V\). The following theorem describes the relationship between the characteristic polynomial of \(T:V \rightarrow V\) and the characteristic polynomial of \(T: W \rightarrow W\). Theorem Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\). If \(W\) is \(T\)-invariant, then the characteristic polynomial of \(T_W\) divides that of \(T\). Proof Let \(\beta_w = \{v_1,...,v_k\}\) be a basis of \(W\). Extend this to a basis for \(V\). so $$ \begin{align*} \beta = \{v_1,...,v_k,v_{k+1},...,v_n\} \end{align*} $$ We can express \(T\) with respect \(\beta\) to get $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= \begin{pmatrix} [T(v_1)]_{\beta_1} &amp; \cdots &amp; [T(v_n)]_{\beta_1} \end{pmatrix} \end{align*} $$ We know that \(T(v_i) \in W\) for \(i = 1,...,k\) since \(W\) is \(T\)-invariant. Therefore, we can express \(T(v_i)\) as a linear combination of just the first \(k\) vectors in \(\beta\). The rest of the coefficients will be zero for the remaining vectors in \(\beta\). So you can imagine below that for the first \(k\) columns of \([T]_{\beta}^{\beta}\), we’ll have zero entries for anything below the \(k\)th entry. Let \(B_1\) be the coefficients that we know will not be zero (at least some). $$ \begin{align*} [T]_{\beta}^{\beta} &amp;= \begin{pmatrix} B_1 &amp; | &amp; B_2 \\ ---- &amp; - &amp; ---- \\ 0 &amp; | &amp; B_3 \\ \end{pmatrix} \end{align*} $$ We claim that \(B_1 = [T]_{\beta_W}^{\beta_W}\). Now, we want to determine the characteristic polynomial of \([T]_{\beta}^{\beta}\). $$ \begin{align*} \det([T]_{\beta}^{\beta} - tI_n) &amp;= \begin{pmatrix} [T]_{\beta_W}^{\beta_W} - tI_k &amp; | &amp; B_2 \\ ---- &amp; - &amp; ---- \\ 0 &amp; | &amp; B_3 - tI_{n-k} \\ \end{pmatrix} \\ &amp;= \det([T]_{\beta_W}^{\beta_W} - tI_k)g(t) \end{align*} $$ T-cyclic Subspaces Definition For \(v \in V\) the \(T\)-cyclic subspace generated by \(v\) is \(W=span\{v, T(v), T^2(v),...\} \subset V\). Observe here that \(W\) is \(T\)-invariant. Why? take any element \(w \in W\), \(T(w)\) is still in \(W\). To see this, notice that $$ \begin{align*} T(a_0v + a_1T(v) + ... + a_kT^k(v)) = a_0T(v) + a_1T^2(v)+...+a_kT^{k+1}(v) \in W \end{align*} $$ Question: Are all \(T\)-invariant subspaces \(T\)-cyclic? The answer is no. Suppose $$ \begin{align*} T \ &amp;: \mathbf{R}^3 \rightarrow \mathbf{R}^3 \\ &amp;(x,y,z) \rightarrow (x,y,0) \end{align*} $$ \(W = \{(x,y,0) \ | \ x, y \in \mathbf{R}\}\) is \(T\)-invariant. We just map it to itself. In fact \(T_W = I_W\). So we see here that \(W\) is not \(T\)-cyclic. Take \((x, y, 0)\), $$ \begin{align*} span\{(x,y,0), T(x,y,0),...\} = span\{(x,y,0)\} \end{align*} $$ Theorem Suppose \(T: V \rightarrow V\) is linear and \(\dim(V) &lt; \infty\). Let \(W\) be a \(T\)-cyclic subspace generated by \(v\). Set \(\dim W = k \leq \dim V\). Then \(\{v,T(v),...,T^{k-1}(v)\}\) is a basis for \(W\) If \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\), then the characteristic polynomial of \(T_W\) \((-1)^{k+1}(a_0 + a_1t + ... + a_{k-1}t^{k-1} - t^k)\) For (a). Since the dimension is not infinite anymore. Then it’s natural to ask if only \(k\) of the infinitely generated vectors is a span for \(W\) and the answer is yes. Proof: We’ll start with a proof of (b) given (a). To say anything about the characteristic polynomial, we need to find a basis and then compute the matrix with respect to the basis. A natural choice is the basis given to us in \(a\) so $$ \begin{align*} \beta_W = \{v,T(v),...,T^{k-1}(v)\} \end{align*} $$ Next we need to compute \([T_W]_{\beta_W}^{\beta_W}\) $$ \begin{align*} [T_W]_{\beta_W}^{\beta_W} &amp;= \begin{pmatrix} [T_W(v)]_{\beta_W} &amp; \cdots &amp; [T_W(T^{k-1}(v))]_{\beta_W} \end{pmatrix}\\ &amp;= \begin{pmatrix} 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\ 1 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1} \\ \end{pmatrix} \end{align*} $$ The first column is the coefficients of \(T_W(v)\) with respect to \(\beta_W\) so that’s just 1 for \(T(v)\) while the rest are 0. Similarly, we have the same thing for the rest of the \(k-1\) column vectors. But for the last column, we need to represent \(T_W(T^{k-1}(v)) = T^k(v)\). We’re given \(T^k(v) = a_0v + ... + a_{k-1}T^{k-1}(v)\) and so the rest of the coefficients are \(a_0,a_1 .... a_{k-1}\). Now, we can compute the determinant expanding across the first row $$ \begin{align*} &amp;\det([T_W]_{\beta_W}^{\beta_W} -tI_k) = \begin{pmatrix} -t &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; a_0 \\ 1 &amp; -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 0 &amp; 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 1 &amp; a_{k-1}-t \\ \end{pmatrix} \\ &amp;= (-1)^{1+1}(-t)\det \begin{pmatrix} -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\ \end{pmatrix} +(-1)^{1+k}a_0 (1) \end{align*} $$ The last determinant for the \(a_0\) component is 1 because that sub matrix is upper triangular so the determinant is the product of the entries on the diagonal which are all 1. Next, we want to compute that new determinant but notice now that it has the same pattern so $$ \begin{align*} &amp;= -t \left( \det \begin{pmatrix} -t &amp; \cdots &amp; \cdots &amp; 0 &amp; a_1 \\ 1 &amp; \ddots &amp; \cdots &amp; 0 &amp; a_2 \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; a_{k-2} \\ 0 &amp; \cdots &amp; \cdots &amp; -t &amp; a_{k-1}-t \\ \end{pmatrix}]]></summary></entry></feed>