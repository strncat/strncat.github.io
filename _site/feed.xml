<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-25T21:17:58-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 9: Basis Vectors and The Replacement Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/30/theorem-1.9.html" rel="alternate" type="text/html" title="Lecture 9: Basis Vectors and The Replacement Theorem" /><published>2024-07-30T01:01:36-07:00</published><updated>2024-07-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/30/theorem-1.9</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/30/theorem-1.9.html"><![CDATA[<p>This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet.</p>
<div class="purdiv">
Theorem 1.9
</div>
<div class="purbdiv">
If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis.
</div>
<p><br />
Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases</p>
<ol>
	<li>\(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done.</li>
	<li>\(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\) </li>
</ol>
<p><br />
<br /></p>

<p>Study notes: <a href="">Here</a>is another proof from the book.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof for theorem 1.9 from the book that doesn’t assume we know the replacement theorem (1.10) yet. Theorem 1.9 If a vector space \(V\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(V\). Hence \(V\) has a finite basis. Proof: Let \(V\) be a vector space generated by a finite set \(G\). If \(G = \emptyset\) or \(G = \{0\}\), then \(V = \{0\}\) and \(\emptyset\) is a subset of \(G\) that is a basis for \(V\). Otherwise \(G\) has at least one non-zero vector \(u_1\). Let \(\beta = \{u_1\}\). We know \(\beta\) is linearly independent. Now, keep adding vectors from \(G\) to \(\beta\) while keeping \(\beta\) linearly independent. Since \(G\) is a finite set, then this process must end with a linearly independent set of vectors \(\{u_1, u_2, ..., u_k\}\). We have two cases \(\beta = G\). This means that \(G\) is linearly independent. We know \(G\) is a generating set for \(V\) and so it is a basis for \(V\) and we are done. \(\beta\) is a proper subset of \(G\) that is linearly independent. We need to show that \(\beta\) generates \(V\). From theorem 1.5 (the span of any subset \(S\) of \(V\) is a subspace of \(V\) that contains \(S\) (\(S \subseteq span(S)\)). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\)), we need to show that \(S \subseteq span(\beta)\). To do this, let \(v \in S\), we need to show that \(v \in span(\beta)\). If \(v \in \beta\), then \(v \in span(\beta)\) and we are done since. Otherwise, if \(v \not\in \beta\), then \(\{v \cup \beta\}\) must be a linearly dependent set since we know \(\beta\) is linearly independent by construction. Since adding \(v\) turned the set into a linearly dependent set, then we know that \(v\) can be written as a linear combination of the elements of \(\beta\) which means that \(v \in span(\beta)\) as we wanted to show. Therefore, \(S \subseteq span(\beta)\). \(\blacksquare\)]]></summary></entry><entry><title type="html">Lecture 12: Linear Transformations Continued</title><link href="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 12: Linear Transformations Continued" /><published>2024-07-29T01:01:36-07:00</published><updated>2024-07-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/29/lec12-more-linear-transformations.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is onto if \(R(T) = W\) or
	$$
	\begin{align*}
	 \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T: V \rightarrow W\) is 1-1 if
	$$
	\begin{align*}
	 T(v_1) = T(v_2) \Rightarrow v_1 = v_2.
	\end{align*}
	$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent 
<ul>
	<li> \(T\) is 1-1. </li>
	<li> \(T\) is onto. </li>
	<li> \(rank(T) = \dim(V)\). </li>
</ul>
</div>
<p><br />
Note here that \(rank(T) = \dim(R(T))\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Recall \(A \in M_{m \times n}\) defines a linear map</p>
<div>
	$$
	\begin{align*}
	L_A:\mathbf{R}^n \rightarrow \mathbf{R}^m, \ \bar{x} \rightarrow A\bar{x} 
	\end{align*}
	$$
</div>
<p>We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example \(T_d: P_3 \rightarrow P_2, \ f \rightarrow f'\) and \(T_i: P_2 \rightarrow P_3, \ f \rightarrow \int_0^x f(t)dt\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Coordinate Expression for a vector</b></h4>
<p>Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. 
<br />
<br />
But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every $v \in V\(,\)v$$ can be expressed uniquely in the form,</p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n
	\end{align*}
	$$
</div>
<p>The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. 
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
	$$
	\begin{align*}
	 [v]_{\beta} = 
\begin{pmatrix}
a_1 \\
.  \\
. \\
. \\
a_n
\end{pmatrix}
\in \mathbf{R}^n
	\end{align*}
	$$
is the coordinate expression for \(v\) with respect to \(\beta\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\).
<br /></p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta} &amp;= 
	 \begin{pmatrix}
	 2 \\
	 1 \\
	 \end{pmatrix} \\
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis,</p>
<div>
	$$
	\begin{align*}
	 v = (2,1) = a_1(1,1) + a_2(1,-1).
\end{align*}
	 $$
</div>
<p>From this, we get</p>
<div>
	$$
	\begin{align*}
	 a_1 + a_2 &amp;= 2 \\
	 a_1 - a_2 &amp;= 1 \\
\end{align*}
	 $$
</div>
<p>Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are</p>
<div>
	$$
	\begin{align*}
	 [v]_{\beta'} &amp;= 
	 \begin{pmatrix}
	 a_1 \\
	 a_2 \\
	 \end{pmatrix} \\
	 &amp;= 
	 \begin{pmatrix}
	 \frac{3}{2} \\
	 \frac{1}{2} \\
	 \end{pmatrix}
\end{align*}
$$
</div>
<p>So we can think of \([\quad]_{\beta'}\) as a map:</p>
<div>
	$$
	\begin{align*}
	 [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n
\end{align*}
$$
</div>
<p>The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance”
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Matrix Representation of linear transformations</b></h4>
<p>Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional.
We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix.
<br />
<br />
Let \(\beta = \{v_1, ..., v_n\}\) be a basis for \(V\) and \(\gamma = \{w_1, ..., w_n\}\). For \(v \in V\), we have</p>
<div>
	$$
	\begin{align*}
	T(v) = a_1w_1 + ... + a_mw_m.
\end{align*}
$$
</div>
<p>For \(v_j\), we have,</p>
<div>
 	$$
 	\begin{align*}
 	T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m.
 \end{align*}
 $$
 </div>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(T: V \rightarrow W\) is onto if \(R(T) = W\) or $$ \begin{align*} \forall w \in W, \exists v \in V \ \text{s.t.} \ T(v) = w. \end{align*} $$ Definition \(T: V \rightarrow W\) is 1-1 if $$ \begin{align*} T(v_1) = T(v_2) \Rightarrow v_1 = v_2. \end{align*} $$ Theorem If \(T: V \rightarrow W\) is linear, then \(T\) is 1-1 if and only if \(N(T)=\{\bar{0}_V\}\) Theorem Let \(T: V \rightarrow W\) be a linear map between vector spaces of the same finite dimension. Then the following are equivalent \(T\) is 1-1. \(T\) is onto. \(rank(T) = \dim(V)\). Note here that \(rank(T) = \dim(R(T))\). Matrix Representation of linear transformations Recall \(A \in M_{m \times n}\) defines a linear map $$ \begin{align*} L_A:\mathbf{R}^n \rightarrow \mathbf{R}^m, \ \bar{x} \rightarrow A\bar{x} \end{align*} $$ We will show that any linear map \(T: V \rightarrow W\) between finite dimensional spaces be represented by a matrix. For example \(T_d: P_3 \rightarrow P_2, \ f \rightarrow f'\) and \(T_i: P_2 \rightarrow P_3, \ f \rightarrow \int_0^x f(t)dt\). Coordinate Expression for a vector Recall that if \(\beta\) is a basis for \(V\), then for any \(v \in V\), \(v\) can be expressed uniquely as an element of \(Span(\beta)\). We proved this previously for any vector space whether infinite or finite dimensional. But if the basis is finite (\(\beta = \{v_1, ... , v_n\}\)), then for every $v \in V\(,\)v$$ can be expressed uniquely in the form, $$ \begin{align*} v = a_1v_1 + ... + a_nv_n \end{align*} $$ The coefficients \(a_1,...a_n\) are unique for \(v\). We want to think of this relationship as a map. Definition $$ \begin{align*} [v]_{\beta} = \begin{pmatrix} a_1 \\ . \\ . \\ . \\ a_n \end{pmatrix} \in \mathbf{R}^n \end{align*} $$ is the coordinate expression for \(v\) with respect to \(\beta\). Example Let \(V = \mathbf{R}^2, v = (2,1), \beta = \{(1,0), (0,1)\}\) and \(\beta' = \{(1,1), (1,-1)\}\). $$ \begin{align*} [v]_{\beta} &amp;= \begin{pmatrix} 2 \\ 1 \\ \end{pmatrix} \\ [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \end{align*} $$ These \(a_1\) and \(a_2\) are the coefficients that appear in writing \(v\) as a linear combinations of the vectors in the basis, $$ \begin{align*} v = (2,1) = a_1(1,1) + a_2(1,-1). \end{align*} $$ From this, we get $$ \begin{align*} a_1 + a_2 &amp;= 2 \\ a_1 - a_2 &amp;= 1 \\ \end{align*} $$ Of course we can use an augmented matrix and put the matrix in a reduced row echelon form to solve the system and find that the coefficients are $$ \begin{align*} [v]_{\beta'} &amp;= \begin{pmatrix} a_1 \\ a_2 \\ \end{pmatrix} \\ &amp;= \begin{pmatrix} \frac{3}{2} \\ \frac{1}{2} \\ \end{pmatrix} \end{align*} $$ So we can think of \([\quad]_{\beta'}\) as a map: $$ \begin{align*} [\quad]_{\beta'}: V \rightarrow \mathbf{R}^n \end{align*} $$ The idea is that each basis \(\beta\) of \(V\) with dimension \(\dim V = n\) gives a map that identifies \(V\) with \(\mathbf{R}^n\). This map is linear, 1-1 and onto. “Bijective Correspondance” Matrix Representation of linear transformations Consider now \(T: V \rightarrow W\) linear and both \(V\) and \(W\) are finite dimensional. We know we can identify \(V\) with \(\mathbf{R}^n\) and we can identify \(W\) with \(\mathbf{R}^m\) but now we want to represent \(T\) as a matrix. Let \(\beta = \{v_1, ..., v_n\}\) be a basis for \(V\) and \(\gamma = \{w_1, ..., w_n\}\). For \(v \in V\), we have $$ \begin{align*} T(v) = a_1w_1 + ... + a_mw_m. \end{align*} $$ For \(v_j\), we have, $$ \begin{align*} T(v_j) = a_{1j}w_1 + ... + a_{mj}w_m. \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 11: Null Space, Range, and Dimension Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem.html" rel="alternate" type="text/html" title="Lecture 11: Null Space, Range, and Dimension Theorem" /><published>2024-07-28T01:01:36-07:00</published><updated>2024-07-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/28/lec11-null-space-range-and-dimension-theorem.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The null space (kernel) of \(T\) is 
	$$
	\begin{align*}
	 N(T) = \{v \in V \ | \ T(v) = \bar{0}_W\} \subset V
	\end{align*}
	$$
The range (image) of \(T\) is
	$$
	\begin{align*}
	 R(T) = \{T(v) \ | v \in V \} \subset W
	\end{align*}
	$$
</div>
<p><br />
Notes: So the null space is any vector that the transformation turns it into the zero vector. The range just like the definition is the set of the images of all the vectors in \(V\). If all the vectors have an image, then the range is equal to the codomain and that’s when the transformation is onto.
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>\(N(T^b_a) = \{\)functions in \(C^0(\mathbf{R})\) whos average over \([a,b]\) is \(0\}\). In other words, it’s the set of all functions whose definite integral is 0. 
<br />
<br />
\(R(T^b_a) = \mathbf{R}\). Here we want all functions whose definite integral is some constant/some real number. In other words, this will be the entire codomain (\(\mathbf{R}\)). We call this map \((T^b_a)\) onto.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \( \ T: V \rightarrow W\) is linear then \(N(T)\) is a subspace of \(V\) and \(R(T)\) is a subspace of \(W\).
</div>
<p><br />
Proof:
<br />
We need to verify the three subspaces properties. To prove that \(R(T)\) is a subspace:</p>
<ul>
<li>We need \(\bar{0}_W \in R(T)\). So we need the zero vector to be in the range. This means that we need a \(v \in V\) such that \(T(v) = \bar{0}_W\). Since \(T\) is linear then \(T(\bar{0}_v) = \bar{0}_W\). (We proved this in the previous lecture) </li>
<li>We need \(R(T)\) to be closed under addition. Suppose \(w_1, w_2 \in R(T)\), then \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1\) and \(v_2\). Therefore,
	<div>
		$$
		\begin{align*}
		 w_1 + w_2 &amp;= T(v_1) + T(v_2) \\
		           &amp;= T(v_1 + v_2) \ \text{because $$T$$ is linear}
		\end{align*}
		$$
	</div>
By definition of a vector space \(v_1 + v_2 \in V\), so the image of the sum \(w_1 + w_2 \in R(T)\) as required.
</li>
<li> We need \(R(T)\) to be closed under multiplication. Suppose \(w_1 \in R(T)\) and \(c\) is a scalar. We need \(cw_1 \in R(T)\). Since \(w_1 \in R(T)\), then there must be a vector \(v_1 \in V\) such that \(w_1 = T(v_1)\). Notice now,
	<div>
		$$
		\begin{align*}
		 w_1 &amp;= T(v_1) \\
		 cw_1 &amp;= cT(v_1) \\
         &amp;= T(cv_1) \ \text{because $$T$$ is linear}
		\end{align*}
		$$
	</div>
By definition of a vector space \(cv_1 \in V\), so the image of the product \(cv_1 \in R(T)\) as required.

</li>
</ul>
<p>From this, we conclude that the \(R(T)\) is a subspace of \(W\).
Proving that \(N(T)\) is a subspace is an exercise.
<br />
<br /></p>
<div class="purdiv">
Theorem (Dimension Theorem)
</div>
<div class="purbdiv">
If \( \ T: V \rightarrow W\) is linear and \(V\) is finite dimensional, then
		$$
		\begin{align*}
		 \dim(N(T)) + \dim(R(T)) = \dim(V).
		\end{align*}
		$$
</div>
<p><br />
We know that \(N(T)\) is a subspace of \(V\). This means that \(\dim(N(T)) \leq \dim(V)\). This theorem tells us that the difference \(\dim(V) - \dim(N(T))\) is the dimension of \(R(T)\). Even if \(W\) is an infinite dimensional space, we know from linearity, it is finite dimensional. Typically, \(\dim(R(T))\) is called the rank of \(T\) and \(\dim(N(T))\) is called the nullity of \(T\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Consider the map \(T: \mathbf{R}^n \rightarrow \mathbf{R}^n\) where</p>
<div>
	$$
	\begin{align*}
	(x_1,...,x_n) \rightarrow (x_1,...,x_m,0,...,0) \quad (m &lt; n)
	\end{align*}
	$$
</div>
<p>The null space is the set of vectors where their images are the zero vector. This means that given some vector \(v=(x_1,...,x_n)\), we want all of \((x_1,...x_m)\) to be zero since we already know anything after \(m\) is zero by the definition of the map. Therefore,</p>
<div>
	$$
	\begin{align*}
	N(T) = \{(x_1,...,x_n) \ | \ x_1=0,...,x_m=0\}
	\end{align*}
	$$
</div>
<p>Therefore, \(\dim(N(T)) = n - m\). For the range of \(T\),</p>
<div>
	$$
	\begin{align*}
	R(T) &amp;= \{(x_1,...,x_m,0,...,0)\} \\
	 \dim(R(T)) &amp;= m
	\end{align*}
	$$
</div>
<p>From this we see that,</p>
<div>
	$$
	\begin{align*}
	 \dim(R(T)) + \dim(N(T)) &amp;= m + n - m \\ 
	                        &amp;= n \\ 
	                        &amp;= \dim(\mathbf{R}^n)
	\end{align*}
	$$
</div>
<h4><b>Proof</b></h4>
<p>Let \(T: V \rightarrow W\) be a linear transformation and \(V\) be finite dimensional. Let \(\dim(V) = n\). Let \(\dim(N(T) = k\). We know that \(N(T)\) is a subspace of \(V\). Therefore, \(k \leq n\). Since \(\dim(N(T)) = k\), this means that any basis of the null space will have \(k\) elements. So let \(\beta_N = \{u_1,...,u_k\}\) be a basis for \(N(T)\).
<br />
<br />
Claim 1: We can extend the basis \(\beta_N\) by \(n-k\) vectors to form a basis of \(V\). To see this, let \(\beta\) be a basis for \(V\). We will use the replacement theorem, setting \(\mathcal{S} = \beta\) and \(\mathcal{U} = \beta_N\). (TODO: How do we know that \(\beta_N\) is linearly independent?) So we can add \(n-k\) elements of \(\beta\) to \(\beta_N\) so the resulting set generates \(V\). Let this resulting set be \(\beta_V\).</p>
<div>
	$$
	\begin{align*}
	Span(\beta_V) &amp;= Span(\{u_1,...,u_k,v_1,...v_{n-k}\}) 
	              &amp;= V.
	\end{align*}
	$$
</div>
<p>We claim that \(\beta_V\) is a basis for \(V\). How do we know this? We know that it spans \(V\). Moreover, it has \(k+n-k = n\) elements which is the dimension of \(V\). To see that the vectors are linearly independent, suppose that it’s not. Since \(\beta_V\) generates \(V\), then by the refinement theorem, we can take away an element and still have a span that generates \(V\). But then if do take an element out, this means that we’ll have \(n-1\) elements in \(\beta_V\). This is a contradiction since we need at least \(n\) elements to span \(V\).
<br />
<br />
The second claim is that \(\{T(v_1),...,T(v_{n-k})\}\) is a basis for \(R(T)\). If we prove this, then we’ll be done since,</p>
<div>
	$$
	\begin{align*}
	 \dim(N(T)) + \dim(R(T)) &amp;= k + (n-k) \\ 
	                        &amp;= n \\ 
	                        &amp;= \dim(V).
	\end{align*}
	$$
</div>
<p>So now we need to prove that \(\{T(v_1),...,T(v_{n-k})\}\) is a basis. To do so, we need to prove that it generates \(R(T)\) and so \(Span(\{T(v_1),...,T(v_{n-k})\}) = R(T)\). Additionally, we need to prove that the vectors are linearly independent. To see that it generates \((R)\), we know that \(R(T) = \{T(v) \ | \ v \in V\}\). Furthermore, we know that any vector \(v \in V\) an be written as a linear combinations in terms of the elements in \(\beta_V\) and we know that \(T\) is linear. So now we can re-write the definition as,</p>
<div>
	$$
	\begin{align*}
	R(T) &amp;= \{T(v) \ | \ v \in V\} \\
	     &amp;= \{T(a_1u_1 + ... + a_ku_k + b_1v_1 + ... + b_{n-k}v_{n-k}) \ | \ a_1,...,a_k \in \mathbf{R}, b_1,...b_{n-k}\in \mathbf{R} \}. \\
	     &amp;= \{a_1T(u_1) + ... + a_kT(u_k) + b_1T(v_1) + ... + b_{n-k}T(v_{n-k}) \} \\
		 &amp;= \{b_1T(v_1) + ... + b_{n-k}T(v_{n-k})\} \quad \text{because $u_1,...u_k$ are in the null space ($T(u_i)=0_W$)} \\
		 &amp;= Span(\{T(v_1),...,T(v_{n-k})\}).
	\end{align*}
	$$
</div>
<p>Next, to see that the set is linearly independent, we need to prove that the following equation</p>
<div>
	$$
	\begin{align*}
	 a_1T(v_1) + ... + a_{n-k}T(v_{n-k}) = \bar{0}_W.
	\end{align*}
	$$
</div>
<p>implies that \(a_1 = 0, ..., a_{n-k} = 0\). To see this, we’ll use the linearity of \(T\) where the image of a linear combination is the same as the linear combination of the images to get,</p>
<div>
	$$
	\begin{align*}
	 a_1T(v_1) + ... + a_{n-k}T(v_{n-k}) &amp;= \bar{0}_W \\
	 aT(a_1v_1 + ... + a_{n-k}v_{n-k})  &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p>If this equation holds, this means that \(a_1v_1 + ... + a_{n-k}v_{n-k}\) is in the null space \(N(T)\) by definition since it’s image is the zero vector. We defined previously \(\beta_N\) to be the basis for the null space. This means that</p>
<div>
	$$
	\begin{align*}
	 \{a_1v_1 + ... + a_{n-k}v_{n-k} &amp;\in Span(\{u_1,...,u_k\}) \\
	 &amp;= Span(\beta_N).
	\end{align*}
	$$
</div>
<p>We claim that this must imply that \(a_1 = 0, ..., a_{n-k} = 0\). Why? Suppose not, then the linear combination \(a_1v_1 + ... + a_{n-k}v_{n-k}\) would be a linear combination of the elements \(u_1,...,u_k\). But this mean that we’ll have a linear combination of all the \(v\) vectors and all the \(u\) vectors equal to the zero vector,</p>
<div>
	$$
	\begin{align*}
	 a_1v_1 + ... + a_{n-k}v_{n-k} &amp;= b_1u_1 + ... + b_ku_k 
	  a_1v_1 + ... + a_{n-k}v_{n-k} - b_1u_1 + ... + b_ku_k &amp;= 0
	\end{align*}
	$$
</div>
<p>But all the \(u\) and \(v\) vectors \(\{u_1,...,u_k,v_1,...,v_{n-k}\}\) are part of a basis, \(\beta_V\). So any linear combination of these vectors equaling the zero vector must imply that the coefficients are zero because otherwise they are linearly dependent and this is a contradiction. \(\blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The null space (kernel) of \(T\) is $$ \begin{align*} N(T) = \{v \in V \ | \ T(v) = \bar{0}_W\} \subset V \end{align*} $$ The range (image) of \(T\) is $$ \begin{align*} R(T) = \{T(v) \ | v \in V \} \subset W \end{align*} $$ Notes: So the null space is any vector that the transformation turns it into the zero vector. The range just like the definition is the set of the images of all the vectors in \(V\). If all the vectors have an image, then the range is equal to the codomain and that’s when the transformation is onto. Example \(N(T^b_a) = \{\)functions in \(C^0(\mathbf{R})\) whos average over \([a,b]\) is \(0\}\). In other words, it’s the set of all functions whose definite integral is 0. \(R(T^b_a) = \mathbf{R}\). Here we want all functions whose definite integral is some constant/some real number. In other words, this will be the entire codomain (\(\mathbf{R}\)). We call this map \((T^b_a)\) onto. Theorem If \( \ T: V \rightarrow W\) is linear then \(N(T)\) is a subspace of \(V\) and \(R(T)\) is a subspace of \(W\). Proof: We need to verify the three subspaces properties. To prove that \(R(T)\) is a subspace: We need \(\bar{0}_W \in R(T)\). So we need the zero vector to be in the range. This means that we need a \(v \in V\) such that \(T(v) = \bar{0}_W\). Since \(T\) is linear then \(T(\bar{0}_v) = \bar{0}_W\). (We proved this in the previous lecture) We need \(R(T)\) to be closed under addition. Suppose \(w_1, w_2 \in R(T)\), then \(w_1 = T(v_1)\) and \(w_2 = T(v_2)\) for some \(v_1\) and \(v_2\). Therefore, $$ \begin{align*} w_1 + w_2 &amp;= T(v_1) + T(v_2) \\ &amp;= T(v_1 + v_2) \ \text{because $$T$$ is linear} \end{align*} $$ By definition of a vector space \(v_1 + v_2 \in V\), so the image of the sum \(w_1 + w_2 \in R(T)\) as required. We need \(R(T)\) to be closed under multiplication. Suppose \(w_1 \in R(T)\) and \(c\) is a scalar. We need \(cw_1 \in R(T)\). Since \(w_1 \in R(T)\), then there must be a vector \(v_1 \in V\) such that \(w_1 = T(v_1)\). Notice now, $$ \begin{align*} w_1 &amp;= T(v_1) \\ cw_1 &amp;= cT(v_1) \\ &amp;= T(cv_1) \ \text{because $$T$$ is linear} \end{align*} $$ By definition of a vector space \(cv_1 \in V\), so the image of the product \(cv_1 \in R(T)\) as required.]]></summary></entry><entry><title type="html">Lecture 10: Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations.html" rel="alternate" type="text/html" title="Lecture 10: Linear Transformations" /><published>2024-07-27T01:01:36-07:00</published><updated>2024-07-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/27/lec10-linear-transformations.html"><![CDATA[<div class="purdiv">
Definition
</div>
<div class="purbdiv">
A map \(T\) from a vector space \(V\) to a vector space \(W\), \(T: V \rightarrow W\) is linear for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\) if
<ol>
	<li>\(T(v_1+v_2) = T(v_1) + T(v_2))\)</li>
	<li>\(T(cv_1) = cT(v_1)\)</li>
</ol>
</div>
<p><br />
Remark: The two conditions can be combined together and so \(T: V \rightarrow W\) is linear above if and only if \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\) for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\).
<br />
<br />
Proof: 
<br />
\(\Rightarrow\): Assume \(T\) is linear. Then,</p>
<div>
	$$
	\begin{align*}
	 T(v_1 + cv_2) &amp;= T(v_1 + cv_2) \\ 
	              &amp;= T(v_1) + T(cv_2) \text{ (By property (1))} \\
				  &amp;= T(v_1) + cT(v_2) \text{ (By property (2))}
	\end{align*}
	$$
</div>
<p>\(\Leftarrow\): Assume \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). Then to show that property (1) is true, notice that</p>
<div>
	$$
	\begin{align*}
	 T(v_1 + v_2) &amp;= T(v_1 + (1)v_2) \\
	              &amp;= T(v_1) + (1)T(v_2) \\
				  &amp;= T(v_1) + T(v_2).
	\end{align*}
	$$
</div>
<p>And to see that property (2) is true, notice that</p>
<div>
	$$
	\begin{align*}
	 T(cv_1) &amp;= T(bar{0}_V + cv_1) \\
	         &amp;= T(\bar{0}_V) + cT(v_1)
	\end{align*}
	$$
</div>
<p>To finish the proof we want to additionally show that \(T(\bar{0}_V) = \bar{0}_W\). How do we do this? We can only use the assumption that \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). To do this notice that,</p>
<div>
	$$
	\begin{align*}
	 \bar{0}_W + T(\bar{0}_V) &amp;= T(\bar{0}_V) \text{ (We're just adding the zero vector)} \\
	         &amp;= T(\bar{0}_V + (1)\bar{0}_V) \text{ (using (c))} \\
			 &amp;= T(\bar{0}_V) + (1)T(\bar{0}_V) \text{ (using (c))} \\
			 &amp;= T(\bar{0}_V) + T(\bar{0}_V) \\
	\bar{0}_W &amp;= T(\bar{0}_V) 
			 
	\end{align*}
	$$
</div>
<p>Another way to do this is the following</p>
<div>
	$$
	\begin{align*}
	 T(\bar{0}_V) &amp;= T(v_1 - v_1) \\
	         &amp;= T(v_1 + (-1)v_1) \\
			 &amp;= T(v_1) + (-1)T(v_1) \\
			 &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p><br />
Remark 2: Suppose we have a linear transformation \(T: V \rightarrow W\)</p>
<div>
	$$
	\begin{align*}
	 T(a_1u_1 + ... + a_ku_k) &amp;= T(a_1u_1 + ... + a_{k-1}u_{k-1}) + a_kT(u_k) \text{ Using property (3)}\\
	 &amp;= a_1T(u_1) + ... + a_kT(u_k)
	\end{align*}
	$$
</div>
<p><br />
This is crucial because this says that the image of a linear combination with coefficients \(a_1, ... a_k\) is again a linear combination in the new vector space with coefficients \(a_1,...a_k\) except that it’s a linear combination of the image of the original vectors.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>For \(V, W\), the map \(T_0: V \rightarrow W, u \rightarrow \bar{0}_W\) is linear.
<br />
<br />
We need to verify that it is linear by verifying \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). This is easy because for any vectors \(v_1, v_2\),</p>
<div>
	$$
	\begin{align*}
	 T_0(v_1 + cv_2) &amp;= \bar{0}_W.
	 \end{align*}
	$$
</div>
<p>Moreover, we also have</p>
<div>
	$$
	\begin{align*}
	 T_0(v_1) + T_0(cv_2) &amp;= \bar{0}_W + c\bar{0}_W \\
	                      &amp;= \bar{0}_W
	\end{align*}
	$$
</div>
<p>The two sides are equal and so \(T_0\) is linear.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The map \(I_V: V \rightarrow V, u \rightarrow u\) is linear as well.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>The map \(T: \mathbf{R}^2 \rightarrow \mathbf{R}^2, (x,y) \rightarrow (-y,x)\) is linear as well. (a rotation by 90 degrees, todo: add pic). To see why it’s linear, notice that</p>
<div>
	$$
	\begin{align*}
	 T((x_1,y_1) + c(x_2,y_2)) &amp;= T((x_1 + cx_2, y_1 + cy_2)) \text{ (add the two vectors to get one vector)}\\ 
	 &amp;= (-y_1-cy_2, x_1+cx_2).
	 \end{align*}
	$$
</div>
<p>Moreover, notice that</p>
<div>
	$$
	\begin{align*}
	 T((x_1,y_1)) + T((cx_2, cy_2)) &amp;= (-y_1, x_1) + (-cy_2, cx_2) \\
	                                &amp;= (-y_1-cy_2, x_1 + cx_2).
	\end{align*}
	$$
</div>
<p>Both sides are equal and so the transformation is linear.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>The map \(T: P \rightarrow P, f(x) \rightarrow f'(x)\) is linear. Note here that the map The map \(T (P_n \rightarrow P_{n-1}): f(x) \rightarrow f'(x)\) is different because the domain and codomain are different here! this is crucial. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>Let \(A \in M_{m \times n}\). The map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\) is linear. Remember here \(A\bar{x}\) are the linear combinations of the column vectors of \(A\) with the coefficients being the entries of \(\bar{x}\). The crucial thing here is that if \(V\) and \(W\) are both finite dimensional can be represented with this kind of transformation (matrix). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6</b></h4>
<p>For \(a &lt; b\), define the map \(T_a^b: C^0(\mathbf{R}) \rightarrow \mathbf{R}, f \rightarrow \int_a^b f(x)dx\) (\(C^0\) is the set of continuous functions on \(\mathbf{R})\)). 
<br />
<br />
Recall the dimension of \(\mathbf{R}\) is 1 and the dimension of \(C^0\) is infinte because the set of all polynomials (which has dimension infinity) is a subset of the set of continuous functions. Therefore, the set of continuous function has dimension infinity as well. This mapping goes from an infinite dimensional space to a finite dimensional space.
<br />
<br />
To prove that this mapping in linear, we notice that</p>
<div>
	$$
	\begin{align*}
	 T_a^b(f + cg) &amp;= \int_a^b (f(x) + cg(x))dx \\
	          &amp;= \int_a^b f(x)dx + c \int_a^b g(x)dx \ \text{ (By Calculus)} \\
	 &amp;= T_a^b(f) + cT_a^b(g).
	\end{align*}
	$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A map \(T\) from a vector space \(V\) to a vector space \(W\), \(T: V \rightarrow W\) is linear for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\) if \(T(v_1+v_2) = T(v_1) + T(v_2))\) \(T(cv_1) = cT(v_1)\) Remark: The two conditions can be combined together and so \(T: V \rightarrow W\) is linear above if and only if \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\) for all \(v_1, v_2 \in V\) and \(c \in \mathbf{R}\). Proof: \(\Rightarrow\): Assume \(T\) is linear. Then, $$ \begin{align*} T(v_1 + cv_2) &amp;= T(v_1 + cv_2) \\ &amp;= T(v_1) + T(cv_2) \text{ (By property (1))} \\ &amp;= T(v_1) + cT(v_2) \text{ (By property (2))} \end{align*} $$ \(\Leftarrow\): Assume \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). Then to show that property (1) is true, notice that $$ \begin{align*} T(v_1 + v_2) &amp;= T(v_1 + (1)v_2) \\ &amp;= T(v_1) + (1)T(v_2) \\ &amp;= T(v_1) + T(v_2). \end{align*} $$ And to see that property (2) is true, notice that $$ \begin{align*} T(cv_1) &amp;= T(bar{0}_V + cv_1) \\ &amp;= T(\bar{0}_V) + cT(v_1) \end{align*} $$ To finish the proof we want to additionally show that \(T(\bar{0}_V) = \bar{0}_W\). How do we do this? We can only use the assumption that \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). To do this notice that, $$ \begin{align*} \bar{0}_W + T(\bar{0}_V) &amp;= T(\bar{0}_V) \text{ (We're just adding the zero vector)} \\ &amp;= T(\bar{0}_V + (1)\bar{0}_V) \text{ (using (c))} \\ &amp;= T(\bar{0}_V) + (1)T(\bar{0}_V) \text{ (using (c))} \\ &amp;= T(\bar{0}_V) + T(\bar{0}_V) \\ \bar{0}_W &amp;= T(\bar{0}_V) \end{align*} $$ Another way to do this is the following $$ \begin{align*} T(\bar{0}_V) &amp;= T(v_1 - v_1) \\ &amp;= T(v_1 + (-1)v_1) \\ &amp;= T(v_1) + (-1)T(v_1) \\ &amp;= \bar{0}_W \end{align*} $$ Remark 2: Suppose we have a linear transformation \(T: V \rightarrow W\) $$ \begin{align*} T(a_1u_1 + ... + a_ku_k) &amp;= T(a_1u_1 + ... + a_{k-1}u_{k-1}) + a_kT(u_k) \text{ Using property (3)}\\ &amp;= a_1T(u_1) + ... + a_kT(u_k) \end{align*} $$ This is crucial because this says that the image of a linear combination with coefficients \(a_1, ... a_k\) is again a linear combination in the new vector space with coefficients \(a_1,...a_k\) except that it’s a linear combination of the image of the original vectors. Example 1 For \(V, W\), the map \(T_0: V \rightarrow W, u \rightarrow \bar{0}_W\) is linear. We need to verify that it is linear by verifying \(T(v_1 + cv_2) = T(v_1) + cT(v_2)\). This is easy because for any vectors \(v_1, v_2\), $$ \begin{align*} T_0(v_1 + cv_2) &amp;= \bar{0}_W. \end{align*} $$ Moreover, we also have $$ \begin{align*} T_0(v_1) + T_0(cv_2) &amp;= \bar{0}_W + c\bar{0}_W \\ &amp;= \bar{0}_W \end{align*} $$ The two sides are equal and so \(T_0\) is linear. Example 2 The map \(I_V: V \rightarrow V, u \rightarrow u\) is linear as well. Example 3 The map \(T: \mathbf{R}^2 \rightarrow \mathbf{R}^2, (x,y) \rightarrow (-y,x)\) is linear as well. (a rotation by 90 degrees, todo: add pic). To see why it’s linear, notice that $$ \begin{align*} T((x_1,y_1) + c(x_2,y_2)) &amp;= T((x_1 + cx_2, y_1 + cy_2)) \text{ (add the two vectors to get one vector)}\\ &amp;= (-y_1-cy_2, x_1+cx_2). \end{align*} $$ Moreover, notice that $$ \begin{align*} T((x_1,y_1)) + T((cx_2, cy_2)) &amp;= (-y_1, x_1) + (-cy_2, cx_2) \\ &amp;= (-y_1-cy_2, x_1 + cx_2). \end{align*} $$ Both sides are equal and so the transformation is linear. Example 4 The map \(T: P \rightarrow P, f(x) \rightarrow f'(x)\) is linear. Note here that the map The map \(T (P_n \rightarrow P_{n-1}): f(x) \rightarrow f'(x)\) is different because the domain and codomain are different here! this is crucial. Example 6 Let \(A \in M_{m \times n}\). The map \(L_A: \mathbf{R}^n \rightarrow \mathbf{R}^m, \bar{x} \rightarrow A\bar{x}\) is linear. Remember here \(A\bar{x}\) are the linear combinations of the column vectors of \(A\) with the coefficients being the entries of \(\bar{x}\). The crucial thing here is that if \(V\) and \(W\) are both finite dimensional can be represented with this kind of transformation (matrix). Example 6 For \(a &lt; b\), define the map \(T_a^b: C^0(\mathbf{R}) \rightarrow \mathbf{R}, f \rightarrow \int_a^b f(x)dx\) (\(C^0\) is the set of continuous functions on \(\mathbf{R})\)). Recall the dimension of \(\mathbf{R}\) is 1 and the dimension of \(C^0\) is infinte because the set of all polynomials (which has dimension infinity) is a subset of the set of continuous functions. Therefore, the set of continuous function has dimension infinity as well. This mapping goes from an infinite dimensional space to a finite dimensional space. To prove that this mapping in linear, we notice that $$ \begin{align*} T_a^b(f + cg) &amp;= \int_a^b (f(x) + cg(x))dx \\ &amp;= \int_a^b f(x)dx + c \int_a^b g(x)dx \ \text{ (By Calculus)} \\ &amp;= T_a^b(f) + cT_a^b(g). \end{align*} $$ References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 9: Basis Vectors and The Replacement Theorem</title><link href="http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem.html" rel="alternate" type="text/html" title="Lecture 9: Basis Vectors and The Replacement Theorem" /><published>2024-07-26T01:01:36-07:00</published><updated>2024-07-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/26/lec09-basis-vectors-replacement-theorem.html"><![CDATA[<p>From the previous lecture, we know that a subset \(\beta \subset V\) is a basis if \(\beta\) is linearly independent and if \(Span(\beta) = V\). Moreover, we learned that for any vector \(u \in V\), we can uniquely express \(u\) interms of the elements in \(\beta\). In this lecture, we’ll start with some standard examples of basis.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>In \(\mathbf{R}^n\), let</p>
<div>
	$$
	\begin{align*}
	 e_1 = (1,0,...,0), e_2=(0,1,0,...,0),...,e_n=(0,...,1)
	\end{align*}
	$$
</div>
<p>\(\beta = \{e_1,e_2,...,e_n\}\) is the standard basis of \(\mathbf{R}^n\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>In the vector space of polynomials of degree at most \(n\) (\(P_n\)), the standard basis is \(\beta = \{1, x, x^2, ..., x^n\}\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Recall the space of all sequences, \(V = \{\{a_n\}\}\) where \(\{a_n\}\) is a sequence. Let \(e_j\) be the sequence</p>
<div>
	$$
	\begin{align*}
	 0,0,...,0,1,0,0....,0,...
	\end{align*}
	$$
</div>
<p>Where the \(j\)th term is the term 1 above. Then, the standard basis is \(\beta = \{e_1, e_2, ....\}\). This basis has infinitely many terms unlike the previous two examples.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>The vector space of all polynomials (\(P\)). The standard basis is \(\beta = \{1, x, x^2, x^3, ...\}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>\(\mathcal{F}(\mathbf{R})\) has a basis … hard to describe but it exists!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.9
</div>
<div class="purbdiv">
If \(V\) has a finite generating set, then \(V\) has a finite basis.
</div>
<p><br />
Proof: This follows from the Refinement Theorem. If \(\{u_1,...,u_k\}\) is a finite generating set, then we can find a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and has span \(Span(\{u_{i1},...,u_{ij}\}) = V\).
<br />
<br />
Study notes: <a href="">Here</a>is another proof from the book.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1.10 (Replacement Theorem)
</div>
<div class="purbdiv">
Suppose \(\mathcal{S} = \{s_1,...,s_n\}\) generates \(V\). If \(\ \mathcal{U} = \{u_1,...,u_k\}\) is a linearly independent subset of \(V\), then \(k \leq n\) and there is a subset \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) such that \(Span(\mathcal{U} \cup \mathcal{T}) = V\).
</div>
<p><br />
Notes: So here, \(\mathcal{U}\) is a linearly independent subset of \(V\). But this doesn’t mean that it’s a basis because it might need some additional vectors added to it. If we know another set \(S\) that generates \(V\), then there is a subset \(\mathcal{T} \subset \mathcal{S}\) such that the span of both \(\mathcal{T}\) and \(\mathcal{U}\) will generate \(V\).
<br />
<br />
<b>Proof</b>: By induction on \(k\).
<br />
Base case: \(k = 0\). This means that \(\mathcal{U} = \emptyset\). The empty set is linearly independent and \(k \leq n\). Also, \(n - k = n\) and We can take \(\mathcal{T} = \mathcal{S}\). We know that \(\mathcal{S}\) generates \(V\), so \(\mathcal{U} \cup \mathcal{T}\) generates \(V\) as required.
<br /><br />
Inductive Step: Assume that the theorem is true for \(j\). We need to show that it’s true for \(j+1\). So suppose \(\mathcal{U}_{j+1} = \{u_1, ..., u_{j+1}\}\) is linearly independent. Specifically, we need to show that:</p>
<ul>
	<li>\(j + 1 \leq n\)</li>
	<li>There exists a subset \(\mathcal{T}_{j+1} \in \mathcal{S}\) of size \(n - (j+1)\) such that \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\).</li>
</ul>
<p>Throw the \(j+1\)th element away. So now we have \(\mathcal{U}_j = \{u_1,...,u_j\}\) which is linearly independent by theorem (from the book: If \(S_1 \subseteq S_2\) and \(S_2\) is linearly independent then, \(S_1\) must be linearly independent). 
<br />
By the inductive hypothesis, we know,</p>
<ul>
	<li>\(j \leq n\)</li>
	<li>There exists a subset \(\mathcal{T}_{j} = \{s_1,...,s_{n-j}\}\) has \(n-j\) elements such that \(Span(\{u_1,...,u_j,s_1,...,s_{n-j}\}) = Span(\mathcal{U}_{j} \cup \mathcal{T}_{j}) = V\).</li>
</ul>
<p>But we know that \(u_{j+1}\) is in \(V\). Since \(Span(\mathcal{U}_{j} \cup \mathcal{T}_{j})\) generates \(V\), then we can write \(u_{j+1}\) in terms of the elements in the span.</p>
<div>
	$$
	\begin{align*}
	 u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}.
	\end{align*}
	$$
</div>
<p>Note here that \(b_1,...b_{n-j}\) can’t be all zeros (because if they are, then \(u_{j+1}\) can be written as a linear combination of the elements of \(\mathcal{U}_{j}\) alone. But that’s not possible since we said that the set \(\mathcal{U}_{j+1}\) is linearly independent.). Without the loss of generality, let \(b_{n-j}\) be non-zero. This means that,</p>
<div>
	$$
	\begin{align*}
	 n - j &amp;\geq 1 \\
	 n &amp;\geq j+1
	\end{align*}
	$$
</div>
<p>as desired. Now, we need to satisfy the second condition and find a subset of \(n-(j+1)\) elements such that the span of \(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}\) generates \(V\). We can’t choose the subset \(\mathcal{T}_{j}\) since it has \(n-j\) elements and we need \(n-j-1\) elements. So the strategy is to remove one element from \(\mathcal{T}_{j}\). 
<br />
<br />
Since we said earlier that \(b_{n-j}\) is not zero along with the inductive hypothesis, \(u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}\), we can re-write this as,</p>
<div>
	$$
	\begin{align*}
	 s_{n-j} = -\frac{1}{b_{n-j}} (a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}+u_{j+1}).
	\end{align*}
	$$
</div>
<p>basically as a linear combination of the other elements from the inductive hypothesis equation. This implies that \(s_{n-j} \in Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\). Let \(\mathcal{T}_{j+1} = s_1,...,s_{n-j-1}\). The last thing to prove is that (\(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1})) = V\). We will do this in two steps. First we will prove that,</p>
<div>
	$$
	\begin{align*}
	 Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}).
	\end{align*}
	$$
</div>
<p>These spans are equal which means that adding \(s_{n-j}\) to the span, didn’t increase the span. This is because we showed earlier that \(s_{n-j}\) is a linear combination of all the other elements \(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}\). Moreover,</p>
<div>
	$$
	\begin{align*}
	Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}).
	\end{align*}
	$$
</div>
<p>This is true because we also showed that \(u_{j+1}\) is a linear combinations of the elements \(\{u_1,...u_{j},s_1,...,s_{n-j}\}\). Therefore from the previous two equations, we can conclude that</p>
<div>
	$$
	\begin{align*}
	Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}).
	\end{align*}
	$$
</div>
<p>But we know that \(Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}) = V\) from the inductive hypothesis. Therefore, \(Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\) also generates \(V\) and so \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) has a finite basis, then any basis of \(V\) has the same number of elements.
</div>
<p><br />
Proof: Let \(\beta\) be a finite basis with \(n\) elements. Let \(\bar{\beta}\) be another basis. We claim that \(\bar{\beta}\) is finite. Suppose for the sake of contradiction that it wasn’t, then \(\bar{\beta}\) contains a set \(\bar{U}\) that contains at least \(n+1\) linearly independent vectors. Apply the Replacement Theorem with \(\mathcal{S} = \beta, \mathcal{U} = \bar{U}\). This means that if the number of elements in \(\bar{U}\) is \(k\) then \(k\) must be less than the number of elements in \(S\). But \(S\) has \(n\) elements and so \(n+1 \leq n\) is a contradiction. Therefore, \(\bar{beta}\) must be finite.
<br />
<br />
To prove that it must have size \(n\), apply the Replacement Theorem again with \((\mathcal{S} = \beta, \mathcal{U} = \bar{\beta})\). We know the size of \(\bar{\beta}\) must be less than or equal to \(n\),</p>
<div>
	$$
	\begin{align*}
	|\bar{\beta}| \leq n.
	\end{align*}
	$$
</div>
<p>But if we apply the replacement theorem with \((\mathcal{S} = \bar{\beta}, \mathcal{U} = \beta)\), then the size of \(\beta\) must be less than or equal to \(\bar{\beta}\),</p>
<div>
	$$
	\begin{align*}
	n \leq |\bar{\beta}|.
	\end{align*}
	$$
</div>
<p>From these two inequalities, we must have \(\bar{\beta} = n\). \(\blacksquare\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(V\) is finite dimensional if it has a finite basis. The number of elements in any basis for \(V\) is the dimension of \(V\). Otherwise we say \(V\) is infinite dimensional.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
<li>\(\dim(\mathbf{R}^n) = n\) </li>
<li> \(\dim(M_{m \times n}) = mn\) </li>
One basis for this space is \(\{E^{ij} \in M_{m \times n} \ | \ a_{ij} = 1, \text{ all other elements are 0}\}\). So We'll have \(mn\) matrices where each matrix will have a 1 in the \((i,j)\) position.<br /><br />

<li> \(\dim(P_n) = n+1\) </li>
<li> \(\dim(P) = \infty\) </li>
</ul>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(W\) be a subspace of \(V\). If \(V\) is finite dimensional, then \(\dim W \leq \dim V\), with \(\dim W = \dim V\) if and only if \(W = V\).
</div>
<p><br />
Proof:
Let \(W\) be a subspace of \(V\). We’re given that \(V\) is finite dimensional. Let \(\dim V = n\). Let \(\beta_V = \{u_1, ..., u_n\}\) be a basis for \(V\). The goal is to find a basis for \(W\) that has fewer elements than the basis of \(V\). (Note here that the strategy should not be modifying the basis for \(V\) since we don’t know if these vectors are even in \(W\). Instead we need to use another tool which is the replacement theorem.)
<br />
<br />
To find a basis for \(W\), we need a subset of \(W\) that is linearly independent and also generates \(W\). Let \(\mathcal{U} = \{w_1, ..., w_k\} \subseteq W\) be a linearly independent. Because \(\beta_V\) generates \(V\) and \(\mathcal{U}\) is a linearly independent set, we can use the Replacement Theorem by setting \(\mathcal{S} = \beta_V\) and \(U = \mathcal{U}\). This gives us the assertion that \(k \leq n\). (Note here that we don’t need to use the other result that the theorem asserts. We just need the assertion about the size).
<br />
<br />
With this observation (\(k \leq n\)), we will construct a basis for \(W\) recursively keeping the set linearly independent in the process,</p>
<ul>
	<li>If \(W = \{\bar{0}_V\}\) (\(W\) is a subspace and it must contain the zero vector), then \(\dim W = 0\) and we are done (\(\dim W \leq \dim V)\). </li>
	<li>Otherwise there is some non-zero vector so choose that vector \(w_1 \neq \bar{0}_v\)</li>
	<li>If \(W = Span(\{w_1\})\), then we stop.</li>
	<li> Otherwise, choose \(w_2\) not in \(Span(\{w_1\})\)). Note here that \(\{w_1, w_2\}\) is a linearly independent set by construction.</li>
</ul>
<p>We repeat this process, if the span is equal to \(W\), we stop. Otherwise we add a new vector if it’s not in the span of the current constructed set. This process will stop at some set \(\{w_1,...,w_k\}\) such that \(W = Span(\{w_1,...,w_k\})\). We know this set is linearly independent and that \(W = Span(\{w_1,...,w_k\})\) by construction. Therefore, it’s a basis for \(W\) and so \(\dim W = k\). By the Replacement Theorem we discussed previously we know that \(k \leq n\) and so \(\dim W \leq \dim V\). \(\blacksquare\) 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[From the previous lecture, we know that a subset \(\beta \subset V\) is a basis if \(\beta\) is linearly independent and if \(Span(\beta) = V\). Moreover, we learned that for any vector \(u \in V\), we can uniquely express \(u\) interms of the elements in \(\beta\). In this lecture, we’ll start with some standard examples of basis. Example 1 In \(\mathbf{R}^n\), let $$ \begin{align*} e_1 = (1,0,...,0), e_2=(0,1,0,...,0),...,e_n=(0,...,1) \end{align*} $$ \(\beta = \{e_1,e_2,...,e_n\}\) is the standard basis of \(\mathbf{R}^n\). Example 2 In the vector space of polynomials of degree at most \(n\) (\(P_n\)), the standard basis is \(\beta = \{1, x, x^2, ..., x^n\}\). Example 3 Recall the space of all sequences, \(V = \{\{a_n\}\}\) where \(\{a_n\}\) is a sequence. Let \(e_j\) be the sequence $$ \begin{align*} 0,0,...,0,1,0,0....,0,... \end{align*} $$ Where the \(j\)th term is the term 1 above. Then, the standard basis is \(\beta = \{e_1, e_2, ....\}\). This basis has infinitely many terms unlike the previous two examples. Example 4 The vector space of all polynomials (\(P\)). The standard basis is \(\beta = \{1, x, x^2, x^3, ...\}\). Example 5 \(\mathcal{F}(\mathbf{R})\) has a basis … hard to describe but it exists! Theorem 1.9 If \(V\) has a finite generating set, then \(V\) has a finite basis. Proof: This follows from the Refinement Theorem. If \(\{u_1,...,u_k\}\) is a finite generating set, then we can find a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and has span \(Span(\{u_{i1},...,u_{ij}\}) = V\). Study notes: Hereis another proof from the book. Theorem 1.10 (Replacement Theorem) Suppose \(\mathcal{S} = \{s_1,...,s_n\}\) generates \(V\). If \(\ \mathcal{U} = \{u_1,...,u_k\}\) is a linearly independent subset of \(V\), then \(k \leq n\) and there is a subset \(\mathcal{T} \subset \mathcal{S}\) of size \(n-k\) such that \(Span(\mathcal{U} \cup \mathcal{T}) = V\). Notes: So here, \(\mathcal{U}\) is a linearly independent subset of \(V\). But this doesn’t mean that it’s a basis because it might need some additional vectors added to it. If we know another set \(S\) that generates \(V\), then there is a subset \(\mathcal{T} \subset \mathcal{S}\) such that the span of both \(\mathcal{T}\) and \(\mathcal{U}\) will generate \(V\). Proof: By induction on \(k\). Base case: \(k = 0\). This means that \(\mathcal{U} = \emptyset\). The empty set is linearly independent and \(k \leq n\). Also, \(n - k = n\) and We can take \(\mathcal{T} = \mathcal{S}\). We know that \(\mathcal{S}\) generates \(V\), so \(\mathcal{U} \cup \mathcal{T}\) generates \(V\) as required. Inductive Step: Assume that the theorem is true for \(j\). We need to show that it’s true for \(j+1\). So suppose \(\mathcal{U}_{j+1} = \{u_1, ..., u_{j+1}\}\) is linearly independent. Specifically, we need to show that: \(j + 1 \leq n\) There exists a subset \(\mathcal{T}_{j+1} \in \mathcal{S}\) of size \(n - (j+1)\) such that \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). Throw the \(j+1\)th element away. So now we have \(\mathcal{U}_j = \{u_1,...,u_j\}\) which is linearly independent by theorem (from the book: If \(S_1 \subseteq S_2\) and \(S_2\) is linearly independent then, \(S_1\) must be linearly independent). By the inductive hypothesis, we know, \(j \leq n\) There exists a subset \(\mathcal{T}_{j} = \{s_1,...,s_{n-j}\}\) has \(n-j\) elements such that \(Span(\{u_1,...,u_j,s_1,...,s_{n-j}\}) = Span(\mathcal{U}_{j} \cup \mathcal{T}_{j}) = V\). But we know that \(u_{j+1}\) is in \(V\). Since \(Span(\mathcal{U}_{j} \cup \mathcal{T}_{j})\) generates \(V\), then we can write \(u_{j+1}\) in terms of the elements in the span. $$ \begin{align*} u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}. \end{align*} $$ Note here that \(b_1,...b_{n-j}\) can’t be all zeros (because if they are, then \(u_{j+1}\) can be written as a linear combination of the elements of \(\mathcal{U}_{j}\) alone. But that’s not possible since we said that the set \(\mathcal{U}_{j+1}\) is linearly independent.). Without the loss of generality, let \(b_{n-j}\) be non-zero. This means that, $$ \begin{align*} n - j &amp;\geq 1 \\ n &amp;\geq j+1 \end{align*} $$ as desired. Now, we need to satisfy the second condition and find a subset of \(n-(j+1)\) elements such that the span of \(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}\) generates \(V\). We can’t choose the subset \(\mathcal{T}_{j}\) since it has \(n-j\) elements and we need \(n-j-1\) elements. So the strategy is to remove one element from \(\mathcal{T}_{j}\). Since we said earlier that \(b_{n-j}\) is not zero along with the inductive hypothesis, \(u_{j+1} = a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}\), we can re-write this as, $$ \begin{align*} s_{n-j} = -\frac{1}{b_{n-j}} (a_1u_1 + ... + a_{j}u_{j}+b_1s_1+...+b_{n-j}s_{n-j}+u_{j+1}). \end{align*} $$ basically as a linear combination of the other elements from the inductive hypothesis equation. This implies that \(s_{n-j} \in Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\). Let \(\mathcal{T}_{j+1} = s_1,...,s_{n-j-1}\). The last thing to prove is that (\(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1})) = V\). We will do this in two steps. First we will prove that, $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}). \end{align*} $$ These spans are equal which means that adding \(s_{n-j}\) to the span, didn’t increase the span. This is because we showed earlier that \(s_{n-j}\) is a linear combination of all the other elements \(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}\). Moreover, $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}). \end{align*} $$ This is true because we also showed that \(u_{j+1}\) is a linear combinations of the elements \(\{u_1,...u_{j},s_1,...,s_{n-j}\}\). Therefore from the previous two equations, we can conclude that $$ \begin{align*} Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\}) = Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}). \end{align*} $$ But we know that \(Span(\{u_1,...u_{j},s_1,...,s_{n-j}\}) = V\) from the inductive hypothesis. Therefore, \(Span(\{u_1,...,u_{j+1},s_1,...,s_{n-j-1}\})\) also generates \(V\) and so \(Span(\mathcal{U}_{j+1} \cup \mathcal{T}_{j+1}) = V\). \(\blacksquare\) Theorem If \(V\) has a finite basis, then any basis of \(V\) has the same number of elements. Proof: Let \(\beta\) be a finite basis with \(n\) elements. Let \(\bar{\beta}\) be another basis. We claim that \(\bar{\beta}\) is finite. Suppose for the sake of contradiction that it wasn’t, then \(\bar{\beta}\) contains a set \(\bar{U}\) that contains at least \(n+1\) linearly independent vectors. Apply the Replacement Theorem with \(\mathcal{S} = \beta, \mathcal{U} = \bar{U}\). This means that if the number of elements in \(\bar{U}\) is \(k\) then \(k\) must be less than the number of elements in \(S\). But \(S\) has \(n\) elements and so \(n+1 \leq n\) is a contradiction. Therefore, \(\bar{beta}\) must be finite. To prove that it must have size \(n\), apply the Replacement Theorem again with \((\mathcal{S} = \beta, \mathcal{U} = \bar{\beta})\). We know the size of \(\bar{\beta}\) must be less than or equal to \(n\), $$ \begin{align*} |\bar{\beta}| \leq n. \end{align*} $$ But if we apply the replacement theorem with \((\mathcal{S} = \bar{\beta}, \mathcal{U} = \beta)\), then the size of \(\beta\) must be less than or equal to \(\bar{\beta}\), $$ \begin{align*} n \leq |\bar{\beta}|. \end{align*} $$ From these two inequalities, we must have \(\bar{\beta} = n\). \(\blacksquare\). Definition \(V\) is finite dimensional if it has a finite basis. The number of elements in any basis for \(V\) is the dimension of \(V\). Otherwise we say \(V\) is infinite dimensional. Examples \(\dim(\mathbf{R}^n) = n\) \(\dim(M_{m \times n}) = mn\) One basis for this space is \(\{E^{ij} \in M_{m \times n} \ | \ a_{ij} = 1, \text{ all other elements are 0}\}\). So We'll have \(mn\) matrices where each matrix will have a 1 in the \((i,j)\) position.]]></summary></entry><entry><title type="html">Lecture 8: More on Linear Dependance</title><link href="http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance.html" rel="alternate" type="text/html" title="Lecture 8: More on Linear Dependance" /><published>2024-07-25T01:01:36-07:00</published><updated>2024-07-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/25/lec08-more-linear-dependance.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \( \{u_1,...,u_k\} \subset V\) is linearly dependent, then there exists one \(u_j\) can be expressed as a linear combination of the others.
</div>
<p><br />
Proof:
Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. This means that for some (not all zero) scalars \(a_1,...,a_k\), we must have</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + ... + a_ju_j + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p>Without the loss of generality that \(a_j \neq 0\) for some \(j\). Multiply the equation by \(1/a_j\)</p>
<div>
	$$
	\begin{align*}
	\frac{1}{a_j}(a_1u_1 + ... + a_ju_j + ... + a_ku_k) &amp;= \frac{1}{a_j}\bar{0} \\
	\frac{a_1}{a_j}u_1 + ... + u_j + ... + \frac{a_k}{a_j}u_k &amp;= \bar{0} \\
	-\frac{a_1}{a_j}u_1 - ... - \frac{a_k}{a_j}u_k &amp;= u_j
	\end{align*}
	$$
</div>
<p>Therefore, \(u_j\) is a linear combination of the other vectors as we wanted to show. \(\blacksquare\)
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (Refinement Theorem)
</div>
<div class="purbdiv">
Suppose \( \{u_1,...,u_k\} \subset V\) is linearly dependent. There is a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and satisfies
	$$
	\begin{align*}
	Span(\{u_{i1},...,u_{il}\}) = Span(\{u_1,...,u_k\}).
	\end{align*}
	$$
</div>
<p><br />
Proof:
Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. By the previous theorem, there exists some \(j\) where \(u_j\) is a linear combination of the other vectors in the set. So,</p>
<div>
	$$
	\begin{align*} 
	u_j = b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k.
	\end{align*}
	$$
</div>
<p>Now, delete \(u_j\) from this set. We claim that</p>
<div>
	$$
	\begin{align*} 
	Span(\{u_1,...,u_j,...,u_k\}) = Span(\{u_1,...\hat{u_j},...,u_k\}).
	\end{align*}
	$$
</div>
<p>(the hat symbol means that the variable has been deleted). To show this, we will show that the two sets are subsets of each others which will imply that the two sets are equal.
<br /><br />
Now, given a vector \(\bar{u} = a_1u_1 + ... + a_{j}u_{j} + ... + a_ku_k\) in \(Span(\{u_1,...,u_j,...,u_k\})\), we want to show that \(\bar{u} \in Span(\{u_1,...\hat{u_j},...,u_k\})\). But since we established that \(u_j\) is a linear combination of the other vectors so substitute \(u_j\) in \(\bar{u}\) as follows,</p>
<div>
	$$
	\begin{align*}
	&amp;= a_1u_1 + ... + a_{j}(b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k) + ... + a_ku_k \\
	&amp;= (a_1+ a_jb_1)u_1 + ... + (a_k+a_jb_k)u_k.
	\end{align*}
	$$
</div>
<p>From this we see that given a vector in \(Span(\{u_1,...,u_j,...,u_k\})\), it is also in \(Span(\{u_1,...\hat{u_j},...,u_k\})\). For the other direction, it’s trivial. If we have a vector \(\bar{u} = a_1u_1 + ... + 0u_{j} + ... + a_ku_k\), then it is in \(Span(\{u_1,...,u_j,...,u_k\})\). Therefore, the two spans are equal.
<br />
<br />
But now, if \(\{u_1,...,u_{j-1},u_{j+1},...,u_k\}\) is linearly independent, we then stop. Otherwise we find another \(u_j\) to throw out. This process will stop since we started with a finite number of vectors.
<br />
<br />
Remark: To find \(u_j\), we start with \(u_1\) and ask/settle the question (Q1) can you \(u_1\) as linear combinations of \(u_2, ... ,u_k\). If the answer is yes set \(u_j = u_1\). If the answer is No, then ask (Q2) can \(u_2\) be written as a linear combination of \(u_1, ... , u_k\)?
<br />
<br />
Conclusion: we can refine a finite subset \(\{u_1,...,u_k\}\) to obtain a linearly independent subset \(\{u_{i1},...,u_{il}\}\) with the same span.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(B \subset V\) is a basis of \(V\) if
	<ol>
		<li>\(B\) is linearly independent.</li>
		<li>\(Span(B) = V\). (\(B\) generates \(V\))</li>
	</ol>
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Every vector space has a basis.
</div>
<p><br />
Proof in 1.7.
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(\beta \subset V\) is a basis then every \(u \in V\) can be expressed in a unique way as an element of \(Span(\beta)\).
</div>
<p><br />
Proof: Let \(u \in V\). Let \(\beta \subset V\) be a basis for \(V\). We can express \(u\) as</p>
<div>
	$$
	\begin{align*}
	 u = a_1u_1 + ... + a_ku_k.
	\end{align*}
	$$
</div>
<p>for \(u_1, ..., u_k \in \beta\) and \(a_1, ....,a_k \in \mathbf{R}\). We claim that this is the only way to express \(u\) in terms of the elements in \(\beta\). To see why, suppose for the sake of contradiction that it is not the only way. This means that we can also express \(u\) as</p>
<div>
 	$$
 	\begin{align*}
 	 u = b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}.
 	\end{align*}
 	$$
</div>
<p>But we know that \(u - u = \bar{0}\). Evaluating \(u-u\),</p>
<div>
	$$
	\begin{align*}
	 u - u &amp;= (a_1u_1 + ... + a_ku_k) - (b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\
	\bar{0} &amp;= (a_1-b_1)u_1 + ... + (a_k-b_k)u_k - (b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\
	\end{align*}
	$$
</div>
<p>We also know that \(\beta\) is linearly independent. So for the linear combination above, all the coefficients must be 0. Therefore, we must have,</p>
<div>
	$$
	a_1 = b_1, a_2 = b_2, ..., a_k=b_k, b_{k+1}=0, b_{l} = 0.
	$$
</div>
<p>This is exactly the first representation of \(u\) which is a contradiction and so \(u\) can only be uniquely expressed in terms of the elements of \(\beta\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem If \( \{u_1,...,u_k\} \subset V\) is linearly dependent, then there exists one \(u_j\) can be expressed as a linear combination of the others. Proof: Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. This means that for some (not all zero) scalars \(a_1,...,a_k\), we must have $$ \begin{align*} a_1u_1 + ... + a_ju_j + ... + a_ku_k = \bar{0}. \end{align*} $$ Without the loss of generality that \(a_j \neq 0\) for some \(j\). Multiply the equation by \(1/a_j\) $$ \begin{align*} \frac{1}{a_j}(a_1u_1 + ... + a_ju_j + ... + a_ku_k) &amp;= \frac{1}{a_j}\bar{0} \\ \frac{a_1}{a_j}u_1 + ... + u_j + ... + \frac{a_k}{a_j}u_k &amp;= \bar{0} \\ -\frac{a_1}{a_j}u_1 - ... - \frac{a_k}{a_j}u_k &amp;= u_j \end{align*} $$ Therefore, \(u_j\) is a linear combination of the other vectors as we wanted to show. \(\blacksquare\) Theorem (Refinement Theorem) Suppose \( \{u_1,...,u_k\} \subset V\) is linearly dependent. There is a subset \(\{u_{i1},...,u_{il}\}\) which is linearly independent and satisfies $$ \begin{align*} Span(\{u_{i1},...,u_{il}\}) = Span(\{u_1,...,u_k\}). \end{align*} $$ Proof: Suppose the set \(\{u_1,...,u_k\} \subset V\) is linearly dependent. By the previous theorem, there exists some \(j\) where \(u_j\) is a linear combination of the other vectors in the set. So, $$ \begin{align*} u_j = b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k. \end{align*} $$ Now, delete \(u_j\) from this set. We claim that $$ \begin{align*} Span(\{u_1,...,u_j,...,u_k\}) = Span(\{u_1,...\hat{u_j},...,u_k\}). \end{align*} $$ (the hat symbol means that the variable has been deleted). To show this, we will show that the two sets are subsets of each others which will imply that the two sets are equal. Now, given a vector \(\bar{u} = a_1u_1 + ... + a_{j}u_{j} + ... + a_ku_k\) in \(Span(\{u_1,...,u_j,...,u_k\})\), we want to show that \(\bar{u} \in Span(\{u_1,...\hat{u_j},...,u_k\})\). But since we established that \(u_j\) is a linear combination of the other vectors so substitute \(u_j\) in \(\bar{u}\) as follows, $$ \begin{align*} &amp;= a_1u_1 + ... + a_{j}(b_1u_1 + ... + b_{j-1}u_{j-1} + b_{j+1}u_{j+1} + ... + b_ku_k) + ... + a_ku_k \\ &amp;= (a_1+ a_jb_1)u_1 + ... + (a_k+a_jb_k)u_k. \end{align*} $$ From this we see that given a vector in \(Span(\{u_1,...,u_j,...,u_k\})\), it is also in \(Span(\{u_1,...\hat{u_j},...,u_k\})\). For the other direction, it’s trivial. If we have a vector \(\bar{u} = a_1u_1 + ... + 0u_{j} + ... + a_ku_k\), then it is in \(Span(\{u_1,...,u_j,...,u_k\})\). Therefore, the two spans are equal. But now, if \(\{u_1,...,u_{j-1},u_{j+1},...,u_k\}\) is linearly independent, we then stop. Otherwise we find another \(u_j\) to throw out. This process will stop since we started with a finite number of vectors. Remark: To find \(u_j\), we start with \(u_1\) and ask/settle the question (Q1) can you \(u_1\) as linear combinations of \(u_2, ... ,u_k\). If the answer is yes set \(u_j = u_1\). If the answer is No, then ask (Q2) can \(u_2\) be written as a linear combination of \(u_1, ... , u_k\)? Conclusion: we can refine a finite subset \(\{u_1,...,u_k\}\) to obtain a linearly independent subset \(\{u_{i1},...,u_{il}\}\) with the same span. Definition \(B \subset V\) is a basis of \(V\) if \(B\) is linearly independent. \(Span(B) = V\). (\(B\) generates \(V\)) Theorem Every vector space has a basis. Proof in 1.7. Theorem If \(\beta \subset V\) is a basis then every \(u \in V\) can be expressed in a unique way as an element of \(Span(\beta)\). Proof: Let \(u \in V\). Let \(\beta \subset V\) be a basis for \(V\). We can express \(u\) as $$ \begin{align*} u = a_1u_1 + ... + a_ku_k. \end{align*} $$ for \(u_1, ..., u_k \in \beta\) and \(a_1, ....,a_k \in \mathbf{R}\). We claim that this is the only way to express \(u\) in terms of the elements in \(\beta\). To see why, suppose for the sake of contradiction that it is not the only way. This means that we can also express \(u\) as $$ \begin{align*} u = b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}. \end{align*} $$ But we know that \(u - u = \bar{0}\). Evaluating \(u-u\), $$ \begin{align*} u - u &amp;= (a_1u_1 + ... + a_ku_k) - (b_1u_1 + ... + b_ku_k + b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\ \bar{0} &amp;= (a_1-b_1)u_1 + ... + (a_k-b_k)u_k - (b_{k+1}u_{k+1} + ... + b_{l}u_{l}) \\ \end{align*} $$ We also know that \(\beta\) is linearly independent. So for the linear combination above, all the coefficients must be 0. Therefore, we must have, $$ a_1 = b_1, a_2 = b_2, ..., a_k=b_k, b_{k+1}=0, b_{l} = 0. $$ This is exactly the first representation of \(u\) which is a contradiction and so \(u\) can only be uniquely expressed in terms of the elements of \(\beta\). References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 7: Linear Dependance</title><link href="http://localhost:4000/jekyll/update/2024/07/24/lec07-linear-dependance.html" rel="alternate" type="text/html" title="Lecture 7: Linear Dependance" /><published>2024-07-24T01:01:36-07:00</published><updated>2024-07-24T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/24/lec07-linear-dependance</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/24/lec07-linear-dependance.html"><![CDATA[<p>In the previous lecture, we saw that for a subset \(S \subset V\) (where \(V\) is a vector space) that the span of \(S\) (\(Span(S)\)) is a subspace of \(V\) and so \(Span(S)\) is a vector space. We also solved the question of whether a particular vector \(w\) belongs to \(Span(S)\) by answering the question of whether we can write \(w\) as a linear combinations of the elements of \(S\). When \(S\) contains a finite collection of elements \(\{u_1, u_2, u_3\}\), we did this by writing \(w\) as linear combination of the elements of \(S\) as follows,</p>
<div>
$$
\begin{align*}
w = x_1 (u_1) + x_2 (u_2) +...+ x_k(u_k).
\end{align*}
$$
</div>
<p>Writing \(w\) as a linear combinations above requires solving the above system of linear equations with \(k\) variables \((x_1, x_2,...,x_k)\). If there is a solution to the system, then we know \(w\) can be written as a linear combination of the elements of \(S\) and it belongs to the span of \(S\). If the system doesn’t have a solution, then this means that \(w\) can’t be written as a linear combination of the elements and so \(w\) doesn’t belong to the span of \(S\).
<br />
<br /> 
But this question can be reversed meaning we can answer this question in terms of the span of a matrix. So if we’re given a linear system of equations with matrix \(A\),</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
a_{11} &amp; a_{12} &amp; ... &amp; a_{1n}  \\
. &amp;  &amp;  &amp; . \\
. &amp;  &amp;  &amp;  . \\
. &amp;  &amp;  &amp;  . \\
a_{m1} &amp; a_{m2} &amp; ... &amp; a_{mn}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>If we view this matrix as a collection of \(n-\)column vectors in \(R^m\) instead. The span of these vectors is a subspace of \(\mathbf{R}^m\). We call this subspace the <b>Column Space of \(A\) \((Col(A))\)</b>.
<br />
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Given a vector
\(
\bar{x} = 
\begin{pmatrix}
x_1 \\
x_2 \\
.  \\
. \\
x_n
\end{pmatrix}.
\in \mathbf{R}^n
\), we define the following operation
\begin{align*}
A\bar{x} = 
\begin{pmatrix}
a_{11} &amp; ... &amp; a_{1n}  \\
. &amp;  &amp;  . \\
. &amp;  &amp;   . \\
. &amp;  &amp;   . \\
a_{m1} &amp; ... &amp; a_{mn}
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2 \\
.  \\
. \\
x_n
\end{pmatrix}
= x_1
\begin{pmatrix}
a_{11}   \\
.  \\
. \\
. \\
a_{m1} 
\end{pmatrix}
+ 
...
+ 
x_n
\begin{pmatrix}
a_{1n} \\
.  \\
. \\
. \\
a_{mn} 
\end{pmatrix}.
\end{align*}
This product is in the column space of \(A\) because it is a linear combinations of the columns of \(A\).
</div>
<p><br />
Note that this product or operations works if the vector has as many entries as the columns of \(A\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>The following matrix has three column vectors in \(\mathbf{R}^2\). The product below is a linear combination of the columns of the matrix.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6
\end{pmatrix}
\begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
&amp;=
1
\begin{pmatrix}
1 \\
4
\end{pmatrix}
+
1
\begin{pmatrix}
2 \\
5
\end{pmatrix}
+
1
\begin{pmatrix}
3 \\
6
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
6 \\
15
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>An Observation</b></h4>
<p>Given \(\bar{b} \in \mathbf{R}^m\) and viewing the \(\bar{x}\) as a variable, the equation</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
A\bar{x} = \bar{b}
\end{pmatrix}.
\end{align*}
$$
</div>
<p>is equivalent to the linear system with augmented matrix \((A\bar{b})\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Test</b></h4>
<p>Let’s verify the above observation. Let 
\(\begin{align*}
A =
\begin{pmatrix}
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6
\end{pmatrix}
,
\bar{b} = 
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix}
\end{align*}\)
Then,</p>
<div>
$$
\begin{align*}
A\bar{x} &amp;= \bar{b} \\
\begin{pmatrix}
1 &amp; 2 &amp; 3  \\
4 &amp; 5 &amp; 6
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2 \\
x_3 \\
\end{pmatrix}
&amp;=
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix} \\
x_1
\begin{pmatrix}
1 \\
4
\end{pmatrix}
+
x_2
\begin{pmatrix}
2 \\
5
\end{pmatrix}
+
x_3
\begin{pmatrix}
3 \\
6
\end{pmatrix}
&amp;=
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix} \\
\begin{pmatrix}
1x_1 + 2x_2 + 3x_3 \\
4x_1 + 5x_2 + 6x_3
\end{pmatrix}
&amp;=
\begin{pmatrix}
\sqrt{2} \\
\pi
\end{pmatrix}
\end{align*}
$$
</div>
<p>This is a linear system of equations with augmented matrix</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3  &amp; \sqrt{2} \\
	4 &amp; 5 &amp; 6 &amp; \pi
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>\(A\bar{x} = \bar{b}\) is consistent if and only if \(\bar{b} \in Col(A)\). This again means that answering the question of whether \(\bar{b}\) is in the span of columns of \(A\) is the same as answering if the system has a solution.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Dependence and Linear Independence</b></h4>
<p>Suppose \(W\) is a subspace of \(V\). We know that spans are subspaces but is \(W\) a span of some elements? or what is the smallest number \(k\) such that \(W\) can be written as</p>
<div>
	$$
	\begin{align*}
	W = Span = \{(w_1, w_2, ..., w_k)\}
	\end{align*}
	$$
</div>
<p>The answer to this question could be that there is no such \(k\) (for example if \(W\) is a subspace of the vector space of all continuous functions).</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A subset \( \{u_1, u_2, u_3,...\} \subset V\) is <b>linearly dependent</b> if there are scalars \(a_1, ... a_k\) not all zero such that 
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p><br />
Note here that <b>not</b> linearly dependent is equivalent to <b>linearly independent</b>. In particular the set \(\{u_1, u_2, u_3,...\}\) is linearly independent if and only if</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}.
	\end{align*}
	$$
</div>
<p>is true only when \(a_1 = 0, a_2 = 0, ... a_k = 0\).
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 0</b></h4>
<p>Consider \(\{\bar{0}\}\). This set is linearly dependent because we can choose a scalar $a_1 \neq 0$ such that \(a_1\bar{0} = \bar{0}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Given a vector \(u\) such that \(u \neq \{\bar{0}\}\), then \(\{u\}\) is linearly independent.
<br />
<br />
Proof: We need to prove that \(au = \bar{0}\) only if \(a = 0\). Now suppose that \(au = \bar{0}\). We have two cases. Case one is when \(a = 0\) and we’re done. Case two is when \(a \neq 0\). If \(a \neq 0\), then</p>
<div>
	$$
	\begin{align*}
	 \frac{1}{a}au &amp;= \frac{1}{a}\bar{0} \\
	 u &amp;= \bar{0}
	\end{align*}
	$$
</div>
<p>But we know that \(u \neq \bar{0}\). Therefore \(a\) must be zero and we’re done.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Given a vector \(u_1, u_2\), prove that \(\{u_1, u_2\}\) is linearly dependent if and only if one vector is a scalar multiple of the other.
<br />
<br />
Proof: For the forward direction, suppose that \(\{u_1, u_2\}\) is linearly dependent. This means that there are scalars (a_1, a_2) not all zero such that</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 &amp;= \bar{0}
	\end{align*}
	$$
</div>
<p>Without the loss of generality, suppose that \(a_1 \neq 0\). Then,</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 &amp;= \bar{0} \\
	a_1u_1 &amp;= - a_2u_2 \\
	u_1 &amp;= -\frac{a_2}{a_1}u_2.
	\end{align*}
	$$
</div>
<p>For the backward direction, suppose that one vector can be written as a scalar multiple of the other. Without the loss of generality, suppose it is \(u_1\). Then \(u_1 = cu_2\) where \(c \neq 0\), We can re-write this as follows,</p>
<div>
	$$
	\begin{align*}
	u_1 &amp;= cu_2 \\
	u_1 + (-c)u_2 &amp;= \bar{0}
	\end{align*}
	$$
</div>
<p>From this we see that there are scalars \(a_1 = 1\) and \(a_2 = -c\), not all zero such that \(a_1u_1 + a_2u_2 = \bar{0}\) which means that they are linearly dependent as we wanted to show.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Determine the set of the following vectors:</p>
<div>
	$$
	\begin{align*}
	\{u_1, u_2, u_3\} = \{(−1, 1, 2), (1, 2, 1), (5, 1, −4) \} 
	\end{align*}
	$$
</div>
<p>is linearly dependent.
<br />
<br />
The set of vectors is linearly independent \(a_1u_1 + a_2u_2 + a_3u_3 = \bar{0}\) implies that \(a_1=0, a_2=0\) and \(a_3=0\). So,</p>
<div>
	$$
	\begin{align*}
	a_1u_1 + a_2u_2 + a_3u_3 &amp;= \bar{0} \\
	a_1(−1, 1, 2) + a_2(1, 2, 1) + a_3(5, 1, −4) &amp;= (0,0,0) \\
	(-a_1+a_2+5a_3, a_2+2a_2+a_3, 2a_1+2a_2-4a_3) &amp;= (0,0,0).
	\end{align*}
	$$
</div>
<p>This is equivalent to solving the following system:</p>
<div>
	$$
	\begin{align*}
	-a_1 + a_2 + 5a_3 &amp;= 0 \\
	a_2+2a_2+a_3 &amp;= 0 \\
	2a_1+2a_2-4a_3 &amp;= 0.
	\end{align*}
	$$
</div>
<p>\(\{u_1, u_2, u_3\}\) is linearly independent if and only if this system has the trivial solution \((0,0,0)\). \((0,0,0)\) will always be a solution. It’s why it is called the trivial solution. So the question now is how do we know from the REF matrix, whether we have the \((0,0,0)\) solution or a non-zero solution?
<br />
<br />
If the matrix has no column without a leading entry (besides last) then we have a unique solution and the set is linearly independent. If the matrix has a column (besides the last) with no leading entry then we have infinitely many solutions (so not just the zero vector) and the set is linearly dependent. So we want to make sure that all columns have leading entries.</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	-1 &amp; 1 &amp; 5  &amp; 0 \\
	1 &amp; 2 &amp; 1 &amp; 0 \\
	2 &amp; 2 &amp; -4 &amp; 0
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>This will eventually be</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	-1 &amp; 1 &amp; 5  &amp; 0 \\
	0 &amp; 3 &amp; 6 &amp; 0 \\
	0 &amp; 0 &amp; 0 &amp; 0
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>The third column has no leading entry so there are infinitely many solutions besides the zero solution this set is linearly dependent.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Consider \(\{ \sin(x), \cos(x)\} \in F(\mathbf{R})\). Is this set linearly dependent or independant?
<br />
<br />
From the definition if \(\sin(x)\) was a non-zero scalar multiple of \(cos(x)\), in other words \(\sin(x) = a\cos(x)\) for some non-zero scalar \(a\), then they are linearly dependent. But two functions are equal when they take the same value at every point.
<br />
<br />
Suppose we choose the point \(x = 0\), then we know that \(\sin(0) = 0\) but \(a\cos(0)=a\). Therefore, these functions are not equal and so the set is linearly independent.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In the previous lecture, we saw that for a subset \(S \subset V\) (where \(V\) is a vector space) that the span of \(S\) (\(Span(S)\)) is a subspace of \(V\) and so \(Span(S)\) is a vector space. We also solved the question of whether a particular vector \(w\) belongs to \(Span(S)\) by answering the question of whether we can write \(w\) as a linear combinations of the elements of \(S\). When \(S\) contains a finite collection of elements \(\{u_1, u_2, u_3\}\), we did this by writing \(w\) as linear combination of the elements of \(S\) as follows, $$ \begin{align*} w = x_1 (u_1) + x_2 (u_2) +...+ x_k(u_k). \end{align*} $$ Writing \(w\) as a linear combinations above requires solving the above system of linear equations with \(k\) variables \((x_1, x_2,...,x_k)\). If there is a solution to the system, then we know \(w\) can be written as a linear combination of the elements of \(S\) and it belongs to the span of \(S\). If the system doesn’t have a solution, then this means that \(w\) can’t be written as a linear combination of the elements and so \(w\) doesn’t belong to the span of \(S\). But this question can be reversed meaning we can answer this question in terms of the span of a matrix. So if we’re given a linear system of equations with matrix \(A\), $$ \begin{align*} \begin{pmatrix} a_{11} &amp; a_{12} &amp; ... &amp; a_{1n} \\ . &amp; &amp; &amp; . \\ . &amp; &amp; &amp; . \\ . &amp; &amp; &amp; . \\ a_{m1} &amp; a_{m2} &amp; ... &amp; a_{mn} \end{pmatrix}. \end{align*} $$ If we view this matrix as a collection of \(n-\)column vectors in \(R^m\) instead. The span of these vectors is a subspace of \(\mathbf{R}^m\). We call this subspace the Column Space of \(A\) \((Col(A))\). Definition Given a vector \( \bar{x} = \begin{pmatrix} x_1 \\ x_2 \\ . \\ . \\ x_n \end{pmatrix}. \in \mathbf{R}^n \), we define the following operation \begin{align*} A\bar{x} = \begin{pmatrix} a_{11} &amp; ... &amp; a_{1n} \\ . &amp; &amp; . \\ . &amp; &amp; . \\ . &amp; &amp; . \\ a_{m1} &amp; ... &amp; a_{mn} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ . \\ . \\ x_n \end{pmatrix} = x_1 \begin{pmatrix} a_{11} \\ . \\ . \\ . \\ a_{m1} \end{pmatrix} + ... + x_n \begin{pmatrix} a_{1n} \\ . \\ . \\ . \\ a_{mn} \end{pmatrix}. \end{align*} This product is in the column space of \(A\) because it is a linear combinations of the columns of \(A\). Note that this product or operations works if the vector has as many entries as the columns of \(A\). Example 1 The following matrix has three column vectors in \(\mathbf{R}^2\). The product below is a linear combination of the columns of the matrix. $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} &amp;= 1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + 1 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + 1 \begin{pmatrix} 3 \\ 6 \end{pmatrix}\\ &amp;= \begin{pmatrix} 6 \\ 15 \end{pmatrix}. \end{align*} $$ An Observation Given \(\bar{b} \in \mathbf{R}^m\) and viewing the \(\bar{x}\) as a variable, the equation $$ \begin{align*} \begin{pmatrix} A\bar{x} = \bar{b} \end{pmatrix}. \end{align*} $$ is equivalent to the linear system with augmented matrix \((A\bar{b})\). Test Let’s verify the above observation. Let \(\begin{align*} A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} , \bar{b} = \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \end{align*}\) Then, $$ \begin{align*} A\bar{x} &amp;= \bar{b} \\ \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ \end{pmatrix} &amp;= \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \\ x_1 \begin{pmatrix} 1 \\ 4 \end{pmatrix} + x_2 \begin{pmatrix} 2 \\ 5 \end{pmatrix} + x_3 \begin{pmatrix} 3 \\ 6 \end{pmatrix} &amp;= \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \\ \begin{pmatrix} 1x_1 + 2x_2 + 3x_3 \\ 4x_1 + 5x_2 + 6x_3 \end{pmatrix} &amp;= \begin{pmatrix} \sqrt{2} \\ \pi \end{pmatrix} \end{align*} $$ This is a linear system of equations with augmented matrix $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; \sqrt{2} \\ 4 &amp; 5 &amp; 6 &amp; \pi \end{pmatrix} \end{align*} $$ \(A\bar{x} = \bar{b}\) is consistent if and only if \(\bar{b} \in Col(A)\). This again means that answering the question of whether \(\bar{b}\) is in the span of columns of \(A\) is the same as answering if the system has a solution. Linear Dependence and Linear Independence Suppose \(W\) is a subspace of \(V\). We know that spans are subspaces but is \(W\) a span of some elements? or what is the smallest number \(k\) such that \(W\) can be written as $$ \begin{align*} W = Span = \{(w_1, w_2, ..., w_k)\} \end{align*} $$ The answer to this question could be that there is no such \(k\) (for example if \(W\) is a subspace of the vector space of all continuous functions). Definition A subset \( \{u_1, u_2, u_3,...\} \subset V\) is linearly dependent if there are scalars \(a_1, ... a_k\) not all zero such that $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}. \end{align*} $$ Note here that not linearly dependent is equivalent to linearly independent. In particular the set \(\{u_1, u_2, u_3,...\}\) is linearly independent if and only if $$ \begin{align*} a_1u_1 + a_2u_2 + ... + a_ku_k = \bar{0}. \end{align*} $$ is true only when \(a_1 = 0, a_2 = 0, ... a_k = 0\). Example 0 Consider \(\{\bar{0}\}\). This set is linearly dependent because we can choose a scalar $a_1 \neq 0$ such that \(a_1\bar{0} = \bar{0}\). Example 1 Given a vector \(u\) such that \(u \neq \{\bar{0}\}\), then \(\{u\}\) is linearly independent. Proof: We need to prove that \(au = \bar{0}\) only if \(a = 0\). Now suppose that \(au = \bar{0}\). We have two cases. Case one is when \(a = 0\) and we’re done. Case two is when \(a \neq 0\). If \(a \neq 0\), then $$ \begin{align*} \frac{1}{a}au &amp;= \frac{1}{a}\bar{0} \\ u &amp;= \bar{0} \end{align*} $$ But we know that \(u \neq \bar{0}\). Therefore \(a\) must be zero and we’re done. Example 2 Given a vector \(u_1, u_2\), prove that \(\{u_1, u_2\}\) is linearly dependent if and only if one vector is a scalar multiple of the other. Proof: For the forward direction, suppose that \(\{u_1, u_2\}\) is linearly dependent. This means that there are scalars (a_1, a_2) not all zero such that $$ \begin{align*} a_1u_1 + a_2u_2 &amp;= \bar{0} \end{align*} $$ Without the loss of generality, suppose that \(a_1 \neq 0\). Then, $$ \begin{align*} a_1u_1 + a_2u_2 &amp;= \bar{0} \\ a_1u_1 &amp;= - a_2u_2 \\ u_1 &amp;= -\frac{a_2}{a_1}u_2. \end{align*} $$ For the backward direction, suppose that one vector can be written as a scalar multiple of the other. Without the loss of generality, suppose it is \(u_1\). Then \(u_1 = cu_2\) where \(c \neq 0\), We can re-write this as follows, $$ \begin{align*} u_1 &amp;= cu_2 \\ u_1 + (-c)u_2 &amp;= \bar{0} \end{align*} $$ From this we see that there are scalars \(a_1 = 1\) and \(a_2 = -c\), not all zero such that \(a_1u_1 + a_2u_2 = \bar{0}\) which means that they are linearly dependent as we wanted to show. Example 3 Determine the set of the following vectors: $$ \begin{align*} \{u_1, u_2, u_3\} = \{(−1, 1, 2), (1, 2, 1), (5, 1, −4) \} \end{align*} $$ is linearly dependent. The set of vectors is linearly independent \(a_1u_1 + a_2u_2 + a_3u_3 = \bar{0}\) implies that \(a_1=0, a_2=0\) and \(a_3=0\). So, $$ \begin{align*} a_1u_1 + a_2u_2 + a_3u_3 &amp;= \bar{0} \\ a_1(−1, 1, 2) + a_2(1, 2, 1) + a_3(5, 1, −4) &amp;= (0,0,0) \\ (-a_1+a_2+5a_3, a_2+2a_2+a_3, 2a_1+2a_2-4a_3) &amp;= (0,0,0). \end{align*} $$ This is equivalent to solving the following system: $$ \begin{align*} -a_1 + a_2 + 5a_3 &amp;= 0 \\ a_2+2a_2+a_3 &amp;= 0 \\ 2a_1+2a_2-4a_3 &amp;= 0. \end{align*} $$ \(\{u_1, u_2, u_3\}\) is linearly independent if and only if this system has the trivial solution \((0,0,0)\). \((0,0,0)\) will always be a solution. It’s why it is called the trivial solution. So the question now is how do we know from the REF matrix, whether we have the \((0,0,0)\) solution or a non-zero solution? If the matrix has no column without a leading entry (besides last) then we have a unique solution and the set is linearly independent. If the matrix has a column (besides the last) with no leading entry then we have infinitely many solutions (so not just the zero vector) and the set is linearly dependent. So we want to make sure that all columns have leading entries. $$ \begin{align*} \begin{pmatrix} -1 &amp; 1 &amp; 5 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 0 \\ 2 &amp; 2 &amp; -4 &amp; 0 \end{pmatrix} \end{align*} $$ This will eventually be $$ \begin{align*} \begin{pmatrix} -1 &amp; 1 &amp; 5 &amp; 0 \\ 0 &amp; 3 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ The third column has no leading entry so there are infinitely many solutions besides the zero solution this set is linearly dependent. Example 4 Consider \(\{ \sin(x), \cos(x)\} \in F(\mathbf{R})\). Is this set linearly dependent or independant? From the definition if \(\sin(x)\) was a non-zero scalar multiple of \(cos(x)\), in other words \(\sin(x) = a\cos(x)\) for some non-zero scalar \(a\), then they are linearly dependent. But two functions are equal when they take the same value at every point. Suppose we choose the point \(x = 0\), then we know that \(\sin(0) = 0\) but \(a\cos(0)=a\). Therefore, these functions are not equal and so the set is linearly independent. References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 2: Echelon Form and Reduced Echelon Form</title><link href="http://localhost:4000/jekyll/update/2024/07/23/elementary-matrices.html" rel="alternate" type="text/html" title="Lecture 2: Echelon Form and Reduced Echelon Form" /><published>2024-07-23T01:01:36-07:00</published><updated>2024-07-23T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/23/elementary-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/23/elementary-matrices.html"><![CDATA[<p>From lecture 2, we know that there are 3 types of elementary row operations:</p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A\) be an \(m \times n\) matrix. Any one of the following three operations on the rows of \(A\) is called an <b>elementary row operation</b>:
<ol>
<li> interchanging any two rows of \(A\).</li>
<li> multiplying any row of \(A\) by a non-zero scalar.</li>
<li> adding any scalar multiple of a row of \(A\) to another row.</li>
</ol>
</div>
<p><br />
What’s more interesting is the next definition:
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of type 1, 2 or 3. according to whether the elementary operation performed on \(I_n\) is a type 1, 2 or 3 operation respectively.
</div>
<p><br />
What does the definition above mean? If we interchange the first two rows of \(I_3\), then this will produce the elementary matrix:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>But why this is useful? This is useful because if we have some matrix \(A\) and want to interchange the first two rows. We can perform this operation by instead multiply this new matrix above by \(A\). So we can turn our elementary row operations into a matrix multiplication! The next theorem states this fact.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 3.1
</div>
<div class="purbdiv">
Let \(A \in M_{m \times n}(F)\), and suppose that \(B\) is obtained from \(A\) by performing an elementary row operation. Then there exists an \(m \times n\) elementary matrix \(E\) such that \(B = EA\). In fact, \(E\) is obtained from \(I_m\) by performing the same elementary row operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m\) matrix, then \(EA\) is the matrix obtained from \(A\) by performing the same elementary row operation as that which produces \(E\) from \(I_m\).
</div>
<p><br />
This confirms what we said earlier. If we have a matrix \(A\) and we want to interchange the first two rows then we can first generate the matrix \(E\) from \(I_m\) by interchanging the first two rows:</p>
<div>
$$
\begin{align*}
E =
\begin{pmatrix}
0 &amp; 1 &amp; 0  \\
1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>And then multiplying \(E\) by \(A\) to produce the same matrix produced from interchanging the first two rows in \(A\).  Finally we have this nice theorem to confirm that this elementary matrix is invertible!</p>
<div class="purdiv">
Theorem 3.2
</div>
<div class="purbdiv">
Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type.
</div>
<p><br />
<b>Proof:</b>
Let \(E\) be an elementary \(n \times n\) matrix. We know that that \(E\) was obtained by some elementary row operation on \(I_n\). We can reverse the steps used to transform \(I_n\) to obtain \(E\) in order to get back \(I_n\). To get \(I_n\) back, we had to use the same type of elementary row operation. By the previous theorem (3.1), this elementary row operation can be done using an elementary matrix \(\bar{E}\) such that \(\bar{E}E = I_n\). Therefore, \(E\) is invertible and \(E^{-1} = \bar{E}\) (By 2.4 Exercise 10!). \(\blacksquare\)</p>

<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241">Linear Algebra 5th Edition by Stephen Friedberg, Arnold Insel, Lawrence Spence</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[From lecture 2, we know that there are 3 types of elementary row operations: Definition Let \(A\) be an \(m \times n\) matrix. Any one of the following three operations on the rows of \(A\) is called an elementary row operation: interchanging any two rows of \(A\). multiplying any row of \(A\) by a non-zero scalar. adding any scalar multiple of a row of \(A\) to another row. What’s more interesting is the next definition: Definition An \(n \times n\) elementary matrix is a matrix obtained by performing an elementary operation on \(I_n\). The elementary matrix is said to be of type 1, 2 or 3. according to whether the elementary operation performed on \(I_n\) is a type 1, 2 or 3 operation respectively. What does the definition above mean? If we interchange the first two rows of \(I_3\), then this will produce the elementary matrix: $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ But why this is useful? This is useful because if we have some matrix \(A\) and want to interchange the first two rows. We can perform this operation by instead multiply this new matrix above by \(A\). So we can turn our elementary row operations into a matrix multiplication! The next theorem states this fact. Theorem 3.1 Let \(A \in M_{m \times n}(F)\), and suppose that \(B\) is obtained from \(A\) by performing an elementary row operation. Then there exists an \(m \times n\) elementary matrix \(E\) such that \(B = EA\). In fact, \(E\) is obtained from \(I_m\) by performing the same elementary row operation as that which was performed on \(A\) to obtain \(B\). Conversely, if \(E\) is an elementary \(m \times m\) matrix, then \(EA\) is the matrix obtained from \(A\) by performing the same elementary row operation as that which produces \(E\) from \(I_m\). This confirms what we said earlier. If we have a matrix \(A\) and we want to interchange the first two rows then we can first generate the matrix \(E\) from \(I_m\) by interchanging the first two rows: $$ \begin{align*} E = \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ And then multiplying \(E\) by \(A\) to produce the same matrix produced from interchanging the first two rows in \(A\). Finally we have this nice theorem to confirm that this elementary matrix is invertible! Theorem 3.2 Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type. Proof: Let \(E\) be an elementary \(n \times n\) matrix. We know that that \(E\) was obtained by some elementary row operation on \(I_n\). We can reverse the steps used to transform \(I_n\) to obtain \(E\) in order to get back \(I_n\). To get \(I_n\) back, we had to use the same type of elementary row operation. By the previous theorem (3.1), this elementary row operation can be done using an elementary matrix \(\bar{E}\) such that \(\bar{E}E = I_n\). Therefore, \(E\) is invertible and \(E^{-1} = \bar{E}\) (By 2.4 Exercise 10!). \(\blacksquare\)]]></summary></entry><entry><title type="html">Lecture 3: Gaussian Elimination</title><link href="http://localhost:4000/jekyll/update/2024/07/22/lec03-gaussian-elimination.html" rel="alternate" type="text/html" title="Lecture 3: Gaussian Elimination" /><published>2024-07-22T01:01:36-07:00</published><updated>2024-07-22T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/22/lec03-gaussian-elimination</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/22/lec03-gaussian-elimination.html"><![CDATA[<p>Gaussian Elimination consists of two passes. The forward pass is used to put the augmented matrix of linear equations into a Row Echelon Form. At this point, it will be clear if this system is inconsistent or has infinitely many solutions. The second stage of the algorithm is to do a backward pass on the augmented matrix to put it into a Reduced Row Echelon Form.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Suppose we have the following augmented matrix:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
0 &amp; 1 &amp; 2 &amp; 3 \\
1 &amp; 1 &amp; 1 &amp; 1 \\
3 &amp; 2 &amp; 1 &amp; 2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>The first stage of the algorithm is the forward pass. We want to put this matrix into Row Echelon Form which means we’ll want non-zero leading entries and zero out the entries below them. In row 1 above, we have a zero, so we’ll swap row 1 and 2.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
3 &amp; 2 &amp; 1 &amp; 2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next we want to zero out any entries below the leading entry. So we want to get rid of the 3 in column 3. To do that, we’ll multiply row 1 by -3 and add it to row 3. We write this as \(R_3 \rightarrow -3R_1 + R_3\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
0 &amp; -1 &amp; -2 &amp; -1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next, we want to ignore row 1 and move to the next row. In row 2, the next leading entry is in the most left non-zero column which is column 2. We’ll repeat the same process by zeroing the entries below it. This means we’ll want to get rid of the -1 in row 3. So we write \(R_3 \rightarrow R_2 + R_3\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 &amp; 2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We repeat this process until we have no more rows. At the end of this process, the matrix will be in Row Echelon Form. Notice here that this system is inconsistent because we have a leading entry in the very last column. 
<br />
<br />
The next phase of the algorithm is a backward pass to put the matrix in Reduced Row Echelon Form by making the leading entries 1 and zeroing out the non-leading entries in each column. We want to do this phase from right to left. For the last row, we need to multiply row 3 by 1/2 to make it 1.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next, we’ll zero out the remaining entries above it by performing the operations \(R_2 \rightarrow -3R_3 + R_2\) and \(R_1 \rightarrow -R_3 + R_1\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We then want to focus on the next leading entry in row 2 which happens to be in column 2. The leading entry is 1 already so we only need to zero out the 1 above it by performing \(R_1 = -R_2 + R_1\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Now this matrix is in Reduced Row Echelon Form.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Find all the solutions to the following system</p>
<div>
$$
\begin{align*}
  x_1 + 2x_2 - x_3  &amp;= 1 \\
  x_1 + x_2 - 2x_3  &amp;= 0 \\
  5x_1 + 8x_2 + x_3 &amp;= 1
\end{align*}
$$
</div>
<p>We’ll put the equations in an augmented matrix form:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
1 &amp; 1 &amp; 2 &amp; 0 \\
5 &amp; 8 &amp; 1 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Next, we’ll perform Gaussian Elimination by first performing the forward pass which will put the matrix in Row Echelon. We’ll make the top most left entry non-zero and zero out the remaining entries below it. So we’ll perform the operations \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; -2 &amp; 6 &amp; -4
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We’ll repeat the same process for the next leading entry in row 2 which is -1. We’ll zero out the entries below it by applying \(R_3 \rightarrow R_3 - 2R_2\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; 0 &amp; 0 &amp; -2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Here we should know that that since there is a leading entry in the last column then this system is inconsistent and the system has no solutions and so we’re done.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Find all the solutions to the following system</p>
<div>
$$
\begin{align*}
x_1 + 2x_2 - x_3  &amp;= 1 \\
x_1 + x_2 - 2x_3 &amp;= 0 \\
5x_1 + 8x_2 + x_3 &amp;= 3
\end{align*}
$$
</div>
<p>We’ll put the equations in an augmented matrix form:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
1 &amp; 1 &amp; 2 &amp; 0 \\
5 &amp; 8 &amp; 1 &amp; 3
\end{pmatrix}.
\end{align*}
$$
</div>
<p>Step one is the forward pass. we’ll start with the top left most entry and make it a non-zero. it is already a non-zero entry. then we’ll zero out the remaining entries in that column by applying \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; -2 &amp; 6 &amp; -2
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We’ll repeat this for the next leading entry in row 2 which is -1 and then zero out the below rows. We’ll apply \(R_3 \rightarrow R_3 - 2R_2\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; -1 &amp; 3 &amp; -1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We have the leading entries 1 and -1. We’re in Row Echelon Form. We can describe the solution set now. but first we want to put it in Reduced Row Echelon Form. so we’ll do the backward pass.
<br />
<br />
We’ll go right to left in the backward pass, making leading entries 1 and zeroing out the remaining entries in those columns. The first leading entry we’ll work on is -1 in row 2. We’ll make it a 1 by multiplying row 2 by -1 so \(R_2 \rightarrow -R_2\).</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; -1 &amp; 1 \\
0 &amp; 1 &amp; -3 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>And then we need to zero out the remaining entries in the same column above that leading entry so apply \(R_1 = -2R_2 + R_1\)</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; 5 &amp; -1 \\
0 &amp; 1 &amp; -3 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>We finally are in RREF (columns with leading entries are zeroed out except for the leading entries and the leading entries are all 1s). We can now use the convention to use a variable to parameterize the solution set for the columns with no leading entries so let \(x_3 = t\). Based on this, we’ll see that</p>
<div>
$$
\begin{align*}
x_1 &amp;= -1-5t \\
x_2 &amp;= 1 + 3t
\end{align*}
$$
</div>
<p>Finally, we can write the solution set as</p>
<div>
$$
\begin{align*}
\{(-1-5t, 1+3t, t) | t \in \mathbf{R}\}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Gaussian Elimination consists of two passes. The forward pass is used to put the augmented matrix of linear equations into a Row Echelon Form. At this point, it will be clear if this system is inconsistent or has infinitely many solutions. The second stage of the algorithm is to do a backward pass on the augmented matrix to put it into a Reduced Row Echelon Form. Example 1 Suppose we have the following augmented matrix: $$ \begin{align*} \begin{pmatrix} 0 &amp; 1 &amp; 2 &amp; 3 \\ 1 &amp; 1 &amp; 1 &amp; 1 \\ 3 &amp; 2 &amp; 1 &amp; 2 \end{pmatrix}. \end{align*} $$ The first stage of the algorithm is the forward pass. We want to put this matrix into Row Echelon Form which means we’ll want non-zero leading entries and zero out the entries below them. In row 1 above, we have a zero, so we’ll swap row 1 and 2. $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 3 &amp; 2 &amp; 1 &amp; 2 \end{pmatrix}. \end{align*} $$ Next we want to zero out any entries below the leading entry. So we want to get rid of the 3 in column 3. To do that, we’ll multiply row 1 by -3 and add it to row 3. We write this as \(R_3 \rightarrow -3R_1 + R_3\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 0 &amp; -1 &amp; -2 &amp; -1 \end{pmatrix}. \end{align*} $$ Next, we want to ignore row 1 and move to the next row. In row 2, the next leading entry is in the most left non-zero column which is column 2. We’ll repeat the same process by zeroing the entries below it. This means we’ll want to get rid of the -1 in row 3. So we write \(R_3 \rightarrow R_2 + R_3\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 2 \end{pmatrix}. \end{align*} $$ We repeat this process until we have no more rows. At the end of this process, the matrix will be in Row Echelon Form. Notice here that this system is inconsistent because we have a leading entry in the very last column. The next phase of the algorithm is a backward pass to put the matrix in Reduced Row Echelon Form by making the leading entries 1 and zeroing out the non-leading entries in each column. We want to do this phase from right to left. For the last row, we need to multiply row 3 by 1/2 to make it 1. $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ Next, we’ll zero out the remaining entries above it by performing the operations \(R_2 \rightarrow -3R_3 + R_2\) and \(R_1 \rightarrow -R_3 + R_1\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ We then want to focus on the next leading entry in row 2 which happens to be in column 2. The leading entry is 1 already so we only need to zero out the 1 above it by performing \(R_1 = -R_2 + R_1\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ Now this matrix is in Reduced Row Echelon Form. Example 2 Find all the solutions to the following system $$ \begin{align*} x_1 + 2x_2 - x_3 &amp;= 1 \\ x_1 + x_2 - 2x_3 &amp;= 0 \\ 5x_1 + 8x_2 + x_3 &amp;= 1 \end{align*} $$ We’ll put the equations in an augmented matrix form: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 1 &amp; 1 &amp; 2 &amp; 0 \\ 5 &amp; 8 &amp; 1 &amp; 1 \end{pmatrix}. \end{align*} $$ Next, we’ll perform Gaussian Elimination by first performing the forward pass which will put the matrix in Row Echelon. We’ll make the top most left entry non-zero and zero out the remaining entries below it. So we’ll perform the operations \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; -2 &amp; 6 &amp; -4 \end{pmatrix}. \end{align*} $$ We’ll repeat the same process for the next leading entry in row 2 which is -1. We’ll zero out the entries below it by applying \(R_3 \rightarrow R_3 - 2R_2\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; -2 \end{pmatrix}. \end{align*} $$ Here we should know that that since there is a leading entry in the last column then this system is inconsistent and the system has no solutions and so we’re done. Example 3 Find all the solutions to the following system $$ \begin{align*} x_1 + 2x_2 - x_3 &amp;= 1 \\ x_1 + x_2 - 2x_3 &amp;= 0 \\ 5x_1 + 8x_2 + x_3 &amp;= 3 \end{align*} $$ We’ll put the equations in an augmented matrix form: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 1 &amp; 1 &amp; 2 &amp; 0 \\ 5 &amp; 8 &amp; 1 &amp; 3 \end{pmatrix}. \end{align*} $$ Step one is the forward pass. we’ll start with the top left most entry and make it a non-zero. it is already a non-zero entry. then we’ll zero out the remaining entries in that column by applying \(R_2 \rightarrow R_2 - R_1\) and \(R_3 \rightarrow R_3 - 5R_1\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; -2 &amp; 6 &amp; -2 \end{pmatrix}. \end{align*} $$ We’ll repeat this for the next leading entry in row 2 which is -1 and then zero out the below rows. We’ll apply \(R_3 \rightarrow R_3 - 2R_2\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 3 &amp; -1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ We have the leading entries 1 and -1. We’re in Row Echelon Form. We can describe the solution set now. but first we want to put it in Reduced Row Echelon Form. so we’ll do the backward pass. We’ll go right to left in the backward pass, making leading entries 1 and zeroing out the remaining entries in those columns. The first leading entry we’ll work on is -1 in row 2. We’ll make it a 1 by multiplying row 2 by -1 so \(R_2 \rightarrow -R_2\). $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; -1 &amp; 1 \\ 0 &amp; 1 &amp; -3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ And then we need to zero out the remaining entries in the same column above that leading entry so apply \(R_1 = -2R_2 + R_1\) $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; 5 &amp; -1 \\ 0 &amp; 1 &amp; -3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ We finally are in RREF (columns with leading entries are zeroed out except for the leading entries and the leading entries are all 1s). We can now use the convention to use a variable to parameterize the solution set for the columns with no leading entries so let \(x_3 = t\). Based on this, we’ll see that $$ \begin{align*} x_1 &amp;= -1-5t \\ x_2 &amp;= 1 + 3t \end{align*} $$ Finally, we can write the solution set as $$ \begin{align*} \{(-1-5t, 1+3t, t) | t \in \mathbf{R}\}. \end{align*} $$ References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry><entry><title type="html">Lecture 2: Echelon Form and Reduced Echelon Form</title><link href="http://localhost:4000/jekyll/update/2024/07/21/lec02-rref.html" rel="alternate" type="text/html" title="Lecture 2: Echelon Form and Reduced Echelon Form" /><published>2024-07-21T01:01:36-07:00</published><updated>2024-07-21T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/07/21/lec02-rref</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/21/lec02-rref.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A matrix is in Row Echelon Form (REF) if
<ol style="list-style-type:lower-alpha">
<li> All zero rows are below all nonzero rows.</li>
<li> The leading entry of of each row to be to the right of the leading entry of the row above.</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>For example, this matrix is in Row Echelon Form:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4  \\
0 &amp; 1 &amp; 1 &amp; 1  \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A matrix is in Reduced Row Echelon Form (RREF) if it is in REF and it satisfies the following two additional properties:
c) The leading entries are all 1.
d) The leading entries are the only non-zero entries in their column
<ol style="list-style-type:lower-alpha">
<li> The leading entries are all 1..</li>
<li> The leading entries are the only non-zero entries in their column.</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The same matrix from example 1 is not in RREF because column 2 for example still has non-zero entries above its leading entry.</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4  \\
0 &amp; 1 &amp; 1 &amp; 1  \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
Every matrix can be put in RREF by a finite sequence of elementary row operations.
<ol>
	<li>\(R_i \leftrightarrow R_j \) </li>
    <li> \(cR_i\) where \(c \neq 0\)</li>
    <li> \(R_i \rightarrow R_i + cR_j\) </li>
</ol>
</div>
<p><br />
Proof: by construction (Gaussian Elimination) … TODO?
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
The reduced RREF form of a matrix is unique. (Note row echelon form is not unique).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 4
</div>
<div class="purbdiv">
The solution set of a linear system with augmented matrix in reduced row echelon form is easily described in a standard way.
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Consider the system with augmented matrix as follows:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
\end{align*}
$$
</div>
<p>The last row has a single non-zero entry in the last column. This tells us that this system will not have a solution. To see this, translate back the system of linear equations</p>
<div>
$$
\begin{array}{ll}
x_1 \quad -x_3 &amp;=0 \\ 
\quad x_2 &amp;=0 \\ 
\quad \quad \quad \quad 0 &amp;= 1
\end{array}
$$
</div>
<p>We can see here that \(0=1\) has no solutions.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Consider the system with augmented matrix as follows:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 0 &amp; -1 &amp; 0  \\
0 &amp; 1 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{align*}
$$
</div>
<p>The last row has has all zeros. This tells us that we will have infinitely many solutions. To see this, translate back the system of linear equations</p>
<div>
$$
\begin{array}{ll}
x_1 \quad -x_3 &amp;=0 \\ 
\quad x_2 &amp;=0 \\ 
\quad \quad \quad \quad 0 &amp;= 0
\end{array}
$$
</div>
<p>This time we have infinitely many solutions. The convention is to look for a column without a leading entry like Column 3. The convention is to use the third variable \(x_3\) to parameterize the solution set. so set \(x_3 = t\), then</p>
<div>
$$
\begin{align*}
x_3 &amp;= t \\
x_1 &amp;= t \\
x_2 &amp;= 0.
\end{align*}
$$
</div>
<p>The solution set is \((x_1,x_2,x_3) = \{(t, t, 0) | t \in \mathbf{R}\}\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>Consider the system with augmented matrix as follows:</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; 1 &amp; 0 &amp; 2  \\
0 &amp; 0 &amp; 1 &amp; 3 &amp; 0 &amp; 3  \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 4  \\
\end{pmatrix}.
\end{align*}
$$
</div>
<p>This is a system of three equations in 5 unknowns. Column one is \(x_1\), column two is \(x_2\) and so on. This matrix is also in Row Reduced Echelon Form because the leading entries are 1 and the leading entries are the only non-zero entries in their columns. Also, we don’t have a leading entry in the very last column so we do have a solution. This translates to the system of linear:</p>
<div>
$$
\begin{array}{ll}
x_1 + 2x_2 \quad \quad + x_4 &amp;= 2 \\ 
\ \ \ \ \ \ \ \quad \quad \quad x_3 + 3x_4 &amp;= 3 \\ 
\quad \quad \quad \quad \quad \quad \quad \quad \quad x_5 &amp;= 4
\end{array}
$$
</div>
<p>We have infinitely many solutions. So we’ll go by the convention which is to look for the columns without a leading entries and assign them variables. Here, columns 2 and 4 have no leading entries so set \(x_2 = t_1\) and \(x_4 = t_2\). This means that \(x_3 = 3 - 3t_2\) and \(x_1 = 2 - 2t_1 - t_2\). Writing this a solution vector:</p>
<div>
$$
\begin{align*}
\{(2-2t_1-t_2, t_1, 3-3t_2, t_2, 4) | t_1, t_2 \in \mathbf{R} \}.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Corollary
</div>
<div class="purbdiv">
a system of linear equations has no solutions, exactly one solution (no parameters), or infinitely many solutions.
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A matrix is in Row Echelon Form (REF) if All zero rows are below all nonzero rows. The leading entry of of each row to be to the right of the leading entry of the row above. Example 1 For example, this matrix is in Row Echelon Form: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ Definition A matrix is in Reduced Row Echelon Form (RREF) if it is in REF and it satisfies the following two additional properties: c) The leading entries are all 1. d) The leading entries are the only non-zero entries in their column The leading entries are all 1.. The leading entries are the only non-zero entries in their column. Example 2 The same matrix from example 1 is not in RREF because column 2 for example still has non-zero entries above its leading entry. $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 0 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ Theorem 2 Every matrix can be put in RREF by a finite sequence of elementary row operations. \(R_i \leftrightarrow R_j \) \(cR_i\) where \(c \neq 0\) \(R_i \rightarrow R_i + cR_j\) Proof: by construction (Gaussian Elimination) … TODO? Theorem 3 The reduced RREF form of a matrix is unique. (Note row echelon form is not unique). Theorem 4 The solution set of a linear system with augmented matrix in reduced row echelon form is easily described in a standard way. Example 3 Consider the system with augmented matrix as follows: $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}. \end{align*} $$ The last row has a single non-zero entry in the last column. This tells us that this system will not have a solution. To see this, translate back the system of linear equations $$ \begin{array}{ll} x_1 \quad -x_3 &amp;=0 \\ \quad x_2 &amp;=0 \\ \quad \quad \quad \quad 0 &amp;= 1 \end{array} $$ We can see here that \(0=1\) has no solutions. Example 4 Consider the system with augmented matrix as follows: $$ \begin{align*} \begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{pmatrix}. \end{align*} $$ The last row has has all zeros. This tells us that we will have infinitely many solutions. To see this, translate back the system of linear equations $$ \begin{array}{ll} x_1 \quad -x_3 &amp;=0 \\ \quad x_2 &amp;=0 \\ \quad \quad \quad \quad 0 &amp;= 0 \end{array} $$ This time we have infinitely many solutions. The convention is to look for a column without a leading entry like Column 3. The convention is to use the third variable \(x_3\) to parameterize the solution set. so set \(x_3 = t\), then $$ \begin{align*} x_3 &amp;= t \\ x_1 &amp;= t \\ x_2 &amp;= 0. \end{align*} $$ The solution set is \((x_1,x_2,x_3) = \{(t, t, 0) | t \in \mathbf{R}\}\). Example 5 Consider the system with augmented matrix as follows: $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 0 &amp; 1 &amp; 0 &amp; 2 \\ 0 &amp; 0 &amp; 1 &amp; 3 &amp; 0 &amp; 3 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 4 \\ \end{pmatrix}. \end{align*} $$ This is a system of three equations in 5 unknowns. Column one is \(x_1\), column two is \(x_2\) and so on. This matrix is also in Row Reduced Echelon Form because the leading entries are 1 and the leading entries are the only non-zero entries in their columns. Also, we don’t have a leading entry in the very last column so we do have a solution. This translates to the system of linear: $$ \begin{array}{ll} x_1 + 2x_2 \quad \quad + x_4 &amp;= 2 \\ \ \ \ \ \ \ \ \quad \quad \quad x_3 + 3x_4 &amp;= 3 \\ \quad \quad \quad \quad \quad \quad \quad \quad \quad x_5 &amp;= 4 \end{array} $$ We have infinitely many solutions. So we’ll go by the convention which is to look for the columns without a leading entries and assign them variables. Here, columns 2 and 4 have no leading entries so set \(x_2 = t_1\) and \(x_4 = t_2\). This means that \(x_3 = 3 - 3t_2\) and \(x_1 = 2 - 2t_1 - t_2\). Writing this a solution vector: $$ \begin{align*} \{(2-2t_1-t_2, t_1, 3-3t_2, t_2, 4) | t_1, t_2 \in \mathbf{R} \}. \end{align*} $$ Corollary a system of linear equations has no solutions, exactly one solution (no parameters), or infinitely many solutions. References: Video Lectures from Math416 by Ely Kerman.]]></summary></entry></feed>