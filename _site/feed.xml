<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-08T20:40:38-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 23: Eigenvalues and Diagonalizability</title><link href="http://localhost:4000/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html" rel="alternate" type="text/html" title="Lecture 23: Eigenvalues and Diagonalizability" /><published>2024-08-20T01:01:36-07:00</published><updated>2024-08-20T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/20/lec23-eigenvalues-and-diagonalizability.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonal if all entries off diagonal are 0. \(A_{ij} = 0\) if \(i \neq j\).
</div>
<p><br />
All computations involving diagonal matrices are simple. If \(A, B\) are diagonal, then</p>
<div>
$$
\begin{align*}
\det(A) = A_{11}...A_{nn}
\end{align*}
$$
</div>
<p>Similarly, matrix multiplication of diagonal matrices is simple. The \(AB_{ij}\)th entry is</p>
<div>
$$
\begin{align*}
AB_{ij} &amp;= \begin{cases} A_{ij}B_{ij} \quad \text{if } i = j \\ 0\phantom{A_{ij}B} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This can be generalized to computing \((A)^k\) where the \(ij\) entry is</p>
<div>
$$
\begin{align*}
(A^k)_{ij} &amp;= \begin{cases} (A_{ij})^k \quad \text{if } i = j \\ 0\phantom{(A_{ij})} \quad \text{if } i \neq j \end{cases}
\end{align*}
$$
</div>
<p>This leads to the question of whether we can transform any matrix to a diagonal matrix so we can perform these computations easily. In the next definition we formalize this,
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(T \in V \rightarrow V\) is diagonalizable if there is a basis \(\beta = \{v_1,...,v_n\}\) of \(V\) such that 
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
Two questions:
<br />
Questions 1: What does such a basis exist?
<br />
Question 2: If it exists, how can we compute it?
<br />
<br />
A basis \(\beta = \{v_1,...,v_n\}\) such that
\([T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}.\)
This is equivalent to</p>
<ul style="list-style: none;">
	<li> \(\leftrightarrow [T(v_j)]_{\beta} =   \begin{pmatrix} 0 \\\vdots \\\lambda_j \\ \vdots \\ 0 \end{pmatrix}\). for \(j = 1,..,n\).</li>
    <li>\(\leftrightarrow [T(v_j)]_{\beta} =  \lambda_j \begin{pmatrix} 0 \\\vdots \\1 \\ \vdots \\ 0 \end{pmatrix} = \lambda_j [v_j]_{\beta} = [\lambda_jv_j]_{\beta} \) for \(j = 1,...,n\). This is true because that coordinate expression is the coordinate expression of the \(j\)th vector in the basis \(\beta\). </li>
	<li>\(\leftrightarrow T[v_j] = \lambda_jv_j\) for \(j = 1,...,n\) and \(\lambda_1,...,\lambda_n \in \mathbf{R}\). We can just write the transformation since they are both with respect to basis \(\beta\).  </li>
	<li>\(\leftrightarrow T[v] = \lambda v\).  </li>
	<li>\(\leftrightarrow T[v] = \lambda I_V(v)\). Since the identity matrix does nothing. </li>
	<li>\(\leftrightarrow T[v] - \lambda I_V(v) = \bar{0}_V\). </li>
	<li>\(\leftrightarrow (T - \lambda I_V)(v) = \bar{0}_V\). </li>
</ul>
<p>The left hand side is a family of linear maps parameterized by \(\lambda\). So we want all the non-zero elements \(v\) of the null space (we don’t care about the zero solution since we want to build a basis).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of a Linear Transformation</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(T \in V \rightarrow V\) is a \(v \neq \bar{0}_V\) such that
$$
\begin{align*}
T(v) = \lambda v
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(T \in V \rightarrow V\) if \(\exists v \neq \bar{0}_V\) such that \(T(v) = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(T \in V \rightarrow V\) is diagonalizable if and only if there is a basis \(\beta = \{v_1,...,v_n\}\) consisting of eigenvectors.
</div>
<!------------------------------------------------------------------------------------>
<p><br />
This is the same as finding</p>
<div>
$$
\begin{align*}
[T]_{\beta}^{\beta} = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}, \lambda_1,...,\lambda_n \text{ eigenvalues}
\end{align*}
$$
</div>
<p>Instead of focusing on these general linear maps. Let’s turn now to focus on matrices.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenvectors and Eigenvalues of Matrices</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n}\) is diagonalizable if \(L_A\) is diagonalizable.
</div>
<p><br />
This is equivalent to “There is a \(Q \in M_{n \times n}\) such that \(Q^{-1}AQ\) is diagonal”.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An eigenvector of \(A \in M_{n \times n}\) is a \(v \neq \bar{0} \in \mathbf{R}^n\) such that \(Av = \lambda v\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
and finally,
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(\lambda \in \mathbf{R}\) is an eigenvalue of \(A \in M_{n \times n}\) if \(\exists v \neq \bar{0}_V\) such that \(Av = \lambda v\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Finding eigenvectors</b></h4>
<p>Okay now that we’ve narrowed down the discussion to matrices, how do we actually find these eigenvectors of \(A\)?
<br />
<br />
Again, let set the null space of \(A\) to \(N(A) = N(L_A)\). Next we will need the following lemma
<br /></p>
<div class="bdiv">
Lemma
</div>
<div class="bbdiv">
\(v \in \mathbf{R}^n\) is an eigenvector of \(A\) with eigenvalue \(\lambda\) if and only if \(v \in N(A - \lambda I_n)\).
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(v\) is an eigenvector of (A). By definition this means that \(Av = \lambda v\). We can re-write this as,</p>
<div>
$$
\begin{align*}
Av &amp;= \lambda I_n v \\
Av - \lambda I_n v &amp;= \bar{0}
\end{align*}
$$
</div>
<p>But this precisely means that \(v \in N(A - \lambda I_n) \ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Find all the eigen values of</p>
<div>
$$
\begin{align*}
A = 
\begin{pmatrix} 
0 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 0 \\
-2 &amp; 0 &amp; 3
\end{pmatrix}
\end{align*}
$$
</div>
<p>with eigenvalue 1.
<br />
<br />
By the lemma, we want all vectors in the null space of \(A - \lambda I_n\).</p>
<div>
$$
\begin{align*}
A - (1)I_n = 
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
\end{align*}
$$
</div>
<p>We’ll put this matrix in row echelon form.</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 2
\end{pmatrix}
R_3 \rightarrow -2R_1 + R_3, R_1 \leftrightarrow -1R_1
\begin{pmatrix} 
1 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>From this we see that the null space consists of vectors of the form</p>
<div>
$$
\begin{align*}
&amp;= \{(x_1, x_2, x_3) \ | \ x_3 = t, x_1 = t, x_2 = 0\}. \\
&amp;= \{(t, 0, t) \ | \ t \in \mathbf{R} \} \\
&amp;= span\{ (1,0,1) \}
\end{align*}
$$
</div>
<p>This is easy because we are given the eigenvalue. But typically, we also need to find the eigenvalues too!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Eigenspace</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
If \(\lambda\) is an eigenvalue of \(A\), then the eigenspace of \(A\) corresponding to \(\lambda\) is
$$
\begin{align*}
E_{\lambda} &amp;= N(A - \lambda I_n) \\
&amp;= \{ \text{eigenvectors for } \lambda \} \cup \{\bar{0}\}
\end{align*}
$$
</div>

<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(A \in M_{n \times n}\) is diagonal if all entries off diagonal are 0. \(A_{ij} = 0\) if \(i \neq j\). All computations involving diagonal matrices are simple. If \(A, B\) are diagonal, then $$ \begin{align*} \det(A) = A_{11}...A_{nn} \end{align*} $$ Similarly, matrix multiplication of diagonal matrices is simple. The \(AB_{ij}\)th entry is $$ \begin{align*} AB_{ij} &amp;= \begin{cases} A_{ij}B_{ij} \quad \text{if } i = j \\ 0\phantom{A_{ij}B} \quad \text{if } i \neq j \end{cases} \end{align*} $$ This can be generalized to computing \((A)^k\) where the \(ij\) entry is $$ \begin{align*} (A^k)_{ij} &amp;= \begin{cases} (A_{ij})^k \quad \text{if } i = j \\ 0\phantom{(A_{ij})} \quad \text{if } i \neq j \end{cases} \end{align*} $$ This leads to the question of whether we can transform any matrix to a diagonal matrix so we can perform these computations easily. In the next definition we formalize this, Definition \(T \in V \rightarrow V\) is diagonalizable if there is a basis \(\beta = \{v_1,...,v_n\}\) of \(V\) such that $$ \begin{align*} [T]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix} \end{align*} $$ Two questions: Questions 1: What does such a basis exist? Question 2: If it exists, how can we compute it? A basis \(\beta = \{v_1,...,v_n\}\) such that \([T]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix}.\) This is equivalent to \(\leftrightarrow [T(v_j)]_{\beta} = \begin{pmatrix} 0 \\\vdots \\\lambda_j \\ \vdots \\ 0 \end{pmatrix}\). for \(j = 1,..,n\). \(\leftrightarrow [T(v_j)]_{\beta} = \lambda_j \begin{pmatrix} 0 \\\vdots \\1 \\ \vdots \\ 0 \end{pmatrix} = \lambda_j [v_j]_{\beta} = [\lambda_jv_j]_{\beta} \) for \(j = 1,...,n\). This is true because that coordinate expression is the coordinate expression of the \(j\)th vector in the basis \(\beta\). \(\leftrightarrow T[v_j] = \lambda_jv_j\) for \(j = 1,...,n\) and \(\lambda_1,...,\lambda_n \in \mathbf{R}\). We can just write the transformation since they are both with respect to basis \(\beta\). \(\leftrightarrow T[v] = \lambda v\). \(\leftrightarrow T[v] = \lambda I_V(v)\). Since the identity matrix does nothing. \(\leftrightarrow T[v] - \lambda I_V(v) = \bar{0}_V\). \(\leftrightarrow (T - \lambda I_V)(v) = \bar{0}_V\). The left hand side is a family of linear maps parameterized by \(\lambda\). So we want all the non-zero elements \(v\) of the null space (we don’t care about the zero solution since we want to build a basis). Eigenvectors and Eigenvalues of a Linear Transformation Definition An eigenvector of \(T \in V \rightarrow V\) is a \(v \neq \bar{0}_V\) such that $$ \begin{align*} T(v) = \lambda v \end{align*} $$ Definition \(\lambda \in \mathbf{R}\) is an eigenvalue of \(T \in V \rightarrow V\) if \(\exists v \neq \bar{0}_V\) such that \(T(v) = \lambda v\). Theorem \(T \in V \rightarrow V\) is diagonalizable if and only if there is a basis \(\beta = \{v_1,...,v_n\}\) consisting of eigenvectors. This is the same as finding $$ \begin{align*} [T]_{\beta}^{\beta} = \begin{pmatrix} \lambda_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \lambda_n \end{pmatrix}, \lambda_1,...,\lambda_n \text{ eigenvalues} \end{align*} $$ Instead of focusing on these general linear maps. Let’s turn now to focus on matrices. Eigenvectors and Eigenvalues of Matrices Definition \(A \in M_{n \times n}\) is diagonalizable if \(L_A\) is diagonalizable. This is equivalent to “There is a \(Q \in M_{n \times n}\) such that \(Q^{-1}AQ\) is diagonal”. Definition An eigenvector of \(A \in M_{n \times n}\) is a \(v \neq \bar{0} \in \mathbf{R}^n\) such that \(Av = \lambda v\). and finally, Definition \(\lambda \in \mathbf{R}\) is an eigenvalue of \(A \in M_{n \times n}\) if \(\exists v \neq \bar{0}_V\) such that \(Av = \lambda v\). Finding eigenvectors Okay now that we’ve narrowed down the discussion to matrices, how do we actually find these eigenvectors of \(A\)? Again, let set the null space of \(A\) to \(N(A) = N(L_A)\). Next we will need the following lemma Lemma \(v \in \mathbf{R}^n\) is an eigenvector of \(A\) with eigenvalue \(\lambda\) if and only if \(v \in N(A - \lambda I_n)\). Proof: Suppose \(v\) is an eigenvector of (A). By definition this means that \(Av = \lambda v\). We can re-write this as, $$ \begin{align*} Av &amp;= \lambda I_n v \\ Av - \lambda I_n v &amp;= \bar{0} \end{align*} $$ But this precisely means that \(v \in N(A - \lambda I_n) \ \blacksquare\). Example Find all the eigen values of $$ \begin{align*} A = \begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 3 \end{pmatrix} \end{align*} $$ with eigenvalue 1. By the lemma, we want all vectors in the null space of \(A - \lambda I_n\). $$ \begin{align*} A - (1)I_n = \begin{pmatrix} -1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ -2 &amp; 0 &amp; 2 \end{pmatrix} \end{align*} $$ We’ll put this matrix in row echelon form. $$ \begin{align*} \begin{pmatrix} -1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ -2 &amp; 0 &amp; 2 \end{pmatrix} R_3 \rightarrow -2R_1 + R_3, R_1 \leftrightarrow -1R_1 \begin{pmatrix} 1 &amp; 0 &amp; -1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix} \end{align*} $$ From this we see that the null space consists of vectors of the form $$ \begin{align*} &amp;= \{(x_1, x_2, x_3) \ | \ x_3 = t, x_1 = t, x_2 = 0\}. \\ &amp;= \{(t, 0, t) \ | \ t \in \mathbf{R} \} \\ &amp;= span\{ (1,0,1) \} \end{align*} $$ This is easy because we are given the eigenvalue. But typically, we also need to find the eigenvalues too! Eigenspace Definition If \(\lambda\) is an eigenvalue of \(A\), then the eigenspace of \(A\) corresponding to \(\lambda\) is $$ \begin{align*} E_{\lambda} &amp;= N(A - \lambda I_n) \\ &amp;= \{ \text{eigenvectors for } \lambda \} \cup \{\bar{0}\} \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 22: Determinants and Invertible Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/19/lec22-determinants-invertible-matrices.html" rel="alternate" type="text/html" title="Lecture 22: Determinants and Invertible Matrices" /><published>2024-08-19T01:01:36-07:00</published><updated>2024-08-19T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/19/lec22-determinants-invertible-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/19/lec22-determinants-invertible-matrices.html"><![CDATA[<p>Last time we proved the theorem that states exactly how row operations change the value of the determinant and then we proved another result where the determinant of upper (lower) triangular matrices is equal to the product of the diagonal entries. This meant that if we put a matrix \(A\) in REF, then we can just multiply the diagonal entries to calculate the determinant in addition to accounting for the type of row operations we applied and how they contribute to the determinant. Specifically, we saw that if we applied some \(k\) row operations on \(A\) to get to \(B\) (REF),</p>
<div>
$$
\begin{align*}
A \xrightarrow{R_k,...,R_2,R_1} B
\end{align*}
$$
</div>
<p>Then,</p>
<div>
$$
\begin{align*}
B_{11}...B_{nn} &amp;= \mathcal{E}(\mathcal{R}_k)...\mathcal{E}(\mathcal{R}_1)\det(A), \\
\end{align*}
$$
</div>
<p>where</p>
<div>
$$
\begin{align*}
\mathcal{E}(R) &amp;= 
\begin{cases} 
-1, \quad \mathcal{R}\text{ type I } \\
c, \quad \mathcal{R}\text{ type II w/ scalar $c$ } \\
1, \quad \mathcal{R}\text{ type III }
\end{cases}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\det(A) &amp;= \frac{B_{11}...B_{nn}}{ \mathcal{E}(\mathcal{R}_k)...\mathcal{E}(\mathcal{R}_1)}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>It is clear that none of these row operations will lead the determinant to be 0. In fact \(\det(A) \neq 0\) is equivalent to</p>
<ul style="list-style: none;">
<li>\(\leftrightarrow \det(REF) \neq 0\)</li>
<li>\(\leftrightarrow REF\) of \(A\) has \(n\) leading entries</li>
<li>\(\leftrightarrow A\) is invertible</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
Next, we want to prove a general theorem that states that \(\det(AB) = \det(A)\det(B)\) but in order to do so, we’ll need a couple of theorems. The first theorem is from <a href="https://strncat.github.io/jekyll/update/2024/08/10/lec18-elementary-matrices.html"> lecture 18</a>. We stated a corollary without a proof but now we’ll call it theorem (a) and prove it.
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (a)
</div>
<div class="purbdiv">
\(A\) is invertible if and only if it can be written as a product of elementary matrices.
</div>
<p><br />
<b>Proof:</b>
<br />
\(A\) is invertible is equivalent to</p>
<ul style="list-style: none;">

<li>\(\leftrightarrow RREF \text{ is } I_n\). When we computed the inverse, we needed  \(A\)'s RREF to be the identity matrix.</li>

<li>\(\leftrightarrow\) This meant that we applied a sequence of elementary row operations on \(A\) to get the identity matrix. These elementary row operations can be represented with matrix multiplication (<a href="https://strncat.github.io/jekyll/update/2024/08/10/lec18-elementary-matrices.html">lecture 18</a>). So \(E_k...E_1A = I_n\). </li>

<li>\(\leftrightarrow\) So we can solve for \(A\) and get \(A = E_1^{-1}...E_k^{-1}\).</li>

<li>\(\leftrightarrow\) These elementary matrices are invertible and they are themselves elementary matrices. So \(A = E_1^{'}...E_k^{'}\) and \(A\) is invertible. \(\blacksquare\)</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
Next, we’ll state the second theorem that we will need
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(A, B\) are invertible if and only if \(AB\) is invertible.
</div>
<p><br />
<br />
<b>Proof:</b>
<br />
\(\Rightarrow\): Suppose \(A\) and \(B\) are invertible. Then, \((AB)^{-1} = B^{-1}A^{-1}\).
<br />
<br />
\(\Leftarrow\): Suppose that \(AB\) is invertible. This implies</p>

<ul style="list-style: none;">

<li>\(\rightarrow L_{ab} = L_a \circ L_b\) is invertible.</li>

<li>\(\rightarrow L_b\) is one to one and \(L_a\) is onto. (We proved this in HW 5 or 6)</li>

<li>\(\rightarrow\) both \(L_a \text{ and } L_b\) map from a vector space to itself. (Note: this was vague but I did prove in the same homework that both matrices must be \(n \times n\) matrices. Here the linear transformations are of the same dimension) as an implication.</li>

<li>\(\rightarrow\) Since they're maps from a vector space to itself, then being onto or one-to-one implies the other so they're both bijective and so both are invertible </li>

<li>\(\rightarrow B, A\) are invertible. \(\blacksquare\) </li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------>
Finally we’re readying to prove the main theorem:
<br />
<br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
For \(A, B \in M_{n \times n}\)
$$
\begin{align*}
\det(AB) = \det(A)(B)
\end{align*}
$$
</div>
<p><br />
<br />
<b>Proof:</b>
<br />
By Theorem (b) if either \(A\) or \(B\) fails to be invertible, then</p>
<div>
$$
\begin{align*}
\det(A)\det(B) = 0 = \det(AB)
\end{align*}
$$
</div>
<p>So assume that \(A\) and \(B\) are both invertible. We have two cases:
<br />
Case 1: \(A\) is an elementary matrix so \(A = E(\mathcal{R})\) for some row operation \(\mathcal{R}\). What do we know about the determinant of \(A\)? We need to apply the row operation on the identity matrix and then figure out the type of row operation to determine the relationship between the determinant of \(A\) and the determinant of \(I\). So for,</p>
<div>
$$
\begin{align*}
I_n \xrightarrow{\mathcal{R}} E(\mathcal{R})
\end{align*}
$$
</div>
<p>We just need to know the type of operation we applied and apply the previous theorem to figure out \(\mathcal{E}(\mathcal{R})\) (whether it’s 1, 1 or \(-c\)). So we’ll have</p>
<div>
$$
\begin{align*}
\det(E(R)) = \mathcal{E}(\mathcal{R}) \det I_n = \mathcal{E}(\mathcal{R})  &amp;= 
\begin{cases} 
-1, \quad \mathcal{R}\text{ type I } \\
c, \quad \mathcal{R}\text{ type II w/ scalar $c$ } \\
1, \quad \mathcal{R}\text{ type III }
\end{cases}
\end{align*}
$$
</div>
<p>Putting all of this together</p>
<div>
$$
\begin{align*}
\det(AB) &amp;= \det(E(\mathcal{R})B) \\ 
         &amp;= \mathcal{E}(\mathcal{R})\det(B) \\
		 &amp;= \det(A)\det(B).
\end{align*}
$$
</div>
<p>For the general case, we know we can write \(A\) and \(B\) as products of elementary matrices by theorem (a) and so</p>
<div>
$$
\begin{align*}
\det(AB) &amp;= \det(E_1E_2...E_kB) \\ 
        &amp;= \det(E_1(E_2...E_kB)) \text{ (matrix multiplication is associative)} \\ 
         &amp;= \det(E_1)\det(E_2...E_kB) \text{ (by Case 1)} \\
		 &amp;= \det(E_1)\det(E_2)...\det(E_k)\det(B)
\end{align*}
$$
</div>
<p>Notice here that the product \(\det(E_1)\det(E_2)\) is just \(\det(E_1E_2)\) because \(E_1\) is an elementary matrix (we’re doing the reverse of Case 1). We can continue doing so until we get \(\det(A)\) as follows</p>
<div>
$$
\begin{align*}
\det(AB) &amp;= \det(E_1)\det(E_2)...\det(E_k)\det(B) \\
         &amp;= \det(E_1E_2)...\det(E_k)\det(B) \\
		 &amp;= \det(E_1E_2E_k)\det(B) \\
		 &amp;= \det(A)\det(B). \blacksquare
\end{align*}
$$
</div>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Cramer's Rule</b></h4>
<p>Observation: If \(A \in M_{n \times n}\) is invertible, then any system \(A\bar{x}=\bar{b}\) has a unique solution.</p>
<div>
$$
\begin{align*}
\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} 
&amp;=
\bar{x}
=
A^{-1}b.
\end{align*}
$$
</div>
<p>Cramer’s rule allows to analyze the component of the solution in this setting. What does this mean? For each \(k = 1,...,n\).</p>
<div>
$$
\begin{align*}
x_ = \frac{\det(M_k)}{\det(A)},
\end{align*}
$$
</div>
<p>where \(M_k\) is obtained from \(A\) by replacing the \(k\)th column by \(\bar{b}\). This allows to solve for a particular component of the solution without having to solve for everything. It also allows us to analyze the dependence on \(A\) and \(\bar{b}\). Why is this true?
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof:</b>
Let</p>
<div>
$$
\begin{align*}
A &amp;= \begin{pmatrix} a_1 &amp; \cdots &amp; a_k \cdots &amp; a_n \end{pmatrix} \\
M_k &amp;= \begin{pmatrix} a_1 &amp; \cdots &amp; \bar{b_k} \cdots &amp; a_n \end{pmatrix}.
\end{align*}
$$
</div>
<p>The claim is that if we devide the determinant of \(A\) by the determinant of \(M_k\), we will get the \(k\)th component. Define the matrix \(X_k\) where we will the identity matrix and replace the \(k\)th column with the \(\bar{x}\).</p>
<div>
$$
\begin{align*}
I_n &amp;= \begin{pmatrix} e_1 &amp; \cdots &amp; \bar{e_k} \cdots &amp; e_n \end{pmatrix} \\
X_n &amp;= \begin{pmatrix} e_1 &amp; \cdots &amp; \bar{x} \cdots &amp; e_n \end{pmatrix}
\end{align*}
$$
</div>
<p>Now, compute the product of \(A\) and \(X\).</p>
<div>
$$
\begin{align*}
AX_n &amp;= \begin{pmatrix} Ae_1 &amp; \cdots &amp; A\bar{x} \cdots &amp; Ae_k \end{pmatrix} \text{ (definition of matrix multiplication)} \\
&amp;= \begin{pmatrix} a_1 &amp; \cdots &amp; \bar{b} \cdots &amp; a_n \end{pmatrix} \text{ (Because $Ae_1$ is just the first column of $A$)}\\
&amp;= M_k
\end{align*}
$$
</div>
<p>So now take the determinant of both sides to see that,</p>
<div>
$$
\begin{align*}
\det(AX_n) &amp;= \det(M_k) \\
\det(A)\det(X_n) &amp;= \det(M_k)  \text{ (By the previous theorem)}\\
\det(X_n) &amp;= \frac{\det(M_k)}{\det(A)}
\end{align*}
$$
</div>
<p>Let’s compute \(\det(X_n)\) along the \(r=k\)th column</p>
<div>
$$
\begin{align*}
\det(X_n) &amp;= \sum_{j=1}^n(-1)^{k+j}(X_n)_{kj}\det(\tilde{X_k})_{kj} \\
          &amp;= (-1)^{k+k}(x_k)\det(I_{n-1}) \\
		  &amp;= x_k. \ \blacksquare
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(F: M_{n \times n} \rightarrow \mathbf{R}\) such that
<ol>
	<li>\(F\) is linear in rows.</li>
	<li>\(F(A) = 0\) if \(A\) has two identical rows.</li>
	<li>\(F(I_n) = 1.\)</li>
</ol>
Then \(F = det\)
</div>
<p><br />
Any map with those three properties has to be the determinant!</p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we proved the theorem that states exactly how row operations change the value of the determinant and then we proved another result where the determinant of upper (lower) triangular matrices is equal to the product of the diagonal entries. This meant that if we put a matrix \(A\) in REF, then we can just multiply the diagonal entries to calculate the determinant in addition to accounting for the type of row operations we applied and how they contribute to the determinant. Specifically, we saw that if we applied some \(k\) row operations on \(A\) to get to \(B\) (REF), $$ \begin{align*} A \xrightarrow{R_k,...,R_2,R_1} B \end{align*} $$ Then, $$ \begin{align*} B_{11}...B_{nn} &amp;= \mathcal{E}(\mathcal{R}_k)...\mathcal{E}(\mathcal{R}_1)\det(A), \\ \end{align*} $$ where $$ \begin{align*} \mathcal{E}(R) &amp;= \begin{cases} -1, \quad \mathcal{R}\text{ type I } \\ c, \quad \mathcal{R}\text{ type II w/ scalar $c$ } \\ 1, \quad \mathcal{R}\text{ type III } \end{cases} \end{align*} $$ Therefore, $$ \begin{align*} \det(A) &amp;= \frac{B_{11}...B_{nn}}{ \mathcal{E}(\mathcal{R}_k)...\mathcal{E}(\mathcal{R}_1)} \end{align*} $$ It is clear that none of these row operations will lead the determinant to be 0. In fact \(\det(A) \neq 0\) is equivalent to \(\leftrightarrow \det(REF) \neq 0\) \(\leftrightarrow REF\) of \(A\) has \(n\) leading entries \(\leftrightarrow A\) is invertible Next, we want to prove a general theorem that states that \(\det(AB) = \det(A)\det(B)\) but in order to do so, we’ll need a couple of theorems. The first theorem is from lecture 18. We stated a corollary without a proof but now we’ll call it theorem (a) and prove it. Theorem (a) \(A\) is invertible if and only if it can be written as a product of elementary matrices. Proof: \(A\) is invertible is equivalent to]]></summary></entry><entry><title type="html">Lecture 21: Determinants and Row Operations</title><link href="http://localhost:4000/jekyll/update/2024/08/18/lec21-determinants-row-operations.html" rel="alternate" type="text/html" title="Lecture 21: Determinants and Row Operations" /><published>2024-08-18T01:01:36-07:00</published><updated>2024-08-18T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/18/lec21-determinants-row-operations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/18/lec21-determinants-row-operations.html"><![CDATA[<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}\). If we perform the following row operations
<ol style="list-style-type:upper-roman;">
	<li>\(A \xrightarrow{R_i \leftrightarrow R_j} B \), then \(\det(B) = -\det(A)\)</li>
	<li>\(A \xrightarrow{R_i \rightarrow cR_i} B \), then \(\det(B) = c\det(A)\)</li>
	<li>\(A \xrightarrow{R_i \rightarrow R_i + cR_j} B \), then \(\det(B) = -\det(A)\)</li>	
</ol>
</div>
<p><br />
We will only need to use Theorem 1 and Corollary 2 from last lecture to prove this result!
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
For (II) Let</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, 
	B = \begin{pmatrix} a_1 \\ \vdots \\ ca_i \\ \vdots \\ a_n \end{pmatrix}, 
	\end{align*}
	$$
</div>
<p>Using theorem 1 we know that \(\det(B) = c\det(A)\) as we wanted to show. For (III) Let</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i  \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	B = \begin{pmatrix} a_1 \\ \vdots \\ a_i+ca_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	C = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	\end{align*}
	$$
</div>
<p>Using theorem 1 we know that \(\det(B) = \det(A) + c\det(C)\). But Corollary 2 implies that \(\det(C) = 0\) since \(C\) has two identical rows. Therefore, \(\det(B) = \det(A)\) as we wanted to show. For (I)</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, 
	B = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, 
	C = \begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix}, 
	\end{align*}
	$$
</div>
<p>One thing we know right away here is that \(\det(C)=0\) since \(C\) has two identical rows (by corollary 2) so</p>
<div>
	$$
	\begin{align*}
	0 = \det\begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix} &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} \\
	&amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} + 
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} +
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} +
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} \\
	&amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} +
	\det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} \\
0	&amp;= \det(A) + \det(B) \\
det(A) &amp;= -\det(B)
	\end{align*}
	$$
</div>
<p>Which is what we wanted to show. \(\blacksquare\)
<!------------------------------------------------------------------------------------>
<br />
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(A \in M_{n \times n }\) is upper(lower) triangular if all entries below (above) diagonal are zero.
</div>
<p><br />
For example any \(n \times n\) matrix in REF is upper triangular. One reason why the Upper/Lower triangular matrices are interesting is the following theorem.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(A \in M_{2 \times 2}\) is upper or lower triangular then
$$
\begin{align*}
\det(A) = A_{11}A_{22}A_{nn}
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
For upper triangular matrices, by induction on \(n\).
<br />
<br />
Base Case: \(n = 2\)</p>
<div>
$$
\begin{align*}
\det
\begin{pmatrix} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix}
= A_{11}A_{22}.
\end{align*}
$$
</div>

<p>Inductive Case: assume this is true for \(n-1 \geq 2\). Consider</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 
A_{11} &amp; A_{12} &amp; \cdots &amp; \cdots &amp; A_{1n} \\ 
0 &amp; A_{22} &amp; \cdots &amp; \cdots &amp; \vdots \\
\vdots &amp; 0 &amp; \ddots &amp; \cdots &amp; \vdots \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; A_{nn} \\
\end{pmatrix}
\end{align*}
$$
</div>
<p>We can compute the determinant by choosing to cofactor along the \(n\)th row and so,</p>
<div>
$$
\begin{align*}
\det(A) = \sum^n_{j=1}(-1)^{n+j}A_{nj}\det(\tilde{A_{nj}}) \\
= (-1)^{n+n}A_{nn}\det(\tilde{A_{nn}}) \\
\end{align*}
$$
</div>
<p>But \(\tilde{A_{nn}}\) is an \(n-1 \times n-1\) upper triangular matrix. So by the inductive hypothesis its determinant is the product of the diagonal entries and so</p>
<div>
$$
\begin{align*}
\det(\tilde{A_{nn}} = A_{11}A_{22}...A_{n-1n-1}
\end{align*}
$$
</div>
<p>And therefore,</p>
<div>
$$
\begin{align*}
\det(A) = A_{11}A_{22}...A_{nn}
\end{align*}
$$
</div>
<p>As we wanted to show. \(\blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Compute the determinant for 
\(A = \begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	1 &amp; 0 &amp; 2 \\
	2 &amp; 1 &amp; 1
	\end{pmatrix}\).
<br />
<br />
To use the theorem, we’ll put \(A\) in REF.</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	1 &amp; 0 &amp; 2 \\
	2 &amp; 1 &amp; 1
	\end{pmatrix}
	R_2 \rightarrow R_2 - R_1
	R_3 \rightarrow R_3 - 2R_1
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; -2 &amp; -1 \\
	0 &amp; -3 &amp; -5
	\end{pmatrix} 
	\end{align*}
	$$
</div>
<p>Note here that these two operations will not change the value of the determinant per thoerem 1 from last lecture. Continuing with REF:</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; -2 &amp; -1 \\
	0 &amp; -3 &amp; -5
	\end{pmatrix}
	R_2 \rightarrow -\frac{1}{2}R_2
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; -3 &amp; -5
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Here, since we scaled by \(-\frac{1}{2}\) then we know that \(\det(A) = -2\det(RREF(A))\).</p>
<div>
	$$
	\begin{align*}
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; -3 &amp; -5
	\end{pmatrix}
	R_3 \rightarrow 3R_2 + R_3
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; 0 &amp; -\frac{7}{2} 
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>This operation will not further change the value of the determinant. So now we can apply the theorem to compute the determinant since \(A\) is an upper triangular matrix</p>
<div>
	$$
	\begin{align*}
	\det
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	1 &amp; 0 &amp; 2 \\
	2 &amp; 1 &amp; 1
	\end{pmatrix}
	= -2\det
	\begin{pmatrix}
	1 &amp; 2 &amp; 3 \\
	0 &amp; 1 &amp; \frac{1}{2} \\
	0 &amp; 0 &amp; -\frac{7}{2} 
	\end{pmatrix}
	= -2(1 * 1 * -\frac{7}{2}) = 7
	\end{align*}
	$$
</div>
<p><br />
<!--------------------------------------------------------------------------></p>
<h4><b>Comparisons</b></h4>
<p>Computing \(\det(A)\) for \(A \in M_{n \times n}\) using the inductive formula is roughly \(n!\) operations. Using row operations, it is roughly \(n^3\) operations.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem Let \(A \in M_{n \times n}\). If we perform the following row operations \(A \xrightarrow{R_i \leftrightarrow R_j} B \), then \(\det(B) = -\det(A)\) \(A \xrightarrow{R_i \rightarrow cR_i} B \), then \(\det(B) = c\det(A)\) \(A \xrightarrow{R_i \rightarrow R_i + cR_j} B \), then \(\det(B) = -\det(A)\) We will only need to use Theorem 1 and Corollary 2 from last lecture to prove this result! Proof For (II) Let $$ \begin{align*} A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, B = \begin{pmatrix} a_1 \\ \vdots \\ ca_i \\ \vdots \\ a_n \end{pmatrix}, \end{align*} $$ Using theorem 1 we know that \(\det(B) = c\det(A)\) as we wanted to show. For (III) Let $$ \begin{align*} A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, B = \begin{pmatrix} a_1 \\ \vdots \\ a_i+ca_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, C = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, \end{align*} $$ Using theorem 1 we know that \(\det(B) = \det(A) + c\det(C)\). But Corollary 2 implies that \(\det(C) = 0\) since \(C\) has two identical rows. Therefore, \(\det(B) = \det(A)\) as we wanted to show. For (I) $$ \begin{align*} A &amp;= \begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix}, B = \begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix}, C = \begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix}, \end{align*} $$ One thing we know right away here is that \(\det(C)=0\) since \(C\) has two identical rows (by corollary 2) so $$ \begin{align*} 0 = \det\begin{pmatrix} a_1 \\ \vdots \\ a_i+a_j \\ \vdots \\ a_i+a_j \\ \vdots \\ a_n \end{pmatrix} &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i + a_j \\ \vdots \\ a_n \end{pmatrix} \\ &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} \\ &amp;= \det\begin{pmatrix} a_1 \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ a_n \end{pmatrix} + \det\begin{pmatrix} a_1 \\ \vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \\ a_n \end{pmatrix} \\ 0 &amp;= \det(A) + \det(B) \\ det(A) &amp;= -\det(B) \end{align*} $$ Which is what we wanted to show. \(\blacksquare\) Definition \(A \in M_{n \times n }\) is upper(lower) triangular if all entries below (above) diagonal are zero. For example any \(n \times n\) matrix in REF is upper triangular. One reason why the Upper/Lower triangular matrices are interesting is the following theorem. Theorem If \(A \in M_{2 \times 2}\) is upper or lower triangular then $$ \begin{align*} \det(A) = A_{11}A_{22}A_{nn} \end{align*} $$ Proof For upper triangular matrices, by induction on \(n\). Base Case: \(n = 2\) $$ \begin{align*} \det \begin{pmatrix} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix} = A_{11}A_{22}. \end{align*} $$]]></summary></entry><entry><title type="html">Section 2.2: Exercise 12</title><link href="http://localhost:4000/jekyll/update/2024/08/17/lec13-exercise-12.html" rel="alternate" type="text/html" title="Section 2.2: Exercise 12" /><published>2024-08-17T01:01:36-07:00</published><updated>2024-08-17T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/17/lec13-exercise-12</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/17/lec13-exercise-12.html"><![CDATA[<div class="ydiv">
2.2 Exercise 12
</div>
<div class="ybdiv">
Let \(\beta=\{v_1,...,v_n\}\) be a basis for a vector space \(V\) and \(T: V \rightarrow V\) be a linear transformation. Prove that \([T]_{\beta}\) is upper triangular if and only if \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n.\)
</div>
<p><br />
Proof: Let \(A = [T]_{\beta}\). We’ll prove both directions as follows:<br />
\(\Rightarrow:\) Suppose that \(A\) is upper triangular. This means that the entries below the diagonal are all zero. In other words, \(A_{ij} = 0\) for \(j = 1,2,...,n\) and  \(i &gt; j\). 
<br />
<br />
Now, let \(v_j \in \beta\). By the definition of matrix multiplication,</p>
<div>
$$
\begin{align*}
T(v_j) &amp;= \sum_{k}^n A_{jk}v_j = A_{j1}v_j + A_{j2}v_j + ... + A_{jn}v_j
\end{align*}
$$
</div>
<p>But we know whenever \(j &gt; k\), then \(A_{jk = 0}\) so 
<br />
<br />
\(\Leftarrow:\) Suppose that \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n\). This means that \(T(v_j)\) can be written as a linear combination of the elements \(\{v_1,...,v_j\}\) so</p>
<div>
$$
\begin{align*}
T(v_j) &amp;= a_1v_1 + a_2v_2 + ... + a_jv_j. \\
\end{align*}
$$
</div>
<p>for some scalars \(a_1,...,a_j\). We also know that</p>
<div>
$$
\begin{align*}
T(v_j) &amp;= [T]_{\beta}[v_j]_{\beta} = [T]_{\beta}v_j. \\
\end{align*}
$$
</div>
<p>\(\ \blacksquare\)</p>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[2.2 Exercise 12 Let \(\beta=\{v_1,...,v_n\}\) be a basis for a vector space \(V\) and \(T: V \rightarrow V\) be a linear transformation. Prove that \([T]_{\beta}\) is upper triangular if and only if \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n.\) Proof: Let \(A = [T]_{\beta}\). We’ll prove both directions as follows: \(\Rightarrow:\) Suppose that \(A\) is upper triangular. This means that the entries below the diagonal are all zero. In other words, \(A_{ij} = 0\) for \(j = 1,2,...,n\) and \(i &gt; j\). Now, let \(v_j \in \beta\). By the definition of matrix multiplication, $$ \begin{align*} T(v_j) &amp;= \sum_{k}^n A_{jk}v_j = A_{j1}v_j + A_{j2}v_j + ... + A_{jn}v_j \end{align*} $$ But we know whenever \(j &gt; k\), then \(A_{jk = 0}\) so \(\Leftarrow:\) Suppose that \(T(v_j) \in span(\{v_1,...,v_j\})\) for \(j = 1,2,...,n\). This means that \(T(v_j)\) can be written as a linear combination of the elements \(\{v_1,...,v_j\}\) so $$ \begin{align*} T(v_j) &amp;= a_1v_1 + a_2v_2 + ... + a_jv_j. \\ \end{align*} $$ for some scalars \(a_1,...,a_j\). We also know that $$ \begin{align*} T(v_j) &amp;= [T]_{\beta}[v_j]_{\beta} = [T]_{\beta}v_j. \\ \end{align*} $$ \(\ \blacksquare\)]]></summary></entry><entry><title type="html">Section 1.3: Exercise 20</title><link href="http://localhost:4000/jekyll/update/2024/08/16/lec05-exercise-20.html" rel="alternate" type="text/html" title="Section 1.3: Exercise 20" /><published>2024-08-16T01:01:36-07:00</published><updated>2024-08-16T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/16/lec05-exercise-20</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/16/lec05-exercise-20.html"><![CDATA[<div class="ydiv">
1.3 Exercise 20
</div>
<div class="ybdiv">
Prove that if \(W\) is a subspace of a vector space \(V\) and \(w_1,w_2,...,w_n\) in \(W\), then \(a_1w_1 + a_2w_2 + ... + a_nw_n \in W\) for any scalars \(a_1, a_2,...,a_n\).
</div>
<p><br />
Proof:
By induction on \(n\):
<br />
<br />
Base Case (\(n=1\)): If \(w_1 \in W\), then \(a_1w_1 \in W\) since \(W\) is a subspace and so it is closed under scalar multiplication.
<br />
<br />
Inductive Case: Suppose this is true for \(k &gt; 1\) so if \(w_1,...,w_k \in W\), then \(a_1w_1+...+a_kw_k \in W\) for any scalars \(a_1,...,a_k\). We will show that this is true for \(k+1\). So suppose that \(w_1,...,w_k, w_{k+1} \in W\), and let \(a_1,...,a_{k+1}\) be any scalars. We know that \(a_{k+1}w_{k+1} \in W\) since \(W\) is closed under scalar multiplication. We also know that \(a_1w_1+...+a_kw_k\) is in \(W\) by the inductive hypothesis. Furthermore, \(a_1w_1+...+a_kw_k + a_{k+1}w_{k+1}\) is also in \(W\) since \(W\) is closed under addition.
<br />
<br />
Therefore, the statmeent is true for all \(n \geq 1. \ \blacksquare\).</p>

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1.3 Exercise 20 Prove that if \(W\) is a subspace of a vector space \(V\) and \(w_1,w_2,...,w_n\) in \(W\), then \(a_1w_1 + a_2w_2 + ... + a_nw_n \in W\) for any scalars \(a_1, a_2,...,a_n\). Proof: By induction on \(n\): Base Case (\(n=1\)): If \(w_1 \in W\), then \(a_1w_1 \in W\) since \(W\) is a subspace and so it is closed under scalar multiplication. Inductive Case: Suppose this is true for \(k &gt; 1\) so if \(w_1,...,w_k \in W\), then \(a_1w_1+...+a_kw_k \in W\) for any scalars \(a_1,...,a_k\). We will show that this is true for \(k+1\). So suppose that \(w_1,...,w_k, w_{k+1} \in W\), and let \(a_1,...,a_{k+1}\) be any scalars. We know that \(a_{k+1}w_{k+1} \in W\) since \(W\) is closed under scalar multiplication. We also know that \(a_1w_1+...+a_kw_k\) is in \(W\) by the inductive hypothesis. Furthermore, \(a_1w_1+...+a_kw_k + a_{k+1}w_{k+1}\) is also in \(W\) since \(W\) is closed under addition. Therefore, the statmeent is true for all \(n \geq 1. \ \blacksquare\).]]></summary></entry><entry><title type="html">Section 1.4: Theorem 1.5</title><link href="http://localhost:4000/jekyll/update/2024/08/15/lec06-theorem-1.5.html" rel="alternate" type="text/html" title="Section 1.4: Theorem 1.5" /><published>2024-08-15T01:01:36-07:00</published><updated>2024-08-15T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/15/lec06-theorem-1.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/15/lec06-theorem-1.5.html"><![CDATA[<div class="purdiv">
Theorem 1.5
</div>
<div class="purbdiv">
The span of any subset \(S\) of a vector space \(V\) is a subspace of \(V\) that contains \(S\). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\).
</div>
<p><br />
Proof:
<br />
<br />
If \(S = \emptyset\), then \(span(S)=\{\bar{0}\}\). \(\{\bar{0}\}\) is a subspace that contains \(S\) and is also contained in any any subspace of \(V\).
<br />
<br />
If \(S \neq \emptyset\), then \(span(S)\) is a subspace because</p>
<ul style="list-style-type:lower-alpha">
	<li>\(\bar{0} \in span(S)\) since \(0v = \bar{0}\) for some vector \(v \in S\).</li>
	<li>\(span(S)\) is closed under addition. For any two vectors \(x, y \in span(S)\), there exists vectors \(v_1,v_2...,v_n,u_1,u_2...,u_m\) in \(S\) and scalars \(a_1,a_2,..,a_n,b_1,b_2,...,b_m\) such that
	<div>
		$$
		\begin{align*}
		x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n \\
		y &amp;= b_1u_1 + b_2u_2 + ... + b_nu_m.
		\end{align*}
		$$
	</div>
The addition of \(x\) and \(y\) is also in \(span(S)\) because it's a linear combination of some elements in \(S\)  as follows:
	<div>
		$$
		\begin{align*}
		x+y &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n + b_1u_1 + b_2u_2 + ... + b_nu_m.
		\end{align*}
		$$
	</div>
</li>
	<li>\(span(S)\) is closed under scalar multiplication. For any vector \(x \in span(S)\), there exists vectors \(v_1,v_2...,v_n\) in \(S\) and scalars \(a_1,a_2,..,a_n\) such that 
	<div>
		$$
		\begin{align*}
		x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n
		\end{align*}
		$$
	</div>
For any scalar \(c\)
	<div>
		$$
		\begin{align*}
		cx &amp;= (ca_1)v_1 + (ca_2)v_2 + ... + (ca_n)v_n
		\end{align*}
		$$
	</div>
which is a linear combination of some elements in \(S\) and so it is in \(span(S)\).
	</li>
</ul>
<p>So \(span(S)\) is a subspace of \(V\). Furthermore, for any element \(v \in S\), \(1.v \in span(S)\) and so \(v \in span(S)\). So the span of \(S\) contains \(S\).
<br />
<br />
For the second part of the statement, suppose that there exists a subspace \(W\) of \(V\) that contains \(S\). We claim that \(W\) must contain \(span(S)\). Let \(w \in span(S)\), we will show that \(w \in W\). Since \(w \in span(S)\) then \(w\) can be expressed a linear combination of some vectors \(w_1,...,w_k \in S\) and some scalars \(c_1,...,c_k\) such that</p>
<div>
	$$
	\begin{align*}
	w = c_1w_1 + c_2w_2 + ... + c_kw_k,
	\end{align*}
	$$
</div>
<p>But we know that \(W\) contains \(S\) and therefore \(w_1,...,w_k \in W\). We previously proved (Lecture 05, Exercise 20 of section 1.3) that if \(W\) is a subspace, then any linear combination of the elements of \(W\) is also in \(W\). Therefore, \(w \in W\) and \(span(S) \subseteq W\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 1.5 The span of any subset \(S\) of a vector space \(V\) is a subspace of \(V\) that contains \(S\). Moreover, any subspace of \(V\) that contains \(S\) must also contain the span of \(S\). Proof: If \(S = \emptyset\), then \(span(S)=\{\bar{0}\}\). \(\{\bar{0}\}\) is a subspace that contains \(S\) and is also contained in any any subspace of \(V\). If \(S \neq \emptyset\), then \(span(S)\) is a subspace because \(\bar{0} \in span(S)\) since \(0v = \bar{0}\) for some vector \(v \in S\). \(span(S)\) is closed under addition. For any two vectors \(x, y \in span(S)\), there exists vectors \(v_1,v_2...,v_n,u_1,u_2...,u_m\) in \(S\) and scalars \(a_1,a_2,..,a_n,b_1,b_2,...,b_m\) such that $$ \begin{align*} x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n \\ y &amp;= b_1u_1 + b_2u_2 + ... + b_nu_m. \end{align*} $$ The addition of \(x\) and \(y\) is also in \(span(S)\) because it's a linear combination of some elements in \(S\) as follows: $$ \begin{align*} x+y &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n + b_1u_1 + b_2u_2 + ... + b_nu_m. \end{align*} $$ \(span(S)\) is closed under scalar multiplication. For any vector \(x \in span(S)\), there exists vectors \(v_1,v_2...,v_n\) in \(S\) and scalars \(a_1,a_2,..,a_n\) such that $$ \begin{align*} x &amp;= a_1v_1 + a_2v_2 + ... + a_nv_n \end{align*} $$ For any scalar \(c\) $$ \begin{align*} cx &amp;= (ca_1)v_1 + (ca_2)v_2 + ... + (ca_n)v_n \end{align*} $$ which is a linear combination of some elements in \(S\) and so it is in \(span(S)\). So \(span(S)\) is a subspace of \(V\). Furthermore, for any element \(v \in S\), \(1.v \in span(S)\) and so \(v \in span(S)\). So the span of \(S\) contains \(S\). For the second part of the statement, suppose that there exists a subspace \(W\) of \(V\) that contains \(S\). We claim that \(W\) must contain \(span(S)\). Let \(w \in span(S)\), we will show that \(w \in W\). Since \(w \in span(S)\) then \(w\) can be expressed a linear combination of some vectors \(w_1,...,w_k \in S\) and some scalars \(c_1,...,c_k\) such that $$ \begin{align*} w = c_1w_1 + c_2w_2 + ... + c_kw_k, \end{align*} $$ But we know that \(W\) contains \(S\) and therefore \(w_1,...,w_k \in W\). We previously proved (Lecture 05, Exercise 20 of section 1.3) that if \(W\) is a subspace, then any linear combination of the elements of \(W\) is also in \(W\). Therefore, \(w \in W\) and \(span(S) \subseteq W\) as we wanted to show. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 2.1: Theorem 2.2</title><link href="http://localhost:4000/jekyll/update/2024/08/14/lec11-theorem-2.2.html" rel="alternate" type="text/html" title="Section 2.1: Theorem 2.2" /><published>2024-08-14T01:01:36-07:00</published><updated>2024-08-14T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/14/lec11-theorem-2.2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/14/lec11-theorem-2.2.html"><![CDATA[<div class="purdiv">
Theorem 2.2
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is linear and \(\beta=\{v_1,...,v_n\}\) is basis for \(V\), then
$$
\begin{align*}
R(T) = span(T(\beta)) = span(\{T(v_1),...,T(v_n)\} 
\end{align*}
$$
</div>
<p><br />
Proof:
<br />
<br />
To show that \(R(T) = span(T(\beta))\) we will show that \(span(T(\beta)) \subseteq R(T)\) and \(R(T) \subseteq span(T(\beta))\).
<br />
<br />
\(span(T(\beta)) \subseteq R(T)\): By definition, we know that for any \(v \in \beta\), \(T(v) \in R(T)\) so \(T(\beta) \subseteq R(T)\). Since \(R(T)\) is a subspace and \(R(T)\) contains the set \(T(\beta)\), then by <a href="https://strncat.github.io/jekyll/update/2024/08/15/lec06-theorem-1.5.html">theorem 1.5</a>, it must contain the span of this set as well and so</p>
<div>
	$$
	\begin{align*}
	span(T(\beta)) = span(\{T(v_1),...,T(v_n)\}) \subseteq R(T)
	\end{align*}
	$$
</div>
<p><br />
\(R(T) \subseteq span(T(\beta))\): Let \(w \in R(T)\). We know that \(w = T(v)\) for some \(v \in V\). Since \(\beta\) is a basis, then we can express \(v\) as a linear combination of the elements in \(\beta\) such that
<br /></p>
<div>
	$$
	\begin{align*}
	v = a_1v_1 + ... + a_nv_n,
	\end{align*}
	$$
</div>
<p>for some scalars \(a_1, ..., a_n\). Since \(T\) is linear then we can see that</p>
<div>
	$$
	\begin{align*}
	w = T(v) &amp;= T(a_1v_1 + ... + a_nv_n) \\
	   &amp;= a_1T(v_1) + ... + a_nT(v_n).
	\end{align*}
	$$
</div>
<p>So \(w\) is a linear combination of the elements of \(T(\beta)\). This means that \(w\) is also in \(span(T(\beta))\) by the definition of a span and so \(R(T) \subseteq span(T(\beta))\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 2.2 If \(T: V \rightarrow W\) is linear and \(\beta=\{v_1,...,v_n\}\) is basis for \(V\), then $$ \begin{align*} R(T) = span(T(\beta)) = span(\{T(v_1),...,T(v_n)\} \end{align*} $$ Proof: To show that \(R(T) = span(T(\beta))\) we will show that \(span(T(\beta)) \subseteq R(T)\) and \(R(T) \subseteq span(T(\beta))\). \(span(T(\beta)) \subseteq R(T)\): By definition, we know that for any \(v \in \beta\), \(T(v) \in R(T)\) so \(T(\beta) \subseteq R(T)\). Since \(R(T)\) is a subspace and \(R(T)\) contains the set \(T(\beta)\), then by theorem 1.5, it must contain the span of this set as well and so $$ \begin{align*} span(T(\beta)) = span(\{T(v_1),...,T(v_n)\}) \subseteq R(T) \end{align*} $$ \(R(T) \subseteq span(T(\beta))\): Let \(w \in R(T)\). We know that \(w = T(v)\) for some \(v \in V\). Since \(\beta\) is a basis, then we can express \(v\) as a linear combination of the elements in \(\beta\) such that $$ \begin{align*} v = a_1v_1 + ... + a_nv_n, \end{align*} $$ for some scalars \(a_1, ..., a_n\). Since \(T\) is linear then we can see that $$ \begin{align*} w = T(v) &amp;= T(a_1v_1 + ... + a_nv_n) \\ &amp;= a_1T(v_1) + ... + a_nT(v_n). \end{align*} $$ So \(w\) is a linear combination of the elements of \(T(\beta)\). This means that \(w\) is also in \(span(T(\beta))\) by the definition of a span and so \(R(T) \subseteq span(T(\beta))\) as we wanted to show. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 2.4: Theorem 2.17 (Corollary)</title><link href="http://localhost:4000/jekyll/update/2024/08/13/lec15-corollary-2.17.html" rel="alternate" type="text/html" title="Section 2.4: Theorem 2.17 (Corollary)" /><published>2024-08-13T01:01:36-07:00</published><updated>2024-08-13T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/13/lec15-corollary-2.17</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/13/lec15-corollary-2.17.html"><![CDATA[<p>This is the proof from the book for the Corollary following theorem 2.17.</p>
<div class="purdiv">
Theorem 2.17 (Corollary)
</div>
<div class="purbdiv">
If \(T: V \rightarrow W\) is invertible, then \(V\) is finite dimensional if and only if \(W\) is finite dimensional. In this case \(\dim V = \dim W\)
</div>
<p><br />
Proof:
<br />
<br />
\(\Rightarrow:\) Suppose that \(V\) is finite dimensional. If \(T\) is invertible, then \(T\) is onto. By the definition of onto, this means that \(R(T)=W\) (or that for any \(w \in W\), there exists a \(v \in V\) such that \(T(v)=w\)). Since \(V\) is finite dimensional, then let \(\beta\) be a finite basis for \(V\). By <a href="https://strncat.github.io/jekyll/update/2024/08/14/lec11-theorem-2.2.html">Theorem 2.2</a>, \(span(T(\beta)) = R(T)\). But \(R(T) = W\). Therefore, \(W\) must be finite dimensional.
<br />
<br />
\(\Leftarrow:\) Suppose that \(W\) is finite dimensional. If \(T\) is invertible, then \(T^{-1}\) is linear and invertible. We can now apply the same argument from before. \(T^{-1}\) is invertible and so \(T^{-1}\) is onto. Since \(W\) is finite dimensional, let \(\gamma\) be a basis for \(W\). Therefore, \(R(T^{-1})=V\) and since \(span(T^{-1}(\gamma)) = R(T^{-1}) = V\), then \(V\) is finite dimensional.
<br />
<br />
So now suppose that \(V\) and \(W\) are finite dimensional. Because \(T\) is one-to-one and onto, then \(nullity(T)=0\) and \(rank(T) = \dim(R(T)) = \dim(W)\). By the dimension theorem we know,</p>
<div>
	$$
	\begin{align*}
	\dim(V) &amp;= \dim(N(T)) + \dim(R(T)) \\
	        &amp;= 0 + \dim(W) \\
	\end{align*}
	$$
</div>
<p>Therefore, \(\dim(V)=\dim(W)\) as we wanted to show. \(\blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This is the proof from the book for the Corollary following theorem 2.17. Theorem 2.17 (Corollary) If \(T: V \rightarrow W\) is invertible, then \(V\) is finite dimensional if and only if \(W\) is finite dimensional. In this case \(\dim V = \dim W\) Proof: \(\Rightarrow:\) Suppose that \(V\) is finite dimensional. If \(T\) is invertible, then \(T\) is onto. By the definition of onto, this means that \(R(T)=W\) (or that for any \(w \in W\), there exists a \(v \in V\) such that \(T(v)=w\)). Since \(V\) is finite dimensional, then let \(\beta\) be a finite basis for \(V\). By Theorem 2.2, \(span(T(\beta)) = R(T)\). But \(R(T) = W\). Therefore, \(W\) must be finite dimensional. \(\Leftarrow:\) Suppose that \(W\) is finite dimensional. If \(T\) is invertible, then \(T^{-1}\) is linear and invertible. We can now apply the same argument from before. \(T^{-1}\) is invertible and so \(T^{-1}\) is onto. Since \(W\) is finite dimensional, let \(\gamma\) be a basis for \(W\). Therefore, \(R(T^{-1})=V\) and since \(span(T^{-1}(\gamma)) = R(T^{-1}) = V\), then \(V\) is finite dimensional. So now suppose that \(V\) and \(W\) are finite dimensional. Because \(T\) is one-to-one and onto, then \(nullity(T)=0\) and \(rank(T) = \dim(R(T)) = \dim(W)\). By the dimension theorem we know, $$ \begin{align*} \dim(V) &amp;= \dim(N(T)) + \dim(R(T)) \\ &amp;= 0 + \dim(W) \\ \end{align*} $$ Therefore, \(\dim(V)=\dim(W)\) as we wanted to show. \(\blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Lecture 11: Exercise 0</title><link href="http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0.html" rel="alternate" type="text/html" title="Lecture 11: Exercise 0" /><published>2024-08-12T01:01:36-07:00</published><updated>2024-08-12T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/12/lec11-ex-0.html"><![CDATA[<div class="ydiv">
Problem 4
</div>
<div class="ybdiv">
Let \(V\) and \(W\) be finite dimensional vector spaces and \(T: V \rightarrow W\) be linear.
<ol style="list-style-type:lower-alpha">
	<li>Prove that if \(\dim(V) &lt; \dim (W)\), then \(T\) cannot be onto.</li>
	<li>Prove that if \(\dim(W) &lt; \dim(V)\), then \(T\) cannot be one-to-one.</li>
</ol>
</div>
<p><br />
Proof:</p>
<ol style="list-style-type:lower-alpha">
	<li>Suppose for the sake of contradiction that \(T\) was onto. This means that \(Rank(T) = \dim(W)\). The dimension theorem states that
	<div>
		$$
		\begin{align*}
		\dim(V) &amp;= Nullity(T) + Rank(T) \\
		       &amp;= Nullity(T) + \dim(W)
		\end{align*}
		$$
	</div>
This implies that \(Nullity(T) = \dim(V) - \dim(W) &lt; 0\) since \(\dim(V) &lt; \dim(W)\). This is a contradiction and so \(T\) cannot be onto. \(\blacksquare\)  
	</li>
	<li>Similarly, suppose for the sake of contradiction that \(T\) is one-to-one. This means that \(Nullity(T) = 0\) and so by the dimension theorem, it must be that \(\dim(V) = \dim(W)\). But this is a contradiction since \(\dim(W) &lt; \dim(V)\).</li> </ol>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.geneseo.edu/~heap/courses/333/exam2_F2007_practice_sol.pdf">Practice Midterm</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Problem 4 Let \(V\) and \(W\) be finite dimensional vector spaces and \(T: V \rightarrow W\) be linear. Prove that if \(\dim(V) &lt; \dim (W)\), then \(T\) cannot be onto. Prove that if \(\dim(W) &lt; \dim(V)\), then \(T\) cannot be one-to-one. Proof: Suppose for the sake of contradiction that \(T\) was onto. This means that \(Rank(T) = \dim(W)\). The dimension theorem states that $$ \begin{align*} \dim(V) &amp;= Nullity(T) + Rank(T) \\ &amp;= Nullity(T) + \dim(W) \end{align*} $$ This implies that \(Nullity(T) = \dim(V) - \dim(W) &lt; 0\) since \(\dim(V) &lt; \dim(W)\). This is a contradiction and so \(T\) cannot be onto. \(\blacksquare\) Similarly, suppose for the sake of contradiction that \(T\) is one-to-one. This means that \(Nullity(T) = 0\) and so by the dimension theorem, it must be that \(\dim(V) = \dim(W)\). But this is a contradiction since \(\dim(W) &lt; \dim(V)\). References: Practice Midterm]]></summary></entry><entry><title type="html">Lecture 19/20: Determinants</title><link href="http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants.html" rel="alternate" type="text/html" title="Lecture 19/20: Determinants" /><published>2024-08-11T01:01:36-07:00</published><updated>2024-08-11T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/11/lec19-determinants.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The determinant is a map
$$
\begin{align*}
\det: &amp;M_{n \times n} \rightarrow \mathbf{R} \\
     &amp;A \rightarrow \det(A)
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Properties of the determinant</b></h4>
<ol>
	<li>\(A\) is invertible iff \(\det(A) \neq 0\)</li>
	<li>\(\det(A)\) has geometric meaning.</li>
	To see this, let 
	<div>
	$$
	\begin{align*}
	[0,1]^n = \{(x_1,...,x_n) \in \mathbf{R}^n \ | \ x_i \in [0,1]\}
	\end{align*}
	$$
	</div>
	What does this represent? In \(\mathbf{R}^2\), this is a unit square. In general, it's a cube determined by the vectors \(\{e_1, e_2,...,e_n\}\). 
	<br />
	What does this have anything to do with the determinant? 
	<br />
	Consider, the matrix \(A \in M_{n \times n}\). When we apply \(A\) on each vector in the standard basis, we get \(Ae_1, Ae_2, ...\).
	<br />
	\(L_A([0,1]^n\) is the parallelepiped determined by \(\{Ae_1, Ae_2, ...,Ae_n\}\) and \(volume(L_A([0,1]^n)) = |det(A)|\).
	<br />
	<br />
	<li>The determinant map is not linear except for \(n = 1\). (It is linear in the rows of \(A\))</li>
	<li>\(\det(AB) = \det(A)\det(B)\).</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Definition of the Determinant</b></h4>
<p>The definition of the \(\det: M_{n \times n} \rightarrow \mathbf{R}\) is inductive on \(n\).
<br />
<br />
<!-------------------n=1------------------->
For \(n = 1\):</p>
<div>
$$
\begin{align*}
&amp;M_{1\times 1} = \{(a)\} \leftrightarrow \mathbf{R} \\
       &amp;\det((a)) = a
\end{align*}
$$
</div>
<p>Checking the four properties:</p>
<ol>
	<li>Since we only have one entry in the matrix, then the inverse exists and is \((a)^{-1} = (\frac{1}{a})\) if and only if \(a = \det((a)) \neq 0\).</li>
	<li>Checking the second property we see that for \(L_{(a)}: \mathbf{R}^1 \rightarrow \mathbf{R}^1\)
<div>
$$
\begin{align*}
L_{(a)} : &amp;\mathbf{R}^1 \rightarrow \mathbf{R}^1 \\
&amp;x \rightarrow ax \\
L_{(a)}([0,1]) &amp;= \begin{cases} [0,a] \quad \text{if } a \geq 0 \\ [a,0] \quad \text{if } a \lt 0\end{cases}\\
volume(L_{(a)}([0,1])) &amp;= |a| = |det((a))|.
\end{align*}
$$
</div>
	</li>
	<li>The determinant is linear for \(n = 1\)</li>
	<li>\( det((a)(b)) = det((ab)) = ab = det((a))det((b))  \) </li>
</ol>
<p><br />
<!-------------------n=2------------------->
For \(n = 2\):</p>
<div>
$$
\begin{align*}
\det: \ &amp;M_{2 \times 2} \rightarrow \mathbf{R} \\
       &amp;\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix} \rightarrow ad - bc
\end{align*}
$$
</div>
<p>Checking the four properties:</p>
<ol>
	<li>We previously proved that \(\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\) is invertible if and only if \(ad - bc \neq 0\).</li>
	<li> This property takes some work to see. Check the book for a nice description of it.</li>
	<li>The determinant is not linear for \(n = 2\)</li>
	<li>We want to check that \(\det(AB) = \det(A)\det(B)\). To see this:
<div>
$$
\begin{align*}
\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\begin{pmatrix}\alpha &amp; \beta \\ \gamma &amp; \delta \end{pmatrix} &amp;= \begin{pmatrix}a\alpha + b\gamma &amp; a\beta + b\delta \\ c\alpha + d\gamma &amp; c\beta + d\delta \end{pmatrix} \\
det(AB) &amp;= ((a\alpha + b\gamma)(c\beta + d\delta) - (a\beta + b\delta)(c\beta + d\delta)) \\
&amp;= a\alpha d\delta + b\gamma c\beta - a\beta d\gamma - b\delta c\alpha \\
&amp;= (ad - bc)(a\delta - \beta \gamma)\\
&amp;= \det(A) \det(B).
 \\
\end{align*}
$$
</div>
	</li>
</ol>
<p><br />
<!-------------------n------------------->
So now what about the general case?</p>
<div>
$$
\begin{align*}
\det: \ &amp;M_{n \times n} \rightarrow \mathbf{R}
\end{align*}
$$
</div>
<p>Define \(\tilde{A_{ij}}\) as the \((n-1)\times(n-1)\) matrix obtained from \(A\) by deleting its \(i\)th row and \(j\)th column.
<br />
<br />
For example</p>
<div>
$$
\begin{align*}
A &amp;= \begin{pmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{pmatrix} \\
\tilde{A_{23}} &amp;= \begin{pmatrix}1 &amp; 2 \\ 7 &amp; 8 \end{pmatrix}
\tilde{A_{31}} = \begin{pmatrix}2 &amp; 3 \\ 5 &amp; 6 \end{pmatrix}  
\end{align*}
$$
</div>
<p>So now for \(n \geq 2\) and \(A \in M_{n \times n}\)</p>
<div>
$$
\begin{align*}
det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) 
\end{align*}
$$
</div>
<p>Remark 1:</p>
<div>
$$
\begin{align*}
(-1)^k &amp;= \begin{cases} 1 \quad \phantom{-} k \text{ even} \\ -1 \quad \text{k odd } \end{cases}
\end{align*}
$$
</div>
<p>Remark 2:</p>
<div>
$$
\begin{align*}
\det\left( \begin{pmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix}\right)  
&amp;= (-1)^{1+1}A_{11} det((A_{22})) + (-1)^{1+2}A_{12}det((A_{21})) \\ 
&amp;= A_{11}A_{22} - A_{12}A_{21}.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Compute the determinant for</p>
<div>
$$
\begin{align*}
\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 1 &amp; 1 \end{pmatrix} 
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
det(\tilde{A_{11}}) = \det \left(
\begin{pmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{pmatrix}\right) &amp;= -2 \\
det(\tilde{A_{12}}) = \det \left(
\begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= -3 \\
det(\tilde{A_{13}}) = \det \left(
\begin{pmatrix} 1 &amp; 0 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= 1 \\
\end{align*}
$$
</div>
<div>
$$
\begin{align*}
\det(A) &amp;= (-1)^{1+1}(1)(-2) + (-1)^{1+2}(2)(-3) + (-1)^{1+3}(3)(1) \\
       &amp;= -2 + 6 + 3 = 7.
\end{align*}
$$
</div>
<p><br /></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
\(\det(A)\) is linear in rows of \(A\).
</div>
<p><br />
What does this mean? Suppose we have three matrices \(A, B\) and \(C\) in \(M_{n \times n}\) which are equal in all rows but the \(r\)th row. And suppose that for the \(r\)th row that \(a_r = b_r + kc_r\).</p>
<div>
$$
\begin{align*}
B = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}, C = \begin{pmatrix} \pi &amp; \pi \\ 1 &amp; 1 \end{pmatrix}, A = \begin{pmatrix} 1+k\pi &amp; 1+k\pi \\ 1 &amp; 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>Here, they’re all equal except for the \(r\)th row which is the first row here so \(r = 1\). The \(r\)th row of \(A\) is \(a_r = b_r + kc_r\). In this case,</p>
<div>
$$
\begin{align*}
\det(A) = \det(B) + k\det(C)
\end{align*}
$$
</div>
<p><b>Proof:</b>
<br />
We have two cases. The \(r\)th row is 1 or the \(r\)th row is some other row other than 1. Why? because the current definition of the determinant that we have right now is “favoring” the first row. So we want to split the cases around this.
<br />
<br />
Case 1 (\(r = 1\)): 
<br />
Suppose the matrices differ in the first row where the first row of \(A\) is some linear combination of the first row in \(B\) and the first row in \(C\). We know that</p>
<div>
$$
\begin{align*}
\det(A) =  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}})
\end{align*}
$$
</div>
<p>However we know that every entry in the first row of \(A\) can be written as a linear combination of the entries in \(B\) and \(C\) and</p>
<div>
$$
\begin{align*}
A_{1j} = B_{1j} + kC_{1j}.
\end{align*}
$$
</div>
<p>Additionally, by the definition that we have of computing the determinant,</p>
<div>
$$
\begin{align*}
\tilde{A_{1j}} = \tilde{B_{1j}} = \tilde{C_{1j}}.
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
\det(A) &amp;=  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} (B_{1j} + kC_{1j}) \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{A_{1j}}) +  \sum_{j=1}^{n} (-1)^{1+j} kC_{1j} \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) +  k\sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\
&amp;= \det(B) + k\det(C).
\end{align*}
$$
</div>
<p>Case 2 (\(r &gt; 1\)): By Induction on \(n\)
<br />
Base Case: When \(n = 1\), the determinat is linear.
<br />
<br />
Inductive Step: Suppose it is true for \(n - 1\). Then,</p>
<div>
$$
\begin{align*}
\det(A) &amp;=  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}})
\end{align*}
$$
</div>
<p>In this case, we know the matrices differ in the \(r\)‘th row where \(r &gt; 1\). and so the first row of each matrix is the same. \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). What about the determinants of \(\tilde{A_{1j}}, \tilde{B_{1j}}\) and \(\tilde{C_{1j}}\)? Well, since they are now of size \(n-1 \times n-1\), then we can use the inductive hypothesis to conclude that,</p>
<div>
$$
\begin{align*}
\det(\tilde{A_{1j}}) &amp;=  \det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}})
\end{align*}
$$
</div>
<p>So now we can use all of these facts to compute the determinant,</p>
<div>
$$
\begin{align*}
\det(A) &amp;=  \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} (\det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}})) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{C_{1j}}) \\
&amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\
&amp;= \det(B) + k\det(C). \ \blacksquare
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
	For any \(r = 1,2,...,n\), we have 
$$
\begin{align*}
\det(A) =  \sum_{j=1}^{n} (-1)^{r+j} A_{rj} \det(\tilde{A_{rj}})
\end{align*}
$$
</div>
<p><br />
The proof for this theorem requires the following lemma
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Lemma
</div>
<div class="purbdiv">
Given \(A\). Let \(B\) be the matrix equal to \(A\) with the \(r\)th row placed by \(e_j\) so 
$$
\begin{align*}
B &amp;= \begin{pmatrix} \bar{b_1} \\ \vdots \\ e_j \\ \vdots \\ \bar{b_n} \end{pmatrix}
\end{align*}
$$
Then,
$$
\begin{align*}
\det(B) &amp;=  (-1)^{r+j} \det(\tilde{B_{rj}})
\end{align*}
$$
</div>
<p><br />
The proof of this lemma is in the textbook (page 214). (TODO: check). So now we’ll do the proof for the theorem.
<!------------------------------------------------------------------------------------>
<br />
<br />
<b>Proof (Using the Lemma)</b>
<br />
Given \(A\). Let \(B_j\) be the matrix equal to \(A\) with the \(r\)th row replaced by \(e_j\). By the technical lemma,</p>
<div>
$$
\begin{align*}
\det(B_j) &amp;=  (-1)^{r+j} \det(\tilde{(B_{j})_{rj}})
\end{align*}
$$
</div>
<p>(<i>Note this is this the same expression as the technical lemma. It’s just that the matrix here is called \(B_j\) and in the technical lemma it is \(B\). Moreover, \(\det(\tilde{(B_{j})_{rj}})\) says take the matrix \(B_j\) and remove the row \(r\) and column \(j\) to compute that determinant)</i> 
<br />
<br />
So now, since we’re computing the determinant by removing the row \(r\) and column \(j\), this determinant should the same exact determinant as \(\det(\tilde{A_{rj}})\) since they only differ in row \(r\) and column \(j\). Therefore, we replace it with \(\det(\tilde{A_{rj}})\) in</p>
<div>
$$
\begin{align*}
\det(B_j) &amp;=  (-1)^{r+j} \det(\tilde{(B_{j})_{rj}}) \\
&amp;=  (-1)^{r+j} \det(\tilde{A_{rj}})
\end{align*}
$$
</div>
<p>Next, notice that \(e_1,...e_j\) .. are vectors of the standard basis for \(\mathbf{R}^n\). So we know that the \(r\)th row of \(A\) can be expressed as a linear combination of these vectors. Moreover, the coefficients of this linear combination are just the entries in the \(r\)th row of \(A\). So by Theorem 1,</p>
<div>
$$
\begin{align*}
\det(A) &amp;= \sum A_{rj} \det(B_j) \\
        &amp;= \sum A_{rj}  (-1)^{r+j} \det(\tilde{A_{rj}}).
\end{align*}
$$
</div>
<p>as we wanted to show. \(\blacksquare\)
<!------------------------------------------------------------------------------------>
<br />
<br />
This gives the following corollary
<br /></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
If \(A\) has a row of all zeros, then \(\det A = 0\).
</div>
<p><br />
This is all fine but unfortunately, it doesn’t still solve the problem of the computation of the determinant taking too long to compute. Since a matrix at random will likely not have a row of all zeros. We still have another corollary
<!------------------------------------------------------------------------------------>
<br /></p>
<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
If \(A\) has two identical rows, then \(\det A = 0\).
</div>
<p><br />
<b>Proof</b>
<br />
By induction on \(n\). 
<br />
<br />
Base Case: \(n = 2\).</p>
<div>
$$
\begin{align*}
\det\begin{pmatrix} a &amp; b \\ a &amp; b \end{pmatrix} = ab - ab = 0.
\end{align*}
$$
</div>
<p>Inductive Step: <br />
Assume this is true for \(n - 1\).</p>
<div>
	$$
	\begin{align*}
	A &amp;= \begin{pmatrix} \bar{a_1} \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ \bar{b_n} \end{pmatrix}, \text{ where $a_i = a_j$}
	\end{align*}
	$$
</div>
<p>The goal is to compute the determinant of \(A\) using the inductive hypothesis. Well if we choose \(r\) such that it is not \(i\) or \(j\), then by the previous theorem we know that</p>
<div>
	$$
	\begin{align*}
	\det(A) &amp;= \sum A_{rj}  (-1)^{r+j} \det(\tilde{A_{rj}}).
	\end{align*}
	$$
</div>
<p>But \(\det(\tilde{A_{rj}})\) is 0 since \(\tilde{A_{rj}}\) has two identical rows and its size is \(n-1 \times n-1\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\det(A) &amp;= 0.
	\end{align*}
	$$
</div>
<p>As we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Video Lectures from Math416 by Ely Kerman.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The determinant is a map $$ \begin{align*} \det: &amp;M_{n \times n} \rightarrow \mathbf{R} \\ &amp;A \rightarrow \det(A) \end{align*} $$ Properties of the determinant \(A\) is invertible iff \(\det(A) \neq 0\) \(\det(A)\) has geometric meaning. To see this, let $$ \begin{align*} [0,1]^n = \{(x_1,...,x_n) \in \mathbf{R}^n \ | \ x_i \in [0,1]\} \end{align*} $$ What does this represent? In \(\mathbf{R}^2\), this is a unit square. In general, it's a cube determined by the vectors \(\{e_1, e_2,...,e_n\}\). What does this have anything to do with the determinant? Consider, the matrix \(A \in M_{n \times n}\). When we apply \(A\) on each vector in the standard basis, we get \(Ae_1, Ae_2, ...\). \(L_A([0,1]^n\) is the parallelepiped determined by \(\{Ae_1, Ae_2, ...,Ae_n\}\) and \(volume(L_A([0,1]^n)) = |det(A)|\). The determinant map is not linear except for \(n = 1\). (It is linear in the rows of \(A\)) \(\det(AB) = \det(A)\det(B)\). Definition of the Determinant The definition of the \(\det: M_{n \times n} \rightarrow \mathbf{R}\) is inductive on \(n\). For \(n = 1\): $$ \begin{align*} &amp;M_{1\times 1} = \{(a)\} \leftrightarrow \mathbf{R} \\ &amp;\det((a)) = a \end{align*} $$ Checking the four properties: Since we only have one entry in the matrix, then the inverse exists and is \((a)^{-1} = (\frac{1}{a})\) if and only if \(a = \det((a)) \neq 0\). Checking the second property we see that for \(L_{(a)}: \mathbf{R}^1 \rightarrow \mathbf{R}^1\) $$ \begin{align*} L_{(a)} : &amp;\mathbf{R}^1 \rightarrow \mathbf{R}^1 \\ &amp;x \rightarrow ax \\ L_{(a)}([0,1]) &amp;= \begin{cases} [0,a] \quad \text{if } a \geq 0 \\ [a,0] \quad \text{if } a \lt 0\end{cases}\\ volume(L_{(a)}([0,1])) &amp;= |a| = |det((a))|. \end{align*} $$ The determinant is linear for \(n = 1\) \( det((a)(b)) = det((ab)) = ab = det((a))det((b)) \) For \(n = 2\): $$ \begin{align*} \det: \ &amp;M_{2 \times 2} \rightarrow \mathbf{R} \\ &amp;\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix} \rightarrow ad - bc \end{align*} $$ Checking the four properties: We previously proved that \(\begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\) is invertible if and only if \(ad - bc \neq 0\). This property takes some work to see. Check the book for a nice description of it. The determinant is not linear for \(n = 2\) We want to check that \(\det(AB) = \det(A)\det(B)\). To see this: $$ \begin{align*} \begin{pmatrix}a &amp; b \\ c &amp; d \end{pmatrix}\begin{pmatrix}\alpha &amp; \beta \\ \gamma &amp; \delta \end{pmatrix} &amp;= \begin{pmatrix}a\alpha + b\gamma &amp; a\beta + b\delta \\ c\alpha + d\gamma &amp; c\beta + d\delta \end{pmatrix} \\ det(AB) &amp;= ((a\alpha + b\gamma)(c\beta + d\delta) - (a\beta + b\delta)(c\beta + d\delta)) \\ &amp;= a\alpha d\delta + b\gamma c\beta - a\beta d\gamma - b\delta c\alpha \\ &amp;= (ad - bc)(a\delta - \beta \gamma)\\ &amp;= \det(A) \det(B). \\ \end{align*} $$ So now what about the general case? $$ \begin{align*} \det: \ &amp;M_{n \times n} \rightarrow \mathbf{R} \end{align*} $$ Define \(\tilde{A_{ij}}\) as the \((n-1)\times(n-1)\) matrix obtained from \(A\) by deleting its \(i\)th row and \(j\)th column. For example $$ \begin{align*} A &amp;= \begin{pmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{pmatrix} \\ \tilde{A_{23}} &amp;= \begin{pmatrix}1 &amp; 2 \\ 7 &amp; 8 \end{pmatrix} \tilde{A_{31}} = \begin{pmatrix}2 &amp; 3 \\ 5 &amp; 6 \end{pmatrix} \end{align*} $$ So now for \(n \geq 2\) and \(A \in M_{n \times n}\) $$ \begin{align*} det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ Remark 1: $$ \begin{align*} (-1)^k &amp;= \begin{cases} 1 \quad \phantom{-} k \text{ even} \\ -1 \quad \text{k odd } \end{cases} \end{align*} $$ Remark 2: $$ \begin{align*} \det\left( \begin{pmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{pmatrix}\right) &amp;= (-1)^{1+1}A_{11} det((A_{22})) + (-1)^{1+2}A_{12}det((A_{21})) \\ &amp;= A_{11}A_{22} - A_{12}A_{21}. \end{align*} $$ Example Compute the determinant for $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 1 &amp; 1 \end{pmatrix} \end{align*} $$ $$ \begin{align*} det(\tilde{A_{11}}) = \det \left( \begin{pmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{pmatrix}\right) &amp;= -2 \\ det(\tilde{A_{12}}) = \det \left( \begin{pmatrix} 1 &amp; 2 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= -3 \\ det(\tilde{A_{13}}) = \det \left( \begin{pmatrix} 1 &amp; 0 \\ 2 &amp; 1 \end{pmatrix}\right) &amp;= 1 \\ \end{align*} $$ $$ \begin{align*} \det(A) &amp;= (-1)^{1+1}(1)(-2) + (-1)^{1+2}(2)(-3) + (-1)^{1+3}(3)(1) \\ &amp;= -2 + 6 + 3 = 7. \end{align*} $$ Theorem \(\det(A)\) is linear in rows of \(A\). What does this mean? Suppose we have three matrices \(A, B\) and \(C\) in \(M_{n \times n}\) which are equal in all rows but the \(r\)th row. And suppose that for the \(r\)th row that \(a_r = b_r + kc_r\). $$ \begin{align*} B = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}, C = \begin{pmatrix} \pi &amp; \pi \\ 1 &amp; 1 \end{pmatrix}, A = \begin{pmatrix} 1+k\pi &amp; 1+k\pi \\ 1 &amp; 1 \end{pmatrix} \end{align*} $$ Here, they’re all equal except for the \(r\)th row which is the first row here so \(r = 1\). The \(r\)th row of \(A\) is \(a_r = b_r + kc_r\). In this case, $$ \begin{align*} \det(A) = \det(B) + k\det(C) \end{align*} $$ Proof: We have two cases. The \(r\)th row is 1 or the \(r\)th row is some other row other than 1. Why? because the current definition of the determinant that we have right now is “favoring” the first row. So we want to split the cases around this. Case 1 (\(r = 1\)): Suppose the matrices differ in the first row where the first row of \(A\) is some linear combination of the first row in \(B\) and the first row in \(C\). We know that $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ However we know that every entry in the first row of \(A\) can be written as a linear combination of the entries in \(B\) and \(C\) and $$ \begin{align*} A_{1j} = B_{1j} + kC_{1j}. \end{align*} $$ Additionally, by the definition that we have of computing the determinant, $$ \begin{align*} \tilde{A_{1j}} = \tilde{B_{1j}} = \tilde{C_{1j}}. \end{align*} $$ Therefore, $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} (B_{1j} + kC_{1j}) \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{A_{1j}}) + \sum_{j=1}^{n} (-1)^{1+j} kC_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) + k\sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\ &amp;= \det(B) + k\det(C). \end{align*} $$ Case 2 (\(r &gt; 1\)): By Induction on \(n\) Base Case: When \(n = 1\), the determinat is linear. Inductive Step: Suppose it is true for \(n - 1\). Then, $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \end{align*} $$ In this case, we know the matrices differ in the \(r\)‘th row where \(r &gt; 1\). and so the first row of each matrix is the same. \(A_{1j} = B_{1j} = C_{1j}\) for all \(j\). What about the determinants of \(\tilde{A_{1j}}, \tilde{B_{1j}}\) and \(\tilde{C_{1j}}\)? Well, since they are now of size \(n-1 \times n-1\), then we can use the inductive hypothesis to conclude that, $$ \begin{align*} \det(\tilde{A_{1j}}) &amp;= \det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}}) \end{align*} $$ So now we can use all of these facts to compute the determinant, $$ \begin{align*} \det(A) &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{A_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} (\det(\tilde{B_{1j}}) + k \det(\tilde{C_{1j}})) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} A_{1j} \det(\tilde{C_{1j}}) \\ &amp;= \sum_{j=1}^{n} (-1)^{1+j} B_{1j} \det(\tilde{B_{1j}}) + k \sum_{j=1}^{n} (-1)^{1+j} C_{1j} \det(\tilde{C_{1j}}) \\ &amp;= \det(B) + k\det(C). \ \blacksquare \end{align*} $$ Theorem For any \(r = 1,2,...,n\), we have $$ \begin{align*} \det(A) = \sum_{j=1}^{n} (-1)^{r+j} A_{rj} \det(\tilde{A_{rj}}) \end{align*} $$ The proof for this theorem requires the following lemma Lemma Given \(A\). Let \(B\) be the matrix equal to \(A\) with the \(r\)th row placed by \(e_j\) so $$ \begin{align*} B &amp;= \begin{pmatrix} \bar{b_1} \\ \vdots \\ e_j \\ \vdots \\ \bar{b_n} \end{pmatrix} \end{align*} $$ Then, $$ \begin{align*} \det(B) &amp;= (-1)^{r+j} \det(\tilde{B_{rj}}) \end{align*} $$ The proof of this lemma is in the textbook (page 214). (TODO: check). So now we’ll do the proof for the theorem. Proof (Using the Lemma) Given \(A\). Let \(B_j\) be the matrix equal to \(A\) with the \(r\)th row replaced by \(e_j\). By the technical lemma, $$ \begin{align*} \det(B_j) &amp;= (-1)^{r+j} \det(\tilde{(B_{j})_{rj}}) \end{align*} $$ (Note this is this the same expression as the technical lemma. It’s just that the matrix here is called \(B_j\) and in the technical lemma it is \(B\). Moreover, \(\det(\tilde{(B_{j})_{rj}})\) says take the matrix \(B_j\) and remove the row \(r\) and column \(j\) to compute that determinant) So now, since we’re computing the determinant by removing the row \(r\) and column \(j\), this determinant should the same exact determinant as \(\det(\tilde{A_{rj}})\) since they only differ in row \(r\) and column \(j\). Therefore, we replace it with \(\det(\tilde{A_{rj}})\) in $$ \begin{align*} \det(B_j) &amp;= (-1)^{r+j} \det(\tilde{(B_{j})_{rj}}) \\ &amp;= (-1)^{r+j} \det(\tilde{A_{rj}}) \end{align*} $$ Next, notice that \(e_1,...e_j\) .. are vectors of the standard basis for \(\mathbf{R}^n\). So we know that the \(r\)th row of \(A\) can be expressed as a linear combination of these vectors. Moreover, the coefficients of this linear combination are just the entries in the \(r\)th row of \(A\). So by Theorem 1, $$ \begin{align*} \det(A) &amp;= \sum A_{rj} \det(B_j) \\ &amp;= \sum A_{rj} (-1)^{r+j} \det(\tilde{A_{rj}}). \end{align*} $$ as we wanted to show. \(\blacksquare\) This gives the following corollary Corollary 1 If \(A\) has a row of all zeros, then \(\det A = 0\). This is all fine but unfortunately, it doesn’t still solve the problem of the computation of the determinant taking too long to compute. Since a matrix at random will likely not have a row of all zeros. We still have another corollary Corollary 2 If \(A\) has two identical rows, then \(\det A = 0\). Proof By induction on \(n\). Base Case: \(n = 2\). $$ \begin{align*} \det\begin{pmatrix} a &amp; b \\ a &amp; b \end{pmatrix} = ab - ab = 0. \end{align*} $$ Inductive Step: Assume this is true for \(n - 1\). $$ \begin{align*} A &amp;= \begin{pmatrix} \bar{a_1} \\ \vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \\ \bar{b_n} \end{pmatrix}, \text{ where $a_i = a_j$} \end{align*} $$ The goal is to compute the determinant of \(A\) using the inductive hypothesis. Well if we choose \(r\) such that it is not \(i\) or \(j\), then by the previous theorem we know that $$ \begin{align*} \det(A) &amp;= \sum A_{rj} (-1)^{r+j} \det(\tilde{A_{rj}}). \end{align*} $$ But \(\det(\tilde{A_{rj}})\) is 0 since \(\tilde{A_{rj}}\) has two identical rows and its size is \(n-1 \times n-1\). Therefore, $$ \begin{align*} \det(A) &amp;= 0. \end{align*} $$ As we wanted to show. \(\blacksquare\) References Video Lectures from Math416 by Ely Kerman.]]></summary></entry></feed>