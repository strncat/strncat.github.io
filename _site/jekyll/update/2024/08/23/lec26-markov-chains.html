<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="nemo&apos;s notebook" />
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">nemo&#39;s notebook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
  
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 26/27: Markov Chains and Transition Matrices</h1>
    <!--
    <p class="post-meta">
      <time class="dt-published" datetime="2024-08-23T04:01:36-04:00" itemprop="datePublished">Aug 23, 2024
      </time></p>
     -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>We have a set of ingredients:</p>
<ul style="list-style-type:lower-alpha">
	<li>Set of states \(S_1,...,S_n\). If we have 10m people in Chicagoland, then we can divide them into \(S_1\) living in the city and \(S_2\) living in a suburb</li>
	<li>Time Step. Example: we are interested in how the number of people living in these two states changes over time. So a time step could be 1 year</li>
	<li>Probability vector at time \(k\)
<div> 
$$
\begin{align*}
\bar{p_k} = 
\begin{pmatrix} 
(\bar{p_k})_1 \\
\vdots \\
(\bar{p_k})_n
\end{pmatrix}
\end{align*}
$$
</div>
where \(\bar{p_k}_1\) is the probability of being in state 1 at time \(k\). In general \(\bar{p_k}_j \geq 0\) and \(\sum_{j=1}^n \bar{p_k}_j = 1\). For example
<div> 
$$
\begin{align*}
(\bar{p_k})_0 = 
\begin{pmatrix} 
.7 \\
.3
\end{pmatrix}
\end{align*}
$$
</div>
</li>
<!------------------------>
<li>Transition matrix \(A \in M_{n \times n}\)
<div> 
$$
\begin{align*}
A = 
\begin{pmatrix} 
A_{11} &amp; \cdots &amp; A_{1n} \\
\vdots &amp; A_{ij} &amp; \vdots \\
A_{n1} &amp; \cdots &amp; A_{nn}
\end{pmatrix}
\end{align*}
$$
</div>
where \(A_{ij}\) is the probability of moving from state \(j\) to state \(i\) in one time step. This transition matrix doesn't change over time. For example
<div> 
$$
\begin{align*}
A = 
\begin{pmatrix} 
.9 &amp; .02 \\
.1 &amp; .98
\end{pmatrix}
\end{align*}
$$
</div>
Here \(A_{ij} \geq 0\) and \(\sum_{i=1}^nA_{ij} = 1\). So \(A_{12} = 0.02\) is the probability of moving from state 1 (city) to the state 2 (suburb).
</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Limit of a Markov Matrix</b></h4>

<p>So \(\bar{p_0}\) is the probability at time step 0. What is \(\bar{p_1}, \bar{p_2},...,\bar{p_n}\)?</p>
<div> 
$$
\begin{align*}
\bar{p_1} &amp;= A\bar{p_0} \\
\bar{p_2} &amp;= A\bar{p_1} = A^2\bar{p_0} \\
\vdots
\bar{p_n} &amp;= A^n\bar{p_0} \\
\lim_{n \rightarrow \infty} \bar{p_n} &amp;= (\lim_{n \rightarrow \infty} A^n) \bar{p_0}
\end{align*}
$$
</div>
<p>What is the limit of this matrix?? This often exists and is easy to describe.
<br />
<br />
Exercise: \(A^d\) is also a transition matrix!
<br />
<br />
What we want to know is what this limit is so we’re going to spend some time studying the properties of this transition matrix.
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem (Mix of 5.18 and 5.19)
</div>
<div class="purbdiv">
Suppose for some \(d \geq 1, A^d\) has all positive entries. Then
<ul style="list-style-type:lower-alpha">
	<li>1 is an eigenvalue of \(A\). \(\dim(E_1) = 1\) and \(E_1 = span\{\bar{u}\}\) where \(\bar{u}\) is a probability vector</li>
	<li>For any other eigenvalue \(\lambda, |\lambda| &lt; 1\).</li>
	<li>
	\(\lim\limits_{k \rightarrow \infty} A^k = 
		\begin{pmatrix} 
		\bar{u} &amp; \bar{u} &amp; \cdots &amp; \bar{u} \\
		\end{pmatrix}
		\)<br />
	As a Consequence for any probability vector \(p\), \(\lim\limits_{k \rightarrow \infty} A^k\bar{p} = 
		\begin{pmatrix} 
		\bar{u} &amp; \bar{u} &amp; \cdots &amp; \bar{u} \\
		\end{pmatrix}\bar{p} = \bar{u} \text{ (for any $\bar{p}$)}
		\)
	</li>
</ul>
</div>
<p><br />
\(c\) is saying that just knowing the eigenvector corresponding to eigenvalue 1, we can predict the future. No matter what initial probability vector we’re given, we know as \(k\) approaches infinity, \(A^k\) is just \(\begin{pmatrix} \bar{u} &amp; \bar{u} &amp; ... &amp; \bar{u} \end{pmatrix}\) and the product of this matrix with any probability vector is just \(\bar{u}\)!
<br />
<br />
<!-------------------------------------------1----------------------------------------->
<b>Proof</b>
<br />
<br />
We’re going to prove these statements in several parts.
<br />
<br />
<b>1 is an eigenvalue of \(A\):</b>
<br />
<br />
FACT: \(A\) and \(A^t\) has the same eigenvalues. Why? This is because \(\det(B) = \det(B^t)\) which means that \(\det(A - tI_n) = \det(A^t - tI_n)\). We will use this fact to prove what we want.
<br />
<br />
Consider \(A^t\). We know that the rows now add to 1 (in \(A\), the columns add to 1). Let \(\bar{v} = \begin{pmatrix} 1 &amp; \cdots &amp; 1 \end{pmatrix}^t\). Now,</p>
<div> 
$$
\begin{align*}
A^t\bar{v} &amp;= \begin{pmatrix} a_1^t &amp; \cdots &amp; a_n^t \end{pmatrix}
\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = a_1^t + ... + a_n^t 
= \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \bar{v}
\end{align*}
$$
</div>
<p>The last step is true because we know in \(A^t\), the sum of each row adds to 1. Therefore, \(\bar{v}\) is an eigenvector of \(A^t\) with eigenvalue is 1. But we know that \(A\) and \(A^t\) have the same eigenvalues and so 1 is an eigenvalue of \(A\).
<br /></p>
<hr />

<!---------------------------------------2--------------------------------------------->
<p><br />
<b>If \(\lambda\) is an eigenvalue of \(A\), then \(|\lambda| \leq 1\):</b>
<br />
<br />
Since \(\lambda\) is an eigenvalue of \(A\), then \(\lambda\) is an eigenvalue of \(A^t\). By definition this means</p>
<div> 
$$
\begin{align*}
A^tv = \lambda v \text{ where } v = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \neq \bar{0}
\end{align*}
$$
</div>
<p>So \(v\) is an eigenvector of \(A^t\). Let \(|v_k| = \max_j \{|v_j|\}\). so \(|v_k|\) is the greatest entry in the vector \(v\) (and we’re going to assume it’s the \(k\)th entry?)
<br />
Now, let’s look at the \(k\)th entry of \(A^tv\). We’re going to compute it in two different ways. First, by the definition of matrix-vector multiplication as follows</p>
<div> 
$$
\begin{align*}
(A^tv)_k = \sum_j (A^t)_{kj}v_j
\end{align*}
$$
</div>
<p>Another way to compute this is by the definition of an eigenvector as follows</p>
<div> 
$$
\begin{align*}
(A^tv)_k = (\lambda v)_k = \lambda v_k
\end{align*}
$$
</div>
<p>Combining both equations</p>
<div> 
$$
\begin{align*}
\lambda v_k &amp;= \sum_j (A^t)_{kj}v_j \\
|\lambda v_k| &amp;= |\sum_j (A^t)_{kj}v_j| \\
&amp;= |\sum_j A_{jk}v_j| \text{ (replace $A^t$ with $A$)}\\
&amp;\leq \sum_j |A_{jk}v_j| \text{ (the absolute value of a sum $\leq$ sum of absolute values)} \\
&amp;\leq \sum_j A_{jk}|v_j| \text{ (the entries in $A$ are nonnegative by how we defined $$A$$ above)} \\
&amp;\leq \sum_j A_{jk}|v_k| \text{ (we defined the greatest entry as $|v_k|$ above )} \\
&amp;\leq |v_k| \sum_j A_{jk} \text{ ($|v_k|$ doesn't depend on the sum)} \\
&amp;\leq |v_k| \text{ (the sum is 1 by definition)} \\
|\lambda| &amp;\leq 1
\end{align*}
$$
</div>
<p>As required.
<br /></p>
<hr />

<!---------------------------------------2--------------------------------------------->
<p><br />
<b>If \(A_{ij} &gt; 0\) for all \(i,j\), then \(\dim E_1 = 1\) and \(\lambda \neq 1 \implies |\lambda| &lt; 1\):</b>
<br />
<br />
Suppose \(|\lambda| = 1\) and \(A^tv = \lambda v\) for \(v \neq \bar{0}\). It suffies to show that</p>
<div> 
$$
\begin{align*}
v = c\begin{pmatrix} 1 \\ \vdots \\ 1 
\end{pmatrix}
\end{align*}
$$
</div>
<p>Why? if the eigenvector was \(\begin{pmatrix} 1 &amp; \cdots &amp; 1 \end{pmatrix}^t\), then \(\lambda\) must be 1. Because</p>
<div> 
$$
\begin{align*}
A\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}  = \lambda \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}
\end{align*}
$$
</div>
<p>so \(\lambda = 1\). So we argue as before,</p>
<div> 
$$
\begin{align*}
|v_k| &amp;= | \lambda v_k | \text { ( because $ \lambda = 1)$}\\
&amp;= |\sum_j A_{jk} v_j | \\
&amp;\leq \sum_j A_{jk} |v_j| \\
&amp;\leq |v_k| \sum_j A_{jk} = |v_k| \\
\end{align*}
$$
</div>
<p>So these inequalities are equalities. \(A_{jk}v_j\) all have the same sign.
[TODO continue this proof .. I’m lost now]
<br />
Instead of proving (c), we will prove a slightly weaker theorem where we will assume that \(A\) is diagonalizable next!
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Diagonalizable Transition Matrix</b></h4>

<div class="purdiv">
Theorem (Corollary to 5.18)
</div>
<div class="purbdiv">
Suppose \(A\) is a transition matrix such that \(A_{ij} &gt; 0\) for \(i, j\) and \(A\) is diagonalizable. Then
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k = \begin{pmatrix} \bar{u} &amp; \cdots &amp; \bar{u} \end{pmatrix}
\end{align*}
$$
where \(\bar{u}\) is a probability vector and \(A\bar{u} = \bar{u}\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
<br />
We will assume the statements we proved in the previous theorem. So 1 is an eigenvalue, \(\dim(E_1)=1\) and any other \(\lambda\) has \(|\lambda| &lt; 1\). On top of this, now we can also assume that \(A\) is diagonalizable. This means we can write \(A\) as</p>
<div> 
$$
\begin{align*}
A = QDQ^{-1}, 
D = 
\begin{pmatrix} 
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>Assume that \(\lambda_1=1\). All the other \(\lambda\)’s absolute value is less than one.</p>
<div> 
$$
\begin{align*}
D = 
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>We know we can compute \(A^k\) easily using</p>
<div> 
$$
\begin{align*}
A^k = Q 
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^k &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n^k
\end{pmatrix}
Q^{-1}
\end{align*}
$$
</div>
<p>We can now apply the limit</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k =
Q
\begin{pmatrix} 
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
Q^{-1}
= L
\end{align*}
$$
</div>
<p>What do we know about \(L\)? What happens if we multiply \(A\) by \(L\)?</p>
<div> 
$$
\begin{align*}
AL &amp;= A\lim\limits_{k \rightarrow \infty} A^k \\
&amp;= \lim\limits_{k \rightarrow \infty} A^{k+1} \\
&amp;= L
\end{align*}
$$
</div>
<p>We know that \(L\) is a limit of a transition matrix so it’s a transition matrix. So the sum of the entries in each column of \(L\) is 1. Moreover, we saw that \(AL = L\). If \(L\) was a vector, then it would be an eigenvector with \(\lambda = 1\). But \(L\) is not a vector. It is a matrix with column vectors. if we apply the matrix-vector definition, then</p>
<div> 
$$
\begin{align*}
AL &amp;= \begin{pmatrix} Al_1 &amp; \cdots &amp; Al_n \end{pmatrix} \\
   &amp;= \begin{pmatrix} l_1 &amp; \cdots &amp; l_n \end{pmatrix} \quad \text{ (because $$AL = L$$)}
\end{align*}
$$
</div>
<p>From this we see that \(Al_1 = l_1, Al_2 = l_2,...\). So each column vector \(l_j\) is in fact an eigenvector of \(A\) with eigenvalue 1. But we also know that the eigenspace of \(\lambda=1\) is one-dimensional so all of the eigenvectors \(l_1,...,l_n\) must be in \(E_{\lambda_1}\). Therefore, \(l_j = c_j\bar{u}\) for some \(c_j\). 
<br />
<br />
We also know that the sum of entries of \(l_j\) is 1. Similarly the sum of entries in \(\bar{u}\) is also 1. So if both sides had vectors where the sum of entries is 1, then \(c_j\) must be 1 so \(l_j = \bar{u}\) and we can write</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k = \begin{pmatrix} \bar{u} &amp; \cdots &amp; \bar{u} \end{pmatrix}
\end{align*}
$$
</div>
<p>Additionally since \(\l_j\) is a probability vector, then we also know that \(\bar{u}\) is a probability vector.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Continuing the same example from before where</p>
<div> 
$$
\begin{align*}
A = 
\begin{pmatrix} 
.9 &amp; .02 \\
.1 &amp; .98
\end{pmatrix}
\end{align*}
$$
</div>
<p>We want to check if \(A\) is diagonalizable. In fact it is</p>
<div> 
$$
\begin{align*}
A = QDQ^{-1}, Q = 
\begin{pmatrix} 
\frac{1}{6} &amp; -\frac{1}{6} \\
\frac{5}{6} &amp; \frac{1}{6}
\end{pmatrix},
D = 
\begin{pmatrix} 
1 &amp; 0 \\
0 &amp; 0.88
\end{pmatrix}
\end{align*}
$$
</div>
<p>To compute \(A^k\), we know from the previous theorem, that the answer has two copies of the eigenvector corresponding to eigenvalue \(\lambda = 1\). You can see above that the eigenvector corresponding to 1 is the first column vector in \(Q\) and so</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} A^k =
\begin{pmatrix} 
\frac{1}{6} &amp; \frac{1}{6} \\
\frac{5}{6} &amp; \frac{5}{6}
\end{pmatrix}
\end{align*}
$$
</div>
<p>and no matter what is our initially probability vector is,</p>
<div> 
$$
\begin{align*}
\lim\limits_{k \rightarrow \infty} \bar{p}_k =
\begin{pmatrix} 
\frac{1}{6} \\
\frac{5}{6} 
\end{pmatrix}
\end{align*}
$$
</div>
<p>\(\frac{1}{6}\) corresponds to state 1 and \(\frac{5}{6}\) correspond to state 2.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>


  </div><div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = /jekyll/update/2024/08/23/lec26-markov-chains.html;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier =  /jekyll/update/2024/08/23/lec26-markov-chains; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://strncat-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/jekyll/update/2024/08/23/lec26-markov-chains.html" hidden></a>
</article>
		 
      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">nemo&#39;s notebook</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">nemo&#39;s notebook</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>personal study notes</p>
      </div>
    </div>

  </div>

</footer>
-->

	
  </body>

</html>
