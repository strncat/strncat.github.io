<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-04-30T20:12:45-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 37: R-Domain</title><link href="http://localhost:4000/jekyll/update/2025/03/01/math417-37-r-domains.html" rel="alternate" type="text/html" title="Lecture 37: R-Domain" /><published>2025-03-01T00:01:36-08:00</published><updated>2025-03-01T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/03/01/math417-37-r-domains</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/03/01/math417-37-r-domains.html"><![CDATA[<p>We’ll start by defining Quotient Rings
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Domain
</div>
<div class="mintbodydiv">
A domain \(R\) is a commutative ring with 1 such that
<ol>
	<li>\(1 \neq 0\).</li>
	<li>If \(a,b \in R/\{0\}\), then \(ab \in R/\{0\}\). So non-zero elements multiply to non-zero elements. In other words, \(ab = 0\) then either \(a = 0\) or \(b = 0\).</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
Fact: Every field is a domain. Other examples:</p>
<ol>
	<li>\(\mathbf{Z}\) and \(K[x]\) are domains.</li>
	<li>The Gaussian integers are a domain \(\mathbf{Z}[i] = \{a + bi \in \mathbf{C} \ | \ a, b \in \mathbf{Z}\).</li>
	<li>\(\mathbf{Z}_4\) is not a domain, \([2] \neq 0\) but \([2][2] = [4] = [0]]\).</li>
	<li>\(\mathbf{Q}/(f_1f_2)\) where \(\deg(f_i) \geq 1\) is not a domain. (Exercise)</li>
</ol>
<p>Fact: If \(K\) is a field and \(R \subseteq K\) is a subring such that \(1_K \in R\), then \(R\) is a domain
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Every domain \(R\) is (isomorphic) a subring of a field \(F = \text{Frac}(R)\) (field of fraction)
</div>
<!----------------------------------------------------------------------------->
<p><br />
So basically every domain can be enlarge to be a field.
<br />
<br />
<b>Proof</b>
<br />
Let \(X = \{(a,b) \ | \ a, b \in R, b \neq 0\}\). Define a relation on \(X\):</p>
<div>
$$
\begin{align*}
(a,b) \sim (a',b') \quad \text{ iff } ab' = ba'
\end{align*}
$$
</div>
<p>We claim that this relation is an equivalence relation. (Exercise, hard to prove but use that \(R\) is a domain Side note: domains have cancellations so if \(ab = bc\) and \(a \neq 0\), then \(b = c\).)
<br />
<br />
Since we have an equivalence relation on a set \(X\), then define \(F: X/\sim\) to be the set of equivalence classes. Write \([\frac{a}{b}] \in F\) for the equivalence class of \((a,b)\). Now, define</p>
<div>
$$
\begin{align*}
\big[\frac{a}{b}\big] + \big[\frac{a'}{b'}\big] &amp;= \big[\frac{ab'+ba'}{bb'}\big] \\
\big[\frac{a}{b}\big]\big[\frac{a'}{b'}\big] &amp;= \big[\frac{aa'}{bb'}\big]
\end{align*}
$$
</div>
<p>Check that these operations are well defined and make \(F\) a field. Finally, define \(\varphi: R \rightarrow F\) by \(\varphi(a) = [\frac{a}{1}]\). Check that \(\varphi\) is an injective ring homomorphism.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ol>
	<li>Suppose \(R = \mathbf{Z}\), then \(\text{Frac}(\mathbf{Z}) \cong \mathbf{Q}\).</li>
	<li>Suppose \(R = K[x]\) the ring of polynomials over a field. Then \(\text{Frac}(K[x]) = K[x]\) is the field of rational polynomials. elements in \(\text{Frac}(K[x]) = K[x]\) are of the form \(\frac{f}{g}\).</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Element Types</b></h4>
<p>We want to discuss factorization in a domain but before doing so, we need to see the types of elements we can find in a domain.</p>
<ol>
	<li>Zero: the element zero is in \(R\).</li>
	<li>Units: These are the elements that have a multiplicative inverse.</li>
	<li>Reducible: If \(a \in R\), \(a \neq 0\) and \(a \not\in R^{\times}\) (not a unit), then there exists \(b,c \in R\) such that \(a = bc\). (\(b\) and \(b\) must both be non-units!)</li>
	<li>Irreducible: Not a zero, not a unit and not reducible. This means \(a \in R\), \(a \not\in R^{\times}\) and if \(a = bc\), then either \(b \in R^{\times}\) or \(c \in R^{\times}\).</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ol>
	<li>\(R = \mathbf{Z}\)</li>
	<ul>
		<li>Units: \(\mathbf{Z}^{\times} = \{-1, +1\}\)</li>
		<li>Reducible: \(\{\pm n, n \in \mathbf{N}, \text{composite}\}\)</li>
		<li>Irreducible: These are the elements that don't factor so \(\{\pm p, p \in \mathbf{N}, \text{prime}\}\)</li>
	</ul>
	<!---------------Field----------------->
	<li>\(R = K\) where \(K\) is a field</li>
	<ul>
		<li>\(0 \in K\)</li>
		<li>Units: \(K^{\times} = K / \{0\}\)</li>
		<li>Reducible: none.</li>
		<li>Irreducible: none.</li>
	</ul>
	<!---------------Polynomial Rings----------------->
	<li>\(R = K[x]\) where \(K\) is a field</li>
	<ul>
		<li>\(0 \in K[x]\)</li>
		<li>Units: these are polynomials of degree 0 so \(K^{\times}\)</li>
		<li>Reducible: polynomials that you can factor into smaller ones. \(f, \deg(f) \geq 1\) such that \(f = gh, 0 &lt; \deg(g), \deg(h) &lt; \deg(f)\)</li>
		<li>Irreducible: polynomials that you can't factor. \(f, \deg(f) \geq 1\) such that there is no \(g,h, \deg(h), \deg(g) &lt; \deg(f)\) such that \(f = gh\)</li>
	</ul>
	<!------------ polynomials over the complex numbers ----------------->
	<li>\(R = \mathbf{C}[x]\)</li>
	<ul>
		<li>\(0 \in C[x]\)</li>
		<li>Units: \(C^{\times}\)</li>
		<li>Reducible: \(f, \deg(f) \geq 2\)</li>
		<li>Irreducible: \(f, \deg(f) = 1\)</li>
	</ul>
	<!--------- polynomials over the real numbers ------------->
	<li>\(R = \mathbf{C}[x]\)</li>
	<ul>
		<li>\(0 \in R[x]\)</li>
		<li>Units: \(R^{\times}\)</li>
		<li>Irreducible: One form is \(ax + b, a \neq 0\) and another form is \(ax^2 + bx + c\) where \(a \neq 0, b^2 - 4ac &lt; 0\)</li>
		<li>Reducible: The rest of polynomials are reducible</li>
	</ul>
</ol>
<p>Warning: If we take a look at the polynomials over the rational numbers, we will find out that there are a lot more irreducible polynomials there. Note here that the ring of polynomials with rational coefficients is contained in the ring of polynomials with the real coefficients and that’s contained in the field of rational functions over \(\mathbf{R}\). In other words, \(\mathbf{Q}[x] \subseteq \mathbf{R}[x] \subseteq \mathbf{R}(x)\). Now, take \(x^p - 2\) where \(p \geq 2\), then \(x^p - 2\) is irreducible in \(Q[x]\), reducible in \(R[x]\). (The proof is not obvious). It is a unit in \(\mathbf{R}(x)\)! so it’s important to consider the context in which we’re in.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Norm Functions</b></h4>
<p>Another Example is the Gaussian Integers: \(R = \mathbf{Z}[i] = \{a + bi \ | \ a, b \in \mathbf{Z}\} \subseteq \mathbf{C}\). So \(R\) is a subring of the complex numbers and it is a domain. Since it’s a domain, we want to categorize the elements in it. But to do so, we need another tool defined as follows
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Norm Functions
</div>
<div class="mintbodydiv">
The norm of \(a + bi\) is defined to be \(N(a + bi)  = a^2 + b^2 = (a + bi)(a - bi) = \lVert a + bi \rVert^2 \)
</div>
<!----------------------------------------------------------------------------->
<p><br />
So this is a function defined on the complex numbers and therefore defined on the Gaussian integers. Some properties:</p>
<ol>
	<li>If \(z \in \mathbf{Z}[i], N(z) \in \mathbf{Z}_{\geq 0}\)</li>
	<li>\(N(z) = 0\) if and only if \(z = 0\)</li>
	<li>If \(z, w \in \mathbf{Z}[i], N(zw) = N(z)N(w)\)</li>
	<li>If \(z \in \mathbf{Z}[i]\), then \(z \in \mathbf{Z}[i]^{\times}\) if and only if \(N(z) = 1\)</li>
</ol>
<p>Proof of (4):
<br />
If \(z \in \mathbf{Z}[i]^{\times}\), then \(z\) is a unit and \(zz^{-1} = 1\). We know that \(z^{-1} \in \mathbf{Z}[i]\) so \(z^{-1}\) is a Gaussian integer. So</p>
<div>
$$
\begin{align*}
1 = N(1) = N(z)N(z^{-1})
\end{align*}
$$
</div>
<p>But \(N(z)\) and \(N(z^{-1})\) are positive integers by (1). Therefore \(N(z) = N(z^{-1}) = 1\). 
<br />
<br />
For the other direction when \(N(z) = 1\). Write \(z = a + bi\) where \(a, b \in \mathbf{Z}\). By definition</p>
<div>
$$
\begin{align*}
1 = N(z) = N(a + bi) = (a + bi)(a - bi)
\end{align*}
$$
</div>
<p>Therefore, \((a + bi)^{-1} = (a - bi)\). 
<br />
<br />
In fact the units are just \(\mathbf{Z}^{-1}[i] = \{ \pm 1, \pm i\}\).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Example: \(2 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). So 2 is also a Gaussian integer since \(\mathbf{Z}\) is a subring of the Gaussian integers. \(2\) is a prime number so it’s irreducible as an integer. However, is it reducible as a Gaussian integer? Observe that</p>
<div>
$$
\begin{align*}
2 = (1 + i)(1 - i)
\end{align*}
$$
</div>
<p>So it’s definitely reducible as a Gaussian integer.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Example: Take \(1 + i \in \mathbf{Z}[i]\). Is it reducible or irreducible in \(\mathbf{Z}[i]\)? It is in fact irreducible. We know this because \(N(1 + i) = 2\) and if there were factors \(1 + i = zw\), then</p>
<div>
$$
\begin{align*}
N(1 + i) = N(z)N(w) = 2
\end{align*}
$$
</div>
<p>But the norm is a non-negative integer. So the only way to make this work is that either \(N(z) = 1\) or \(N(w) = 1\) so one of the factors is a unit. Thus, by definition, \(z\) is irreducible. 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Associate Elements</b></h4>
<p>Next, we have this definition for when two elements are associate when one element is a product of some unit times the other element.
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
\(a, b \in R\) are the <b>same up-to-units</b> or <b>associate</b> (we write \(a \sim b\)) if there exists an element \(u \in R^{\times}\) such that \(b = ua\).
</div>
<p><br />
<!----------------------------------------------------------------------------->
This is in fact an equivalence relation (Exercise)
<br />
<br />
Example: In \(\mathbf{Z}\), associate classes are \(\{0\}\), \(\{\pm n\}, n &gt; 0\). Another example is \(K[x]\), \(f \sim g\) if \(g = fc\) for some \(c \in K/\{0\}\). 
<br />
<br />
Fact: If two elements are associate, then they are of the same type!
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>\(3 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). Is it reducible? It is irreducible in \(\mathbf{Z}\) since it is prime. In \(\mathbf{Z}[i]\), we know that \(N(3) = N(3 + 0i) = 9\). Suppose it was reducible. This means that there exists \(z,w \in \mathbf{Z}[i]\)</p>
<div>
$$
\begin{align*}
9 = N(3) = N(z)N(w)
\end{align*}
$$
</div>
<p>To be reducible, neither \(z\) or \(w\) can be units so they can’t have a norm of 1. Therefore, we must have \(N(z) = N(w) = 3\). This a contradiction since we don’t have any Gaussian integers with norm 3 (square root of 3 from the origin).</p>
<div>
$$
\begin{align*}
N(a + bi) = a^2 + b^2, \quad a^2, b^2 \in \{0,1,4,9,16,...\}
\end{align*}
$$
</div>
<p>So it’s not possible to construct an element of the form \(a + bi\) in \(\mathbf{Z}[i]\) such that \(a^2 + b^2 = 3\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>\(4 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). Is it reducible? It is reducible in \(\mathbf{Z}\). In \(\mathbf{Z}[i]\), we know that \(4 = (2)(2)\) and \(2\) is not a unit in \(\mathbf{Z}[i]\). Not just that but we also saw that \(2\) can be factored using the previous example \(2 = (1 + i)(1 - i)\). Furthermore, we also have another way too to factor 4. Below are some of the ways</p>
<div>
$$
\begin{align*}
4 = 2(2) = (1 + i)^2(1 - i)^2 = -(1 + i)^4
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>\(5 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). Is it reducible in \(\mathbf{Z}[i]\)? It is reducible because</p>
<div>
$$
\begin{align*}
5 = (2 + i)(2 - i)
\end{align*}
$$
</div>
<p>The factors themselves \((2+i)\) and \((2-i)\) are irreducible since the the norm \(N(2 + i) = 5\). Also note that \((2+i)\) and \((2-i)\) are not associate. 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>When is \(p\) reducible in \(\mathbf{Z}[i]\)?</b></h4>
<p>We went through a few examples of doing this calculation but in fact we do have a proposition about this as follows
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(p \in \mathbf{N}\) is a prime number, then \(p\) is reducible in \(\mathbf{Z}[i]\) if and only if \(p = a^2 + b^2\) for some \(a,b \in \mathbf{Z}\)
</div>
<p><br />
<!----------------------------------------------------------------------------->
<b>Proof</b>
<br />
\(\Leftarrow:\) Suppose that \(p = a^2 + b^2\). Then we can write</p>
<div>
$$
\begin{align*}
p = (a - bi)(a + bi)
\end{align*}
$$
</div>
<p>Recall that \(N(a + bi) = a^2 + b^2\) but that means \(N(a + bi) = p\) and \(p\) is prime so it’s not 1. So neither (a + bi) or (a - bi) is a unit. Therefore, \(p\) is reducible. 
<br />
<br />
\(\Rightarrow:\) Suppose that \(p\) is reducible so \(p = zw\). Therefore,</p>
<div>
$$
\begin{align*}
N(p) = N(z)N(w) = p^2 \quad (\text{since } p \in \mathbf{Z}, N(p)=p^2)
\end{align*}
$$
</div>
<p>But \(p\) is prime and so we must have \(N(z)=N(w)=p\). This means that there exists some \(a, b\) such that \(z = a + bi\) where \(N(z) = p\). This means</p>
<div>
$$
\begin{align*}
N(z) = a^2 + b^2 = p.
\end{align*}
$$
</div>
<p>So \(p = a^2 + b^2\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>The proof shows that for a prime to be a sum of squares is related to whether it factors as a Gaussian integer</p>

<div>
<table style="max-width: 700px; margin: 20px auto;">
  <tr>
    <td>Reducible in \(Z[i]\)</td>
	<td>Irreducible in \(Z[i]\)</td>
  </tr>
  <tr>
    <td>\(2 = 1^2 + 1^2\)</td>
	<td>3</td>
  </tr>
  <tr>
    <td>\(5 = 2^2 + 1^2\)</td>
	<td>7</td>
  </tr>
  <tr>
    <td>\(13 = 3^2 + 2^2\)</td>
	<td>11</td>
  </tr>
  <tr>
    <td>\(17 = 4^2 + 2^2\)</td>
	<td>19</td>
  </tr>
  <tr>
    <td>\(29 = 5^2 + 2^2\)</td>
	<td>23</td>
  </tr>
  <tr>
    <td>\(37 = 6^2 + 1^2\)</td>
	<td>31</td>
  </tr>
</table>
</div>
<p>Notice the pattern here. The left prime numbers are congruent to 1 mod 4 while the right prime numbers are congruent to -1 mod 4.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Irreducible Elements in \(R\)</b></h4>
<p>Lecture 38: To state things again, \(a \in R\) is irreducible if</p>
<ol>
	<li>\(a \neq 0\)</li>
	<li>\(a \not\in R^{\times}\)</li>
	<li>If \(a = bc\), then one of \(b\) or \(c\) is a unit</li>	
</ol>
<p>This notion of irreducible element is kind of the replacement we have to the notion of a prime number. In fact, we can re-write this condition to make it closer.</p>
<ul>
	<li>Given two elements in a domain \(a,b \in R\), we say that \(a\) divides \(b\) (write \(a \ | \ b\)) if there exists an element \(c \in R\) such that \(b = ac\)</li>
	<li>In terms of ideals. This is equivalent to saying \(b\) is in \(Ra\). It is also equivalent to saying that \(Rb \subseteq Ra\). </li>
	<li>\(a\) is irreducible if whenever \(b \ | \ a\), either \(b\) is a unit or \(b \sim a\). This is similar to how we defined prime numbers. if an element divides \(p\), then it's either \(p\) or \(1\)</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Prime Elements in \(R\)</b></h4>
<p>The reason we’re discussing this is because of another definition that might make things confusing.
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
\(p \neq 0 \in R\) is prime if for every \(a, b \in R\), if \(p \ | \ ab\), then either \(p \ | \ a \) or \(p \ | \ b\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
This is the definition a prime element in a domain. The confusing part is that the definition of an irreducible element is the one equivalent to a prime number. 
<br />
<br />
Now, this notion is actually related to irreducibility. In fact, in \(R = \mathbf{Z}\), the prime elements and the irreducible elements are exactly the same. 
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(p \in R\) where \(R\) is a domain. Then if \(p\) is prime, then \(p\) is irreducible.
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
Suppose that \(p\) is prime. We want to show that \(p\) is irreducible so suppose that \(p = ab\) where \(a, b \in R\). We want to show that either \(a\) or \(b\) is a unit so \(a \in R^{\times}\) or \(b \in R^{\times}\). Since \(p = ab\), then \(p \ | \ ab\). But prime means that \(p \ | \ a\) or \(p \ | \ b\). So, suppose \(p \ | \ a\). This means that \(p\) is a factor of \(a\) so \(a = pc\) for some \(c \in R\). Plugging this back in the original equation to see that</p>
<div>
$$
\begin{align*}
p &amp;= ab \\
p &amp;= pcb \\
1 &amp;= cb \quad (p \neq 0)
\end{align*}
$$
</div>
<p>But this means that \(b\) is a unit. Therefore, \(p\) is irreducible. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’ll start by defining Quotient Rings Definition: Domain A domain \(R\) is a commutative ring with 1 such that \(1 \neq 0\). If \(a,b \in R/\{0\}\), then \(ab \in R/\{0\}\). So non-zero elements multiply to non-zero elements. In other words, \(ab = 0\) then either \(a = 0\) or \(b = 0\). Fact: Every field is a domain. Other examples: \(\mathbf{Z}\) and \(K[x]\) are domains. The Gaussian integers are a domain \(\mathbf{Z}[i] = \{a + bi \in \mathbf{C} \ | \ a, b \in \mathbf{Z}\). \(\mathbf{Z}_4\) is not a domain, \([2] \neq 0\) but \([2][2] = [4] = [0]]\). \(\mathbf{Q}/(f_1f_2)\) where \(\deg(f_i) \geq 1\) is not a domain. (Exercise) Fact: If \(K\) is a field and \(R \subseteq K\) is a subring such that \(1_K \in R\), then \(R\) is a domain Proposition Every domain \(R\) is (isomorphic) a subring of a field \(F = \text{Frac}(R)\) (field of fraction) So basically every domain can be enlarge to be a field. Proof Let \(X = \{(a,b) \ | \ a, b \in R, b \neq 0\}\). Define a relation on \(X\): $$ \begin{align*} (a,b) \sim (a',b') \quad \text{ iff } ab' = ba' \end{align*} $$ We claim that this relation is an equivalence relation. (Exercise, hard to prove but use that \(R\) is a domain Side note: domains have cancellations so if \(ab = bc\) and \(a \neq 0\), then \(b = c\).) Since we have an equivalence relation on a set \(X\), then define \(F: X/\sim\) to be the set of equivalence classes. Write \([\frac{a}{b}] \in F\) for the equivalence class of \((a,b)\). Now, define $$ \begin{align*} \big[\frac{a}{b}\big] + \big[\frac{a'}{b'}\big] &amp;= \big[\frac{ab'+ba'}{bb'}\big] \\ \big[\frac{a}{b}\big]\big[\frac{a'}{b'}\big] &amp;= \big[\frac{aa'}{bb'}\big] \end{align*} $$ Check that these operations are well defined and make \(F\) a field. Finally, define \(\varphi: R \rightarrow F\) by \(\varphi(a) = [\frac{a}{1}]\). Check that \(\varphi\) is an injective ring homomorphism. Examples Suppose \(R = \mathbf{Z}\), then \(\text{Frac}(\mathbf{Z}) \cong \mathbf{Q}\). Suppose \(R = K[x]\) the ring of polynomials over a field. Then \(\text{Frac}(K[x]) = K[x]\) is the field of rational polynomials. elements in \(\text{Frac}(K[x]) = K[x]\) are of the form \(\frac{f}{g}\). Element Types We want to discuss factorization in a domain but before doing so, we need to see the types of elements we can find in a domain. Zero: the element zero is in \(R\). Units: These are the elements that have a multiplicative inverse. Reducible: If \(a \in R\), \(a \neq 0\) and \(a \not\in R^{\times}\) (not a unit), then there exists \(b,c \in R\) such that \(a = bc\). (\(b\) and \(b\) must both be non-units!) Irreducible: Not a zero, not a unit and not reducible. This means \(a \in R\), \(a \not\in R^{\times}\) and if \(a = bc\), then either \(b \in R^{\times}\) or \(c \in R^{\times}\). Examples \(R = \mathbf{Z}\) Units: \(\mathbf{Z}^{\times} = \{-1, +1\}\) Reducible: \(\{\pm n, n \in \mathbf{N}, \text{composite}\}\) Irreducible: These are the elements that don't factor so \(\{\pm p, p \in \mathbf{N}, \text{prime}\}\) \(R = K\) where \(K\) is a field \(0 \in K\) Units: \(K^{\times} = K / \{0\}\) Reducible: none. Irreducible: none. \(R = K[x]\) where \(K\) is a field \(0 \in K[x]\) Units: these are polynomials of degree 0 so \(K^{\times}\) Reducible: polynomials that you can factor into smaller ones. \(f, \deg(f) \geq 1\) such that \(f = gh, 0 &lt; \deg(g), \deg(h) &lt; \deg(f)\) Irreducible: polynomials that you can't factor. \(f, \deg(f) \geq 1\) such that there is no \(g,h, \deg(h), \deg(g) &lt; \deg(f)\) such that \(f = gh\) \(R = \mathbf{C}[x]\) \(0 \in C[x]\) Units: \(C^{\times}\) Reducible: \(f, \deg(f) \geq 2\) Irreducible: \(f, \deg(f) = 1\) \(R = \mathbf{C}[x]\) \(0 \in R[x]\) Units: \(R^{\times}\) Irreducible: One form is \(ax + b, a \neq 0\) and another form is \(ax^2 + bx + c\) where \(a \neq 0, b^2 - 4ac &lt; 0\) Reducible: The rest of polynomials are reducible Warning: If we take a look at the polynomials over the rational numbers, we will find out that there are a lot more irreducible polynomials there. Note here that the ring of polynomials with rational coefficients is contained in the ring of polynomials with the real coefficients and that’s contained in the field of rational functions over \(\mathbf{R}\). In other words, \(\mathbf{Q}[x] \subseteq \mathbf{R}[x] \subseteq \mathbf{R}(x)\). Now, take \(x^p - 2\) where \(p \geq 2\), then \(x^p - 2\) is irreducible in \(Q[x]\), reducible in \(R[x]\). (The proof is not obvious). It is a unit in \(\mathbf{R}(x)\)! so it’s important to consider the context in which we’re in. Norm Functions Another Example is the Gaussian Integers: \(R = \mathbf{Z}[i] = \{a + bi \ | \ a, b \in \mathbf{Z}\} \subseteq \mathbf{C}\). So \(R\) is a subring of the complex numbers and it is a domain. Since it’s a domain, we want to categorize the elements in it. But to do so, we need another tool defined as follows Definition: Norm Functions The norm of \(a + bi\) is defined to be \(N(a + bi) = a^2 + b^2 = (a + bi)(a - bi) = \lVert a + bi \rVert^2 \) So this is a function defined on the complex numbers and therefore defined on the Gaussian integers. Some properties: If \(z \in \mathbf{Z}[i], N(z) \in \mathbf{Z}_{\geq 0}\) \(N(z) = 0\) if and only if \(z = 0\) If \(z, w \in \mathbf{Z}[i], N(zw) = N(z)N(w)\) If \(z \in \mathbf{Z}[i]\), then \(z \in \mathbf{Z}[i]^{\times}\) if and only if \(N(z) = 1\) Proof of (4): If \(z \in \mathbf{Z}[i]^{\times}\), then \(z\) is a unit and \(zz^{-1} = 1\). We know that \(z^{-1} \in \mathbf{Z}[i]\) so \(z^{-1}\) is a Gaussian integer. So $$ \begin{align*} 1 = N(1) = N(z)N(z^{-1}) \end{align*} $$ But \(N(z)\) and \(N(z^{-1})\) are positive integers by (1). Therefore \(N(z) = N(z^{-1}) = 1\). For the other direction when \(N(z) = 1\). Write \(z = a + bi\) where \(a, b \in \mathbf{Z}\). By definition $$ \begin{align*} 1 = N(z) = N(a + bi) = (a + bi)(a - bi) \end{align*} $$ Therefore, \((a + bi)^{-1} = (a - bi)\). In fact the units are just \(\mathbf{Z}^{-1}[i] = \{ \pm 1, \pm i\}\). Example 1 Example: \(2 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). So 2 is also a Gaussian integer since \(\mathbf{Z}\) is a subring of the Gaussian integers. \(2\) is a prime number so it’s irreducible as an integer. However, is it reducible as a Gaussian integer? Observe that $$ \begin{align*} 2 = (1 + i)(1 - i) \end{align*} $$ So it’s definitely reducible as a Gaussian integer. Example 2 Example: Take \(1 + i \in \mathbf{Z}[i]\). Is it reducible or irreducible in \(\mathbf{Z}[i]\)? It is in fact irreducible. We know this because \(N(1 + i) = 2\) and if there were factors \(1 + i = zw\), then $$ \begin{align*} N(1 + i) = N(z)N(w) = 2 \end{align*} $$ But the norm is a non-negative integer. So the only way to make this work is that either \(N(z) = 1\) or \(N(w) = 1\) so one of the factors is a unit. Thus, by definition, \(z\) is irreducible. Associate Elements Next, we have this definition for when two elements are associate when one element is a product of some unit times the other element. Definition \(a, b \in R\) are the same up-to-units or associate (we write \(a \sim b\)) if there exists an element \(u \in R^{\times}\) such that \(b = ua\). This is in fact an equivalence relation (Exercise) Example: In \(\mathbf{Z}\), associate classes are \(\{0\}\), \(\{\pm n\}, n &gt; 0\). Another example is \(K[x]\), \(f \sim g\) if \(g = fc\) for some \(c \in K/\{0\}\). Fact: If two elements are associate, then they are of the same type! Example 3 \(3 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). Is it reducible? It is irreducible in \(\mathbf{Z}\) since it is prime. In \(\mathbf{Z}[i]\), we know that \(N(3) = N(3 + 0i) = 9\). Suppose it was reducible. This means that there exists \(z,w \in \mathbf{Z}[i]\) $$ \begin{align*} 9 = N(3) = N(z)N(w) \end{align*} $$ To be reducible, neither \(z\) or \(w\) can be units so they can’t have a norm of 1. Therefore, we must have \(N(z) = N(w) = 3\). This a contradiction since we don’t have any Gaussian integers with norm 3 (square root of 3 from the origin). $$ \begin{align*} N(a + bi) = a^2 + b^2, \quad a^2, b^2 \in \{0,1,4,9,16,...\} \end{align*} $$ So it’s not possible to construct an element of the form \(a + bi\) in \(\mathbf{Z}[i]\) such that \(a^2 + b^2 = 3\). Example 4 \(4 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). Is it reducible? It is reducible in \(\mathbf{Z}\). In \(\mathbf{Z}[i]\), we know that \(4 = (2)(2)\) and \(2\) is not a unit in \(\mathbf{Z}[i]\). Not just that but we also saw that \(2\) can be factored using the previous example \(2 = (1 + i)(1 - i)\). Furthermore, we also have another way too to factor 4. Below are some of the ways $$ \begin{align*} 4 = 2(2) = (1 + i)^2(1 - i)^2 = -(1 + i)^4 \end{align*} $$ Example 5 \(5 \in \mathbf{Z} \subseteq \mathbf{Z}[i]\). Is it reducible in \(\mathbf{Z}[i]\)? It is reducible because $$ \begin{align*} 5 = (2 + i)(2 - i) \end{align*} $$ The factors themselves \((2+i)\) and \((2-i)\) are irreducible since the the norm \(N(2 + i) = 5\). Also note that \((2+i)\) and \((2-i)\) are not associate. When is \(p\) reducible in \(\mathbf{Z}[i]\)? We went through a few examples of doing this calculation but in fact we do have a proposition about this as follows Proposition If \(p \in \mathbf{N}\) is a prime number, then \(p\) is reducible in \(\mathbf{Z}[i]\) if and only if \(p = a^2 + b^2\) for some \(a,b \in \mathbf{Z}\) Proof \(\Leftarrow:\) Suppose that \(p = a^2 + b^2\). Then we can write $$ \begin{align*} p = (a - bi)(a + bi) \end{align*} $$ Recall that \(N(a + bi) = a^2 + b^2\) but that means \(N(a + bi) = p\) and \(p\) is prime so it’s not 1. So neither (a + bi) or (a - bi) is a unit. Therefore, \(p\) is reducible. \(\Rightarrow:\) Suppose that \(p\) is reducible so \(p = zw\). Therefore, $$ \begin{align*} N(p) = N(z)N(w) = p^2 \quad (\text{since } p \in \mathbf{Z}, N(p)=p^2) \end{align*} $$ But \(p\) is prime and so we must have \(N(z)=N(w)=p\). This means that there exists some \(a, b\) such that \(z = a + bi\) where \(N(z) = p\). This means $$ \begin{align*} N(z) = a^2 + b^2 = p. \end{align*} $$ So \(p = a^2 + b^2\) as we wanted to show. \(\ \blacksquare\) Examples The proof shows that for a prime to be a sum of squares is related to whether it factors as a Gaussian integer]]></summary></entry><entry><title type="html">Lecture 36: Homomorphism Theorem and Isomorphism Theorem</title><link href="http://localhost:4000/jekyll/update/2025/02/28/math417-36-ring-homomorphisms-isomorphisms.html" rel="alternate" type="text/html" title="Lecture 36: Homomorphism Theorem and Isomorphism Theorem" /><published>2025-02-28T00:01:36-08:00</published><updated>2025-02-28T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/28/math417-36-ring-homomorphisms-isomorphisms</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/28/math417-36-ring-homomorphisms-isomorphisms.html"><![CDATA[<p>We’ll start by defining Quotient Rings
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Quotient Ring
</div>
<div class="mintbodydiv">
Let \(R\) be a ring and \(I\) be an ideal in \(R\). Then let the quotient ring \(R/I\)
$$
\begin{align*}
R/I = \{a + I \ | \ a + R\}
\end{align*}
$$
be the set of all \(I\)-cosets in \(R\) with operations:
<ol>
	<li>Addition: \((a + I) + (b + I) = (a + b) + I\).</li>
	<li>Multiplication: \((a + I) + (b + I) = (ab) + I\).</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
Note that there are other notations used for \(I + a\). One notation is \([a]\) or \(\bar{a}\).
<br />
<br />
The claim is that these operations are well defined and will make the quotient set with the operations defined a ring. For addition, it’s already part of how we defined quotient groups? For multiplication, we need to show that if \(a + I = a' + I\) and \(b + I = b' + I\). Then</p>
<div>
$$
\begin{align*}
ab + I = a'b' + 1
\end{align*}
$$
</div>
<p>To see this, write \(a' = a + u\) and \(b' = b + v\) for some \(u, v \in I\). Then</p>
<div>
$$
\begin{align*}
a'b' &amp;= (a + u)(b + v) \\
     &amp;= ab + av + ub + uv
\end{align*}
$$
</div>
<p>\(av + ub + uv\) is in \(I\) since \(I\) is an ideal. Therefore, \(a'b' + (av+ub+uv)\) and \(ab + I\) represent the same ideal. 
<br />
<br />
Next, we need to show that \(R/I\) is an abelian group with addition. And then we need to show that multiplication is associative and distributes over addition. (exercise)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Quotient Homomorphism</b></h4>

<div class="mintheaderdiv">
Definition: Quotient Homomorphism
</div>
<div class="mintbodydiv">
Let \(R\) be a ring and \(I\) be an ideal in \(R\). Define
$$
\begin{align*}
\pi: R &amp;\rightarrow R/I \\
     \pi(a) &amp;= a + I
\end{align*}
$$
</div>
<!------------------------------------------------------------------------->
<p><br />
So just send any element to its coset. We already know this makes a group homomorphism for quotient groups. It also makes a ring homomorphism. We just need to check. It is also surjective because every coset comes from some element in \(I\). The kernel of the homomorphism \(\ker(\pi) = I\).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Take \(R = \mathbf{Z}\) and take \(I\) to be a principle ideal generated by some element \(d\) so \(I = (d) = \mathbf{Z}d\). Then</p>
<div>
$$
\begin{align*}
R/I = \mathbf{Z}/\mathbf{Z}d = \mathbf{Z}_d
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Take the ring of polynomials \(R[x]\). If we quotient this ring by the principle ideal generated by \((x^2 + 1)\), then we get the ring of complex numbers. \(R[x]/(x^2+1) \cong \mathbf{C}\)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Quotient Rings of Polynomial Rings</b></h4>
<p>Let \(K\) be a field. Let \(R = K[x]\) be the ring of polynomials over the field \(K\) and let \(I \subseteq R\). From this, form the quotient ring \(S = K[x]/I\).  Notice here that since the ideal is in a ring over a field, then we know every ideal is principle. So \(I\) is a principle ideal generated by some polynomial and we can write \(I = (f)\) for some \(f \in R\). 
<br />
<br />
For example, \(R/(0) \cong R\). If \(I\) is not \(\{0\}\), then choose \(f\) to be monic of minimal degree. Now write</p>
<div>
$$
\begin{align*}
f = x^n + a_{n-1}x + ... + a_0, a_0,...,a_{n-1} \in K
\end{align*}
$$
</div>
<p>Then \(S = K[x] /(f)\). We have the following claim based in this
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Every element in \(S\) has the form \(r + I\) for some unique \(r \in K[x]\) such that \(\deg(r) &lt; n\). 
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
Use division with remainder. If \(g + I \in S\) where \(g \in K[x]\), then there exists unique \(q, r \in K[x]\) such that \(g = qf + r\) where \(\deg(r) &lt; n = \deg(f)\). This means we can write \(g - r = qf\). We know that \(qf \in I\) since it’s a multiple of \(f\). Since \(g - r = qf\), then \(g-r\) is also in \(I\). This implies that \(g\) and \(r\) are in the same \(I\)-coset. Therefore \(g+I = r+I\). \(\blacksquare\).
<br />
<br />
\(r + I\) is the canonical form of an element in \(S = K[x] / (f)\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose \(K = \mathbf{Q}\) be the rationals ring, \(S = \mathbf{Q}[x]/(f)\) be the quotient ring and let \(f = x^2 - 2\) so \(I = (f)\) is the ideal generated by this polynomial \(f=x^2-2\). 
<br />
<br />
Using the previous result, we know that Every element of \(S\) can be written as \(r + I\) for a unique \(r \in Q[x]\) such that \(\deg(r) &lt; \deg(f)=2\). Therefore, we can write \(r + I\) as</p>
<div>
$$
\begin{align*}
(a + bx) + I = a + b\bar{x} \quad a,b \in \mathbf{Q}
\end{align*}
$$
</div>
<p>This notation is a shorthand for: given \(a \in \mathbf{Q}\), then write \(a\) or \(\bar{a}\) instead of writing \(a + I \in S\)  where \(a+I\) is the coset of constant polynomials in \(S\). Also write \(\bar{x}\) for the element \(x + I\). So, then</p>
<div>
$$
\begin{align*}
(a + b\bar{x}) + (a' + b'\bar{x}) &amp;= (a+a') + (b+b')\bar{x} \\
(a + b\bar{x}) (a' + b'\bar{x}) &amp;= aa' + ab'\bar{x} + ba'\bar{x} + bb'\bar{x}^2
\end{align*}
$$
</div>
<p>but now we have \(\bar{x}^2\). That’s because we still need to divide by \(f\). If we divide \(\bar{x}^2\) by \(x^2 - 2\), the remainder is 2. So \(\bar{x}^2\) is really 2 in the ring \(R/(f)\). Then we can simplify this to</p>
<div>
$$
\begin{align*}
(a + b\bar{x}) (a' + b'\bar{x}) &amp;= aa' + ab'\bar{x} + ba'\bar{x} + 2bb' \\
                                &amp;= (aa' + 2bb') +(ab'+ba')\bar{x}
\end{align*}
$$
</div>
<p>Remark: This quotient ring \(S\) is isomorphic to \(\mathbf{Q}(\sqrt{2})\) which is a subring of \(\mathbf{R}\). \(\mathbf{Q}(\sqrt{2})\) is the ring</p>
<div>
$$
\begin{align*}
\{a+b\sqrt{2} \ | \ a,b \in \mathbf{Q}\}
\end{align*}
$$
</div>
<p>The isomorphism between these rings is</p>
<div>
$$
\begin{align*}
a+b\bar{x} \rightarrow a + b\sqrt{2}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Homomorphism Theorem</b></h4>
<p>We start by a recipe to form a new homomorphism from a quotient ring.
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Homomorphism Theorem
</div>
<div class="yellowbodydiv">
Let \(\varphi: R \rightarrow S\) be a ring homomorphism and \(I \subseteq R\) be an ideal. If \(I \subseteq \ker(\varphi)\) (ie. \(\varphi(I) = \{0\}\)), then there exists a ring homomorphism \(\bar{\varphi}: R/I \rightarrow S\). Such that \(\bar{\varphi}(a + I) = \varphi(a)\)
</div>
<!------------------------------------------------------------------------->
<p><br />
To prove this we need to show that the new homomorphism \(\bar{\phi}\) is well defined and then we need to show that it’s a ring homomorphism. And then we have the isomorphism theorem as follows
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Isomorphism Theorem
</div>
<div class="yellowbodydiv">
Let \(\varphi: R \rightarrow S\) be a <b>surjective</b> ring homomorphism and \(I \subseteq R\) be an ideal. If \(I = \ker(\varphi)\), then we have an isomorphism of rings 
$$
\begin{align*}
\bar{\varphi}: R/I &amp;\rightarrow S \\ 
\bar{\varphi}(a + I) &amp;= \varphi(a)
\end{align*}
$$
</div>
<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let \(R = \mathbf{Q}[x]\) and \(S = \mathbf{R}\). Since we want to build a homomorphism from a polynomial ring, then we should always think of the substitution principle. Recall that the substitution principle says that given that we have a homomorphism (in this case let this homomorphism be the identity function), then we can create the following evaluation homomorphism</p>
<div>
$$
\begin{align*}
\varphi: \mathbf{Q}[x] &amp;\rightarrow \mathbf{R} \\ 
\varphi(a) &amp;= a \quad \text{if } a \in \mathbf{Q} \subseteq \mathbf{Q}[x] \\
\varphi(x) &amp;= \sqrt{2} \\
\end{align*}
$$
</div>
<p>So in this case, it’s just an evaluation. So, given some polynomial \(f\), then</p>
<div>
$$
\begin{align*}
\varphi: \mathbf{Q}[x] &amp;\rightarrow \mathbf{R} \\ 
\phi(f) &amp;= f(\sqrt{2})
\end{align*}
$$
</div>
<p>What is the kernel of this homomorphism?</p>
<div>
$$
\begin{align*}
I = \ker(\varphi) &amp;= \{ f \in \mathbf{Q}[x] \ | \ \varphi(f) = 0 \} \\
 &amp;= \{ f \in \mathbf{Q}[x] \ | \ f(\sqrt{2}) = 0 \}
\end{align*}
$$
</div>
<p>So it’s any polynomial that has \(\sqrt{2}\) as a root. Since \(\mathbf{Q}\) is a field, then we know that the kernel is principle. So it’s generated by a single monic polynomial of minimal degree. First of all, this kernel isn’t trivial. For example \(x^2 - 2\) has a \(\sqrt{2}\) as a root of degree 2. If we know that we don’t have constant polynomials or polynomials of degree 1, then we know \(x^2 - 2\) is a generator for the group. Constant non-zero polynomials don’t have roots. Linear polynomials can’t have \(\sqrt{2}\) as a root. The roots will be rationals since the coefficients are rational.
<br />
<br />
So now we have a homomorphism \(\varphi\) from \(\mathbf{Q}[x]\) to \(\mathbf{R}\). Therefore, we can apply the homomorphism theorem to get a new homomorphism (quotient homomorphism) from \(\mathbf{Q}[x]\) to \(\mathbf{Q}[x]/(x^2-2)\).</p>
<div>
$$
\begin{align*}
\pi: \mathbf{Q}[x] &amp;\rightarrow \mathbf{Q}[x]/(x^2-2) \\ 
\phi(f) &amp;= f + (x^2 - 2)
\end{align*}
$$
</div>
<p>The homomorphism takes a polynomial \(f\) in \(\mathbf{Q}[x]\) to its coset.
<br />
<br />
Notice now that we have a homomorphism \(\varphi: \mathbf{Q}[x] \rightarrow \mathbf{R}\) and we have the canonical homomorphism \(\pi: \mathbf{Q}[x] \rightarrow \mathbf{Q}[x]/(x^2-2)\). Before we can apply the isomorphism theorem, we have one issue which is that \(\varphi\) is not surjective. We can fix this by restricting the target to only the image of \(\varphi\) so</p>
<div>
$$
\begin{align*}
\varphi': \mathbf{Q}[x] &amp;\rightarrow \varphi(\mathbf{Q}[x]) \\ 
f(x) &amp;\rightarrow f(\sqrt{2}) \\
\end{align*}
$$
</div>
<p>So now we have a surjective homomorphism where the kernel is \(I = (x^2 - 2)\) and we formed the quotient ring \(\mathbf{Q}[x]/(x^2-2)\). Therefore, we can now apply the isomorphism to conclude that \(\mathbf{Q}[x]/(x^2-2)\) is isomorphic to \(\varphi(\mathbf{Q}[x])\). Moreover, we have an isomorphism defined by</p>
<div>
$$
\begin{align*}
\bar{\varphi}: \mathbf{Q}[x]/(x^2-2) &amp;\rightarrow \varphi(\mathbf{Q}[x]) \\ 
f + (x^2 - 2) &amp;\rightarrow \varphi(f) = f(\sqrt{2})
\end{align*}
$$
</div>
<p>Note that \(\varphi(\mathbf{Q}[x])\) are all the real numbers we can get by plugging in \(\sqrt{2}\) into polynomials with rational coefficients. So</p>
<div>
$$
\begin{align*}
f(x) &amp;= a_0 + a_1x + ... + a_nx^n, \quad a_i \in \mathbf{Q} \\
f(\sqrt{2}) &amp;= a_0 + a_1(\sqrt{2}) + ... + a_n(\sqrt{2})^n, \quad a_i \in \mathbf{Q}
\end{align*}
$$
</div>
<p>But \((\sqrt{2}) = 2, (\sqrt{2})^3 = 2\sqrt{2}\) and in general \((\sqrt{2})^2k = 2^k\) while \((\sqrt{2})^{2k+1} = 2^k\sqrt{2}\). So any \(f(\sqrt{2})\) simplifies to \(a + b\sqrt{2}\). Therefore</p>
<div>
$$
\begin{align*}
\varphi(\mathbf{Q}) = \{ a + b\sqrt{2} \ | \ a,b \in \mathbf{Q} \}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose we have a homomorphism which is just the identity function \(id\) from \(\mathbf{R} \rightarrow \mathbf{R}\). Based on this create the evaluation homomorphism</p>
<div>
$$
\begin{align*}
\varphi: \mathbf{R}[x] &amp;\rightarrow \mathbf{C} \\ 
\varphi(a) &amp;\rightarrow a \\
\varphi(x) &amp;\rightarrow i
\end{align*}
$$
</div>
<p>This homomorphism is in fact surjective. The kernel of this homomorphism is generated by \((x^2+1)\). Therefore,</p>
<div>
$$
\begin{align*}
\mathbf{R}[x]/(x^2+1) \cong \mathbf{C}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>This take, let’s take the rational numbers instead so</p>
<div>
$$
\begin{align*}
\varphi: \mathbf{Q}[x] &amp;\rightarrow \mathbf{R} \\ 
\varphi(a) &amp;\rightarrow a \\
\varphi(x) &amp;\rightarrow \sqrt[3]{2}
\end{align*}
$$
</div>
<p>The kernel of this homomorphism is generated by \((x^3-2)\). The image of this homomorphism is a subset of \(\mathbf{R}\).</p>
<div>
$$
\begin{align*}
\mathbf{Q}[x]/(x^3-2) \cong \varphi(\mathbf{Q}[x]) \subseteq \mathbf{R}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’ll start by defining Quotient Rings Definition: Quotient Ring Let \(R\) be a ring and \(I\) be an ideal in \(R\). Then let the quotient ring \(R/I\) $$ \begin{align*} R/I = \{a + I \ | \ a + R\} \end{align*} $$ be the set of all \(I\)-cosets in \(R\) with operations: Addition: \((a + I) + (b + I) = (a + b) + I\). Multiplication: \((a + I) + (b + I) = (ab) + I\). Note that there are other notations used for \(I + a\). One notation is \([a]\) or \(\bar{a}\). The claim is that these operations are well defined and will make the quotient set with the operations defined a ring. For addition, it’s already part of how we defined quotient groups? For multiplication, we need to show that if \(a + I = a' + I\) and \(b + I = b' + I\). Then $$ \begin{align*} ab + I = a'b' + 1 \end{align*} $$ To see this, write \(a' = a + u\) and \(b' = b + v\) for some \(u, v \in I\). Then $$ \begin{align*} a'b' &amp;= (a + u)(b + v) \\ &amp;= ab + av + ub + uv \end{align*} $$ \(av + ub + uv\) is in \(I\) since \(I\) is an ideal. Therefore, \(a'b' + (av+ub+uv)\) and \(ab + I\) represent the same ideal. Next, we need to show that \(R/I\) is an abelian group with addition. And then we need to show that multiplication is associative and distributes over addition. (exercise) Quotient Homomorphism]]></summary></entry><entry><title type="html">Lecture 35: Principal Ideal</title><link href="http://localhost:4000/jekyll/update/2025/02/27/math417-35-principal-ideal.html" rel="alternate" type="text/html" title="Lecture 35: Principal Ideal" /><published>2025-02-27T00:01:36-08:00</published><updated>2025-02-27T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/27/math417-35-principal-ideal</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/27/math417-35-principal-ideal.html"><![CDATA[<p>Last time we introduced the notion of an ideal in a ring. Stating it again
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Ideal
</div>
<div class="mintbodydiv">
An ideal in \(R\) is a subset \(I \subseteq R\) such that
<ol>
	<li>\(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well.</li>
	<li>If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication.</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
We said that usually we refer to this definition as the “Two sided ideal” since other variants can exists (left and right ideals).
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(\{I_{\alpha}\}\) is a collection of ideals, then \(I = \bigcap_{\alpha} I_{\alpha}\) is an ideal. 
</div>
<p><br />
As a consequence of this, we have the following defintion
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Ideal Generated by \(S\)
</div>
<div class="mintbodydiv">
Given a subset \(S \subseteq R\) ring, define 
$$
\begin{align*}
(S) = \bigcap_{\text{all ideas such that }S \subseteq I} I
\end{align*}
$$
\((S)\) is an Ideal in \(R\) generated by \(S\). It is in fact the smallest Ideal containing the subset \(S\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
It is the smallest, since by definition for any \(I \subseteq R\), \(S\) is contained in \(I\), so \((S)\) is also in \(I\). This is a nice formal definition but it’s hard to use. An explicit description of the Ideal is as follows
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(R\) is a ring with 1 and \(S \subseteq R\) then,
$$
\begin{align*}
(S) = \{0\} \cup \{a_1s_1b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\}
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->
<p><br />
The elements are of the form \(a_is_ib_i\) since by definition, if an element \(s_i\) is in \(I\), then for any \(a_i \in R\), \(a_is_i \in I\) and for any \(b_i \in R\), \(s_ib_i \in R\). So we have \(a_is_ib_i\) to account for all possible products.
<br />
<br />
<b>Proof</b>
<br />
Let \(T = \{a_1s_2b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\}\). Then we need to show</p>
<ol>
<li>Step (1) is showing that \(T\) is an Ideal where \(S \subseteq T\).</li>
<li>Step (2) is if any \(I \subseteq R\) is any idea such that \(S \subseteq I\), then \(T \subseteq I\).</li>
</ol>
<p>(1) and (2) Together imply that \(T = (S)\). <br />
[TODO: Write full proof]
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Principle Ideal</b></h4>
<p>A special case of the above definition is when \(R\) is commutative. If it is, then the definition simplifies to</p>
<div>
$$
\begin{align*}
(S) = \{0\} \cup \{a_1s_2 + ... + a_ks_k \ | \ k \geq 1, s_1,...s_k \in S, a_i \in R\}
\end{align*}
$$
</div>
<p>We don’t need to multiply on both the left and right since \(a_1s_2 = s_2a_1\) so we get to have all products still with a simpler defintion.
<br />
<br />
Another special case is the Principle Ideal.
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Principle Ideal
</div>
<div class="mintbodydiv">
A Principle Ideal \(I\) is an ideal such that \(I = (r)\) for a single \(r = R\) so
$$
\begin{align*}
(r) = \{a_1rb_1 + ... + a_krb_k \ | \  a_i,b_i \in R\}
\end{align*}
$$
</div>
<p><br />
Note that if \(R\) is a commutative ring with identity then</p>
<div>
$$
\begin{align*}
(r) &amp;= \{a_1r + ... + a_kr \ | \  a_1,...a_k \in R\} \\
    &amp;= \{(a_1 + ... + a_k)r \ | \  a_1,...a_k \in R\} \\
	&amp;= \{ ar \ | \ a \in R \} \quad \text{(because $a_1+...+a_k \in R$)}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>Example 1: If \(K\) is a field. The only ideals are \(\{0\}\) and \(K\). \(\{0\}\) is generated by \((0)\) and \(K\) is generated by \((1)\). The idea generated by 1 is always the whole ring.
<br />
<br />
Example 2: Take \(R = \mathbf{Z}\). All ideals are of the form \((d) = \mathbf{Z}d\) for some \(d \geq 0\). In fact, all ideals are principle. Why? 
<br />
<br />
<b>Proof</b>
<br />
We know that for any ideal, it is a subgroup (with addition) of \(R=\mathbf{Z}\) so \(I \leq \mathbf{Z}\). If \(I = \{0\}\), then we’re done since \(\{0\}\) is a principle ideal generated by \((0)\). Suppose it is not so \(I \neq \{0\}\), then there exists a non-zero element in \(I\). Choose \(d \in I \cap \mathbf{Z}d\) to be the smallest positive element. 
<br />
<br />
Claim: \(I = \mathbf{Z}d\). We can show this by:
<br />
<br />
\(\mathbf{Z}d \subseteq I\) (easy): Since \(d \in I\), then any multiple of \(d\) is in \(I\) so \(\mathbf{Z}d \subseteq I\).
<br />
\(I \subseteq \mathbf{Z}d\) (hard): Consider any \(a \in I\). Use division with remainder to see that \(a = qd + r\). \(r \in \mathbf{Z}\) where \(0 \leq r &lt; d\). So \(r = a - qd\). \(r\) is in \(I\) because \(a \in I\) and \(qd \in I\). But \(d\) is the smallest element in \(I\). So \(r\) must be zero. Therefore, \(a = qd\) so \(a \in \mathbf{Z}d\). So \(I \in \mathbf{Z}d\). \(\blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>More Propositions</b></h4>
<p>We have more propositions about the principle ideal. One is the following
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Suppose that \(K\) is is a field and \(R = K[x]\). Every ideal in \(R\) is principle. In fact, if \(I \subseteq R\), then there exists a unique \(f\) such that \((f) = I\) and either \(f = 0\) or \(f\) is monic.
</div>
<!----------------------------------------------------------------------------->
<p><br />
A monic polynomial is \(f = a_nx^n + a_{n-1}x^{n-1} + ... + a_0\) where \(a_n = 1\).
<br />
<br />
<b>Proof</b>
<br />
If \(I \subseteq K[x]\) and \(I\) is an ideal. Then if \(I = \{0\}\), then \(I\) is generated by \(0\) so \(I = (0)\). Now, suppose that \(I \neq \{0\}\). Then, pick an element \(f \in I/\{0\}\) of minimal degree. We can choose</p>
<div>
$$
\begin{align*}
f = a_nx^n +... +a_0
\end{align*}
$$
</div>
<p>to be monic. If it’s not monic, then use \(f' = a_n^{-1}f\). This is fine because \(a_n\) is a unit in \(K\) and has an inverse. Both \(f\) and \(f'\) have the same degree. 
<br />
<br />
Note: This polynomial of minimal degree is unique. Why? Suppose \(f, g \in I/\{0\}\) and \(f\) and \(g\) are monic of the same minimal degree. Then their difference is in the ideal so \(f-g \in I\). (side note: reminder for any two polynomials in \(I\), their difference is in \(I\) since \(I\) is closed under addition/multiplication). In fact, since \(f\) and \(g\) are monic, then their leading terms are 1 so when we take the difference, this new polynomial \(f-g\) must have degree lower than \(f\) and \(g\). But this is a contradiction since we said that \(f\) and \(g\) are both of minimal degree. Thus, \(f - g = 0\) and \(f = g\). So the monic polynomial of minimal degree in \(I\) is unique. (Another side note: This is not true inside all of \(K[x]\).)
<br />
<br />
So now the claim is that \(I = (f)\) so \(I\) is generated by this minimal monic polynomial. <br />
\((f) \subseteq I\) (easy): This is immediate since \(f\) is in \(I\) so all multiplies of \(f\) are in \(I\).<br /><br />
\(I \subseteq (f)\) (hard): We know \((f) = \{fg \ | \ g \in K[x]\}\). So suppose that \(p \in I\). Then use division with remainder \((p \div f)\) to get \(p = qf + r\) for some polynomials \(q, r \in K[x]\) with \(\deg(r) &lt; \deg(f)\). So now \(r = p - qf\). We know \(p \in I\) and \(qf \in I\) since \(f \in I\). So \(r\) must be in \(I\). But that’s impossible since we said that \(f\) has the smallest degree in \(I\) so \(r = 0\). Therefore, \(p = qf\) is in \((f)\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose that \(K\) is a field with \(c \in K\). Consider the evaluation homomorphism of rings where when \(r \in K \subseteq K[x]\), then \(ev_c(r) = r\)</p>
<div>
$$
\begin{align*}
ev_c: K[x] &amp;\rightarrow K \\
      ev_c(f) &amp;= f(c)
\end{align*}
$$
</div>
<p>The kernel is an ideal. \(\ker(ev_c) = I\). By definition</p>
<div>
$$
\begin{align*}
\ker(ev_c) = \{f(x) \in K[x] \ | \ f(c) = 0\}.
\end{align*}
$$
</div>
<p>So it’s the set of polynomials that vanish at \(c\) or have \(c\) as a root. But since \(K\) is a field, then \(I\) is a principle ideal so it’s generated by some unique monic polynomial. One monic polynomial that satisfies this is \(x - c\). So \(\ker(ev_c) = (x-c)\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Consequences</b></h4>
<!----------------------------------------------------------------------------->
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Suppose that \(K\) is is a field, then every non-zero polynomial \(f \in K[x]\) has only finitely many roots in \(K\) in \(R\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
If \(c \in K\) and \(f(c) = 0\), then \(f\) is generated by \((x - c)\). The ideal generated by \((x-c)\) is the set of multiples of this generator so</p>
<div>
$$
\begin{align*}
I = \{ (x-c)g \ | \ g \in K[x]\}.
\end{align*}
$$
</div>
<p>So \(f\) must be of the from \((x-c)g\) for some polynomial \(g \in K[x]\) and if \(c\) is a root, then we can factor off the linear term \((x - c)\). Inductively, we can iterate this process on \(g\) and in each iteration notice that the degree of the polynomial goes down by one until we reach the form</p>
<div>
$$
\begin{align*}
f = (x - c_1) (x - c_2)...(x - c_k)g', \quad c_1,...,c_k \in K
\end{align*}
$$
</div>
<p>where \(g' \in K[x]\) is a polynomial with no roots in \(K\). Now, we can see that we found the roots \(c_1,c_2,...,c_k \in K\) but \(g\) has no roots in \(K\) (otherwise, we would’ve kept factoring). The degree of \(f\) is</p>
<div>
$$
\begin{align*}
\deg(f) &amp;= \deg((x - c_1) (x - c_2)...(x - c_k)g') \\
n &amp;= k + \deg(g')
\end{align*}
$$
</div>
<p>So \(k\) at most \(n\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(K = \mathbf{R} \subseteq \mathbf{C}\) be a subring. Define the following function using the substitution principle</p>
<div>
$$
\begin{align*}
ev_i : \mathbf{R}[x] &amp;\rightarrow \mathbf{C} \\
               ev_i(r) &amp;= r \\
			   rv_i(x) &amp;= i.
\end{align*}
$$
</div>
<p>In other words, \(ev_i(f) = f(i) \in \mathbf{C}\). So it’s like plugging in \(i\) in the function. What is the kernel? We know the kernel is an ideal that is generated by a single monic polynomial. In the previous example, we were evaluating at \(c\) where \(c \in K\). But now, \(i\) is not in the field \(K\). The question is now: Do we know a monic polynomial that has \(i\) as a root? It is \(x^2 + 1\) so \(I = \ker(ev_i) = (x^2 + 1)\).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we introduced the notion of an ideal in a ring. Stating it again Definition: Ideal An ideal in \(R\) is a subset \(I \subseteq R\) such that \(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well. If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication. We said that usually we refer to this definition as the “Two sided ideal” since other variants can exists (left and right ideals). Proposition If \(\{I_{\alpha}\}\) is a collection of ideals, then \(I = \bigcap_{\alpha} I_{\alpha}\) is an ideal. As a consequence of this, we have the following defintion Definition: Ideal Generated by \(S\) Given a subset \(S \subseteq R\) ring, define $$ \begin{align*} (S) = \bigcap_{\text{all ideas such that }S \subseteq I} I \end{align*} $$ \((S)\) is an Ideal in \(R\) generated by \(S\). It is in fact the smallest Ideal containing the subset \(S\). It is the smallest, since by definition for any \(I \subseteq R\), \(S\) is contained in \(I\), so \((S)\) is also in \(I\). This is a nice formal definition but it’s hard to use. An explicit description of the Ideal is as follows Proposition If \(R\) is a ring with 1 and \(S \subseteq R\) then, $$ \begin{align*} (S) = \{0\} \cup \{a_1s_1b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\} \end{align*} $$ The elements are of the form \(a_is_ib_i\) since by definition, if an element \(s_i\) is in \(I\), then for any \(a_i \in R\), \(a_is_i \in I\) and for any \(b_i \in R\), \(s_ib_i \in R\). So we have \(a_is_ib_i\) to account for all possible products. Proof Let \(T = \{a_1s_2b_1 + ... + a_ks_kb_k \ | \ k \geq 1, s_1,...s_k \in S, a_i,b_i \in R\}\). Then we need to show Step (1) is showing that \(T\) is an Ideal where \(S \subseteq T\). Step (2) is if any \(I \subseteq R\) is any idea such that \(S \subseteq I\), then \(T \subseteq I\). (1) and (2) Together imply that \(T = (S)\). [TODO: Write full proof] Principle Ideal A special case of the above definition is when \(R\) is commutative. If it is, then the definition simplifies to $$ \begin{align*} (S) = \{0\} \cup \{a_1s_2 + ... + a_ks_k \ | \ k \geq 1, s_1,...s_k \in S, a_i \in R\} \end{align*} $$ We don’t need to multiply on both the left and right since \(a_1s_2 = s_2a_1\) so we get to have all products still with a simpler defintion. Another special case is the Principle Ideal. Definition: Principle Ideal A Principle Ideal \(I\) is an ideal such that \(I = (r)\) for a single \(r = R\) so $$ \begin{align*} (r) = \{a_1rb_1 + ... + a_krb_k \ | \ a_i,b_i \in R\} \end{align*} $$ Note that if \(R\) is a commutative ring with identity then $$ \begin{align*} (r) &amp;= \{a_1r + ... + a_kr \ | \ a_1,...a_k \in R\} \\ &amp;= \{(a_1 + ... + a_k)r \ | \ a_1,...a_k \in R\} \\ &amp;= \{ ar \ | \ a \in R \} \quad \text{(because $a_1+...+a_k \in R$)} \end{align*} $$ Examples Example 1: If \(K\) is a field. The only ideals are \(\{0\}\) and \(K\). \(\{0\}\) is generated by \((0)\) and \(K\) is generated by \((1)\). The idea generated by 1 is always the whole ring. Example 2: Take \(R = \mathbf{Z}\). All ideals are of the form \((d) = \mathbf{Z}d\) for some \(d \geq 0\). In fact, all ideals are principle. Why? Proof We know that for any ideal, it is a subgroup (with addition) of \(R=\mathbf{Z}\) so \(I \leq \mathbf{Z}\). If \(I = \{0\}\), then we’re done since \(\{0\}\) is a principle ideal generated by \((0)\). Suppose it is not so \(I \neq \{0\}\), then there exists a non-zero element in \(I\). Choose \(d \in I \cap \mathbf{Z}d\) to be the smallest positive element. Claim: \(I = \mathbf{Z}d\). We can show this by: \(\mathbf{Z}d \subseteq I\) (easy): Since \(d \in I\), then any multiple of \(d\) is in \(I\) so \(\mathbf{Z}d \subseteq I\). \(I \subseteq \mathbf{Z}d\) (hard): Consider any \(a \in I\). Use division with remainder to see that \(a = qd + r\). \(r \in \mathbf{Z}\) where \(0 \leq r &lt; d\). So \(r = a - qd\). \(r\) is in \(I\) because \(a \in I\) and \(qd \in I\). But \(d\) is the smallest element in \(I\). So \(r\) must be zero. Therefore, \(a = qd\) so \(a \in \mathbf{Z}d\). So \(I \in \mathbf{Z}d\). \(\blacksquare\). More Propositions We have more propositions about the principle ideal. One is the following Proposition Suppose that \(K\) is is a field and \(R = K[x]\). Every ideal in \(R\) is principle. In fact, if \(I \subseteq R\), then there exists a unique \(f\) such that \((f) = I\) and either \(f = 0\) or \(f\) is monic. A monic polynomial is \(f = a_nx^n + a_{n-1}x^{n-1} + ... + a_0\) where \(a_n = 1\). Proof If \(I \subseteq K[x]\) and \(I\) is an ideal. Then if \(I = \{0\}\), then \(I\) is generated by \(0\) so \(I = (0)\). Now, suppose that \(I \neq \{0\}\). Then, pick an element \(f \in I/\{0\}\) of minimal degree. We can choose $$ \begin{align*} f = a_nx^n +... +a_0 \end{align*} $$ to be monic. If it’s not monic, then use \(f' = a_n^{-1}f\). This is fine because \(a_n\) is a unit in \(K\) and has an inverse. Both \(f\) and \(f'\) have the same degree. Note: This polynomial of minimal degree is unique. Why? Suppose \(f, g \in I/\{0\}\) and \(f\) and \(g\) are monic of the same minimal degree. Then their difference is in the ideal so \(f-g \in I\). (side note: reminder for any two polynomials in \(I\), their difference is in \(I\) since \(I\) is closed under addition/multiplication). In fact, since \(f\) and \(g\) are monic, then their leading terms are 1 so when we take the difference, this new polynomial \(f-g\) must have degree lower than \(f\) and \(g\). But this is a contradiction since we said that \(f\) and \(g\) are both of minimal degree. Thus, \(f - g = 0\) and \(f = g\). So the monic polynomial of minimal degree in \(I\) is unique. (Another side note: This is not true inside all of \(K[x]\).) So now the claim is that \(I = (f)\) so \(I\) is generated by this minimal monic polynomial. \((f) \subseteq I\) (easy): This is immediate since \(f\) is in \(I\) so all multiplies of \(f\) are in \(I\). \(I \subseteq (f)\) (hard): We know \((f) = \{fg \ | \ g \in K[x]\}\). So suppose that \(p \in I\). Then use division with remainder \((p \div f)\) to get \(p = qf + r\) for some polynomials \(q, r \in K[x]\) with \(\deg(r) &lt; \deg(f)\). So now \(r = p - qf\). We know \(p \in I\) and \(qf \in I\) since \(f \in I\). So \(r\) must be in \(I\). But that’s impossible since we said that \(f\) has the smallest degree in \(I\) so \(r = 0\). Therefore, \(p = qf\) is in \((f)\) as we wanted to show. \(\ \blacksquare\) Example Suppose that \(K\) is a field with \(c \in K\). Consider the evaluation homomorphism of rings where when \(r \in K \subseteq K[x]\), then \(ev_c(r) = r\) $$ \begin{align*} ev_c: K[x] &amp;\rightarrow K \\ ev_c(f) &amp;= f(c) \end{align*} $$ The kernel is an ideal. \(\ker(ev_c) = I\). By definition $$ \begin{align*} \ker(ev_c) = \{f(x) \in K[x] \ | \ f(c) = 0\}. \end{align*} $$ So it’s the set of polynomials that vanish at \(c\) or have \(c\) as a root. But since \(K\) is a field, then \(I\) is a principle ideal so it’s generated by some unique monic polynomial. One monic polynomial that satisfies this is \(x - c\). So \(\ker(ev_c) = (x-c)\). Consequences Proposition Suppose that \(K\) is is a field, then every non-zero polynomial \(f \in K[x]\) has only finitely many roots in \(K\) in \(R\). Proof If \(c \in K\) and \(f(c) = 0\), then \(f\) is generated by \((x - c)\). The ideal generated by \((x-c)\) is the set of multiples of this generator so $$ \begin{align*} I = \{ (x-c)g \ | \ g \in K[x]\}. \end{align*} $$ So \(f\) must be of the from \((x-c)g\) for some polynomial \(g \in K[x]\) and if \(c\) is a root, then we can factor off the linear term \((x - c)\). Inductively, we can iterate this process on \(g\) and in each iteration notice that the degree of the polynomial goes down by one until we reach the form $$ \begin{align*} f = (x - c_1) (x - c_2)...(x - c_k)g', \quad c_1,...,c_k \in K \end{align*} $$ where \(g' \in K[x]\) is a polynomial with no roots in \(K\). Now, we can see that we found the roots \(c_1,c_2,...,c_k \in K\) but \(g\) has no roots in \(K\) (otherwise, we would’ve kept factoring). The degree of \(f\) is $$ \begin{align*} \deg(f) &amp;= \deg((x - c_1) (x - c_2)...(x - c_k)g') \\ n &amp;= k + \deg(g') \end{align*} $$ So \(k\) at most \(n\). Example Let \(K = \mathbf{R} \subseteq \mathbf{C}\) be a subring. Define the following function using the substitution principle $$ \begin{align*} ev_i : \mathbf{R}[x] &amp;\rightarrow \mathbf{C} \\ ev_i(r) &amp;= r \\ rv_i(x) &amp;= i. \end{align*} $$ In other words, \(ev_i(f) = f(i) \in \mathbf{C}\). So it’s like plugging in \(i\) in the function. What is the kernel? We know the kernel is an ideal that is generated by a single monic polynomial. In the previous example, we were evaluating at \(c\) where \(c \in K\). But now, \(i\) is not in the field \(K\). The question is now: Do we know a monic polynomial that has \(i\) as a root? It is \(x^2 + 1\) so \(I = \ker(ev_i) = (x^2 + 1)\). References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 34: Homomorphisms of Rings and the Substitution Principle</title><link href="http://localhost:4000/jekyll/update/2025/02/26/math417-34-homomorphisms-rings.html" rel="alternate" type="text/html" title="Lecture 34: Homomorphisms of Rings and the Substitution Principle" /><published>2025-02-26T00:01:36-08:00</published><updated>2025-02-26T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/26/math417-34-homomorphisms-rings</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/26/math417-34-homomorphisms-rings.html"><![CDATA[<p>Last time we introduced polynomial rings. Given a commutative ring with 1, then \(R[x]\) is a commutative polynomial ring with 1 and coefficients in \(R\). Today, we will introduce a few more definitions
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Homomorphisms of Rings
</div>
<div class="mintbodydiv">
Let \(R\) and \(S\) be rings. A homomorphism \(\varphi: R \rightarrow S\) is a function such that
<ol>
	<li>\(\varphi: (R_1, +) \rightarrow (S, +)\) is a homomorphism of groups (ie: \(\varphi(a + b) = \varphi(a) + \varphi(b)\)</li>
	<li>\(\varphi(a,b) = \varphi(a)\varphi(b)\) for all \(a,b \in R\)</li>
</ol>
</div>
<!--------------------------------------------------------------------------->
<p><br />
Remark: If \(R\) and \(S\) have a multiplication identity, \(\varphi\) can fail to take 1 to 1. So \(\varphi(1)\) could be something other than 1. Unlike for zero, where zero needs to go zero. This is because rings are required to have an additive inverses and because we have additive inverses, then zero will be mapped to zero. But we don’t necessarily have multiplicative inverses for all elements (that’s a field). So 1 won’t necessarily get mapped to 1. Based on this define
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Homomorphisms of Rings
</div>
<div class="mintbodydiv">
Let \(R\) and \(S\) be rings. A unital homomorphism is a ring homomorphism such that also \(\varphi(1_R) = 1_S\)
</div>
<p><br /></p>
<h4><b>Example 1</b></h4>
<p>Let \(\pi: \mathbf{Z} \rightarrow \mathbf{Z}_n\) where \(\pi(a) = [a]_n\). Then \(\pi\) is a unital homomorphism of rings.
<br />
<br /></p>
<h4><b>Example 2</b></h4>
<p>If \(R\) is any ring with 1. Define \(\varphi \mathbf{Z} \rightarrow R\) by \(\varphi(n) = n1_R\). \(\varphi\) is a unital ring homomorphism. For example</p>
<div>
$$
\begin{align*}
\varphi(2) &amp;= 1_R + 1_R \\
\varphi(3) &amp;= 1_R + 1_R + 1_R \\
\varphi(2)\varphi(3) &amp;= (1_R + 1_R)(1_R + 1_R + 1_R) = \varphi(6).
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Isomorphism of Rings</b></h4>
<p>More simple definitions:
<br /></p>
<div class="mintheaderdiv">
Definition: Isomorphism of Rings
</div>
<div class="mintbodydiv">
Let \(R\) and \(S\) be rings. An Isomorphism of rings is a homomorphism of rings which is also a bijection.
</div>
<p><br />
Since an isomorphism is a bijection, this means that we have an inverse function \(\varphi^{-1}: S \rightarrow R\) that is also an isomorphism of rings
<!------------------------------------------------------------------------->
<br /></p>
<div class="mintheaderdiv">
Definition: Automorphism of Rings
</div>
<div class="mintbodydiv">
Let \(R\) be a ring. An Automorphism \(\varphi: R \rightarrow R\) is an isomorphism of the ring to itself.
</div>
<p><br />
<!------------------------------------------------------------------------->
For example, take the set of matrices such that</p>
<div>
$$
\begin{align*}
S = \big\{ \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix} \big\} 
\subseteq \text{Mat}_{2\times 2}(\mathbf{R})
\end{align*}
$$
</div>
<p>with \(a, b \in \mathbf{R}\). \(S\) is a subring. We have an isomorphism of rings between \(S\) and the complex numbers as follows</p>
<div>
$$
\begin{align*}
\varphi: \mathbf{C} &amp;\rightarrow S \\
         a + bi &amp;\rightarrow \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix}
\end{align*}
$$
</div>
<p>Another example is \(\varphi: \mathbf{C} \rightarrow \mathbf{C}\) where \(\varphi(z) = \bar{z}\). So \(\varphi(a+bi) = a - bi\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Substitution Principle</b></h4>
<p>The substitution principle is a method for constructing any ring homomorphism where the domain is a polynomial ring
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition: Substitution Principle
</div>
<div class="peachbodydiv">
Let \(R\) and \(S\) be commutative rings with identity. Given that
<ol> 
	<li>\(\varphi: R \rightarrow S\) is a unital homomorphism</li>
	<li>\(c \in S\)</li>
</ol>
Then there exists a unique ring homomorphism
$$
\begin{align*}
\varphi_c: R[x] \rightarrow S
\end{align*}
$$
such that
<ol> 
	<li>\(\varphi_c(r) = \varphi(r)\)</li>
	<li>\(\varphi_c(x) = c\)</li>
</ol>
Furthermore, if \(f = \sum_{k=0}^n a_kx^k \in R[x], a_k \in R\). Then
$$
\begin{align*}
\varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k
\end{align*}
$$
</div>
<p><br />
<!---------------------------------------------------------------------->
This means given some unital homomorphism between the two rings. Then</p>
<ul>
<li>If we know how the homomorphism act on constant polynomials (so these are the elements in \(R\) which will be the constant polynomials in \(R[x]\)</li> 
<li>And if we know how it acts on \(x\) (we're sending it to \(c\))</li>
</ul>
<p>Then we will get this new homomorphism \(\varphi_c\) and we will know how it acts on all the other polynomials using \(\varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k\).
<br />
<br />
But now you might ask that it’s kind of like evaluating the polynomial at \(c\)?? This is true for a special case when \(\phi\) is the identity function. So when \(R = S\) and \(\phi:R \rightarrow R\) is just the identity. Then, given any \(c\), we will see that</p>
<div>
$$
\begin{align*}
\phi_c(f) = ev_c(f) = \sum_{k=0}^n a_k c^k \in R \quad \text{ev for evaluation}
\end{align*}
$$
</div>
<p>which is basically like plugging in \(c\) for \(x\) so \(f(c)\). However, this \(ev_c: R[x] \rightarrow R\) is actually a ring homomorphism. This ring homomorphism has the following properties.</p>
<ol>
	<li>\(ev_c(r) = r\) if \(r \in R \subseteq R(x)\)</li>
	<li>\(ev_(f+g) = ev_c(f) + ev_c(g)\). This is equivalent to us doing \((f+g)(c) = f(c)+f(g)\)</li>
	<li>\(ev_c(fg) = ev_c(f)ev_c(g)\). This is equivalent to us doing \((fg)(c) = f(c)g(c)\)</li>
</ol>
<p>So again, if we have a unital homomorphism and we have those two conditions where we know what it does to \(c\) and we know what it does to constant polynomials. Then that formula must work. It’s the only way. But then we need to check that this formula is actually a ring homomorphism (Excercise)
<br />
<br />
Observation: \(R\) is a commutative ring with 1. Any polynomial \(f \in R[x]\) gives a function</p>
<div>
$$
\begin{align*}
func_c: R &amp;\rightarrow R \\
c &amp;\rightarrow ev_c(f) = "f(c)"
\end{align*}
$$
</div>
<p>Warning: We can have \(f \neq g\) but \(func_f = func_g\)!.
<br />
Example 1: Let \(R = \mathbf{Z}_p\) where \(p\) is prime. Then</p>
<div>
$$
\begin{align*}
f &amp;= x \in \mathbf{Z}_p[x] \implies func_f = id: \mathbf{Z}_p \rightarrow \mathbf{Z}_p \\
g &amp;= x^p \in \mathbf{R}_p[x] \implies func_g(c) = c^p: \mathbf{Z}_p \rightarrow \mathbf{Z}_p 
\end{align*}
$$
</div>
<p>These are two different polynomials but they are the same function because due to Fermat’s little theorem. \(c^p \equiv c (\bmod p)\) so \(c^p = c\) in \(\mathbf{Z}_p\).
<br />
<br />
Example 2: Let \(h = g-f \in \mathbf{Z}_p\) so \(h = x^p - x\) and \(func_h = 0\) for any \(x \in \mathbf{Z}_p\). So any element in \(\mathbf{Z}_p\) is a root of \(h(x)\) and \(h(x)\) has at least \(p\) roots in \(\mathbf{Z}_p\). In otherwords, we have a polynomial where any element we input, we get zero and so we can’t distinguish it as a function from the zero function.</p>
<ul>
<li>So as a polynomial, $$h(x) = x - x^p$$ is not the zero polynomial. Its coefficients are not all zero. </li>
<li>But as a function, it is the zero function. It evalutes to zero for any input in \(\mathbf{Z}_p\)</li>
</ul>
<p>This happens because we’re in \(\mathbf{Z}_p\). If we’re in an inifinite field, then when \(f \neq g\), we will always get \(func_g \neq func_f\). Also in general, polynomials give us functions but they are not exactly functions!
<br />
<br />
Side study note: A polynomial belongs to the ring \(R[x]\). A function is a mapping from \(R\) to \(R\). It assigns to each input \(r\) in \(R\), another value \(f(r)\) in \(R\). The function lives in the set of functions from \(R\) to \(R\). If \(R\) is finite, two different polynomials in \(R[x]\) may define the same function! So \(x^2\) and \(x\), will define the same function from \(\mathbf{Z}_2 \rightarrow \mathbf{Z}_2\). If \(R\) was an infinite field, then each distinct polynmoial will give a distinct function.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Ideals</b></h4>
<p>So far, we’ve seen groups, subgroups and normal subgroups. Normal subgroups were special since they show up as kernels of homomorphism and we can form quotient groups using them. 
<br />
<br />
Similarly, we have rings and subrings. And we also have ideals. Ideals show up as kernels of ring homomorphisms similar to nomral groups. And we can also form quotient rings using them. 
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition: Ideal
</div>
<div class="mintbodydiv">
An ideal in \(R\) is a subset \(I \subseteq R\) such that
<ol>
	<li>\(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well.</li>
	<li>If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication.</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
So it’s not just a subring. It’s more special because of the closure under the product. Warning: This definition is also reffered as a “Two Sided Ideal”. Because we do have another variant where we only require half the second condition (left and right ideals).
<br />
<br />
Example: In any ring \(R\), \(\{0\}\) and \(R\) are both ideas in \(R\). \(\{0\}\) is usually called the trivial ideal. \(R\) is called the unit ideal.
<br />
<br />
Observation: If \(R\) is a ring with 1 and \(I \subseteq R\) is an ideal. Then, if \(1 \in I\), then \(I = R\). Why? The ideal must be closed under multiplication so if the ideal contains 1, then it contains every element in \(R\). 
<br />
<br />
Observation: If \(I\) contains a unit \(a^x\), then \(I = R\). This is because if \(a \in I\), then \(a^{-1}a = 1 \in I\). so \(1 \in I\) and therefore, every element is in \(I\). So \(I = R\). 
<br />
<br />
Example: If \(K\) is a field, then we have exactly two ideals \(K\) and \(\{0\}\). Because fields have only units in them. 
<br />
<br />
Example: If \(K\) is a field and \(n \geq 1\), then \(S = \text{Mat}_{n \times n}(K)\) has two ideals: \(\{0\}\) and \(S\) (not trivial but not hard to prove).
<br />
<br />
Example: If \(R = \mathbf{Z}\), the ideals in \mathbf{Z}\(all have form\){\mathbf{Z}_n = {an \ | \ a \in \mathbf{Z}}$$. 
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
If \(\varphi: R \rightarrow S\) is a ring homomorphism. Its kernel is 
$$
\begin{align*}
\ker(\varphi) = \{r \in R \ | \ \varphi(r) = 0\}
\end{align*}
$$
</div>
<p><br />
Fact: \(\ker(\varphi)\) is an ideal in \(R\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last time we introduced polynomial rings. Given a commutative ring with 1, then \(R[x]\) is a commutative polynomial ring with 1 and coefficients in \(R\). Today, we will introduce a few more definitions Definition: Homomorphisms of Rings Let \(R\) and \(S\) be rings. A homomorphism \(\varphi: R \rightarrow S\) is a function such that \(\varphi: (R_1, +) \rightarrow (S, +)\) is a homomorphism of groups (ie: \(\varphi(a + b) = \varphi(a) + \varphi(b)\) \(\varphi(a,b) = \varphi(a)\varphi(b)\) for all \(a,b \in R\) Remark: If \(R\) and \(S\) have a multiplication identity, \(\varphi\) can fail to take 1 to 1. So \(\varphi(1)\) could be something other than 1. Unlike for zero, where zero needs to go zero. This is because rings are required to have an additive inverses and because we have additive inverses, then zero will be mapped to zero. But we don’t necessarily have multiplicative inverses for all elements (that’s a field). So 1 won’t necessarily get mapped to 1. Based on this define Definition: Homomorphisms of Rings Let \(R\) and \(S\) be rings. A unital homomorphism is a ring homomorphism such that also \(\varphi(1_R) = 1_S\) Example 1 Let \(\pi: \mathbf{Z} \rightarrow \mathbf{Z}_n\) where \(\pi(a) = [a]_n\). Then \(\pi\) is a unital homomorphism of rings. Example 2 If \(R\) is any ring with 1. Define \(\varphi \mathbf{Z} \rightarrow R\) by \(\varphi(n) = n1_R\). \(\varphi\) is a unital ring homomorphism. For example $$ \begin{align*} \varphi(2) &amp;= 1_R + 1_R \\ \varphi(3) &amp;= 1_R + 1_R + 1_R \\ \varphi(2)\varphi(3) &amp;= (1_R + 1_R)(1_R + 1_R + 1_R) = \varphi(6). \end{align*} $$ Isomorphism of Rings More simple definitions: Definition: Isomorphism of Rings Let \(R\) and \(S\) be rings. An Isomorphism of rings is a homomorphism of rings which is also a bijection. Since an isomorphism is a bijection, this means that we have an inverse function \(\varphi^{-1}: S \rightarrow R\) that is also an isomorphism of rings Definition: Automorphism of Rings Let \(R\) be a ring. An Automorphism \(\varphi: R \rightarrow R\) is an isomorphism of the ring to itself. For example, take the set of matrices such that $$ \begin{align*} S = \big\{ \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix} \big\} \subseteq \text{Mat}_{2\times 2}(\mathbf{R}) \end{align*} $$ with \(a, b \in \mathbf{R}\). \(S\) is a subring. We have an isomorphism of rings between \(S\) and the complex numbers as follows $$ \begin{align*} \varphi: \mathbf{C} &amp;\rightarrow S \\ a + bi &amp;\rightarrow \begin{pmatrix} a &amp; -b \\ b &amp; a \end{pmatrix} \end{align*} $$ Another example is \(\varphi: \mathbf{C} \rightarrow \mathbf{C}\) where \(\varphi(z) = \bar{z}\). So \(\varphi(a+bi) = a - bi\). Substitution Principle The substitution principle is a method for constructing any ring homomorphism where the domain is a polynomial ring Proposition: Substitution Principle Let \(R\) and \(S\) be commutative rings with identity. Given that \(\varphi: R \rightarrow S\) is a unital homomorphism \(c \in S\) Then there exists a unique ring homomorphism $$ \begin{align*} \varphi_c: R[x] \rightarrow S \end{align*} $$ such that \(\varphi_c(r) = \varphi(r)\) \(\varphi_c(x) = c\) Furthermore, if \(f = \sum_{k=0}^n a_kx^k \in R[x], a_k \in R\). Then $$ \begin{align*} \varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k \end{align*} $$ This means given some unital homomorphism between the two rings. Then If we know how the homomorphism act on constant polynomials (so these are the elements in \(R\) which will be the constant polynomials in \(R[x]\) And if we know how it acts on \(x\) (we're sending it to \(c\)) Then we will get this new homomorphism \(\varphi_c\) and we will know how it acts on all the other polynomials using \(\varphi_c(f): \sum_{k=0}^n \varphi(a_k) c^k\). But now you might ask that it’s kind of like evaluating the polynomial at \(c\)?? This is true for a special case when \(\phi\) is the identity function. So when \(R = S\) and \(\phi:R \rightarrow R\) is just the identity. Then, given any \(c\), we will see that $$ \begin{align*} \phi_c(f) = ev_c(f) = \sum_{k=0}^n a_k c^k \in R \quad \text{ev for evaluation} \end{align*} $$ which is basically like plugging in \(c\) for \(x\) so \(f(c)\). However, this \(ev_c: R[x] \rightarrow R\) is actually a ring homomorphism. This ring homomorphism has the following properties. \(ev_c(r) = r\) if \(r \in R \subseteq R(x)\) \(ev_(f+g) = ev_c(f) + ev_c(g)\). This is equivalent to us doing \((f+g)(c) = f(c)+f(g)\) \(ev_c(fg) = ev_c(f)ev_c(g)\). This is equivalent to us doing \((fg)(c) = f(c)g(c)\) So again, if we have a unital homomorphism and we have those two conditions where we know what it does to \(c\) and we know what it does to constant polynomials. Then that formula must work. It’s the only way. But then we need to check that this formula is actually a ring homomorphism (Excercise) Observation: \(R\) is a commutative ring with 1. Any polynomial \(f \in R[x]\) gives a function $$ \begin{align*} func_c: R &amp;\rightarrow R \\ c &amp;\rightarrow ev_c(f) = "f(c)" \end{align*} $$ Warning: We can have \(f \neq g\) but \(func_f = func_g\)!. Example 1: Let \(R = \mathbf{Z}_p\) where \(p\) is prime. Then $$ \begin{align*} f &amp;= x \in \mathbf{Z}_p[x] \implies func_f = id: \mathbf{Z}_p \rightarrow \mathbf{Z}_p \\ g &amp;= x^p \in \mathbf{R}_p[x] \implies func_g(c) = c^p: \mathbf{Z}_p \rightarrow \mathbf{Z}_p \end{align*} $$ These are two different polynomials but they are the same function because due to Fermat’s little theorem. \(c^p \equiv c (\bmod p)\) so \(c^p = c\) in \(\mathbf{Z}_p\). Example 2: Let \(h = g-f \in \mathbf{Z}_p\) so \(h = x^p - x\) and \(func_h = 0\) for any \(x \in \mathbf{Z}_p\). So any element in \(\mathbf{Z}_p\) is a root of \(h(x)\) and \(h(x)\) has at least \(p\) roots in \(\mathbf{Z}_p\). In otherwords, we have a polynomial where any element we input, we get zero and so we can’t distinguish it as a function from the zero function. So as a polynomial, $$h(x) = x - x^p$$ is not the zero polynomial. Its coefficients are not all zero. But as a function, it is the zero function. It evalutes to zero for any input in \(\mathbf{Z}_p\) This happens because we’re in \(\mathbf{Z}_p\). If we’re in an inifinite field, then when \(f \neq g\), we will always get \(func_g \neq func_f\). Also in general, polynomials give us functions but they are not exactly functions! Side study note: A polynomial belongs to the ring \(R[x]\). A function is a mapping from \(R\) to \(R\). It assigns to each input \(r\) in \(R\), another value \(f(r)\) in \(R\). The function lives in the set of functions from \(R\) to \(R\). If \(R\) is finite, two different polynomials in \(R[x]\) may define the same function! So \(x^2\) and \(x\), will define the same function from \(\mathbf{Z}_2 \rightarrow \mathbf{Z}_2\). If \(R\) was an infinite field, then each distinct polynmoial will give a distinct function. Ideals So far, we’ve seen groups, subgroups and normal subgroups. Normal subgroups were special since they show up as kernels of homomorphism and we can form quotient groups using them. Similarly, we have rings and subrings. And we also have ideals. Ideals show up as kernels of ring homomorphisms similar to nomral groups. And we can also form quotient rings using them. Definition: Ideal An ideal in \(R\) is a subset \(I \subseteq R\) such that \(I\) is a subgroup of the abelian group with addition \((R, +)\). So the ideal must have \(0 \in I\), closed under addition, additive inverses are in \(I\) as well. If \(a \in I, r \in R\), then \(ra, ar \in I\) so it is also closed under multiplication. So it’s not just a subring. It’s more special because of the closure under the product. Warning: This definition is also reffered as a “Two Sided Ideal”. Because we do have another variant where we only require half the second condition (left and right ideals). Example: In any ring \(R\), \(\{0\}\) and \(R\) are both ideas in \(R\). \(\{0\}\) is usually called the trivial ideal. \(R\) is called the unit ideal. Observation: If \(R\) is a ring with 1 and \(I \subseteq R\) is an ideal. Then, if \(1 \in I\), then \(I = R\). Why? The ideal must be closed under multiplication so if the ideal contains 1, then it contains every element in \(R\). Observation: If \(I\) contains a unit \(a^x\), then \(I = R\). This is because if \(a \in I\), then \(a^{-1}a = 1 \in I\). so \(1 \in I\) and therefore, every element is in \(I\). So \(I = R\). Example: If \(K\) is a field, then we have exactly two ideals \(K\) and \(\{0\}\). Because fields have only units in them. Example: If \(K\) is a field and \(n \geq 1\), then \(S = \text{Mat}_{n \times n}(K)\) has two ideals: \(\{0\}\) and \(S\) (not trivial but not hard to prove). Example: If \(R = \mathbf{Z}\), the ideals in \mathbf{Z}\(all have form\){\mathbf{Z}_n = {an \ | \ a \in \mathbf{Z}}$$. Definition If \(\varphi: R \rightarrow S\) is a ring homomorphism. Its kernel is $$ \begin{align*} \ker(\varphi) = \{r \in R \ | \ \varphi(r) = 0\} \end{align*} $$ Fact: \(\ker(\varphi)\) is an ideal in \(R\). References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 33: Polynomial Rings</title><link href="http://localhost:4000/jekyll/update/2025/02/25/math417-33-polynomial-rings.html" rel="alternate" type="text/html" title="Lecture 33: Polynomial Rings" /><published>2025-02-25T00:01:36-08:00</published><updated>2025-02-25T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/25/math417-33-polynomial-rings</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/25/math417-33-polynomial-rings.html"><![CDATA[<p>We’ve introduced rings last lecture and we said that rings can be commutative, contain a multiplicative identity or can also be a field. We saw the subset that includes elements with multiplicative inverses that’s also a group. This is the Units group or \(R^{\times}\) which contains an element in \(R\) such that it has a multiplication inverse. We also saw an example of a ring which is</p>
<div>
$$
\begin{align*}
R[i] = \{\text{Formal expressions } a+bi, a, b \in R \}, \quad i^2 = -1
\end{align*}
$$
</div>

<!----------------------------------------------------------------------------->
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Given a commutative ring with 1. Define \(R[x]\) to be the set of "Formal Expression"
$$
\begin{align*}
f = \sum_{k = 0}^n a_kx^k = a_0 + a_1x + ... + a_nx^n \quad a_0,...a_n \in R, \quad x \text{ new symbol}
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------->
<p><br />
Warning: We want to think of \(3 + 4x + 17x^2 + 0x^3 = 3 + 4x + 17x^2\). So if terms has a zero, then we can take it out. 
<br />
We really identify an \(f \in R[x]\) with an infinite sequence \((a_k)_{k \geq 0} = (a_0,a_1,a_2,...)\) where each \(a_i \in R\) and because this polynomial is finite and we want an infinite sequence, we’ll say that there exists an \(n\) such that \(a_i = 0\) for all \(i &gt; n\). So after some point \(n\), all the terms will be zero after.
<br />
<br />
The claim is that \(R[x]\) is a commutative ring with 1. So we need to define multiplication and addition. Therefore, let \(f = \sum_i a_ix^i, g = \sum_j b_jx^j\) where \(a_i,b_j \in R\). Then</p>
<div>
$$
\begin{align*}
f + g := \sum_k (a_k + b_k)x^k \\
fg := \sum_k (\sum_{i = 0}^k a_ib_{k-i})x^k
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------->
<p>The identity element is an example of a constant polynomial. A constant polynomial is \(f \in R[i]\) where \(f = \sum_ka_k x^k\) such that \(a_k = 0\) for all \(k \geq 1\) So \(f = a_0\).
<br />
<br />
Let \(C \subseteq R[i]\) be the subset of constant. Then \(C\) is a subring. Define a bijection from</p>
<div>
$$
\begin{align*}
\lambda: R &amp;\rightarrow C \\
         a &amp;\rightarrow \text{constant polynomial $a$}
\end{align*}
$$
</div>
<p>This bijection is in fact an isomorphism of rings between \(R\) and \(C\). 
<br />
<br />
Convention: We identify an element \(a\) in the original ring \(R\) with the corresponding constant polynomial in \(R[x]\). So we can think of \(R\) as a subring of \(R[x]\). 
<br />
<br />
Remark: In a similar way, we can form \(R[x,y]\) or \(R[x_1,...,x_n]\) (polynomial ring of several variables). In fact, \(R[x,y] = (R[x])[y]\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------></p>
<h4><b>The Degree of a Polynomial</b></h4>
<p>We now want to focus on a special case of a polynomial ring where the coefficient ring is actually a field. So let \(R = K\) be a field and let</p>
<div>
$$
\begin{align*}
f = \sum_k a_kx^k \in K[x], a_k \in K
\end{align*}
$$
</div>
<p>Then, the degree of \(f\), \(\deg(f)\) is the largest integer \(n\) such that \(a_n \neq 0\). For example if</p>
<div>
$$
\begin{align*}
f = 7x^3 + \frac{1}{2}x - 17
\end{align*}
$$
</div>
<p>Then, \(\deg(f) = 3\). Warning: if</p>
<div>
$$
\begin{align*}
f = \sum_{k=0}^{n} a_kx^k \in K[x]
\end{align*}
$$
</div>
<p>Then, we only know that \(\deg(f) \leq n\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Degree of the Zero Polynomial</b></h4>
<p>If \(f\) is a constant polynomial, what is the degree of \(f\)? We defined the degree as the largest \(n\) such that \(a_n \neq 0\). In a constant polynomial, all the terms are zero except for \(a_0\). If the constant is non-zero, then the degree is zero. But if the polynomial is zero itself so \(f = 0\), then now all the coefficients \(a_i\)’s are zero so in this case the degree is undefined but we’re going to let the degree in this case be \(-\infty\). So deg\((f)=0\) if and only if \(f = 0\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Degree of a Polynomial when the Coefficient Ring is a Field</b></h4>
<p>Next, we’re going to prove a proposition where we will see why we needed the coefficient ring to be a field.
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(K\) is a field and \(f,g \in K[x]\). Then
<ol> 
	<li>\(\text{deg}(fg) = \text{deg}(f) + \text{deg}(g)\)</li>
	<li>\(\text{deg}(f+g) \leq \max\{\text{deg}(f), \text{deg}(g)\}\)</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
Convention:</p>
<ul>
	<li>\(-\infty + \text{ anything } = -\infty\)</li>
	<li>\(-\infty \leq \text{ anything }\)</li>
</ul>
<p><b>Proof of (1)</b>
<br />
Let \(f = a_0 + a_1x + ... + a_mx^m\) and \(g = b_0 + b_1x + ... + b_nx^n\). Then</p>
<div>
$$
\begin{align*}
fg &amp;= a_0b_0 + (a_1b_0+a_0b_1)x + ... + (a_ma_n)x^{m+n} \\
\end{align*}
$$
</div>
<p>and if \(a_m \neq 0\) and \(b_n \neq 0\), then \(a_mb_n \neq 0\). This is because \(K\) is a field so if \(a,b \in K\) and \(a \neq 0\), \(b \neq 0\), then \(ab \neq 0\). (because \(K^{\times} = K\{0\}\)). 
<br />
<br /></p>
<hr />

<p><br />
Non-example: Take \(R = \mathbf{Z}_4 = \{0,1,2,3\}\). \(\mathbf{Z}_4\) is not a field since element 2 doesn’t have an inverse in \(\mathbf{Z}_4\). Note that only elements 1 and 3 have inverses. (\(3*3 = 1\) and \(1*1 = 1\) in \(\mathbf{Z}_4\). Now consider \(\mathbf{Z}_4[x]\) and let \(f = 1 + 2x \in \mathbf{Z}_4[x]\). Observe that</p>
<div>
$$
\begin{align*}
ff = f^2 = (1 + 2x)^2 = 1 + 2x + 2x + (2x)(2x) = 1 + 4x + 4x^2 = 1
\end{align*}
$$
</div>
<p>The proposition says the degree of \(fg\) should be \(1+1=2\) but here so the degree of \(f^2\) is zero. So we’re failing here because we’re not working in a field. Note that this shows that \(f \in \mathbf{Z_4}[x]^{\times}\) is a unit and it’s multiplicative inverse is itself.  Here is a consequence of the proposition:
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Corollary
</div>
<div class="peachbodydiv">
If \(K\) is a field, then the units in the polynomial ring \(K[x]\) are exactly the elements of \(K^{\times}\). In other words, \(K[x]^{\times} = K^{\times}\).
</div>
<!------------------------------------------------------------------------>
<p><br />
So if \(K\) is a field, then all the units in the ring \(K[x]\) will be the nonzero constant polynomials. Those units are called \(K[x]^{\times}\). They form the multiplicative group of the ring \(K[x]\). Reminder: \(K\) is a field so every element except zero has a multiplicative inverse. \(K^{\times}\) is not a field, it is a group \((K^{\times},\cdot)\) that excludes zero so every single element has a multiplicative inverse. 
<br />
<br />
<b>Proof</b>
<br />
One direction: If we have a constant non-zero polynomial and we’re working in a field, then it has an inverse that’s also a constant polynomial.
<br />
Other direction: If we have two polynomials \(f,g\) such that \(fg = 1\), then computing the degree of both sides we see that</p>
<div>
$$
\begin{align*}
\deg(f,g) &amp;= \deg(1) \\
\deg(f) + \deg(g) &amp;= 0 \\
\deg(f) &amp;= -\deg(g).
\end{align*}
$$
</div>
<p>But the only solution is that \(\deg(f) = \deg(g) = 0\). So the units always have to be constant.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Division Algorithm for \(K[x]\)</b></h4>
<!------------------------------------------------------------------------>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(K\) be a field. Let \(p, d \in K[x]\) where \(\deg(d) \geq 0\) so \(d\) is not a constant polynomial. Then there exists unique \(q,r \in K[x]\) such that
<ol>
	<li>\(p = qd + r\)</li>
	<li>\(\deg(r) &lt; \deg(d)\)</li>
</ol>
</div>
<!------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
Let</p>
<div>
$$
\begin{align*}
p &amp;= \sum_i a_ix^i \text{ with } \deg(p)=m, \ \text{ so } \ a_m \neq 0, a_k &gt; 0 \text{ if } k &gt; m\\
d &amp;= \sum_j b_ix^j, \text{ with } \deg(d)=n \geq 0, \ \text{ so } \ b_n \neq 0, b_k &gt; 0 \text{ if } k &gt; n
\end{align*}
$$
</div>
<p>First, show that if \(m &gt; n\) so the degree of \(p\) is greater than the degree of \(d\), then there exists a monomial \(cx^k\) such that</p>
<ol>
	<li>\(p = (cx^k)d + p'\)</li>
	<li>\(\deg(p') &lt; m = \deg(p)\)</li>
</ol>
<p>So we can in a sense subtract that extra monomial where its degree is less than \(m\). Now, re-write both polynomials such that highest term is in the front</p>
<div>
$$
\begin{align*}
p &amp;= a_mx^m + \text{ lower polynomial } \\
d &amp;= b_nx^n + \text{ lower polynomial }
\end{align*}
$$
</div>
<p>Let \(c = a_mb_n^{-1} = \frac{a_m}{b_n}\). This is fine becasuse \(b_n \neq 0\). Let \(k = m - n\). Then</p>
<div>
$$
\begin{align*}
p' &amp;= p - (cx^k)d = (a_mx^m + \text{ lower polynomial }) - (a_nb_n^{-1}x^{m-n})(b_nx^n + \text{ lower polynomial }) \\
&amp;= (a_mx^m - a_nx^m) \text{ lower polynomial }) \\
&amp;= \text{ lower polynomial }
\end{align*}
$$
</div>
<p>Therefore, \(\deg(p') &lt; m\). 
<br />
<br />
To prove the proposition, we use induction on \(m = \deg(p)\). <br />
If \(m &lt; n = \deg(d)\), let \(q = 0, r = p\) so that \(p = 0d + r\). Therefore, \(\deg(r) = \deg(p) &lt; n\). 
<br />
<br />
If \(m \geq n\), use induction. I can write \(p = (cx^k)d + p'\) where \(\deg(p') &lt; m\). By induction, there exists a \(q'\) such that \(q'd + r\) where \(\deg(r) &lt; n\) so</p>
<div>
$$
\begin{align*}
p &amp;= (cx^k + p')d + r.
\end{align*}
$$
</div>
<p>[TODO: This proof is unclear and a mess … ]
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’ve introduced rings last lecture and we said that rings can be commutative, contain a multiplicative identity or can also be a field. We saw the subset that includes elements with multiplicative inverses that’s also a group. This is the Units group or \(R^{\times}\) which contains an element in \(R\) such that it has a multiplication inverse. We also saw an example of a ring which is $$ \begin{align*} R[i] = \{\text{Formal expressions } a+bi, a, b \in R \}, \quad i^2 = -1 \end{align*} $$]]></summary></entry><entry><title type="html">Lecture 32: Rings</title><link href="http://localhost:4000/jekyll/update/2025/02/24/math417-32-rings.html" rel="alternate" type="text/html" title="Lecture 32: Rings" /><published>2025-02-24T00:01:36-08:00</published><updated>2025-02-24T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/24/math417-32-rings</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/24/math417-32-rings.html"><![CDATA[<p>Let \(G\) be a group that acts on \(X\). Define
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A Ring: \(R, +, \cdot\) is a \(R\)-set with \(+, \cdot\) operations on \(R\) such that
<ul>
	<li>\(R, +\) is an abelian group with \(0\) as the identity and \(-a\) as the inverse of \(a \in R\).</li>
	<li>Multiplication is associative so \((ab)c = a(bc)\) for all \(a,b,c \in R\).</li>
	<li>Distributive Law: \((a + b)c = (ac) + (bc)\) and \((a(b + c) = (ab) + (ac)\).</li>
</ul>
</div>
<!----------------------------------------------------------------------------->
<p><br />
Warning: we don’t know if \(a + b \cdot c\) should be \((a + b) \cdot c\) or \(a + (b \cdot c)\). The convention is to use the second (operator precedence).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>More Terminology</b></h4>
<ul>
	<li><b>Ring with identity</b>: This means a rin with a multiplicative identity since a ring always has an additive identity but not necessarily a multiplicative identity. This is a ring \(R\) such that \(\exists 1 \in R\) so that \(1a = a1 = a\) for all \(a \in R\).</li>
	<!----->
	<li><b>Commutative Ring</b>: A ring \(R\) such that \(ab = ba\) for all \(a, b \in R\).</li>
	<!---->
	<li>If \(R\) has a multiplicative identity, then \(a \in R\) is a unit if there exists a \(b\) in \(R\) such that \(ab = 1 = ba\). We call \(b\) an inverse of \(a\) and write \(b = a^{-1}\). (So a unit is an element with a multiplicative inverse)</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Basic Facts in a Ring</b></h4>
<ol>
	<li>\(a0 = 0 = 0a\) for all \(a \in R\). To show this, use the distributive law to see that
		<div>
		$$
		\begin{align*}
		0a = (0 + 0)a = 0a + 0a. 
		\end{align*}
		$$
		</div>
	But \(R\) is abelian with respect to addition so we can cancel \(0a\) from both sides to see that \(0 = 0a\).</li>
	<!----->
	<li>If \(1 \in R\), then \(-a = (-1)a\). So this says the additive inverse is -1 multiplied by \(a\). To see why, observe that
		<div>
		$$
		\begin{align*}
		(1 + (-1))a &amp;= 1a + (-1)a \\
		     0a &amp;= 0.
		\end{align*}
		$$
		</div>
	  </li>
	  <!----->
	  <li>The multiplicative identity is unique if it exists.</li>
	  <!----->
	  <li>If \(1 \in R\) and \(a\) is unit in \(R\), then it has a unique inverse</li>
	  <!----->
	  <li>If \(1 \in R\), then define \((R^{\times},\cdot)\) as the set of units in \(R\) so \((R^{\times},\cdot) = \{\text{units }a \in R\}\). So this is set of the elements that have a multiplicative inverse. This set with the multiplication operation is a group. As a consequence, if \(a\) and \(b\) are units, then their product is in \(R^{\times}\). In fact, \((ab)^{-1} = b^{-1}a^{-1}\).</li>	  
</ol>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>The Zero Ring</b></h4>
<p>This is a ring \(R\) with element \(R = \{0\}\). \(0 + 0 = 0\) and \(0 \cdot 0 = 0\). This is a commutative ring with identity. The multiplicative identity is \(1 = 0\) in this ring. In fact, If \(R\) is a ring with 1, then \(1 = 0\) if and only if \(R = \{0\}\). Sometimes this ring is excluded from the definition of rings …
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Fields</b></h4>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A Field is a commutative ring with 1 such that every nonzero element is a unit and \(1 \neq 0\).
</div>
<p><br />
So we’re excluding the zero ring here.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
	<li>\(\mathbf{Z}\) with addition and multiplication is a commutative ring with identity.</li>
	<!----->
	<li>\(\mathbf{Z}2 = \{2n \ | \ n \in \mathbf{Z}\}\) is a commutative ring but it doesn't have the identity \(1 \not\in \mathbf{Z}2\).</li>
	<!---->
	<li>\(\mathbf{Q}, \mathbf{R}, \mathbf{C}\) are all fields.</li>
	<!---->
	<li>For \(n \geq 1\), \(\mathbf{Z}_n\) is a commutative ring with \(1 = [1]_n\). It is a field if and only if \(n\) is a prime.</li>
	<!---->
	<li>Let \(R\) be any ring. Then \(S = \text{Mat}_{n \times n}(R)\) the set of matrices with entries in \(R\) is 
	also a ring with matrix addition/multiplication.</li>
	    <ul>
	    <li>If \(1 \in R\) so \(R\) includes the multiplicative identity. Then, \(I \in S\).</li>
	    <li>If \(R\) is commutative. Then, \(S\) might not be commutative.</li>
	   </ul>
	<!---->
	<li>Rings of functions: if \(X\) is an arbitrary set and \(R\) is an arbitrary function, then \(F(X,R) = \{ \text{ all functions } \ | \ f: X \rightarrow R \ \}\) is a ring via a pointwise operation. What's a pointwise operation? Given two different functions \(f, g: X \rightarrow R\), then define 
		<div>
		$$
		\begin{align*}
		(f + g)(x) &amp;:= f(x) + g(x) \\
		(fg)(x) &amp;:= f(x)g(x) \quad \text{ for } x \in X
		\end{align*}
		$$
		</div>
	As long as the target is a ring this works. No restriction on \(X\).
	</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Subrings</b></h4>
<p>A subring \(S\) of ring \(R\) is a subset, which is a ring via operation inherited from \(R\). This implies that if \(a, b \in S\), then \(a + b, ab \in S\). To show \(S\) is a subring, we have the following proposition
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
\(R\) ring, a subset \(S \subseteq R\) is a subring if and only if
<ul>
	<li>\(0 \in S\) or \(S \neq \emptyset\).</li>
	<li>if \(a,b \in S\), then \(a + b, ab \in S\).</li>
	<li>If \(s \in S\) then \(-a \in S\).</li>
</ul>
</div>
<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<ul>
	<li>\(\mathbf{Z}\) is a ring and \(\mathbf{Z}2\) is a subring.</li>
	<!----->
	<li>\(\mathbf{Z} \subseteq \mathbf{Q} \subseteq \mathbf{R} \subseteq \mathbf{C}\) are subrings.</li>
	<!----->
	<li> \(R = \text{Mat}_{n \times n}(\mathbf{R})\) be the ring of 2 by 2 matrices with entries in \(\mathbf{R}\). Then, \(S = \big\{ \begin{pmatrix}a &amp; -b \\ b &amp; a \end{pmatrix} \big\}, a, b \in R\) is a subring. In fact, \(S \cong \mathbf{C}\) as rings</li>
</ul>
<p>Warning: we can have a subring \(S \subseteq R\) such that \(1_S \in S\) and \(1_R \in R\) but \(1_S \neq 1_S\).
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Complex Numbers</b></h4>
<p>One way to describe the complex numbers is to say that the elements of \(\mathbf{C}\) are “formal expressions” \(a + bi\) where \(a, b \in \mathbf{R}\) and \(i\) is a new symbol. We add in the usual way and when we multiply, we use the identity \(i^2 = -1\). 
<br />
<br />
Another way to describe the complex numbers is to say that the elements of \(C\) are vectors \((a,b)\) in \(\mathbf{R}^2\) where we have a special notation for \(1 = (1, 0)\) and \(i = (0, 1)\). Now, define \(+\) by vector addition and define \(\cdot\) by</p>
<div>
$$
\begin{align*}
(a, b) \cdot (a', b') = (aa' - bb', ab' + ba')
\end{align*}
$$
</div>
<p>In terms of the older notation where \((a,b) = a+bi\), it is</p>
<div>
$$
\begin{align*}
(a + bi)(a' + b'i) = (aa' - bb') + (ab' + ba')i
\end{align*}
$$
</div>
<p>\(\mathbf{C}\) is a commutative ring with 1. In fact, \(\mathbf{C}\) is a field. Why? because we have straight forward formula to finding the inverse. First observe what happens when we multiply by the complex conjugate</p>
<div>
$$
\begin{align*}
(a + bi)(a' - b'i) = (a^2 + b^2) + 0i.
\end{align*}
$$
</div>
<p>Fact: if \(a, b \in \mathbf{R}\) and \((a,b) \neq (0,0)\), then \(a^2 + b^2 &gt; 0\). From this we get the inverse formula</p>
<div>
$$
\begin{align*}
\frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i = (a + bi)^{-1} 
\end{align*}
$$
</div>
<p>So every non-zero element has a multiplicative inverse and so it’s a field because we have this formula.
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Let \(R = \mathbf{Z}_3[i]\) is ring. This is the set of formal expressions \(a + bi\) where \(a, b \in \mathbf{Z}_3\). \(i\) is a symbol such that \(i^2 = [-1]_3 = -1\).
<br />
<br />
We can check very easily that \(R\) is a commutative ring with identity. Is \(R\) a field? It’s obvious but the answer is yes. So for any \(a, b \in \mathbf{Z}_3\) where \((a,b) \neq (0,0)\), then \(a^2 + b^2 \neq 0\). Why? observe that, \(0^2 = 0\), \(1^2 = 1\) but \(2^2 = 1\) in \(\mathbf{Z}_3\). So the only way to get \(a^2 + b^2 = 0\) is to have \(a = b = 0\). The formula for a multiplicative inverse is</p>
<div>
$$
\begin{align*}
(a + bi)^{-1} = \frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i 
\end{align*}
$$
</div>
<p>Therefore, \(\mathbf{Z}_3[i]\) is a field with 9 elements. Call this \(\mathbf{F}_9\).
<br />
<br />
What about \(\mathbf{Z}_5[i]\)? is it a field? No. Because \((2 + i)\) doesn’t have a multiplicative in \(\mathbf{Z}_5[i]\). To see this, observe that</p>
<div>
$$
\begin{align*}
(2 + i) \cdot (2 - i) = 2^2 + i^2 + 5 = 0 
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(G\) be a group that acts on \(X\). Define Definition A Ring: \(R, +, \cdot\) is a \(R\)-set with \(+, \cdot\) operations on \(R\) such that \(R, +\) is an abelian group with \(0\) as the identity and \(-a\) as the inverse of \(a \in R\). Multiplication is associative so \((ab)c = a(bc)\) for all \(a,b,c \in R\). Distributive Law: \((a + b)c = (ac) + (bc)\) and \((a(b + c) = (ab) + (ac)\). Warning: we don’t know if \(a + b \cdot c\) should be \((a + b) \cdot c\) or \(a + (b \cdot c)\). The convention is to use the second (operator precedence). More Terminology Ring with identity: This means a rin with a multiplicative identity since a ring always has an additive identity but not necessarily a multiplicative identity. This is a ring \(R\) such that \(\exists 1 \in R\) so that \(1a = a1 = a\) for all \(a \in R\). Commutative Ring: A ring \(R\) such that \(ab = ba\) for all \(a, b \in R\). If \(R\) has a multiplicative identity, then \(a \in R\) is a unit if there exists a \(b\) in \(R\) such that \(ab = 1 = ba\). We call \(b\) an inverse of \(a\) and write \(b = a^{-1}\). (So a unit is an element with a multiplicative inverse) Basic Facts in a Ring \(a0 = 0 = 0a\) for all \(a \in R\). To show this, use the distributive law to see that $$ \begin{align*} 0a = (0 + 0)a = 0a + 0a. \end{align*} $$ But \(R\) is abelian with respect to addition so we can cancel \(0a\) from both sides to see that \(0 = 0a\). If \(1 \in R\), then \(-a = (-1)a\). So this says the additive inverse is -1 multiplied by \(a\). To see why, observe that $$ \begin{align*} (1 + (-1))a &amp;= 1a + (-1)a \\ 0a &amp;= 0. \end{align*} $$ The multiplicative identity is unique if it exists. If \(1 \in R\) and \(a\) is unit in \(R\), then it has a unique inverse If \(1 \in R\), then define \((R^{\times},\cdot)\) as the set of units in \(R\) so \((R^{\times},\cdot) = \{\text{units }a \in R\}\). So this is set of the elements that have a multiplicative inverse. This set with the multiplication operation is a group. As a consequence, if \(a\) and \(b\) are units, then their product is in \(R^{\times}\). In fact, \((ab)^{-1} = b^{-1}a^{-1}\). The Zero Ring This is a ring \(R\) with element \(R = \{0\}\). \(0 + 0 = 0\) and \(0 \cdot 0 = 0\). This is a commutative ring with identity. The multiplicative identity is \(1 = 0\) in this ring. In fact, If \(R\) is a ring with 1, then \(1 = 0\) if and only if \(R = \{0\}\). Sometimes this ring is excluded from the definition of rings … Fields Definition A Field is a commutative ring with 1 such that every nonzero element is a unit and \(1 \neq 0\). So we’re excluding the zero ring here. Examples \(\mathbf{Z}\) with addition and multiplication is a commutative ring with identity. \(\mathbf{Z}2 = \{2n \ | \ n \in \mathbf{Z}\}\) is a commutative ring but it doesn't have the identity \(1 \not\in \mathbf{Z}2\). \(\mathbf{Q}, \mathbf{R}, \mathbf{C}\) are all fields. For \(n \geq 1\), \(\mathbf{Z}_n\) is a commutative ring with \(1 = [1]_n\). It is a field if and only if \(n\) is a prime. Let \(R\) be any ring. Then \(S = \text{Mat}_{n \times n}(R)\) the set of matrices with entries in \(R\) is also a ring with matrix addition/multiplication. If \(1 \in R\) so \(R\) includes the multiplicative identity. Then, \(I \in S\). If \(R\) is commutative. Then, \(S\) might not be commutative. Rings of functions: if \(X\) is an arbitrary set and \(R\) is an arbitrary function, then \(F(X,R) = \{ \text{ all functions } \ | \ f: X \rightarrow R \ \}\) is a ring via a pointwise operation. What's a pointwise operation? Given two different functions \(f, g: X \rightarrow R\), then define $$ \begin{align*} (f + g)(x) &amp;:= f(x) + g(x) \\ (fg)(x) &amp;:= f(x)g(x) \quad \text{ for } x \in X \end{align*} $$ As long as the target is a ring this works. No restriction on \(X\). Subrings A subring \(S\) of ring \(R\) is a subset, which is a ring via operation inherited from \(R\). This implies that if \(a, b \in S\), then \(a + b, ab \in S\). To show \(S\) is a subring, we have the following proposition Proposition \(R\) ring, a subset \(S \subseteq R\) is a subring if and only if \(0 \in S\) or \(S \neq \emptyset\). if \(a,b \in S\), then \(a + b, ab \in S\). If \(s \in S\) then \(-a \in S\). Examples \(\mathbf{Z}\) is a ring and \(\mathbf{Z}2\) is a subring. \(\mathbf{Z} \subseteq \mathbf{Q} \subseteq \mathbf{R} \subseteq \mathbf{C}\) are subrings. \(R = \text{Mat}_{n \times n}(\mathbf{R})\) be the ring of 2 by 2 matrices with entries in \(\mathbf{R}\). Then, \(S = \big\{ \begin{pmatrix}a &amp; -b \\ b &amp; a \end{pmatrix} \big\}, a, b \in R\) is a subring. In fact, \(S \cong \mathbf{C}\) as rings Warning: we can have a subring \(S \subseteq R\) such that \(1_S \in S\) and \(1_R \in R\) but \(1_S \neq 1_S\). Complex Numbers One way to describe the complex numbers is to say that the elements of \(\mathbf{C}\) are “formal expressions” \(a + bi\) where \(a, b \in \mathbf{R}\) and \(i\) is a new symbol. We add in the usual way and when we multiply, we use the identity \(i^2 = -1\). Another way to describe the complex numbers is to say that the elements of \(C\) are vectors \((a,b)\) in \(\mathbf{R}^2\) where we have a special notation for \(1 = (1, 0)\) and \(i = (0, 1)\). Now, define \(+\) by vector addition and define \(\cdot\) by $$ \begin{align*} (a, b) \cdot (a', b') = (aa' - bb', ab' + ba') \end{align*} $$ In terms of the older notation where \((a,b) = a+bi\), it is $$ \begin{align*} (a + bi)(a' + b'i) = (aa' - bb') + (ab' + ba')i \end{align*} $$ \(\mathbf{C}\) is a commutative ring with 1. In fact, \(\mathbf{C}\) is a field. Why? because we have straight forward formula to finding the inverse. First observe what happens when we multiply by the complex conjugate $$ \begin{align*} (a + bi)(a' - b'i) = (a^2 + b^2) + 0i. \end{align*} $$ Fact: if \(a, b \in \mathbf{R}\) and \((a,b) \neq (0,0)\), then \(a^2 + b^2 &gt; 0\). From this we get the inverse formula $$ \begin{align*} \frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i = (a + bi)^{-1} \end{align*} $$ So every non-zero element has a multiplicative inverse and so it’s a field because we have this formula. Example Let \(R = \mathbf{Z}_3[i]\) is ring. This is the set of formal expressions \(a + bi\) where \(a, b \in \mathbf{Z}_3\). \(i\) is a symbol such that \(i^2 = [-1]_3 = -1\). We can check very easily that \(R\) is a commutative ring with identity. Is \(R\) a field? It’s obvious but the answer is yes. So for any \(a, b \in \mathbf{Z}_3\) where \((a,b) \neq (0,0)\), then \(a^2 + b^2 \neq 0\). Why? observe that, \(0^2 = 0\), \(1^2 = 1\) but \(2^2 = 1\) in \(\mathbf{Z}_3\). So the only way to get \(a^2 + b^2 = 0\) is to have \(a = b = 0\). The formula for a multiplicative inverse is $$ \begin{align*} (a + bi)^{-1} = \frac{a}{a^2+b^2} + \frac{-b}{a^2+b^2}i \end{align*} $$ Therefore, \(\mathbf{Z}_3[i]\) is a field with 9 elements. Call this \(\mathbf{F}_9\). What about \(\mathbf{Z}_5[i]\)? is it a field? No. Because \((2 + i)\) doesn’t have a multiplicative in \(\mathbf{Z}_5[i]\). To see this, observe that $$ \begin{align*} (2 + i) \cdot (2 - i) = 2^2 + i^2 + 5 = 0 \end{align*} $$ References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 31: Fixed Point Theorem and Cauchy Theorem</title><link href="http://localhost:4000/jekyll/update/2025/02/23/math417-31-fixed-point-theorem-cauchy-theorem.html" rel="alternate" type="text/html" title="Lecture 31: Fixed Point Theorem and Cauchy Theorem" /><published>2025-02-23T00:01:36-08:00</published><updated>2025-02-23T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/23/math417-31-fixed-point-theorem-cauchy-theorem</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/23/math417-31-fixed-point-theorem-cauchy-theorem.html"><![CDATA[<p>Let \(G\) be a group that acts on \(X\). Define
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Define \(X^G = \{x \in X \ | \ gx = x \text{ for all } g \in G\} \subseteq X \). This set is called the Fixed Point set of the action. 
</div>
<!----------------------------------------------------------------------------->
<p><br />
Compare this to the definition for any \(g \in G\), then.</p>
<div>
$$
\begin{align*}
\text{Fix}(g) = \{x \in X \ | \ gx = x\}
\end{align*}
$$
</div>
<p>So this is the set of elements fixed by \(g\) but what we defined above is the set of elements in \(X\) such that they’re always fixed by any \(g\). This means that we can re-write the definition to</p>
<div>
$$
\begin{align*}
X^G = \bigcap_{g \in G}\text{Fix}(g)
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->
<p>We can also describe this set another way. Recall that an element \(x\) is fixed by every element \(g \in G\) if and only if its orbit contains only the element \(x\) itself. So now we can re-write the definition to be</p>
<div>
$$
\begin{align*}
X^G = \{x \in X \ | \ O(x) = \{x\} \}
\end{align*}
$$
</div>
<!----------------------------------------------------------------------------->
<p>Recall now that \(\text{Stab}(x) = \{g \in G \ | \ gx = x\}\). So we can re-write this definition to say</p>
<div>
$$
\begin{align*}
X^G = \{x \in X \ | \ \text{Stab}(x) = G \}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Fixed Point Theorem</b></h4>
<p>We’ll study one theorem about this. But first define
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(p\) be a prime number. A \(p\)-group is a group of order \(p^k\) for some \(k \geq 1\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
For example \(\mathbf{Z}_{p^k}\) is a \(p\)-group. The cyclic group like \(\mathbf{Z}_{p^i} \times \mathbf{Z}_{p^j}\) is another \(p\)-group. Or the dihedral group \(D_{2^k}\) which has order \(2^{k+1}\) so this is a 2-group.
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(G\) be \(p\)-group which acts on a finite set \(X\). Then \(|X^G| \equiv |X| (\bmod p)\)
</div>
<!----------------------------------------------------------------------------->
<p><br /></p>
<p>Proof</p>
<p>The idea is that since \(G\) acts on \(X\), then it partitions \(X\) into non-empty disjoint orbits. So we can write \(O_1,O_2,...O_r\) for the orbits of the action. Then</p>
<div>
$$
\begin{align*}
|X| = |O_1| + |O_2| + ... + |O_r|
\end{align*}
$$
</div>
<p>But we know that \(|G|=p^k\) and we also know that any orbit size must divide the group order. Therefore, \(|O_i| \in \{1,p,p^2,...p^k\}\). Break the orbits into two types. Let</p>
<div>
$$
\begin{align*}
O_1, O_2, ..., O_d
\end{align*}
$$
</div>
<p>be orbits of size 1 and let</p>
<div>
$$
\begin{align*}
O_{d+1}, O_{d+2}, ..., O_r
\end{align*}
$$
</div>
<p>be orbits of size \(r\). So now</p>
<div>
$$
\begin{align*}
|X| = (|O_1| + |O_2| + ... + |O_d|) + (|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|)
\end{align*}
$$
</div>
<p>where \(|O_1| + |O_2| + ... + |O_d|=d\) and \(|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|\) is divisible by \(p\) so it’s some multiple \(k\) of \(p\). More precisely, \(d\) is the number of elements that are in orbits of size 1. By definition, this is the fixed set of the action so \(|X^G| = d\). Therefore</p>
<div>
$$
\begin{align*}
|X| &amp;= d + kp \\
|X| - d &amp;= kp \\
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
|X| &amp;\equiv d (\bmod p) \\
|X| &amp;\equiv |X^G| (\bmod p) 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Application of the Fixed Point Theorem</b></h4>
<p>Here are some application of this theorem
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(G\) be \(p\)-group. Then the center of the group \(Z(G) = \{ g \in G \ | \ gh = hg \text{ for all } h \in H\}\) is non-trivial so \(Z(G) \neq \{e\}\).
</div>
<!----------------------------------------------------------------------------->
<p><br /></p>
<p>Proof</p>
<p>Let \(G\) acts on \(X = G\) itself by conjugation. So we have \(c: G \rightarrow Sym(X)\). The fixed points of this action are</p>
<div>
$$
\begin{align*}
X^G &amp;= \{x \in X \ | \ gx = x \text{ for all } g \in G\} \\
 &amp;= \{x \in X \ | \ c(g)(x) = x \text{ for all } g \in G\} \quad \text{(the action is the conjugation action)} \\
&amp;= \{x \in X \ | gxg^{-1} = x \text{ for all } g \in G\} \\
&amp;= \{x \in X \ | gx = xg \text{ for all } g \in G\} 
\end{align*}
$$
</div>
<p>So in the conjugation action, the fixed point set is the center of the set. By the previous theorem we know that</p>
<div>
$$
\begin{align*}
|X^G| &amp;\equiv |X| (\bmod p) \\
|X^G| &amp;\equiv p^k (\bmod p) \quad \text{(order of $|G|$ is $p^k$)} \\
|X^G| &amp;\equiv 0 (\bmod p)  \quad \text{because ($p^k \equiv 0 (\bmod p)$)}\\
|Z(G)| &amp;\equiv 0 (\bmod p)  \quad \text{(we just showed this)}\\
\end{align*}
$$
</div>
<p>Therefore, \(Z(G) - 0 = pm\) for some \(m \in Z\). This means that \(p \ | \ Z(G)\). But \(Z(G)\) is a subgroup so it includes at least the identity element. So its size is at least 1. Therefore, \(|Z(G)| \geq p \geq 2\). So we must have at least one non-trivial element in the center. \(\ \blacksquare\)
<br />
<br />
So again, \(p-\)groups will always have a non-trivial center. Next, we have a corollary of this
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(p\) be a prime number. Then every group of order \(p^2\) is abelian.
</div>
<!----------------------------------------------------------------------------->
<p><br />
Using this, we can now use the elementary divisor theorem to classify these groups. In fact, \(G\) of order (p^2) is isomorphic to either \(\mathbf{Z}_{p^2}\) or \(\mathbf{Z}_p \times \mathbf{Z}_p\). 
<br />
<br />
<b>Proof</b>
Let \(|G| = p^2\). By the proposition, \(Z(G)\) is not trivial. But we also know that it is a subgroup. So its order must divide the order of the group. So its order must either be \(p\) or \(p^2\). If the order is \(p^2\), then every element commute with every other element so \(G\) must be abelian. So we only have case which is when \(|Z(G)| = p\). Now, recall that \(Z(G)\) is a normal subgroup in \(G\). Therefore, we can form the quotient group \(G/Z(G)\). The order of this quotient group is \(p^2/p = p\). But we also know that every group of prime order is cyclic so \(G/Z(G)\) is cyclic. By Homework ?, if we have a group \(G\) where its quotient group mod its center is cyclic (\(G/Z(G)\)), then \(G\) is abelian. So \(G\) is abelian in this case too. \(\ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cauchy Theorem</b></h4>
<p>There is one more application of the fixed point theorem. 
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(G\) be a finite group. If \(p\) is a prime number that divides \(|G|\), then \(G\) must have element of order \(p\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
A special case of this is the even-order theorem. As a reminder, let’s revisit this proof in terms of group actions before proving the actual theorem. 
<br />
<br />
<b>Proof (Even Order Theorem)</b>
<br />
Let \(|G| = n\) where \(n\) is even. Let \(C = \langle \varphi \rangle = \{e, \varphi\}\) be a cyclic group of order 2. Let \(C\) act on the set \(X = C\) which is the group itself. We know the identity elements acts as the identity function on \(X\). So we only have to define the action for \(\varphi\). So let’s define what \(\varphi(g)\) is for every element of the group. Let \(\varphi(g)\) be</p>
<div>
$$
\begin{align*}
\varphi(g) = g^{-1} \quad \text{for } g \in G
\end{align*}
$$
</div>
<p>This is in fact is a bijection. Note here, \(\varphi \circ \varphi = id\) so composing the action \(\varphi\) with itself gives us back the identity function. Now, \(C\) is a 2-group since its of order 2. So we can apply the fixed point theorem which states that</p>
<div>
$$
\begin{align*}
|X^C| &amp;\equiv |X| (\bmod 2) \\
|X^C| &amp;\equiv 2 (\bmod 2) \quad \text{(We know $|X| = 2$)}\\
|X^C| &amp;\equiv 0 (\bmod 2)
\end{align*}
$$
</div>
<p>So \(|X^C|\) must be even. \(X^C\) is the set of elements that are fixed by the action \(\varphi\) so</p>
<div>
$$
\begin{align*}
X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\
    &amp;= \{x \in X \ | \ x^{-1} = x\} \\
	&amp;= \{x \in X \ | \ x^2 = e\}.
\end{align*}
$$
</div>
<p>So this group has an even number of elements and must at least include the identity element. Therefore, it must have at least one more non-trivial element of order 2. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------->
<b>Proof (Cauchy’s Theorem)</b>
<br />
Let \(G\) be a group of order \(n\). Suppose \(p\) is a prime number such that \(p \ | \ n\). Let \(C = \langle \varphi \rangle\) be a cyclic group of order \(p\). Let \(X\) be the set</p>
<div>
$$
\begin{align*}
X = \{(a_1,...,a_p) \in G^p \ | \ a_1,...a_p \in G, a_1a_2...a_p = e\}
\end{align*}
$$
</div>
<p>So it’s the set of all \(p\) tuples such that when we multiply any tuple’s elements, we get the identity. But we can re-write this as</p>
<div>
$$
\begin{align*}
a_p =  (a_1,a_2...a_{p-1})^{-1}
\end{align*}
$$
</div>
<p>So the last element \(a_p\) is the inverse of the previous elements all multiplied. So for any \(g\) to be in \(G^p\), we can pick \(p-1\) elements from \(G\) and then form the last element by taking their product and taking the inverse of that product. 
<br />
<br />
What is \(|X|\)? we have \(n\) choices for the first \(p-1\) elements but only 1 choice for the last element. This implies that \(|X| = n^{p-1}\). Moreover, by assumption we know that \(p\) divides \(|G|=n\). So \(n = pk\) for some \(k\). So we can write \(|X| = (pk)^{p-1}\). But \(p\) is prime so it’s at least 2. Therefore, \(p\) must divide \(|X|\) as well.
<br />
<br />
Now, let \(C\) act on \(X\). Define</p>
<div>
$$
\begin{align*}
\varphi \cdot (a_1,a_2,...,a_{p-1},a_p) = (a_p,a_1,...,a_{p-1})
\end{align*}
$$
</div>
<p>So the action permutes the elements cyclicly. We need to verify that the product is in \(X\). This means if we multiply the first \(p-1\) elements and take their inverse, we should get the last element. To show this notice that \((a_1,a_2,...,a_{p-1},a_p)\) is in \(X\) by assumption so we know that the product of the elements is \(e\). Now conjugate this product by \(a^{p}\) to see that</p>
<div>
$$
\begin{align*}
(a_1a_2...a_{p-1}a_p) &amp;= e \\
a_p(a_1a_2...a_{p-1}a_p)a_p^{-1} &amp;= a_pea_p^{-1} \\
a_pa_1a_2...a_{p-1} &amp;= e.
\end{align*}
$$
</div>
<p>So we can see that \(a_pa_1a_2...a_{p-1} \in X\) which is what we wanted to show. Additionally, if we apply \(\varphi\) \(p\) times, we will see that \(\varphi \circ \varphi \circ ... \varphi = id\) it will take us to the identity function or action. So this action or permutation has order \(p\). 
<br />
<br />
So now we have \(|G| = n\) and \(|X| = n^{p-1}\). We know \(p\) divides both. We can apply the fixed point theorem but what is \(X^C\)? By definition, it’s the set of elements fixed by any \(g \in \langle \varphi \rangle\). But since \(\langle \varphi \rangle\) is cyclic, then if an element gets fixed by \(\varphi\), it get fixed by any power of \(\varphi\). Therefore</p>
<div>
$$
\begin{align*}
X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\
    &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and } \varphi \cdot (a_1,...,a_p) = (a_1,...,a_p)\} \\
    &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and }  (a_p,a_1...,a_{p-1}) = (a_1,a_2...,a_p)\}.
\end{align*}
$$
</div>
<p>This last condition says that \(a_p=a_1\), \(a_1=a_2\), … \(a_{p-1}=a_p\). This means that all the elements are the same. So we can write \(X^C\) as</p>
<div>
$$
\begin{align*}
X^C &amp;= \{ (a,a...,a) \ | \ a \in G, aa...a = a^p = e\}.
\end{align*}
$$
</div>
<p>Therefore, the size of this set, is the number of elements in \(G\) which have order \(p\). So</p>
<div>
$$
\begin{align*}
|X^C| &amp;= |\{ a \in G \ | \ a^p = e\}|.
\end{align*}
$$
</div>
<p>We know \(e \in X^C\) since e\(e^p = e\). Moreover, by the Fixed Point Theorem,</p>
<div>
$$
\begin{align*}
|X^C| &amp;\equiv |X| (\bmod p) \\
|X^C| &amp;\equiv 0 (\bmod p) \quad \text{(because $p \ | \ |X|$)}
\end{align*}
$$
</div>
<p>So \(|X^C|\) must be divisible by \(p\) and since \(|X^C| \geq 1\), then \(|X^C| \geq p\). But this means that \(G\) has at least one non-trivial element of order \(p\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Classification of Groups of Order 6</b></h4>
<p>We’ve seen before \(\mathbf{Z}_6 \cong \mathbf{Z}_2 \times \mathbf{Z}_3\) and we’ve also seen \(D_3 \cong S_3\). 
<br />
<br />
Suppose \(|G| = 6 = 2(3)\). These are prime factors, so we can use Cauchy’s Theorem twice to conclude that we must have an element of order 2 and another element of order 3. So let</p>
<div>
$$
\begin{align*}
A &amp;= \langle a \rangle \text{ where $|A| = 2$ } \\
N &amp;= \langle 3 \rangle \text{ where $|N| = 3$}
\end{align*}
$$
</div>
<p>This implies that</p>
<div>
$$
\begin{align*}
[G:N] = \frac{|G|}{|N|} = \frac{6}{3} = 2.
\end{align*}
$$
</div>
<p>But we’ve seen in one of the homeworks, that this implies that \(N\) is normal. We also know the following facts</p>
<ul>
	<li>\(N \cap A = \{e\}\) since elements of \(N\) have order 1 or 3 and elements of \(A\) have orders 1 and 2</li>
	<li>\(NA\) is a subgroup of \(G\) because one of the groups is normal by Corollary (26.4).</li>
	<li>By the Diamond Isomorphism Theorem, \(A/A \cap N \cong NA/N\). But since \(A \cap N = \{e\}\). Then \(A \cong NA/N\). This means that \(|NA|/|N| = |A|\). So \(|NA| = |A||N| = 6\)</li>
	<li>Since \(NA\) is a subgroup of size 6, then it's \(G\) so \(NA = G\)</li>
</ul>
<p>So we know 4 important things</p>
<ul>
	<li>\(A\) is a subgroup of \(G\).</li>
	<li>\(N\) is a normal subgroup of \(G\).</li>
	<li>\(G = NA\).</li>
	<li>\(A \cap N = \{e\}\).</li>
</ul>
<p>These are the 4 conditions so we can apply the the recognization theorem to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups</p>
<div>
$$
\begin{align*}
N \rtimes_{\gamma} A \cong G
\end{align*}
$$
</div>
<p>\(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order 3 so it’s isomorphic to \(\mathbf{Z}_3\). We know that \(\text{Aut}(\mathbf{Z}_3) \cong \Phi(3)\) But \(\Phi(3)\) is of order 2 so it’s isomorphic to \(\mathbf{Z}_2\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\) if both groups are cyclic of order 2? There are only two choices.</p>
<ul>
	<li>The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_3\)</li>
	<li>The non-trivial homomorphism gives us the dihedral group \(D_3\).</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Study Notes on \(D_3\)</b></h4>
<p>How does the non-trivial homomorphism gives us the dihedral group? how does this happen? We have</p>
<ul>
	<li>\(N = \langle n \rangle \cong \mathbf{Z}_3\) where \(n^3 = e\).</li>
	<li>\(A = \langle a \rangle \cong \mathbf{Z}_2\) where \(a^2 = e\)</li>
</ul>
<p>Define</p>
<div>
	$$
	\begin{align*}
	\gamma: A &amp;\rightarrow \text{Aut}(\mathbf{Z}_3) \\
	\gamma(a) &amp;= \alpha \text{ where } \alpha(n) = n^{-1} \\
	\gamma(a)(n) &amp;= n^{-1}
	\end{align*}
	$$
</div>
<p>So the element \(a \in A\) acts on \(n \in N\) by inverting it. Observe that \(a^2 = e\) and \(\alpha^2 = (n^{-1})^{-1} = \text{id}\) so \(\gamma(\alpha^2) = \gamma(\alpha)^2\). So now multiplication in semi-direct groups is defined as</p>
<div>
	$$
	\begin{align*}
	(n^i, a^j)(n^k, a^l) = (n^i \cdot \gamma_{a^j}(n^k), a^{j}a^{j})
	\end{align*}
	$$
</div>
<p>\(a\) has order 2 so \(\gamma_{a^j}(n^k)\) is defined as</p>
<ul>
	<li>When \(j = 0\), then \(a^0 = 0\), then \(\gamma_{e} = id\).</li>
	<li>When \(j = 1\), then \(a^1 = a\), then \(\gamma_{a} = \alpha\) where \(\alpha(n) = n^{-k}\).</li>
</ul>
<p>So now if we apply the semidirect product multiplication, using the homomorphism we defined, we will see that for any \(a\), that we get the relationship \(ana^{-1} = n^{-1}\). 
<br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Classification of Groups of Order \(2p\)</b></h4>
<p>where \(p\) is an odd prime. We know two groups \(\mathbf{Z}_{2p} =\mathbf{Z}_{2} \times \mathbf{Z}_{p}\) and \(D_p\). 
<br />
<br />
Let \(|G| = 2p\). By Cauchy, \(2\) divides \(|G|\). Therefore, we have an element of order \(2\). From this, we get \(A = \langle a \rangle\) where \(|A| = 2\). We also have an element of order \(p\). From this we get \(N = \langle N \rangle\). Again, we will see that</p>
<div>
$$
\begin{align*}
[G:N] = \frac{|G|}{|N|} = \frac{2p}{p} = 2.
\end{align*}
$$
</div>
<p>Therefore \(N\) is normal. With a similar argument to the previous example. We will see that</p>
<ul>
	<li>\(A\) is a subgroup of \(G\).</li>
	<li>\(N\) is a normal subgroup of \(G\).</li>
	<li>\(G = NA\).</li>
	<li>\(A \cap N = \{e\}\).</li>
</ul>
<p>So we can use the recognization theorem again to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups</p>
<div>
$$
\begin{align*}
N \rtimes_{\gamma} A \cong G
\end{align*}
$$
</div>
<p>\(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order \(p\) so it’s isomorphic to \(\mathbf{Z}_p\) (cyclic). We know that \(\text{Aut}(\mathbf{Z}_p) \cong \Phi(p)\) where \(\Phi(p)\) is of order \(p-1\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\).</p>
<ul>
	<li>The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_p\)</li>
	<li>The non-trivial homomorphism gives us the dihedral group \(D_p\).</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>Classification of Groups of Order \(pq\)</b></h4>
<p>\(p\) and \(q\) are distinct primes where \(p &gt; q\). So again we have \(|G|=pq\). By Cauchy, we have two cyclic subgroups such that</p>
<div>
$$
\begin{align*}
A &amp;= \langle a \rangle \text{ where $|A| = q$ } \\
N &amp;= \langle b \rangle \text{ where $|N| = p$}
\end{align*}
$$
</div>
<p>This implies that</p>
<div>
$$
\begin{align*}
[G:N] = \frac{|G|}{|N|} = q.
\end{align*}
$$
</div>
<p>\(q\) is the smallest prime dividing the order of \(|G|\). By some homework assignment that said that if we have a subgroup of order “the smallest prime dividing the order”, then this subgroup is normal. So now again,</p>
<ul>
	<li>\(A\) is a subgroup of \(G\).</li>
	<li>\(N\) is a normal subgroup of \(G\).</li>
	<li>\(G = NA\).</li>
	<li>\(A \cap N = \{e\}\) because \(p \neq q\).</li>
</ul>
<p>So we can use the recognization theorem AGAIN to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups</p>
<div>
$$
\begin{align*}
N \rtimes_{\gamma} A \cong G
\end{align*}
$$
</div>
<p>Where</p>
<div>
	$$
	\begin{align*}
	\gamma: A &amp;\rightarrow \text{Aut}(N) \\
	\gamma: \mathbf{Z}_p &amp;\rightarrow \Phi(p)
	\end{align*}
	$$
</div>
<p>There are two cases:</p>
<ul>
	<li>\(q \ \not\mid \ (p-1)\): In this case, the generator of the group \(A\)'s \(q\)th power has to go to the identity because \(q\) doesn't divide \(p - 1\) which is the order of \(\Phi(p)\). The only possible \(\gamma\) is \(\gamma(a) = e\) so send everything to the identity and we get the direct product \(\mathbf{Z}_p \times \mathbf{Z}_q\).</li>
	<li>\(q \ | \ (p-1)\): So we get the non-trivial homomorphism \(\gamma\). Because \(q \ (p-1)\) which is the order of \(\Phi(p-1)\), then by Cauchy there exists an element of order \(q\) in \(\Phi(p-1)\). So \(\mathbf{Z}_{pq}\) and another non abelian group.</li>
</ul>
<p><br />
<br /></p>
<hr />

<p><br />
<!-------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(G\) be a group that acts on \(X\). Define Definition Define \(X^G = \{x \in X \ | \ gx = x \text{ for all } g \in G\} \subseteq X \). This set is called the Fixed Point set of the action. Compare this to the definition for any \(g \in G\), then. $$ \begin{align*} \text{Fix}(g) = \{x \in X \ | \ gx = x\} \end{align*} $$ So this is the set of elements fixed by \(g\) but what we defined above is the set of elements in \(X\) such that they’re always fixed by any \(g\). This means that we can re-write the definition to $$ \begin{align*} X^G = \bigcap_{g \in G}\text{Fix}(g) \end{align*} $$ We can also describe this set another way. Recall that an element \(x\) is fixed by every element \(g \in G\) if and only if its orbit contains only the element \(x\) itself. So now we can re-write the definition to be $$ \begin{align*} X^G = \{x \in X \ | \ O(x) = \{x\} \} \end{align*} $$ Recall now that \(\text{Stab}(x) = \{g \in G \ | \ gx = x\}\). So we can re-write this definition to say $$ \begin{align*} X^G = \{x \in X \ | \ \text{Stab}(x) = G \} \end{align*} $$ Fixed Point Theorem We’ll study one theorem about this. But first define Definition Let \(p\) be a prime number. A \(p\)-group is a group of order \(p^k\) for some \(k \geq 1\). For example \(\mathbf{Z}_{p^k}\) is a \(p\)-group. The cyclic group like \(\mathbf{Z}_{p^i} \times \mathbf{Z}_{p^j}\) is another \(p\)-group. Or the dihedral group \(D_{2^k}\) which has order \(2^{k+1}\) so this is a 2-group. Theorem Let \(G\) be \(p\)-group which acts on a finite set \(X\). Then \(|X^G| \equiv |X| (\bmod p)\) Proof The idea is that since \(G\) acts on \(X\), then it partitions \(X\) into non-empty disjoint orbits. So we can write \(O_1,O_2,...O_r\) for the orbits of the action. Then $$ \begin{align*} |X| = |O_1| + |O_2| + ... + |O_r| \end{align*} $$ But we know that \(|G|=p^k\) and we also know that any orbit size must divide the group order. Therefore, \(|O_i| \in \{1,p,p^2,...p^k\}\). Break the orbits into two types. Let $$ \begin{align*} O_1, O_2, ..., O_d \end{align*} $$ be orbits of size 1 and let $$ \begin{align*} O_{d+1}, O_{d+2}, ..., O_r \end{align*} $$ be orbits of size \(r\). So now $$ \begin{align*} |X| = (|O_1| + |O_2| + ... + |O_d|) + (|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|) \end{align*} $$ where \(|O_1| + |O_2| + ... + |O_d|=d\) and \(|O_{d+1}| + |O_{d+2}| + ... + |O_{r}|\) is divisible by \(p\) so it’s some multiple \(k\) of \(p\). More precisely, \(d\) is the number of elements that are in orbits of size 1. By definition, this is the fixed set of the action so \(|X^G| = d\). Therefore $$ \begin{align*} |X| &amp;= d + kp \\ |X| - d &amp;= kp \\ \end{align*} $$ Therefore, $$ \begin{align*} |X| &amp;\equiv d (\bmod p) \\ |X| &amp;\equiv |X^G| (\bmod p) \end{align*} $$ Application of the Fixed Point Theorem Here are some application of this theorem Proposition Let \(G\) be \(p\)-group. Then the center of the group \(Z(G) = \{ g \in G \ | \ gh = hg \text{ for all } h \in H\}\) is non-trivial so \(Z(G) \neq \{e\}\). Proof Let \(G\) acts on \(X = G\) itself by conjugation. So we have \(c: G \rightarrow Sym(X)\). The fixed points of this action are $$ \begin{align*} X^G &amp;= \{x \in X \ | \ gx = x \text{ for all } g \in G\} \\ &amp;= \{x \in X \ | \ c(g)(x) = x \text{ for all } g \in G\} \quad \text{(the action is the conjugation action)} \\ &amp;= \{x \in X \ | gxg^{-1} = x \text{ for all } g \in G\} \\ &amp;= \{x \in X \ | gx = xg \text{ for all } g \in G\} \end{align*} $$ So in the conjugation action, the fixed point set is the center of the set. By the previous theorem we know that $$ \begin{align*} |X^G| &amp;\equiv |X| (\bmod p) \\ |X^G| &amp;\equiv p^k (\bmod p) \quad \text{(order of $|G|$ is $p^k$)} \\ |X^G| &amp;\equiv 0 (\bmod p) \quad \text{because ($p^k \equiv 0 (\bmod p)$)}\\ |Z(G)| &amp;\equiv 0 (\bmod p) \quad \text{(we just showed this)}\\ \end{align*} $$ Therefore, \(Z(G) - 0 = pm\) for some \(m \in Z\). This means that \(p \ | \ Z(G)\). But \(Z(G)\) is a subgroup so it includes at least the identity element. So its size is at least 1. Therefore, \(|Z(G)| \geq p \geq 2\). So we must have at least one non-trivial element in the center. \(\ \blacksquare\) So again, \(p-\)groups will always have a non-trivial center. Next, we have a corollary of this Proposition Let \(p\) be a prime number. Then every group of order \(p^2\) is abelian. Using this, we can now use the elementary divisor theorem to classify these groups. In fact, \(G\) of order (p^2) is isomorphic to either \(\mathbf{Z}_{p^2}\) or \(\mathbf{Z}_p \times \mathbf{Z}_p\). Proof Let \(|G| = p^2\). By the proposition, \(Z(G)\) is not trivial. But we also know that it is a subgroup. So its order must divide the order of the group. So its order must either be \(p\) or \(p^2\). If the order is \(p^2\), then every element commute with every other element so \(G\) must be abelian. So we only have case which is when \(|Z(G)| = p\). Now, recall that \(Z(G)\) is a normal subgroup in \(G\). Therefore, we can form the quotient group \(G/Z(G)\). The order of this quotient group is \(p^2/p = p\). But we also know that every group of prime order is cyclic so \(G/Z(G)\) is cyclic. By Homework ?, if we have a group \(G\) where its quotient group mod its center is cyclic (\(G/Z(G)\)), then \(G\) is abelian. So \(G\) is abelian in this case too. \(\ \blacksquare\). Cauchy Theorem There is one more application of the fixed point theorem. Theorem Let \(G\) be a finite group. If \(p\) is a prime number that divides \(|G|\), then \(G\) must have element of order \(p\). A special case of this is the even-order theorem. As a reminder, let’s revisit this proof in terms of group actions before proving the actual theorem. Proof (Even Order Theorem) Let \(|G| = n\) where \(n\) is even. Let \(C = \langle \varphi \rangle = \{e, \varphi\}\) be a cyclic group of order 2. Let \(C\) act on the set \(X = C\) which is the group itself. We know the identity elements acts as the identity function on \(X\). So we only have to define the action for \(\varphi\). So let’s define what \(\varphi(g)\) is for every element of the group. Let \(\varphi(g)\) be $$ \begin{align*} \varphi(g) = g^{-1} \quad \text{for } g \in G \end{align*} $$ This is in fact is a bijection. Note here, \(\varphi \circ \varphi = id\) so composing the action \(\varphi\) with itself gives us back the identity function. Now, \(C\) is a 2-group since its of order 2. So we can apply the fixed point theorem which states that $$ \begin{align*} |X^C| &amp;\equiv |X| (\bmod 2) \\ |X^C| &amp;\equiv 2 (\bmod 2) \quad \text{(We know $|X| = 2$)}\\ |X^C| &amp;\equiv 0 (\bmod 2) \end{align*} $$ So \(|X^C|\) must be even. \(X^C\) is the set of elements that are fixed by the action \(\varphi\) so $$ \begin{align*} X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\ &amp;= \{x \in X \ | \ x^{-1} = x\} \\ &amp;= \{x \in X \ | \ x^2 = e\}. \end{align*} $$ So this group has an even number of elements and must at least include the identity element. Therefore, it must have at least one more non-trivial element of order 2. \(\ \blacksquare\) Proof (Cauchy’s Theorem) Let \(G\) be a group of order \(n\). Suppose \(p\) is a prime number such that \(p \ | \ n\). Let \(C = \langle \varphi \rangle\) be a cyclic group of order \(p\). Let \(X\) be the set $$ \begin{align*} X = \{(a_1,...,a_p) \in G^p \ | \ a_1,...a_p \in G, a_1a_2...a_p = e\} \end{align*} $$ So it’s the set of all \(p\) tuples such that when we multiply any tuple’s elements, we get the identity. But we can re-write this as $$ \begin{align*} a_p = (a_1,a_2...a_{p-1})^{-1} \end{align*} $$ So the last element \(a_p\) is the inverse of the previous elements all multiplied. So for any \(g\) to be in \(G^p\), we can pick \(p-1\) elements from \(G\) and then form the last element by taking their product and taking the inverse of that product. What is \(|X|\)? we have \(n\) choices for the first \(p-1\) elements but only 1 choice for the last element. This implies that \(|X| = n^{p-1}\). Moreover, by assumption we know that \(p\) divides \(|G|=n\). So \(n = pk\) for some \(k\). So we can write \(|X| = (pk)^{p-1}\). But \(p\) is prime so it’s at least 2. Therefore, \(p\) must divide \(|X|\) as well. Now, let \(C\) act on \(X\). Define $$ \begin{align*} \varphi \cdot (a_1,a_2,...,a_{p-1},a_p) = (a_p,a_1,...,a_{p-1}) \end{align*} $$ So the action permutes the elements cyclicly. We need to verify that the product is in \(X\). This means if we multiply the first \(p-1\) elements and take their inverse, we should get the last element. To show this notice that \((a_1,a_2,...,a_{p-1},a_p)\) is in \(X\) by assumption so we know that the product of the elements is \(e\). Now conjugate this product by \(a^{p}\) to see that $$ \begin{align*} (a_1a_2...a_{p-1}a_p) &amp;= e \\ a_p(a_1a_2...a_{p-1}a_p)a_p^{-1} &amp;= a_pea_p^{-1} \\ a_pa_1a_2...a_{p-1} &amp;= e. \end{align*} $$ So we can see that \(a_pa_1a_2...a_{p-1} \in X\) which is what we wanted to show. Additionally, if we apply \(\varphi\) \(p\) times, we will see that \(\varphi \circ \varphi \circ ... \varphi = id\) it will take us to the identity function or action. So this action or permutation has order \(p\). So now we have \(|G| = n\) and \(|X| = n^{p-1}\). We know \(p\) divides both. We can apply the fixed point theorem but what is \(X^C\)? By definition, it’s the set of elements fixed by any \(g \in \langle \varphi \rangle\). But since \(\langle \varphi \rangle\) is cyclic, then if an element gets fixed by \(\varphi\), it get fixed by any power of \(\varphi\). Therefore $$ \begin{align*} X^C &amp;= \{x \in X \ | \ \varphi(x) = x\} \\ &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and } \varphi \cdot (a_1,...,a_p) = (a_1,...,a_p)\} \\ &amp;= \{(a_1,...,a_p) \in G \ | \ a_1...a_p = e \text{ and } (a_p,a_1...,a_{p-1}) = (a_1,a_2...,a_p)\}. \end{align*} $$ This last condition says that \(a_p=a_1\), \(a_1=a_2\), … \(a_{p-1}=a_p\). This means that all the elements are the same. So we can write \(X^C\) as $$ \begin{align*} X^C &amp;= \{ (a,a...,a) \ | \ a \in G, aa...a = a^p = e\}. \end{align*} $$ Therefore, the size of this set, is the number of elements in \(G\) which have order \(p\). So $$ \begin{align*} |X^C| &amp;= |\{ a \in G \ | \ a^p = e\}|. \end{align*} $$ We know \(e \in X^C\) since e\(e^p = e\). Moreover, by the Fixed Point Theorem, $$ \begin{align*} |X^C| &amp;\equiv |X| (\bmod p) \\ |X^C| &amp;\equiv 0 (\bmod p) \quad \text{(because $p \ | \ |X|$)} \end{align*} $$ So \(|X^C|\) must be divisible by \(p\) and since \(|X^C| \geq 1\), then \(|X^C| \geq p\). But this means that \(G\) has at least one non-trivial element of order \(p\). \(\ \blacksquare\) Classification of Groups of Order 6 We’ve seen before \(\mathbf{Z}_6 \cong \mathbf{Z}_2 \times \mathbf{Z}_3\) and we’ve also seen \(D_3 \cong S_3\). Suppose \(|G| = 6 = 2(3)\). These are prime factors, so we can use Cauchy’s Theorem twice to conclude that we must have an element of order 2 and another element of order 3. So let $$ \begin{align*} A &amp;= \langle a \rangle \text{ where $|A| = 2$ } \\ N &amp;= \langle 3 \rangle \text{ where $|N| = 3$} \end{align*} $$ This implies that $$ \begin{align*} [G:N] = \frac{|G|}{|N|} = \frac{6}{3} = 2. \end{align*} $$ But we’ve seen in one of the homeworks, that this implies that \(N\) is normal. We also know the following facts \(N \cap A = \{e\}\) since elements of \(N\) have order 1 or 3 and elements of \(A\) have orders 1 and 2 \(NA\) is a subgroup of \(G\) because one of the groups is normal by Corollary (26.4). By the Diamond Isomorphism Theorem, \(A/A \cap N \cong NA/N\). But since \(A \cap N = \{e\}\). Then \(A \cong NA/N\). This means that \(|NA|/|N| = |A|\). So \(|NA| = |A||N| = 6\) Since \(NA\) is a subgroup of size 6, then it's \(G\) so \(NA = G\) So we know 4 important things \(A\) is a subgroup of \(G\). \(N\) is a normal subgroup of \(G\). \(G = NA\). \(A \cap N = \{e\}\). These are the 4 conditions so we can apply the the recognization theorem to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups $$ \begin{align*} N \rtimes_{\gamma} A \cong G \end{align*} $$ \(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order 3 so it’s isomorphic to \(\mathbf{Z}_3\). We know that \(\text{Aut}(\mathbf{Z}_3) \cong \Phi(3)\) But \(\Phi(3)\) is of order 2 so it’s isomorphic to \(\mathbf{Z}_2\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\) if both groups are cyclic of order 2? There are only two choices. The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_3\) The non-trivial homomorphism gives us the dihedral group \(D_3\). Study Notes on \(D_3\) How does the non-trivial homomorphism gives us the dihedral group? how does this happen? We have \(N = \langle n \rangle \cong \mathbf{Z}_3\) where \(n^3 = e\). \(A = \langle a \rangle \cong \mathbf{Z}_2\) where \(a^2 = e\) Define $$ \begin{align*} \gamma: A &amp;\rightarrow \text{Aut}(\mathbf{Z}_3) \\ \gamma(a) &amp;= \alpha \text{ where } \alpha(n) = n^{-1} \\ \gamma(a)(n) &amp;= n^{-1} \end{align*} $$ So the element \(a \in A\) acts on \(n \in N\) by inverting it. Observe that \(a^2 = e\) and \(\alpha^2 = (n^{-1})^{-1} = \text{id}\) so \(\gamma(\alpha^2) = \gamma(\alpha)^2\). So now multiplication in semi-direct groups is defined as $$ \begin{align*} (n^i, a^j)(n^k, a^l) = (n^i \cdot \gamma_{a^j}(n^k), a^{j}a^{j}) \end{align*} $$ \(a\) has order 2 so \(\gamma_{a^j}(n^k)\) is defined as When \(j = 0\), then \(a^0 = 0\), then \(\gamma_{e} = id\). When \(j = 1\), then \(a^1 = a\), then \(\gamma_{a} = \alpha\) where \(\alpha(n) = n^{-k}\). So now if we apply the semidirect product multiplication, using the homomorphism we defined, we will see that for any \(a\), that we get the relationship \(ana^{-1} = n^{-1}\). Classification of Groups of Order \(2p\) where \(p\) is an odd prime. We know two groups \(\mathbf{Z}_{2p} =\mathbf{Z}_{2} \times \mathbf{Z}_{p}\) and \(D_p\). Let \(|G| = 2p\). By Cauchy, \(2\) divides \(|G|\). Therefore, we have an element of order \(2\). From this, we get \(A = \langle a \rangle\) where \(|A| = 2\). We also have an element of order \(p\). From this we get \(N = \langle N \rangle\). Again, we will see that $$ \begin{align*} [G:N] = \frac{|G|}{|N|} = \frac{2p}{p} = 2. \end{align*} $$ Therefore \(N\) is normal. With a similar argument to the previous example. We will see that \(A\) is a subgroup of \(G\). \(N\) is a normal subgroup of \(G\). \(G = NA\). \(A \cap N = \{e\}\). So we can use the recognization theorem again to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups $$ \begin{align*} N \rtimes_{\gamma} A \cong G \end{align*} $$ \(A\) is of order 2 so it must be isomorphic to \(\mathbf{Z}_2\). \(N\) is of order \(p\) so it’s isomorphic to \(\mathbf{Z}_p\) (cyclic). We know that \(\text{Aut}(\mathbf{Z}_p) \cong \Phi(p)\) where \(\Phi(p)\) is of order \(p-1\). So how many homomorphisms can we have from \(A\) to \(\text{Aut}(N)\). The trivial homomorphism gives us the product group \(\mathbf{Z}_2 \times \mathbf{Z}_p\) The non-trivial homomorphism gives us the dihedral group \(D_p\). Classification of Groups of Order \(pq\) \(p\) and \(q\) are distinct primes where \(p &gt; q\). So again we have \(|G|=pq\). By Cauchy, we have two cyclic subgroups such that $$ \begin{align*} A &amp;= \langle a \rangle \text{ where $|A| = q$ } \\ N &amp;= \langle b \rangle \text{ where $|N| = p$} \end{align*} $$ This implies that $$ \begin{align*} [G:N] = \frac{|G|}{|N|} = q. \end{align*} $$ \(q\) is the smallest prime dividing the order of \(|G|\). By some homework assignment that said that if we have a subgroup of order “the smallest prime dividing the order”, then this subgroup is normal. So now again, \(A\) is a subgroup of \(G\). \(N\) is a normal subgroup of \(G\). \(G = NA\). \(A \cap N = \{e\}\) because \(p \neq q\). So we can use the recognization theorem AGAIN to conclude that there exists a homomorphism \(\gamma: A \rightarrow \text{Aut}(N)\) such that there is an isomorphism of groups $$ \begin{align*} N \rtimes_{\gamma} A \cong G \end{align*} $$ Where $$ \begin{align*} \gamma: A &amp;\rightarrow \text{Aut}(N) \\ \gamma: \mathbf{Z}_p &amp;\rightarrow \Phi(p) \end{align*} $$ There are two cases: \(q \ \not\mid \ (p-1)\): In this case, the generator of the group \(A\)'s \(q\)th power has to go to the identity because \(q\) doesn't divide \(p - 1\) which is the order of \(\Phi(p)\). The only possible \(\gamma\) is \(\gamma(a) = e\) so send everything to the identity and we get the direct product \(\mathbf{Z}_p \times \mathbf{Z}_q\). \(q \ | \ (p-1)\): So we get the non-trivial homomorphism \(\gamma\). Because \(q \ (p-1)\) which is the order of \(\Phi(p-1)\), then by Cauchy there exists an element of order \(q\) in \(\Phi(p-1)\). So \(\mathbf{Z}_{pq}\) and another non abelian group. References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 30: Finite Subgroups of SO(3)</title><link href="http://localhost:4000/jekyll/update/2025/02/22/math417-30-finite-subgroups.html" rel="alternate" type="text/html" title="Lecture 30: Finite Subgroups of SO(3)" /><published>2025-02-22T00:01:36-08:00</published><updated>2025-02-22T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/22/math417-30-finite-subgroups</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/22/math417-30-finite-subgroups.html"><![CDATA[<p>Today, we’ll analyze finite subgroups of \(SO(3)\). Let’s describe the ones we already know about</p>
<ul>
	<li>The trivial subgroup \(\{I\}\).</li>
	<!-------------------------------------->
	<li>Subgroups that are cyclic so these subgroups are isomorphic to \(\mathbf{Z}_n\). For each \(n\), we have infinitely many subgroups. For example, there are infinitely many cyclic subgroups of order 3 in \(SO(3)\). Pick any axis and consider the rotation by angle \(\frac{2\pi}{3}\) around this axis. This makes a subgroup of order 3. So you can get a new subgroup for whatever axis you pick. Therefore, we have infinitely many. In fact, all these subgroups that are rotations by \(\frac{2\pi}{3}\) are conjugate (see the lecture before last. If we have a rotation by \(\theta\) around a unit vector \(v\) and we have the same rotation around another unit vector \(u\), then we change \(u\) to \(v\) by some rotation \(A \in SO(3)\).</li>
	<!-------------------------------------->
	<li>Subgroups isomorphic to the dihedral groups \(D_n\) where \(n \geq 2\). These are symmetries of a regular \(n-\)gon. Pick any \(n-\) and fix one of the vertices on the \(x-\)axis. The symmetries of the \(n-\) is a group isomorphic to \(D_n\).</li>
	<!-------------------------------------->
	<li>Symmetry group of the tetrahedron which is isomorphic to \(A_4\).</li>
	<!-------------------------------------->
	<li>Symmetry group of the cube/octahedron which is isomorphic to \(S_4\).</li>
	<!-------------------------------------->
	<li>Symmetry group of the icosahedron/dodecahedron which is isomorphic to \(A_5\).</li>
</ul>
<p>In fact we have the following theorem
<br />
<!-----------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Every finite subgroup of \(SO(3)\) is in the list above.
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
\(SO(3)\) acts on \(\mathbf{R}^3\) (we multiply a matrix in \(SO(3)\) by a vector in \(\mathbf{R}^3\)). So now suppose that \(G\) is a subgroup of \(SO(3)\). \(G\) acts on \(\mathbf{R}^3\). We’ll focus on the pole subset in \(\mathbf{R}^3\). What is a poll?
<br />
<br />
A pole for \(G \subseteq SO(3)\) is a unit vector \((u) \in \mathbf{R}^3\) where \(\lVert u \rVert = 1\) such that \(|Stab_G(u)|\geq 2\).
<br />
<br />
In other words, there exists an element \(g\) in \(\text{Stab}_G(u)\) other than the identity such that \(gu = u\). If \(g \neq I\), then \(g = Rot_u(\theta)\) where \(\theta\) is not a multiple of \(2\pi\). Recall, that a rotation in space only fixes vectors along the axis of rotation. But since we constrained this to unit vectors, then it will only fix the unit vector on the axis of rotation. So \(g\) fixes only \(\pm u\). As a consequence, poles come in pairs. If \(u\) is a pole, then \(-u\) is a pole too. So again, a pole is a unit vector in \(\mathbf{R}^3\) that gets fixed by some rotation in \(G\). In other words, \(u\) is a unit vector along the axis of some rotation in \(G\). As a consequence, every non-identity element in \(G\) can contribute 2 poles to the set of poles. If we let \(X\) to be the set of poles, then</p>
<div>
$$
\begin{align*}
|X| \leq 2(|G| - 1) \quad \text{(-1 for the identity)}
\end{align*}
$$
</div>
<p>So the set of poles \(X\) is the collection of unit vectors along the axes of rotations of elements in the group \(G\). \(G\) acts on the subset \(X \in \mathbf{R}^3\). For example, if \(g \in G\) and \(x \in X\), then we know that \(gx\) and \(g^{-1}x\) are both in \(X\). Why?
<br />
If \(x\) is a pole, we want to show that \(gx\) is a pole. Since \(x\) is a pole, then we know that \(|\text{Stab}_G(x)| \geq 2\). But we also know that \(\text{Stab}(gx) = g\text{Stab}(x)g^{-1}\) (By HW10, Problem 2). As a consequence, they both have the same size. Therefore, \(|\text{Stab}_G(gx)| \geq 2\). Therefore, \(gx\) is also a pole.
<br />
<br />
Now, since \(G\) acts on the set of poles \(X\), then the set \(X\) decomposes have orbits \(O_1, O_2,...,O_k \subset X\) of the \(G\) action. In the following table, we’re going to list the orbits in decreasing order of their sizes so \(|O_1| \geq |O_2| ... \geq |O_k|\). We will also define \(c_i\) to be the size of the stabilizer in \(G\) of \(x\) for any \(x \in O_i\). Because elements in the same orbit have stabilizers of the same order. We also know that \(|O| = \frac{n}{c_i}\) by the orbits stabilizer theorem.</p>
<ol>
	<li>\(Z_n\): Cyclic groups are rotations around a single axis. We have 2 poles for each direction. The poles end up being in separate orbits. So each orbit is of size 1. So the corresponding stabilizer groups are the whole groups</li>
	<!---------------------------------->
	<li>\(D_m\): \(D_m\) is acting on the set of poles here where a pole is a unit vector such that there is a non-trivial element of \(D_m\) that fixes it. The rotations in \(D_m\) are rotations around the \(z-\)axis. The unit vector along the \(z-\)axis is fixed by all rotations in \(D_m\). So this unit vector \((0,0,1)\) must be in the set of poles and its stabilizer consists of all the rotations in \(D_m\). So it has size \(m\). By the orbits-stabilizer theorem, the size of the orbit is 2. We know it's 2 because \(-u\) is also a pole.
	<br /><br />
	For the reflections, we know from assignment 10 that they are in one orbit. For the flips, these are axes in the \(xy\) plane that we flip around. Each flip, will have two poles associated to them. They live in orbits each of size 2. Notice here that the orbits will be of size \(m\).</li>
	<!---------------------------------->
	<li>\(A_4\): For the regular polyhedra, we also have 3 orbits in each. For the tetrahedron, the orbits are of size 6 (midpoint of edges), 4 (centroid of faces) and 4 (vertex-vertex). We can divide by the $$|G|$$ to get the sizes for the stabilizer. Note here the the number of poles is basically the sum of the orbit sizes since the action breaks the set into disjoint orbits.</li>
</ol>

<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>Groups \(G\)</td>
    <td>\(n = |G|\)</td>
    <td>\(k = \) orbits</td>
    <td>\(|O_1| \geq ... \geq |O_k|\)</td>
    <td>\(c_i \leq ... \leq c_k\)</td>
  </tr>
  <tr>
	<td>\(\{I\}\)</td>
	<td>1</td>
	<td>1</td>
	<td>0</td>
	<td></td>
  </tr>
  <tr>
	<td>\(\mathbf{Z}_n, n \geq 2\)</td>
	<td>\(n\)</td>
	<td>2</td>
	<td>1,1</td>
	<td>\(n,n\)</td>
  </tr>
  <tr>
	<td>\(D_m, m \geq 2\)</td>
	<td>\(2m\)</td>
	<td>3</td>
	<td>\(m,m,2\)</td>
	<td>\(2,2,m\)</td>
  </tr>
  <tr>
	<td>\(A_4\)</td>
	<td>\(12\)</td>
	<td>3</td>
	<td>\(6,4,2\)</td>
	<td>\(2,3,3\)</td>
  </tr>
  <tr>
	<td>\(S_4\)</td>
	<td>\(24\)</td>
	<td>3</td>
	<td>\(12,8,6\)</td>
	<td>\(2,3,4\)</td>
  </tr>
  <tr>
	<td>\(A_5\)</td>
	<td>\(60\)</td>
	<td>3</td>
	<td>\(30,20,12\)</td>
	<td>\(2,3,5\)</td>
  </tr>
<!-------------------->
</table>
<p>So now we want to know if there is another subgroup \(G\) missing from the table. Let’s use the burnside theorem. We have an action by \(G \leq SO(3)\) on the set of poles \(X\). We don’t know what \(G\) is. The burnside theorem says that number of orbits of this action is</p>
<div>
$$
\begin{align*}
\frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)|
\end{align*}
$$
</div>
<p>where \(\text{Fix}(g) = \{x \in X \ | \ gx = x \} \subseteq X\) is the set of elements fixed by \(g\). We’ve calculate the number of orbits in the above table. What about \(\text{Fix}(e)\)? It’s everything in \(X\) so</p>
<div>
$$
\begin{align*}
\text{Fix}(e) = |G| = |O_1| + |O_2| + ... + |O_k|
\end{align*}
$$
</div>
<p>What about any other element in \(G\)? It’s not the identity element so it’s a rotation around some axis. Therefore, it fixes two unit vectors along the axis \(\pm u\) where \(g = Rot_u(\theta)\). In particular, \(\text{Fix}(g) = 2\). So now let’s apply the formula to see that</p>
<div>
$$
\begin{align*}
k = \frac{1}{n}\big[ |O_1| + ... + |O_k|  + 2(n-1) \big]
\end{align*}
$$
</div>
<p>We can simplify this because we know that \(|O_i| = \frac{n}{c_i}\). Therefore</p>
<div>
$$
\begin{align*}
k &amp;= \frac{1}{n}\big[ \frac{n}{c_1} + ... + \frac{n}{c_k} + 2(n-1) \big] \\
&amp;= c_1 + ... + c_k + 2 - \frac{2}{n}. 
\end{align*}
$$
</div>
<p>We know \(k \geq 0\). We know \(n \geq 1\). We also know that \(c_1,...,c_k\) are integers and they’re all greater than 2. Finally, \(c_i \ | \ n\) because each \(c_i\) is a subgroup. With these constraints, the only solutions are the rows in the table we constructed!!! the is the main step in classifying the finite subgroups in \(SO(3)\).
<br />
<br />
So how do we solve it? For example, say we know we have two orbits each of size 1, so we have two poles. This means that they must be opposite to each other since poles come in pairs. Therefore, everything is rotation around a single axis. Therefore, it must be cyclic. 
<br />
<br />
Step 1: We will show that we can only have 0, 2 or 3 orbits so \(k \in \{0,2,3\}\). Re-write the equations such that</p>
<div>
$$
\begin{align*}
k &amp;= \frac{1}{n}\big[ \frac{n}{c_1} + ... + \frac{n}{c_k} + 2(n-1) \big] \\
&amp;= c_1 + ... + c_k + 2 - \frac{2}{n} \\
 (1 - \frac{1}{c_i}) + ... + (1 - \frac{1}{c_k}) &amp;= 2 - \frac{2}{n}.
\end{align*}
$$
</div>
<p>The observation is that \(\frac{2}{n}\) can be between 0 and less than 2. The right hand side, each \(1 - \frac{1}{c}\) needs to be in \([\frac{1}{2},1)\). So \(k\) can’t be 1. If \(k = 4\), then the left hand side is greater than 2 but the right hand side must be less than 2. So \(k\) can’t be 4.
<br />
<br />
Step 2: If \(k = 0\), this gives us \(n = 1\). That’s the trivial group.
<br />
<br />
Step 3: Re-write the equation so that</p>
<div>
$$
\begin{align*}
\frac{2}{n} = \frac{1}{c_1} + \frac{1}{c_2}
\end{align*}
$$
</div>
<p>This shows …. [TODO .. I’m lost now]
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Today, we’ll analyze finite subgroups of \(SO(3)\). Let’s describe the ones we already know about The trivial subgroup \(\{I\}\). Subgroups that are cyclic so these subgroups are isomorphic to \(\mathbf{Z}_n\). For each \(n\), we have infinitely many subgroups. For example, there are infinitely many cyclic subgroups of order 3 in \(SO(3)\). Pick any axis and consider the rotation by angle \(\frac{2\pi}{3}\) around this axis. This makes a subgroup of order 3. So you can get a new subgroup for whatever axis you pick. Therefore, we have infinitely many. In fact, all these subgroups that are rotations by \(\frac{2\pi}{3}\) are conjugate (see the lecture before last. If we have a rotation by \(\theta\) around a unit vector \(v\) and we have the same rotation around another unit vector \(u\), then we change \(u\) to \(v\) by some rotation \(A \in SO(3)\). Subgroups isomorphic to the dihedral groups \(D_n\) where \(n \geq 2\). These are symmetries of a regular \(n-\)gon. Pick any \(n-\) and fix one of the vertices on the \(x-\)axis. The symmetries of the \(n-\) is a group isomorphic to \(D_n\). Symmetry group of the tetrahedron which is isomorphic to \(A_4\). Symmetry group of the cube/octahedron which is isomorphic to \(S_4\). Symmetry group of the icosahedron/dodecahedron which is isomorphic to \(A_5\). In fact we have the following theorem Theorem Every finite subgroup of \(SO(3)\) is in the list above. Proof \(SO(3)\) acts on \(\mathbf{R}^3\) (we multiply a matrix in \(SO(3)\) by a vector in \(\mathbf{R}^3\)). So now suppose that \(G\) is a subgroup of \(SO(3)\). \(G\) acts on \(\mathbf{R}^3\). We’ll focus on the pole subset in \(\mathbf{R}^3\). What is a poll? A pole for \(G \subseteq SO(3)\) is a unit vector \((u) \in \mathbf{R}^3\) where \(\lVert u \rVert = 1\) such that \(|Stab_G(u)|\geq 2\). In other words, there exists an element \(g\) in \(\text{Stab}_G(u)\) other than the identity such that \(gu = u\). If \(g \neq I\), then \(g = Rot_u(\theta)\) where \(\theta\) is not a multiple of \(2\pi\). Recall, that a rotation in space only fixes vectors along the axis of rotation. But since we constrained this to unit vectors, then it will only fix the unit vector on the axis of rotation. So \(g\) fixes only \(\pm u\). As a consequence, poles come in pairs. If \(u\) is a pole, then \(-u\) is a pole too. So again, a pole is a unit vector in \(\mathbf{R}^3\) that gets fixed by some rotation in \(G\). In other words, \(u\) is a unit vector along the axis of some rotation in \(G\). As a consequence, every non-identity element in \(G\) can contribute 2 poles to the set of poles. If we let \(X\) to be the set of poles, then $$ \begin{align*} |X| \leq 2(|G| - 1) \quad \text{(-1 for the identity)} \end{align*} $$ So the set of poles \(X\) is the collection of unit vectors along the axes of rotations of elements in the group \(G\). \(G\) acts on the subset \(X \in \mathbf{R}^3\). For example, if \(g \in G\) and \(x \in X\), then we know that \(gx\) and \(g^{-1}x\) are both in \(X\). Why? If \(x\) is a pole, we want to show that \(gx\) is a pole. Since \(x\) is a pole, then we know that \(|\text{Stab}_G(x)| \geq 2\). But we also know that \(\text{Stab}(gx) = g\text{Stab}(x)g^{-1}\) (By HW10, Problem 2). As a consequence, they both have the same size. Therefore, \(|\text{Stab}_G(gx)| \geq 2\). Therefore, \(gx\) is also a pole. Now, since \(G\) acts on the set of poles \(X\), then the set \(X\) decomposes have orbits \(O_1, O_2,...,O_k \subset X\) of the \(G\) action. In the following table, we’re going to list the orbits in decreasing order of their sizes so \(|O_1| \geq |O_2| ... \geq |O_k|\). We will also define \(c_i\) to be the size of the stabilizer in \(G\) of \(x\) for any \(x \in O_i\). Because elements in the same orbit have stabilizers of the same order. We also know that \(|O| = \frac{n}{c_i}\) by the orbits stabilizer theorem. \(Z_n\): Cyclic groups are rotations around a single axis. We have 2 poles for each direction. The poles end up being in separate orbits. So each orbit is of size 1. So the corresponding stabilizer groups are the whole groups \(D_m\): \(D_m\) is acting on the set of poles here where a pole is a unit vector such that there is a non-trivial element of \(D_m\) that fixes it. The rotations in \(D_m\) are rotations around the \(z-\)axis. The unit vector along the \(z-\)axis is fixed by all rotations in \(D_m\). So this unit vector \((0,0,1)\) must be in the set of poles and its stabilizer consists of all the rotations in \(D_m\). So it has size \(m\). By the orbits-stabilizer theorem, the size of the orbit is 2. We know it's 2 because \(-u\) is also a pole. For the reflections, we know from assignment 10 that they are in one orbit. For the flips, these are axes in the \(xy\) plane that we flip around. Each flip, will have two poles associated to them. They live in orbits each of size 2. Notice here that the orbits will be of size \(m\). \(A_4\): For the regular polyhedra, we also have 3 orbits in each. For the tetrahedron, the orbits are of size 6 (midpoint of edges), 4 (centroid of faces) and 4 (vertex-vertex). We can divide by the $$|G|$$ to get the sizes for the stabilizer. Note here the the number of poles is basically the sum of the orbit sizes since the action breaks the set into disjoint orbits.]]></summary></entry><entry><title type="html">Lecture 29: Burnside Formula</title><link href="http://localhost:4000/jekyll/update/2025/02/21/math417-29-burnside-formula.html" rel="alternate" type="text/html" title="Lecture 29: Burnside Formula" /><published>2025-02-21T00:01:36-08:00</published><updated>2025-02-21T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/21/math417-29-burnside-formula</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/21/math417-29-burnside-formula.html"><![CDATA[<p>Today we will discuss a formula for counting orbits calling the burnside formula. The book illustrates this with a necklace beads counting example. So how many necklaces are there with two orange, two blue beads? If we count naively, we have 4 slots. The two blue beads will go into two of the slots. So there are \(\binom{4}{2} = 6\) to do this. Then the orange beads will get the remaining two spots. So</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/1.png" width="90%" class="center" /></p>
<div>
$$
\begin{align*}
X = \{OOBB, OBOB, OBBO, BOBO, BOOB, BBOO\}
\end{align*}
$$
</div>
<p>But that’s not right. There are only 2 distinct elements. This in fact is the same as counting orbits of a group action. 2 here is the number of orbits of a group action. The group action is</p>
<div>
$$
\begin{align*}
G = \{ D_4 \}, |G| = 8 \quad \text{acts on } X = \text{ The set of linear arrangements where $|X|=6$}
\end{align*}
$$
</div>
<p>To see this, imagine the beads moving by 90 degrees (which is the rotation \(r\)) to get the next shape in the picture. The symmetries of the beads are the same as the symmetries of the dihedral group. We can also flip the necklace on the \(x\)-axis which is the same as the flip \(j \in D_4\). Now, the orbits of this action are as follows</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/2.png" width="90%" class="center" /></p>
<div>
$$
\begin{align*}
A &amp;= \{OOBB, OBBO, BOOB, BBOO\} \\
B &amp;= \{OBOB, BOBO\}
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>How many necklaces with 9 beads: 4 red, 3 white, 2 yellow? This isn’t an easy question. But there is an easier question which is counting the linear arrangements of the 9 beads with the colors we have. Let \(X\) be the set of linear arrangements of 9 beads with 4 red, 3 while, 2 yellow. If they were all of different colors, then there will be \(9!\) arrangements. But only have 3 colors so some of the beads are indistinguishable. Therefore, we have</p>
<div>
$$
\begin{align*}
|X| = \frac{9!}{4!3!2!} = 1260.
\end{align*}
$$
</div>
<p>So it’s impossible to physically see which arrangements will unique. We said earlier that this is equivalent to having an action by \(D_9\) on the set \(X\). So we just to count the orbits of this action with something called the burnside formula.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Burnside Formula</b></h4>
<p>First, we need the following notation</p>
<div class="mintheaderdiv">
Action by \(G\) on \(X\)
</div>
<div class="mintbodydiv">
Given \(g \in G\), let \(\text{Fix}(g) = \{x \in X \ | \ gx = x \} \subseteq X \) be the set of elements fixed by \(g\).
</div>
<!----------------------------------------------------------------------------->
<p><br />
For example \(Fix(e) = X\). Now,
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Burnside Formula
</div>
<div class="peachbodydiv">
If \(G\) acts on a set \(X\), and \(|G| &lt; \infty, |X| &lt; \infty\). Then, the number of orbits is
$$
\begin{align*}
\frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)|
\end{align*}
$$
</div>
<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>Let’s apply the formula on Example 1. We had \(G = D_4 = \{e,r,r^2,j,rj,r^2j\}\) where \(|G| = 8\) and \(|X| = 6\). For example, take \(r\). We want to know the arrangements that are fixed by \(r\) (the rotation by 90 degrees). What does it mean for an arrangement to be fixed by \(r\). It means that the arrangement when rotated by 90 degrees, it will still be the same arrangement. Observe what happens to the following arrangement.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/3.png" width="55%" class="center" /></p>
<p>This arrangement is not fixed by \(r\). In fact, all the beads have to have the same color for \(r\) to be able to fix them. So the fix set is zero for \(r\). Note here that \(r^{-1} = r^{3}\) will have the same result for the same reason.
<br />
<br />
What about \(r^2\) so a rotation by 180 degrees. Observe what happens to the following arrangement</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/4.png" width="55%" class="center" /></p>
<p>So the beads on the opposite sides have to have the same color for the arrangement to work. We have two arrangements exactly of this kind. So \(|Fix(r^2)| = 2\)
<br />
<br />
What about \(j\)? Let \(j\) be the flip across the \(x\)-axis. Here, for the arrangement to be fixed, notice that the beads on the axis will not change. So we just need the top and bottom beads to have the same color. The number of arrangements with this condition is 2. In fact, \(r^2j\) (flip around the \(y\)-axis) will have the same number of arrangement fixed.
<br />
<br />
\(rj\) is a flip around the axis \(x=y\). So the beads on either side of the following axis must have the same color just like following picture</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/5.png" width="25%" class="center" /></p>

<p>In summary,</p>
<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>\(g\)</td>
    <td>\(|\text{Fix}(g)|\)</td>
  </tr>
  <tr>
	<td>\(e\)</td>
	<td>6</td>
  </tr>
  <tr>
	<td>\(r, r^3\)</td>
	<td>0</td>
  </tr>
  <tr>
    <td>\(j, r^2j\)</td>
	<td>2</td>
  </tr>
  <tr>
	<td>\(rj, r^3j\)</td>
	<td>2</td>
  </tr>
</table>
<p>So now let’s apply the burnside formula</p>
<div>
$$
\begin{align*}
\text{number of orbits } &amp;= \frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)| \\
                         &amp;= \frac{1}{8}(6+0+2+0+2+2+2+2) \\
						 &amp;= \frac{16}{8} = 2.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Here, we had \(G = D_9 = \{e, r, r^2, ..., r^8, j, rj, ..., r^8j\}\). Recall that \(|X| = 1260\). Doing the same analysis</p>
<ul>
	<li>\(e\): This is just the entire group so \(|\text{Fix}(e)| = 1260\).</li>
	<li>\(r\): Like before, we can't fix anything with a 90 degrees rotation. All the beads have to have the same color. This is the same for its inverse \(r^8\).</li>
	<li>\(r^2\): Note that \(r^2\) has order 9 as well. No possible arrangements here as well. In fact, the same thing happens with \(r^4\) and its inverse \(r^5\).</li>
	<li>\(r^3\): We have 9 beads. Here we need every third bead to match colors. So like in the following picture
	<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/6.png" width="45%" class="center" /></p>
	So vertices 1,4 and 7 have to have the same color. Vertices 2, 5 and 8 have to have the same color. Same for vertices 3, 6 and 9. But we recall that we have 4 red beads, 3 white beads, and 2 yellow beads.
	</li>
	<li>\(j\): A flip around the \(x\)-axis. So now there is a black bead fixed by the access itself and the remaining beads must match as follows
	<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec29/7.png" width="45%" class="center" /></p>
	We have 4 red beads, 3 white beads and 2 yellow. So the fixed bead must be a white bead. So now we have the remaining two whites. We need to put one white bead in one of the 4 slots (say in the bottom). We have 4 slots so 4 choices for the second white bead. Next, if we place the yellow bead, we'll have 3 slots available to choose from. Lastly, we have 2 red beads so they must go in the remaining slots. So 12 choices total.
	</li>
</ul>

<p><br /></p>
<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>\(g\)</td>
    <td>\(|\text{Fix}(g)|\)</td>
  </tr>
  <tr>
	<td>\(e\)</td>
	<td>1260</td>
  </tr>
  <tr>
	<td>\(r, r^8\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^2, r^7\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^3, r^6\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^4, r^5\)</td>
	<td>0</td>
  </tr>
  <!-------flips--------->
  <tr>
    <td>\(j, r^2j,...,r^8j\)</td>
	<td>12</td>
  </tr>
</table>
<p>So now let’s apply the burnside formula</p>
<div>
$$
\begin{align*}
\text{number of orbits } &amp;= \frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)| \\
                         &amp;= \frac{1}{18}(1260+9(12)) \\
						 &amp;= \frac{1374}{18} = 76.
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Proof of Burnside Formula</b></h4>
<p>Let \(F = \{(g,x) \ | \ g \in G, x \in X, gx = x\}\). (So we’re writing it differently than before but it’s the same. We’re just counting any element that gets fixed by any \(g\). So all of them). We’re going to count this set in two different ways</p>
<ol>
	<li>\(|F| = \sum_{g \in G} | x \in X \ | \ gx = x \} = \sum_{g \in G} |\text{Fix}(g)| \)</li>
	<li>\(|F| = \sum_{x \in X} | g \in G \ | \ gx = X \} = \sum_{x \in X} |\text{Stab}(g)| \)</li>
</ol>
<p>From this, observe that</p>
<div>
$$
\begin{align*}
\frac{1}{|G|} \sum_{g \in G} |\text{Fix}(g)| &amp;= \frac{1}{|G|} \sum_{x \in X} |\text{Stab}(g)| \\
                                             &amp;= \sum_{x \in X} \frac{|\text{Stab}(g)|}{|G|}.
\end{align*}
$$
</div>
<p>By the Orbit-Stabilizer Theorem, \(|O(x)| = \frac{|G|}{\text{|Stab(x)}|}\). Therefore</p>
<div>
$$
\begin{align*}
\frac{1}{|G|} \sum_{g \in G} |\text{Fix}(g)| &amp;= \frac{1}{|G|} \sum_{x \in X} |\text{Stab}(g)| \\
                                             &amp;= \sum_{x \in X} \frac{|\text{Stab}(g)|}{|G|} \\
											 &amp;= \sum_{x \in X} \frac{1}{|O(x)|} \\
											 &amp;= \sum_{\text{Orbits}} \sum_{x \in O} \frac{1}{|O|} \\
											 &amp;= \text{number of orbits}
											 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example 3 (Variant of Example 2)</b></h4>
<p>Consider example 2 again. We had a fixed number of beads (4 red, 3 white, 2 yellow). Now, suppose that we don’t have a constraint on the count for each color. We just want a necklace made of 3 colors and 9 beads. Then \(G = D_9\) acts on \(X\) which is the linear arrangements of beads of 3 colors. What is the size of \(X\)? The first bead can be any of the three colors, the second bead can be any of the three colors and so on. So</p>
<div>
$$
\begin{align*}
|X| = 3^9 = 19,683								 
\end{align*}
$$
</div>

<p>Here, we had \(G = D_9 = \{e, r, r^2, ..., r^8, j, rj, ..., r^8j\}\). Recall that \(|X| = 1260\). Doing the same analysis</p>
<ul>
	<li>\(e\): This is just the entire group so \(|\text{Fix}(e)| = 1260\).
	</li>
	<li>\(r\): Like before, we can't fix anything with a 90 degrees rotation. All the beads have to have the same color. This is the same for its inverse \(r^8\) and as well as \(r^2, r^4, r^5\).
	</li>
	<li>\(r^3\): We have 9 beads. Again we need every third bead to have the same color. We have three groups. Each group will need to be of the same color and it can be any color so 3 choices for every group. Therefore, we have \(3^3 = 27\) choices.
	</li>
	<li>\(j\): A flip around the \(x\)-axis. Like before, we had a bead fixed on the axis and 4 other groups of two beads each that needed to match colors. Each of these has a choice of 3 colors so \(3^5\) choices.
	</li>
</ul>

<p><br /></p>
<table style="max-width: 400px; margin: 20px auto;">
  <tr>
    <td>\(g\)</td>
    <td>\(|\text{Fix}(g)|\)</td>
  </tr>
  <tr>
	<td>\(e\)</td>
	<td>\(3^9\)</td>
  </tr>
  <tr>
	<td>\(r, r^8, r^2, r^4, r^5\)</td>
	<td>0</td>
  </tr>
  <tr>
	<td>\(r^3, r^6\)</td>
	<td>\(3^3 = 27\)</td>
  </tr>
  <!-------flips--------->
  <tr>
    <td>\(j, r^2j,...,r^8j\)</td>
	<td>\(3^5\)</td>
  </tr>
</table>
<p>So now let’s apply the burnside formula</p>
<div>
$$
\begin{align*}
\text{number of orbits } &amp;= \frac{1}{|G|}\sum_{g \in G} |\text{Fix}(g)| \\
                         &amp;= \frac{1}{18}(3^9 + 6*3 + 2*3^2 + 9*3^5) \\
						 &amp;= \frac{1374}{18} = 1219.
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Today we will discuss a formula for counting orbits calling the burnside formula. The book illustrates this with a necklace beads counting example. So how many necklaces are there with two orange, two blue beads? If we count naively, we have 4 slots. The two blue beads will go into two of the slots. So there are \(\binom{4}{2} = 6\) to do this. Then the orange beads will get the remaining two spots. So $$ \begin{align*} X = \{OOBB, OBOB, OBBO, BOBO, BOOB, BBOO\} \end{align*} $$ But that’s not right. There are only 2 distinct elements. This in fact is the same as counting orbits of a group action. 2 here is the number of orbits of a group action. The group action is $$ \begin{align*} G = \{ D_4 \}, |G| = 8 \quad \text{acts on } X = \text{ The set of linear arrangements where $|X|=6$} \end{align*} $$ To see this, imagine the beads moving by 90 degrees (which is the rotation \(r\)) to get the next shape in the picture. The symmetries of the beads are the same as the symmetries of the dihedral group. We can also flip the necklace on the \(x\)-axis which is the same as the flip \(j \in D_4\). Now, the orbits of this action are as follows]]></summary></entry><entry><title type="html">Lecture 28: Group Actions</title><link href="http://localhost:4000/jekyll/update/2025/02/20/math417-28-group-actions.html" rel="alternate" type="text/html" title="Lecture 28: Group Actions" /><published>2025-02-20T00:01:36-08:00</published><updated>2025-02-20T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/20/math417-28-group-actions</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/20/math417-28-group-actions.html"><![CDATA[<p>In the previous lecture, we introduced actions by a group \(G\) on a set \(X\) which we defined as a homomorphism from the group to a permutation group of the set \(X\).</p>
<div>
$$
\begin{align*}
\varphi: G &amp;\rightarrow Sym(X) \\
         gx &amp;= \phi(g)(x)
\end{align*}
$$
</div>
<p>The set \(X\) that gets acted on, gets partitioned into orbits. An orbit is</p>
<div>
$$
\begin{align*}
O(x) = \{ gx \ | \ g \in G \}
\end{align*}
$$
</div>
<p>We also saw that for each element in the set \(X\) that \(G\) acted on, we have a stabilizer. A stabilizer of an element \(x\) is the set of elements in \(G\) that fix \(x\) so</p>
<div>
$$
\begin{align*}
\text{Stab}(x) = \{ g \in G \ | \ gx = x \}
\end{align*}
$$
</div>
<p>Note here that \(\text{Stab}(x)\) is a subgroup of \(G\). Finally, there are a number of relationships between orbits and stabilizer but the most important one is the orbits/stabilizer theorem which gives a bijection between the orbit of an element and the index of its stabilizer group</p>
<div>
$$
\begin{align*}
|O(x)| = [G: \text{Stab}(x)]
\end{align*}
$$
</div>
<p>We’ve also discussed a few other things like calling an action transitive if there is only one orbit.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Conjugation by \(G\) on \(X = G\)</b></h4>
<p>The next thing we talked about was the conjugation action when the set \(G\) acted on was the set \(G\) itself. This was defined as</p>
<div>
	$$
	\begin{align*}
	c: G &amp;\rightarrow Aut(G) \leq Sym(G) \\
	c(g)(x) &amp;= gxg^{-1}
	\end{align*}
	$$
</div>
<p>We defined this action last lecture from \(G\) to \(Sym(G)\) but in fact, \(c\) is a homomorphism from \(G\) to \(Aut(G)\). \(Aut(G)\) is a subgroup of \(Sym(G)\). The conjugation action is the one that’s really useful for understanding things about a group \(G\). 
<br />
<br />
The <b>orbits</b> of the conjugation action are called <b>conjugacy classes.</b></p>
<div>
	$$
	\begin{align*}
	Cl(x) = \{gxg^{-1} \ | \ g \in G\}.
	\end{align*}
	$$
</div>
<p>We also studied <b>centralizers</b> and these are the <b>centralizers</b> of the action. So</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(x) &amp;= \{g \in G \ | \ gxg^{-1} = x\} \\
	               &amp;= \{g \in G \ | \ gx= xg\} .
	\end{align*}
	$$
</div>
<p>In other words, the centralizers are those elements that commute with the element \(x\). Using the orbit/stabilizer theorem</p>
<div>
	$$
	\begin{align*}
	|Cl(x)| &amp;= [G: \text{Cent}(x)]
	\end{align*}
	$$
</div>
<p>Additionally, the kernel of \(c\) is what we call the center \(c\). It’s the collection of elements that commute with every other element in the group. So</p>
<div>
	$$
	\begin{align*}
	Ker(c) = Z(G) &amp;= \{g \in G \ | \ gh = hg \ \forall h \in G \} \\
	              &amp;= \bigcap_{x \in G} \text{Cent}(x)
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>What is the conjugacy class of the identity? It’s always \(Cl(e) = \{e\}\). This means that the conjugation action is never transitive unless we’re acting on the trivial group. Since if \(G\) contains other elements, then they’ll be in a different orbit.
<br />
<br />
Now, suppose that \(b \in Z(G)\). So \(b\) commutes with every other element in \(G\). What is the conjugacy class of \(b\). It is \(Cl(b) = \{b\}\). Why? since \(b\) commutes with every element \(g\) in \(G\), then \(Cl(b) = \{gbg^{-1} \ | \ g \in G\}\) but</p>
<div>
	$$
	\begin{align*}
	gbg^{-1} = gg^{-1}b = b
	\end{align*}
	$$
</div>
<p>So \(Cl\) contains only \(b\). So if you’re in the center, then the conjugacy class contains only you. The other way direction is true. If \(Cl(g) = \{g\}\), \(g \in Z(G)\). 
<br />
<br />
Consequence: If \(G\) is abelian, then \(Z(G) = G\) and every conjugacy class in \(G\) in its own class. So \(Cl(g) = \{g\}\) for all \(g \in G\). What about the stabilizer or Center\((g)\)? It’s also all of \(G\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Conjugacy Classes in \(SO(3)\)</b></h4>
<p>Let \(G = SO(3) = \{Rot_u(\theta)\}\) where \(u\) is a unit vector. \(Rot_u(\theta)\) is a rotation by angle \(\theta\) around the unit vector \(u\). Recall</p>
<div>
	$$
	\begin{align*}
	 Rot_u(\theta + 2\pi n) &amp;= Rot_u(\theta), n \in \mathbf{Z} \\
	 Rot_{-u}(\theta) &amp;= Rot_{u}(-\theta) \\
	 Rot_{u}(0) &amp;= I
	\end{align*}
	$$
</div>

<p>How do we conjugate elements in this group? Recall from lecture 2? that if \(A \in SO(3)\), then</p>
<div>
	$$
	\begin{align*}
	ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)
	\end{align*}
	$$
</div>
<p>So if we conjugate the rotation by matrix \(A\), we get another rotation but now it’s about the axis \(Au\) instead of \(u\). This is the conjugation formula for elements in \(SO(3)\).
<br />
<br />
So now, what are the conjugacy classes of \(SO(3)\)</p>
<div>
	$$
	\begin{align*}
	Cl(Rot_u(\theta)) &amp;= \{A Rot_u(\theta)A^{-1} \ | \ A \in SO(3)\} \\
	                  &amp;= \{A Rot_{Au}(\theta) \ | \ A \in SO(3)\} \\
	\end{align*}
	$$
</div>
<p>For example. Take the identity mattress. What is the conjugacy class of the identity element?</p>
<div>
	$$
	\begin{align*}
	Cl(Rot_u(0)) = Cl(I) &amp;= I
	\end{align*}
	$$
</div>
<p>What about a non-identity matrix? so \(\theta\) is not zero. Fix a unit vector in space. What vectors do we get if we act on it by \(A\)? Suppose we limit \(\theta\) to \((0, 2\pi)\). Then</p>
<div>
	$$
	\begin{align*}
	Cl(Rot_u(0)) = \{Rot_v(\theta), v \in \mathbf{R}, \lVert v \rVert = 1 \}
	\end{align*}
	$$
</div>
<p>This means that we get all the rotations with the same \(\theta\) but the axis is changing to a different unit vector. 
<br />
<br />
Note: Both \(Rot_u(\theta)\) and \(Rot_u(-\theta)\) are in the same conjugacy class. For example, \(Rot_{e_3}(-120) = Rot_{-e_3}(120)\). The exception is 180 degrees because \(Rot_u(\pi) = Rot_u(-\pi)\). [Side note: Given two rotations \(Rot_{u_1}(\theta)\) and \(Rot_{u_2}(\theta)\) that have the same angle but different axes. They are in the same conjugacy class if \(u_1\) can be rotated into \(u_2\) by an element in \(SO(3)\)].
<br />
<br />
The centralizer of the identity is any element that commutes with the identity element. So it’s easy to see that \(\text{Cent}(x) = SO(3)\). What about the centralizer of \(Rot_u(\theta)\) where \(\theta \in (0,2\pi)\). We’re looking for elements such that \(ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)\).</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(Rot_u(\theta)) = \{A = Rot_u(\alpha) \ | \ \alpha \in \mathbf{R}\} 
	\end{align*}
	$$
</div>
<p>This is because rotations around the same axis commute with each other. There is exception for  \(\pi = 180\) because in addition to all the arbitrary rotations around the same axis, we also get another set which is the 180 rotations around any vector such that \(u \cdot v = 0\).</p>
<div>
	$$
	\begin{align*}
	\text{Cent}(Rot_u(\theta) = \{A = Rot_u(\alpha) \ | \ \alpha \in \mathbf{R}\} \cup \{Rot_v(\pi) \ | \ \text{any } v \text{ such that } v \cdot u = 0\}.
	\end{align*}
	$$
</div>
<p>Study notes since I was confused here. The idea is that for any rotation in \(SO(3)\), we have a conjugation formula that states \(ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)\). This means that rotation about the axis \(u\) by \(\theta\) and then rotating by \(A\) is the same as rotating about the axis \(Au\) by the same angle. This tells us that two rotations in \(SO(3)\) are conjugate if it’s the same angle and if they can be related by some matrix \(A\). So now the questions is? given some unit vector \(u\)? and given a rotation around \(u\) by angle \(\theta\). Are we guaranteed to find another vector \(v\) such that \(ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)\)? The answer is yes for \(SO(3)\) specifically. For any two unit vectors \(u\) and \(v\), there exists an \(A\) such that \(Au = v\). Therefore, if we keep the same angle, then all the rotations by another other vector \(v\) around the same angle are in the same conjugacy class as the rotation around the original vector by \(\theta\).
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Symmetries of the Cube \(\leq SO(3)\)</b></h4>
<p>As a reminder, we did this in the previous lecture</p>
<div>
<table style="margin: 20px auto;">
  <tr>
    <td>Axis (Cube)</td>
	<td># of Axes</td>
    <td>Angle of Rotation</td>
	<td>Order of Symmetry</td>
	<td># of Symmetries of This Type</td>
    <td>Axis (Octahedron)</td>
	<td>Cycle Type</td>
  </tr>
  <tr>
    <td>Vertex/Vertex</td>
	<td>4</td>
    <td>\(\pm \frac{2\pi}{3}\)</td>
    <td>3</td>
	<td>8</td>
	<td>Faces/Face</td>
	<td>3-cycle</td>
  </tr>
  <tr>
    <td>Edge/Edge</td>
	<td>6</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>6</td>
	<td>Edge/Edge</td>
	<td>2-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>3</td>
    <td>\(\pm \frac{2\pi}{4}\)</td>
    <td>4</td>
	<td>6</td>
    <td>Vertex/Vertex</td>
	<td>4-cycle</td>
  </tr>
  <tr>
    <td>Face/Face</td>
	<td>3</td>
    <td>\(\frac{2\pi}{2}\)</td>
    <td>2</td>
	<td>3</td>
	<td></td>
	<td>2+2</td>
  </tr>
  <tr>
    <td>Identity Rotation</td>
    <td></td>
    <td></td>
	<td>1</td>
	<td>1</td>
	<td></td>
	<td>e</td>
  </tr>
</table>
</div>
<p>For example, for the diagonals between vertices. We have 4 diagonals but for each diagonal we could rotate by \(\frac{2\pi}{3}\) or \(-\frac{2\pi}{3}\). So we have 8 symmetries of that kind. We also have the axes that goes through opposite edges. Those have to be of 180 degrees rotations. We have 6 of them. Finally we have the ones that goes through the faces. We have two types of these. The quarter turns which can go \(\frac{\pi}{2}\) in either clockwise or counter clockwise and then we have the half turn rotations which are 180 degrees. Since we have 6 faces, then we have 3 axes. For the quarter turns, we have (2*3) symmetries and for the half turns, we have \(3\) symmetries.
<br />
<br />
In fact, these are exactly the conjugacy classes of the cube (5 classes). But we said earlier that rotations with the same angle but different axis are all going to be in the same conjugacy class in \(SO(3)\). So why do we have for example, two classes that have rotations by 180 degrees here (look at the edge/edge and face/face). While they are conjugate in \(SO(3)\), they are not conjugate in \(G\) (cube). Reminder, the conjugation formula is</p>
<div>
	$$
	\begin{align*}
	ARot_u(\theta)A^{-1} = Rot_{Au}(\theta)
	\end{align*}
	$$
</div>
<p>What we want is an \(A\) such that it sends \(u\) to \(Au\). We don’t have that in the cube.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Conjugacy Classes of \(\S_n\)</b></h4>
<p>We have formula already for this</p>
<div>
	$$
	\begin{align*}
	g(a_1 \ a_2 \ ... \ g_k)g^{-1} = (g(a_1) \ g(a_2) \ ... \ g(a_k))
	\end{align*}
	$$
</div>
<p>So we get another \(k-\)cycle.
<br />
<br />
Example: suppose $\sigma = (1 \ 2 \ 3)(4 \ 5)\(and we want to conjugate by\)g = (1 \ 5 \ 2 \ 4 \ 6)$$. Then</p>
<div>
	$$
	\begin{align*}
	g(1 \ 2 \ 3)(4 \ 5)g^{-1} &amp;= g(1 \ 2 \ 3)g^{-1 } \circ g(4 \ 5)g^{-1} \\
	                          &amp;= (g(1) \ g(2) \ g(3)) \circ (g(4) \ g(5)) \\
							  &amp;= (5 \ 4 \ 3)(6 \ 2)
	\end{align*}
	$$
</div>
<p>Observation: If we start with a product of disjoint cycles and we conjugate by some element, then we get the same exact cycle type (a product of the same cycle type). Conjugation preserves the cycle type. Conversely if we have two products of the same cycle type, we can find some \(g\) takes us from one one element to the other. This leads to the following fact</p>
<div class="peachheaderdiv">
Fact
</div>
<div class="peachbodydiv">
The conjugacy class of an element \(g\) is all elements of the same cycle type of \(g\).
</div>
<p><br />
For example, consider \(S_4\). Take the cycle type \((a \ b)\). There are \(\binom{4}{2} = 6\) choices for the two elements. We can arrange them in \(2!\) but we need to divide by 2 since \((a \ b) = (b \ a)\). Therefore, we have \(6 * 2! / 2 = 6\) distinct permutations. We can consider \((a \ b \ c)\) next. We have 4 choices for the elements but we need to multiply by \(3!\) and divide by 3 to get 8. For \((a \ b)(c \ d)\). We have \(binom{4}{2}\) for the first cycle but then we need to multiply by \(2!\) and divide by 2. So we get 6 elements. For the second cycle, we have \(\binom{2}{2}*2!/2 = 1\) element. But then we also need to divide by \(2\) since \((a \ b)(c \ d) = (c \ d)(a \ b)\). So the total is \(6 * 1 / 2 = 3\).
<br /></p>
<div>
<table style="margin: 20px auto;">
  <tr>
    <td>Cycle</td>
	<td>Number</td>
    <td>Cycle Type</td>
  </tr>
  <tr>
    <td>\((a \ b \ c)\)</td>
	<td>8</td>
    <td>\(3+1\)</td>
  </tr>
  <tr>
    <td>\((a \ b)\)</td>
	<td>6</td>
    <td>\(2+1+1\)</td>
  </tr>
  <tr>
    <td>\((a \ b \ c \ d)\)</td>
	<td>6</td>
    <td>\(4\)</td>
  </tr>
  <tr>
    <td>\((a \ b)(c \ d)\)</td>
	<td>3</td>
    <td>\(2+2\)</td>
  </tr>
  <tr>
    <td>Identity</td>
    <td>1</td>
    <td>\(1 + 1 + 1 + 1\)</td>
  </tr>
</table>
</div>
<p>In fact this group is isomorphic to the symmetry group of the cube that we did earlier.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In the previous lecture, we introduced actions by a group \(G\) on a set \(X\) which we defined as a homomorphism from the group to a permutation group of the set \(X\). $$ \begin{align*} \varphi: G &amp;\rightarrow Sym(X) \\ gx &amp;= \phi(g)(x) \end{align*} $$ The set \(X\) that gets acted on, gets partitioned into orbits. An orbit is $$ \begin{align*} O(x) = \{ gx \ | \ g \in G \} \end{align*} $$ We also saw that for each element in the set \(X\) that \(G\) acted on, we have a stabilizer. A stabilizer of an element \(x\) is the set of elements in \(G\) that fix \(x\) so $$ \begin{align*} \text{Stab}(x) = \{ g \in G \ | \ gx = x \} \end{align*} $$ Note here that \(\text{Stab}(x)\) is a subgroup of \(G\). Finally, there are a number of relationships between orbits and stabilizer but the most important one is the orbits/stabilizer theorem which gives a bijection between the orbit of an element and the index of its stabilizer group $$ \begin{align*} |O(x)| = [G: \text{Stab}(x)] \end{align*} $$ We’ve also discussed a few other things like calling an action transitive if there is only one orbit. Conjugation by \(G\) on \(X = G\) The next thing we talked about was the conjugation action when the set \(G\) acted on was the set \(G\) itself. This was defined as $$ \begin{align*} c: G &amp;\rightarrow Aut(G) \leq Sym(G) \\ c(g)(x) &amp;= gxg^{-1} \end{align*} $$ We defined this action last lecture from \(G\) to \(Sym(G)\) but in fact, \(c\) is a homomorphism from \(G\) to \(Aut(G)\). \(Aut(G)\) is a subgroup of \(Sym(G)\). The conjugation action is the one that’s really useful for understanding things about a group \(G\). The orbits of the conjugation action are called conjugacy classes. $$ \begin{align*} Cl(x) = \{gxg^{-1} \ | \ g \in G\}. \end{align*} $$ We also studied centralizers and these are the centralizers of the action. So $$ \begin{align*} \text{Cent}(x) &amp;= \{g \in G \ | \ gxg^{-1} = x\} \\ &amp;= \{g \in G \ | \ gx= xg\} . \end{align*} $$ In other words, the centralizers are those elements that commute with the element \(x\). Using the orbit/stabilizer theorem $$ \begin{align*} |Cl(x)| &amp;= [G: \text{Cent}(x)] \end{align*} $$ Additionally, the kernel of \(c\) is what we call the center \(c\). It’s the collection of elements that commute with every other element in the group. So $$ \begin{align*} Ker(c) = Z(G) &amp;= \{g \in G \ | \ gh = hg \ \forall h \in G \} \\ &amp;= \bigcap_{x \in G} \text{Cent}(x) \end{align*} $$ Examples What is the conjugacy class of the identity? It’s always \(Cl(e) = \{e\}\). This means that the conjugation action is never transitive unless we’re acting on the trivial group. Since if \(G\) contains other elements, then they’ll be in a different orbit. Now, suppose that \(b \in Z(G)\). So \(b\) commutes with every other element in \(G\). What is the conjugacy class of \(b\). It is \(Cl(b) = \{b\}\). Why? since \(b\) commutes with every element \(g\) in \(G\), then \(Cl(b) = \{gbg^{-1} \ | \ g \in G\}\) but $$ \begin{align*} gbg^{-1} = gg^{-1}b = b \end{align*} $$ So \(Cl\) contains only \(b\). So if you’re in the center, then the conjugacy class contains only you. The other way direction is true. If \(Cl(g) = \{g\}\), \(g \in Z(G)\). Consequence: If \(G\) is abelian, then \(Z(G) = G\) and every conjugacy class in \(G\) in its own class. So \(Cl(g) = \{g\}\) for all \(g \in G\). What about the stabilizer or Center\((g)\)? It’s also all of \(G\). Conjugacy Classes in \(SO(3)\) Let \(G = SO(3) = \{Rot_u(\theta)\}\) where \(u\) is a unit vector. \(Rot_u(\theta)\) is a rotation by angle \(\theta\) around the unit vector \(u\). Recall $$ \begin{align*} Rot_u(\theta + 2\pi n) &amp;= Rot_u(\theta), n \in \mathbf{Z} \\ Rot_{-u}(\theta) &amp;= Rot_{u}(-\theta) \\ Rot_{u}(0) &amp;= I \end{align*} $$]]></summary></entry></feed>