<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-29T13:23:37-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 33: Least Squares, Adjoint Maps</title><link href="http://localhost:4000/jekyll/update/2024/09/03/lec33-adjoint-maps.html" rel="alternate" type="text/html" title="Lecture 33: Least Squares, Adjoint Maps" /><published>2024-09-03T01:01:36-07:00</published><updated>2024-09-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/03/lec33-adjoint-maps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/03/lec33-adjoint-maps.html"><![CDATA[<p>We’re going to use the projection we studied in the previous lecture in studying systems of linear equations that are inconsistent. So consider the system of equations</p>
<div>
$$
\begin{align*}
Ax = b	 
\end{align*}
$$
</div>
<p>and suppose that this system is inconsistent. This means that \(b\) can’t be written as \(Ax\) and so \(b \not\in \{Ax \ | \ x \in \mathbf{R}^n\}\). What is \(Ax\)? This is the range of the linear operator \(L_A\), the set of images when \(L_A\) is applied on \(x\) so \(R(L_A)\). But this is also the column space of \(A\) and so</p>
<div>
$$
\begin{align*}
b \not\in \{Ax \ | \ x \in \mathbf{R}^n\} = R(L_A) = Col(A)
\end{align*}
$$
</div>
<p>From theorem 2 however, we know that there is a vector in the column space of \(A\) that is closest to \(b\) (with respect to the standard inner product). The closest vector has the form \(Ax_0\) and satisfies</p>
<div>
$$
\begin{align*}
Ax_0 = proj_{Col(A)}(b)
\end{align*}
$$
</div>
<p>Note here that \(Ax_0\) is unique but \(x_0\) is not. Such an \(x_0\) is called a least squares approximate solution to \(Ax = b\). So how do we find \(x_0\)? We can just compute \(proj_{Col(A)}(b)\) directly by finding an orthonormal basis for \(Col(A)\) using Gram Schmidt but it is an intensive process. It turns out there is a alternative way to find this vector. Formally we have the following theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 3
</div>
<div class="purbdiv">
If \(Ax = b\) is inconsistent and rank\((A)=n\), then there is a unique least squares approximate solution
$$
\begin{align*}
x_0 = (A^tA)^{-1}A^tb
\end{align*}
$$
</div>
<p><br />
But why is this true? to be able to prove this theorem we need a few other definitions and lemmas first. 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Adjoint Linear Maps</b></h4>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that
$$
\begin{align*}
\langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W
\end{align*}
$$
</div>
<p><br />
Note here that the inner product on the left is the inner product of \(W\) but the one on the right is the inner product of \(V\). 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
For \(A \in M_{m \times n}(\mathbf(F)), \ \) \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\)
Set \(A^* = (\bar{A})^t\)
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>The matrix</p>
<div>
$$
\begin{align*}
\begin{pmatrix}
1 &amp; 2 &amp; i \\
i+1 &amp; 3 &amp; 3
\end{pmatrix}^*
=
\begin{pmatrix}
1 &amp; i-1 \\
2 &amp; 3  \\
-i &amp; 3
\end{pmatrix}
\end{align*}
$$
</div>
<p>Now we are ready to prove the first result that we need to prove the theorem we introduced earlier (theorem 3). 
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Lemma 1
</div>
<div class="purbdiv">
\(A \in M_{m \times n}(\mathbf{F})\), \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) 
$$
\begin{align*}
L_A: \ \mathbf{F}^n \rightarrow  \mathbf{F}^m
\end{align*}
$$
has adjoint
$$
\begin{align*}
L_{A^*}: \ \mathbf{F}^m \rightarrow  \mathbf{F}^n
\end{align*}
$$
In other words, \((L_A)^* = L_{A^*}\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof</b>
<br />
In \(\mathbf{F}^n\), we’re going to re-write the standard inner product as</p>
<div>
$$
\begin{align*}
\langle x, y \rangle &amp;= x_1 \bar{y_1} + ... + x_n \bar{y_n} \\
&amp;= \begin{pmatrix}
\bar{y_1} &amp; \cdots &amp; \bar{y_n}
\end{pmatrix}
\begin{pmatrix}
x_1 \\ 
\vdots \\ 
x_n
\end{pmatrix} \\
&amp;= (\bar{y})^t x \\
&amp;= y^*x
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p>With this observation, we are now ready to prove \((L_A)^* = L_{A^*}\). Specifically, we want to show that</p>
<div>
$$
\begin{align*}
\langle A(x), y \rangle = \langle x, A(y) \rangle \quad \forall x \in \mathbf{F}^n, y \in \mathbf{F}^m
\end{align*}
$$
</div>
<p>Expanding the right hand side:</p>
<div>
$$
\begin{align*}
\langle A(x), y \rangle &amp;= y^*(Ax) \quad \text{(by the previous observation)}\\ 
                       &amp;= (y^*A)x \\
					   &amp;= (A^* (y^*)^*)^* x \quad \text{because $(AB)^* = B^*A^*$}\\
					   &amp;= (A^*y)^* x \quad \text{because $(A^*)^* = A$}\\
					   &amp;= \langle x, A^*y \rangle \quad \blacksquare
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
This is great but we still need another result before we are ready to prove theorem 3.
<br />
<br /></p>
<div class="purdiv">
Lemma 2
</div>
<div class="purbdiv">
$$
\begin{align*}
rank(A^*A) = rank(A)
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
The dimension theorem implies that</p>
<div>
$$
\begin{align*}
\dim(N(A)) + \text{rank}(A) = n
\end{align*}
$$
</div>
<p>Applying the dimension theorem on \(A^*A\), we see that</p>
<div>
$$
\begin{align*}
\dim(N(A^*A)) + \text{rank}(A^*A) = n \quad \text{(becasuse $A^*A$ is an $n \times n$ matrix)}
\end{align*}
$$
</div>
<p>From these two equations, \(n=n\), we want to prove that \(\text{rank}(A^*A) = \text{rank}(A)\). Therefore, it suffices to show that \(N(A) = N(A^*A)\).
<br />
<br />
To show that \(N(A) = N(A^*A)\), we want to show that \(N(A) \subseteq N(A^*A)\) and \(N(A^*A) \subseteq N(A)\). But it should be clear that \(N(A) \subseteq N(A^*A)\). Why? because \(A^*A = 0\) implies that \(A = 0\). Next, we will show that \(N(A^*A) \subseteq N(A)\). So suppose that \(x \in N(A^*A)\), then this implies that</p>
<div>
$$
\begin{align*}
&amp;\implies A^*Ax = \bar{0}_{\mathbf{F}^n} \\
&amp;\implies \langle A^*Ax, x \rangle = \langle \bar{0}, x \rangle \quad \text{(take the inner product of both sides with $x$)}\\
&amp;\implies \langle A^*Ax, x \rangle = 0 \in \mathbf{F}\\
&amp;\implies \langle Ax, (A^*)^*x \rangle = 0 \quad \text{(by the definition of adjoint above)} \\
&amp;\implies \langle Ax, Ax \rangle = 0 \quad ((A^*)^* =A) \\
&amp;\implies Ax = \bar{0} \in \mathbf{F}^m \quad \text{(property (b) of the norms theorem)} \\
&amp;\implies x \in N(A) \quad \blacksquare
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Theorem 3 Proof</b></h4>
<p>We’re going to set \(\mathbf{F} = \mathbf{R}\). Therefore, \(A^* = A^t\). We are given that rank\((A)=n\). We want to show that</p>
<div>
$$
\begin{align*}
Ax_0 = \text{proj}_{Col(A)}b \ \Longleftrightarrow \ x_0 = (A^tA)^{-1}A^tb
\end{align*}
$$
</div>
<p>Which is what theorem 3 is asserting. The solution is unique and given by the above formula. One thing we immediately see is that \(\text{rank}(A) = n\) implies \(\text{rank}(A^tA) = n\) by lemma 2. This implies means that \(A^tA\) is invertible.
<br />
<br />
We also know that \(\text{proj}_{Col(A)}b\) is the unique vector \(w\) from theorem 1. In theorem 1, we asserted that every vector \(x\) (here it is \(b\)) can be decomposed into two components \(w \in W\) and \(z \in W^{\perp}\) such that \(b = w + z\). Here we have \(w = \text{proj}_{Col(A)}b = Ax_0\) but \(b - w = z\) so \(z = b -  Ax_0\) and we want \(z\) to be orthogonal to the column space of \(A\) or \((Col(A))\).</p>
<div>
$$
\begin{align*}
Ax_0 = \text{proj}_{Col(A)}b \ &amp;\Longleftrightarrow \ \langle b - Ax_0, Ax \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{(By Theorem 1)} \\
&amp; \Longleftrightarrow \ \langle A^t (b - Ax_0), x \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{move the adjoint to the other side} \\
&amp; \Longleftrightarrow \  A^t (b - Ax_0) = \bar{0} \\
&amp; \Longleftrightarrow \  A^tb - A^tAx_0 = \bar{0} \\
&amp; \Longleftrightarrow \  x_0 = (A^tA)^{-1}A^tb \quad \blacksquare \\
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[We’re going to use the projection we studied in the previous lecture in studying systems of linear equations that are inconsistent. So consider the system of equations $$ \begin{align*} Ax = b \end{align*} $$ and suppose that this system is inconsistent. This means that \(b\) can’t be written as \(Ax\) and so \(b \not\in \{Ax \ | \ x \in \mathbf{R}^n\}\). What is \(Ax\)? This is the range of the linear operator \(L_A\), the set of images when \(L_A\) is applied on \(x\) so \(R(L_A)\). But this is also the column space of \(A\) and so $$ \begin{align*} b \not\in \{Ax \ | \ x \in \mathbf{R}^n\} = R(L_A) = Col(A) \end{align*} $$ From theorem 2 however, we know that there is a vector in the column space of \(A\) that is closest to \(b\) (with respect to the standard inner product). The closest vector has the form \(Ax_0\) and satisfies $$ \begin{align*} Ax_0 = proj_{Col(A)}(b) \end{align*} $$ Note here that \(Ax_0\) is unique but \(x_0\) is not. Such an \(x_0\) is called a least squares approximate solution to \(Ax = b\). So how do we find \(x_0\)? We can just compute \(proj_{Col(A)}(b)\) directly by finding an orthonormal basis for \(Col(A)\) using Gram Schmidt but it is an intensive process. It turns out there is a alternative way to find this vector. Formally we have the following theorem Theorem 3 If \(Ax = b\) is inconsistent and rank\((A)=n\), then there is a unique least squares approximate solution $$ \begin{align*} x_0 = (A^tA)^{-1}A^tb \end{align*} $$ But why is this true? to be able to prove this theorem we need a few other definitions and lemmas first. Adjoint Linear Maps Definition Suppose \(T: V \rightarrow W\) b/w inner product spaces. An adjoint of T is a linear map \(T^*: W \rightarrow V\) such that $$ \begin{align*} \langle T(x), y \rangle = \langle x, T^*(y) \rangle \quad \forall x \in V, y \in W \end{align*} $$ Note here that the inner product on the left is the inner product of \(W\) but the one on the right is the inner product of \(V\). Definition For \(A \in M_{m \times n}(\mathbf(F)), \ \) \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) Set \(A^* = (\bar{A})^t\) Example The matrix $$ \begin{align*} \begin{pmatrix} 1 &amp; 2 &amp; i \\ i+1 &amp; 3 &amp; 3 \end{pmatrix}^* = \begin{pmatrix} 1 &amp; i-1 \\ 2 &amp; 3 \\ -i &amp; 3 \end{pmatrix} \end{align*} $$ Now we are ready to prove the first result that we need to prove the theorem we introduced earlier (theorem 3). Lemma 1 \(A \in M_{m \times n}(\mathbf{F})\), \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\) $$ \begin{align*} L_A: \ \mathbf{F}^n \rightarrow \mathbf{F}^m \end{align*} $$ has adjoint $$ \begin{align*} L_{A^*}: \ \mathbf{F}^m \rightarrow \mathbf{F}^n \end{align*} $$ In other words, \((L_A)^* = L_{A^*}\) Proof In \(\mathbf{F}^n\), we’re going to re-write the standard inner product as $$ \begin{align*} \langle x, y \rangle &amp;= x_1 \bar{y_1} + ... + x_n \bar{y_n} \\ &amp;= \begin{pmatrix} \bar{y_1} &amp; \cdots &amp; \bar{y_n} \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \\ &amp;= (\bar{y})^t x \\ &amp;= y^*x \end{align*} $$ With this observation, we are now ready to prove \((L_A)^* = L_{A^*}\). Specifically, we want to show that $$ \begin{align*} \langle A(x), y \rangle = \langle x, A(y) \rangle \quad \forall x \in \mathbf{F}^n, y \in \mathbf{F}^m \end{align*} $$ Expanding the right hand side: $$ \begin{align*} \langle A(x), y \rangle &amp;= y^*(Ax) \quad \text{(by the previous observation)}\\ &amp;= (y^*A)x \\ &amp;= (A^* (y^*)^*)^* x \quad \text{because $(AB)^* = B^*A^*$}\\ &amp;= (A^*y)^* x \quad \text{because $(A^*)^* = A$}\\ &amp;= \langle x, A^*y \rangle \quad \blacksquare \end{align*} $$ This is great but we still need another result before we are ready to prove theorem 3. Lemma 2 $$ \begin{align*} rank(A^*A) = rank(A) \end{align*} $$ Proof The dimension theorem implies that $$ \begin{align*} \dim(N(A)) + \text{rank}(A) = n \end{align*} $$ Applying the dimension theorem on \(A^*A\), we see that $$ \begin{align*} \dim(N(A^*A)) + \text{rank}(A^*A) = n \quad \text{(becasuse $A^*A$ is an $n \times n$ matrix)} \end{align*} $$ From these two equations, \(n=n\), we want to prove that \(\text{rank}(A^*A) = \text{rank}(A)\). Therefore, it suffices to show that \(N(A) = N(A^*A)\). To show that \(N(A) = N(A^*A)\), we want to show that \(N(A) \subseteq N(A^*A)\) and \(N(A^*A) \subseteq N(A)\). But it should be clear that \(N(A) \subseteq N(A^*A)\). Why? because \(A^*A = 0\) implies that \(A = 0\). Next, we will show that \(N(A^*A) \subseteq N(A)\). So suppose that \(x \in N(A^*A)\), then this implies that $$ \begin{align*} &amp;\implies A^*Ax = \bar{0}_{\mathbf{F}^n} \\ &amp;\implies \langle A^*Ax, x \rangle = \langle \bar{0}, x \rangle \quad \text{(take the inner product of both sides with $x$)}\\ &amp;\implies \langle A^*Ax, x \rangle = 0 \in \mathbf{F}\\ &amp;\implies \langle Ax, (A^*)^*x \rangle = 0 \quad \text{(by the definition of adjoint above)} \\ &amp;\implies \langle Ax, Ax \rangle = 0 \quad ((A^*)^* =A) \\ &amp;\implies Ax = \bar{0} \in \mathbf{F}^m \quad \text{(property (b) of the norms theorem)} \\ &amp;\implies x \in N(A) \quad \blacksquare \end{align*} $$ Theorem 3 Proof We’re going to set \(\mathbf{F} = \mathbf{R}\). Therefore, \(A^* = A^t\). We are given that rank\((A)=n\). We want to show that $$ \begin{align*} Ax_0 = \text{proj}_{Col(A)}b \ \Longleftrightarrow \ x_0 = (A^tA)^{-1}A^tb \end{align*} $$ Which is what theorem 3 is asserting. The solution is unique and given by the above formula. One thing we immediately see is that \(\text{rank}(A) = n\) implies \(\text{rank}(A^tA) = n\) by lemma 2. This implies means that \(A^tA\) is invertible. We also know that \(\text{proj}_{Col(A)}b\) is the unique vector \(w\) from theorem 1. In theorem 1, we asserted that every vector \(x\) (here it is \(b\)) can be decomposed into two components \(w \in W\) and \(z \in W^{\perp}\) such that \(b = w + z\). Here we have \(w = \text{proj}_{Col(A)}b = Ax_0\) but \(b - w = z\) so \(z = b - Ax_0\) and we want \(z\) to be orthogonal to the column space of \(A\) or \((Col(A))\). $$ \begin{align*} Ax_0 = \text{proj}_{Col(A)}b \ &amp;\Longleftrightarrow \ \langle b - Ax_0, Ax \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{(By Theorem 1)} \\ &amp; \Longleftrightarrow \ \langle A^t (b - Ax_0), x \rangle = 0 \ \forall x \in \mathbf{R}^n \quad \text{move the adjoint to the other side} \\ &amp; \Longleftrightarrow \ A^t (b - Ax_0) = \bar{0} \\ &amp; \Longleftrightarrow \ A^tb - A^tAx_0 = \bar{0} \\ &amp; \Longleftrightarrow \ x_0 = (A^tA)^{-1}A^tb \quad \blacksquare \\ \end{align*} $$ References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 32: Decomposition of a Vector, Orthogonal Complement</title><link href="http://localhost:4000/jekyll/update/2024/09/02/lec32-decomposition-complement.html" rel="alternate" type="text/html" title="Lecture 32: Decomposition of a Vector, Orthogonal Complement" /><published>2024-09-02T01:01:36-07:00</published><updated>2024-09-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/02/lec32-decomposition-complement</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/02/lec32-decomposition-complement.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The orthogonal complement to \(S\) is 
$$
\begin{align*}
S^{\perp} = \{x \in V \ | \ \langle x, y \rangle = 0 \forall y \in S\}
\end{align*}
$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>Suppose \(S = \{(0,0,1)\} \subset \mathbf{R}^3\). The orthogonal complement to \(S\) is any element in \(\mathbf{R}^3\) such that when we take the inner product between any element in \(S\) and any element \(\mathbf{R}^3\), the product must be zero.</p>
<div>
$$
\begin{align*}
S^{\perp} &amp;= \{(x,y,z) \in V \ | \ \langle (x,y,z), (0,0,1) \rangle = 0\} \\
          &amp;= \{(x,y,z) \in V \ | \ z = 0\} \\
          &amp;= \{(x,y,0)\} \\
\end{align*}
$$
</div>
<p>Note here if we chose \(S' = span(S)\), then the orthogonal complement will be the same! 
<br />
Exercise 1: \(S^{\perp}\) is a subspace of \(V\).
<br />
Exercise 2: If \(S\) is a subspace, then \(S \cap S^{\perp} = \{0\}\).
<br />
<br />
How to use these orthogonal complements? We have the following theorem
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem 1
</div>
<div class="purbdiv">
Suppose \(W\) is finite dimensional subspace of inner produce space \(V\). For each \(x \in V\) there is a unique \(w \in W\) and \(z \in W^{\perp}\) such that 
$$
\begin{align*}
x = w + z
\end{align*}
$$
<br />
If \(\{u_1,...,u_k\}\) is an orthonormal basis for \(W\), then \(w = \langle x, u_1 \rangle + ... + \langle x, u_k \rangle u_k\)
</div>
<!------------------------------------------------------------------------------------>
<p><br />
(TODO: Add pic)
<br />
<b>Proof</b>
<br />
We can always find \(\{u_1, ..., u_k\}\) using Gram-Schmidt. So we know \(w\) and we’re given \(x\). All we need is to verify that \(z = x - w\) and show that \(w\) and \(z\) are unique. In other words, it suffices to show for</p>
<div>
$$
\begin{align*}
w = \sum_{j=1}^{k} \langle x, u_j \rangle u_j
\end{align*}
$$
</div>
<p>that \(z = x - w\) is in \(W^{\perp}\) plus the uniqueness of this decomposition.</p>
<div>
$$
\begin{align*}
&amp;z = x - w \in W^{\perp} \\
\Leftrightarrow \ &amp;\langle z, y \rangle = 0 \ \forall y \in W
\end{align*}
$$
</div>
<p>It isn’t easy to verify this for every vector \(y \in W\). But we do have a basis for \(W\) and any \(y\) can be written as a linear combination of the basis elements. So we can instead check this for every basis element</p>
<div>
$$
\begin{align*}
\Leftrightarrow \ &amp;\langle z, u_j \rangle = 0 \ \forall j =1,...,k
\end{align*}
$$
</div>
<p>So let’s check for every basis element that it’s orthogonal to \(x - w\).</p>
<div>
$$
\begin{align*}
\Leftrightarrow \ \langle z, u_j \rangle &amp;= \langle x-w, u_j \rangle \\ 
 &amp;= \langle x - \sum_{j=1}^k \langle x, u_j \rangle u_j  , u_j \rangle \\
 &amp;= \langle x, u_j \rangle - \langle \sum_{i=1}^k \langle x, u_i \rangle u_i  , u_j \rangle  \\
 &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \langle u_i  , u_j \rangle  \\
 &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \delta_{ij} \\
 &amp;= \langle x, u_j \rangle - \langle x, u_j \rangle \\
 &amp;= 0
\end{align*}
$$
</div>
<p>So now we see that \(x = w + z\) where \(w \in W\) and \(z \in W^{\perp}\). We still need to show that this decomposition is unique. Suppose it’s not and that \(x = w + z = \tilde{w} + \tilde{z}\). This implies</p>
<div>
$$
\begin{align*}
w - \tilde{w} = z - \tilde{z} = \bar{0}_V
\end{align*}
$$
</div>
<p>But \(w,\tilde{w} \in W\) and \(z, \tilde{z} \in Z\) and since \(W \cap W^{\perp} = \bar{0}_V\), then \(w = \tilde{w}\) and \(z = \tilde{z}\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Projection of \(x\) onto \(W\)</b></h4>
<p>The \(w\) vector we found in the last theorem has a special name. It is the projection of \(x\) onto the subspace \(W\).
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
\(w\) above is called the orthogonal projection of \(x\) onto \(W\) and is denoted as
$$
\begin{align*}
proj_W(x) = w
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<br />
It also has a special geometric interpretation. It is the closest vector to \(x\) in \(W\).
<br />
<br /></p>
<div class="purdiv">
Theorem 2
</div>
<div class="purbdiv">
\(w = proj_W(x)\) is the vector in \(W\) closest to \(x\) in the following sense
$$
\begin{align*}
\Vert x - w \Vert \leq \Vert x - y \Vert \ \forall y \in W
\end{align*}
$$
</div>
<p><br />
<b>Proof</b>
<br />
<br />
It’s easier to square things since we don’t have deal with squareroots so</p>
<div>
$$
\begin{align*}
\Vert x - y \Vert^2 &amp;= \Vert (w + z) - y \Vert^2 \text{ (by theorem 1)} \\
                 &amp;= \Vert (w - y) + z \Vert^2 \text{ (w and y are both in $W$)} \\
                 &amp;= \langle (w - y) + z, (w - y) + z \rangle \\
                 &amp;= \langle (w - y), (w - y) \rangle 
				 + \langle (w - y), z \rangle 
				 + \langle z, z \rangle 
				 + \langle z, (w - y) \rangle \\
				 &amp;= \Vert w - y \Vert^2 + \langle (w - y), z \rangle  + \langle z, (w - y) \rangle + \Vert z \Vert^2 \\
				 &amp;= \Vert w - y \Vert^2 + \Vert z \Vert^2 \text{ ($z \in W^{\perp}$ and $w, y \in W$)} \\
				 &amp;\geq \Vert z \Vert^2 \\
				 &amp;=  \Vert x - w \Vert^2. \ \blacksquare
				
				 
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Projection as a Map</b></h4>
<p>In general we can think of the projection onto \(W\) as a map.</p>
<div>
$$
\begin{align*}
proj_W: \ &amp; V \rightarrow W \\
		&amp;x \rightarrow proj_W(x)		 
\end{align*}
$$
</div>
<p>where the formula is</p>
<div>
$$
\begin{align*}
w = \sum_{j = 1}^{k} \langle x, u_j \rangle u_j		 
\end{align*}
$$
</div>
<p>This formula tells us that the projection is linear in \(x\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The orthogonal complement to \(S\) is $$ \begin{align*} S^{\perp} = \{x \in V \ | \ \langle x, y \rangle = 0 \forall y \in S\} \end{align*} $$ Example 2 Suppose \(S = \{(0,0,1)\} \subset \mathbf{R}^3\). The orthogonal complement to \(S\) is any element in \(\mathbf{R}^3\) such that when we take the inner product between any element in \(S\) and any element \(\mathbf{R}^3\), the product must be zero. $$ \begin{align*} S^{\perp} &amp;= \{(x,y,z) \in V \ | \ \langle (x,y,z), (0,0,1) \rangle = 0\} \\ &amp;= \{(x,y,z) \in V \ | \ z = 0\} \\ &amp;= \{(x,y,0)\} \\ \end{align*} $$ Note here if we chose \(S' = span(S)\), then the orthogonal complement will be the same! Exercise 1: \(S^{\perp}\) is a subspace of \(V\). Exercise 2: If \(S\) is a subspace, then \(S \cap S^{\perp} = \{0\}\). How to use these orthogonal complements? We have the following theorem Theorem 1 Suppose \(W\) is finite dimensional subspace of inner produce space \(V\). For each \(x \in V\) there is a unique \(w \in W\) and \(z \in W^{\perp}\) such that $$ \begin{align*} x = w + z \end{align*} $$ If \(\{u_1,...,u_k\}\) is an orthonormal basis for \(W\), then \(w = \langle x, u_1 \rangle + ... + \langle x, u_k \rangle u_k\) (TODO: Add pic) Proof We can always find \(\{u_1, ..., u_k\}\) using Gram-Schmidt. So we know \(w\) and we’re given \(x\). All we need is to verify that \(z = x - w\) and show that \(w\) and \(z\) are unique. In other words, it suffices to show for $$ \begin{align*} w = \sum_{j=1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ that \(z = x - w\) is in \(W^{\perp}\) plus the uniqueness of this decomposition. $$ \begin{align*} &amp;z = x - w \in W^{\perp} \\ \Leftrightarrow \ &amp;\langle z, y \rangle = 0 \ \forall y \in W \end{align*} $$ It isn’t easy to verify this for every vector \(y \in W\). But we do have a basis for \(W\) and any \(y\) can be written as a linear combination of the basis elements. So we can instead check this for every basis element $$ \begin{align*} \Leftrightarrow \ &amp;\langle z, u_j \rangle = 0 \ \forall j =1,...,k \end{align*} $$ So let’s check for every basis element that it’s orthogonal to \(x - w\). $$ \begin{align*} \Leftrightarrow \ \langle z, u_j \rangle &amp;= \langle x-w, u_j \rangle \\ &amp;= \langle x - \sum_{j=1}^k \langle x, u_j \rangle u_j , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \langle \sum_{i=1}^k \langle x, u_i \rangle u_i , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \langle u_i , u_j \rangle \\ &amp;= \langle x, u_j \rangle - \sum_{i=1}^k \langle x, u_i \rangle \delta_{ij} \\ &amp;= \langle x, u_j \rangle - \langle x, u_j \rangle \\ &amp;= 0 \end{align*} $$ So now we see that \(x = w + z\) where \(w \in W\) and \(z \in W^{\perp}\). We still need to show that this decomposition is unique. Suppose it’s not and that \(x = w + z = \tilde{w} + \tilde{z}\). This implies $$ \begin{align*} w - \tilde{w} = z - \tilde{z} = \bar{0}_V \end{align*} $$ But \(w,\tilde{w} \in W\) and \(z, \tilde{z} \in Z\) and since \(W \cap W^{\perp} = \bar{0}_V\), then \(w = \tilde{w}\) and \(z = \tilde{z}\). \(\ \blacksquare\) Projection of \(x\) onto \(W\) The \(w\) vector we found in the last theorem has a special name. It is the projection of \(x\) onto the subspace \(W\). Definition \(w\) above is called the orthogonal projection of \(x\) onto \(W\) and is denoted as $$ \begin{align*} proj_W(x) = w \end{align*} $$ It also has a special geometric interpretation. It is the closest vector to \(x\) in \(W\). Theorem 2 \(w = proj_W(x)\) is the vector in \(W\) closest to \(x\) in the following sense $$ \begin{align*} \Vert x - w \Vert \leq \Vert x - y \Vert \ \forall y \in W \end{align*} $$ Proof It’s easier to square things since we don’t have deal with squareroots so $$ \begin{align*} \Vert x - y \Vert^2 &amp;= \Vert (w + z) - y \Vert^2 \text{ (by theorem 1)} \\ &amp;= \Vert (w - y) + z \Vert^2 \text{ (w and y are both in $W$)} \\ &amp;= \langle (w - y) + z, (w - y) + z \rangle \\ &amp;= \langle (w - y), (w - y) \rangle + \langle (w - y), z \rangle + \langle z, z \rangle + \langle z, (w - y) \rangle \\ &amp;= \Vert w - y \Vert^2 + \langle (w - y), z \rangle + \langle z, (w - y) \rangle + \Vert z \Vert^2 \\ &amp;= \Vert w - y \Vert^2 + \Vert z \Vert^2 \text{ ($z \in W^{\perp}$ and $w, y \in W$)} \\ &amp;\geq \Vert z \Vert^2 \\ &amp;= \Vert x - w \Vert^2. \ \blacksquare \end{align*} $$ Projection as a Map In general we can think of the projection onto \(W\) as a map. $$ \begin{align*} proj_W: \ &amp; V \rightarrow W \\ &amp;x \rightarrow proj_W(x) \end{align*} $$ where the formula is $$ \begin{align*} w = \sum_{j = 1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ This formula tells us that the projection is linear in \(x\). References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Lecture 31/32: Orthonormal Sets, Gram Schmidt and Fourier coefficients</title><link href="http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets.html" rel="alternate" type="text/html" title="Lecture 31/32: Orthonormal Sets, Gram Schmidt and Fourier coefficients" /><published>2024-09-01T01:01:36-07:00</published><updated>2024-09-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/09/01/lec31-orthonormal-orthogonal-sets.html"><![CDATA[<div class="bdiv">
Definition
</div>
<div class="bbdiv">
<ul>
	<li>\(x, y \in V\) are orthogonal (perpendicular) if \(\langle x, y \rangle = 0\)</li>
	<li>\(S \in V\) is orthogonal if for any \(x, y\) distinct in \(S\), \(\langle x, y \rangle = 0\)</li>
	<li>\(S \subset V\) is orthonormal if it is orthogonal and for each \(x \in S, \Vert x \Vert = 1\)</li>
</ul>
</div>
<p><br />
Remark: \(x \in V\) is a unit vector if \(\Vert x \Vert = 1\). If \(x \neq \bar{0}_V\), then \(\frac{x}{\Vert x \Vert}\) is a unit vector. This process is normalization.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div> 
$$
\begin{align*}
V = C^0([0,1]), \langle f,g \rangle = \int_0^1 f(t)g(t)dt
\end{align*}
$$
</div>
<p>Show \(\sin(2\pi t)\) and \(\cos(2\pi t)\) are orthogonal. To do this, we’ll compute their inner product.</p>
<div> 
$$
\begin{align*}
\langle \sin(2\pi t), \cos(2\pi t) \rangle &amp;= \int_0^1 \sin(2\pi t) \cos(2\pi t) \\
                                           &amp;= \frac{\sin^2 2 \pi t}{4\pi} \Big|^1_0 = 0
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>The standard basis \(\beta = \{e_1, e_2, ..., e_n \}\) of \(\mathbf{R}^n\) is an orthonormal subset. For every distinct two vectors in \(\beta\), their inner product is zero. Moreover, for any vector \(e_i \in \beta\), \(\langle e_i, e_i \rangle = 1\). In general,</p>
<div> 
$$
\begin{align*}
\langle e_i, e_j \rangle &amp;= \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j\end{cases}\\
\end{align*}
$$
</div>
<p>In general, if \(\{v_1,...,v_k\}\) is orthonormal, then \(\langle v_i, v_j \rangle = \delta_{ij}\). The next theorem tells us why orthonormal sets are useful.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Suppose \(S = \{v_1,...,v_k\} \subseteq V\) is orthonormal. If \(y \in span(S)\), then
$$
\begin{align*}
y = \sum_{j=1}^k \langle y, v_j \rangle v_j
\end{align*}
$$
</div>
<p><br />
So we don’t need to solve a system of linear equations to figure out the coefficients if \(y\) written with respect to \(S\). 
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>:
<br />
<br />
We know that \(y \in span(S)\) Therefore, we can write \(y\) as</p>
<div>
$$
\begin{align*}
y = \sum_{j=1}^k a_j v_j
\end{align*}
$$
</div>
<p>for scalars \(a_1,...,a_k\). We also know that the \(S\) is an orthonormal set. Taking the inner product of both sides with respect to one vector from \(S\), we see that</p>
<div>
$$
\begin{align*}
\langle y, v_i \rangle &amp;= \langle \sum_{j=1}^k a_j v_j, v_i \rangle \text{ for all $i = 1,...,k$} \\
 &amp;= \sum_{j=1}^k a_j \langle v_j, v_i \rangle \text{ (the inner product is linear in its first argument)} \\
 &amp;= \sum_{j=1}^k a_j \delta_{ij} \\
 &amp;= a_i \text{ ($\delta_{ij}$ is 1 only for $i = j$)}
\end{align*}
$$
</div>
<p>Therefore,</p>
<div>
$$
\begin{align*}
y = \sum_{j=1}^k \langle y, v_j \rangle v_j. \quad \blacksquare
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
What about orthogonal subsets, can we say anything about them? Yes!
<br />
<br /></p>
<div class="purdiv">
Corollary 1
</div>
<div class="purbdiv">
If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\). If \(y \in span(S)\), then
$$
\begin{align*}
y = \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>:
<br />
<br />
If \(S = \{v_1,...,v_k\}\) is orthogonal. We can turn this set into an orthonormal set by normalizing the set so</p>
<div>
$$
\begin{align*}
\{\frac{v_1}{\Vert v_1 \Vert^2},...,\frac{v_k}{\Vert v_k \Vert^2}\}
\end{align*}
$$
</div>
<p>By the previous theorem, then</p>
<div>
$$
\begin{align*}
y &amp;= \sum_{j=1}^k \langle y, \frac{v_j}{\Vert v_j \Vert }\rangle \frac{v_j}{\Vert v_j \Vert}  \\
  &amp;= \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j. \quad \blacksquare
\end{align*}
$$
</div>
<!--------------------------------------------------------------------------------->
<div class="purdiv">
Corollary 2
</div>
<div class="purbdiv">
If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\), then \(S\) is linearly independent. 
</div>
<p><br />
<br />
<b>Proof:</b>
To see that it’s linearly independent, then the only solution to the equation</p>
<div>
	$$
	\begin{align*}
	a_1v_1 + ... + a_kv_k = \bar{0}_V
	\end{align*}
	$$
</div>
<p>is the trivial solution. But by corollary 1, if \(\bar{0}_V \in span(S)\), then we know the coefficients when it’s written relative to (S). Specifically the \(j\)’s coefficient is</p>
<div>
	$$
	\begin{align*}
	a_j = \frac{\langle \bar{0}_V, v_j \rangle}{\Vert v_j \Vert^2} = 0. \quad \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<!---------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
If \(V\) is finite dimensional inner product space, then it has an orthonormal basis.
</div>
<p><br />
<br />
This will follow from the procedure we will study next …
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Gram-Schmidt Process</b></h4>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
Let \(\{w_1,...,w_k\}\) be a linearly independent subset of \(V\). Set 
	$$
	\begin{align*}
	u_1 &amp;= \frac{w_1}{\Vert w_1 \Vert} \\
	u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle}{\Vert w_2 - \langle w_2, u_1 \rangle \Vert} \\
	&amp;\vdots \\
	u_k &amp;= \frac{w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j}{\Vert w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j \Vert}
	\end{align*}
	$$
Then \(\{u_1,...,u_k\}\) is orthonormal and has same span as \(\{w_1,...,w_k\}\).
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof</b>
<br />
<br />
The basic idea of the proof (by induction) is that given \(\{w_1,...,w_k\}\) is linearly independent and given than \(\{u_1, u_2\}\) is orthonormal with the same span as \(\{w_1, w_2\}\), we want \(u_3\) such that \(\{u_1, u_2, u_3\}\) is orthonormal and \(span(\{u_1, u_2, u_3\}) = span(\{w_1,w_2,w_3\})\)
<br />
<br />
To show that the two spans are the same, it suffices to show that \(w_3 \in span(\{u_1, u_2, u_3\})\). In this case, we know by the theorem above what the coefficients should be:</p>
<div>
	$$
	\begin{align*}
	w_3 = \langle w_3, u_1 \rangle u_1 + \langle w_3, u_2 \rangle u_2 + \langle w_3, u_3 \rangle u_3
	\end{align*}
	$$
</div>
<p>Therefore, we can use the above equation to solve for \(u_3\). But we don’t want to divide by \(\langle w_3, u_3 \rangle\) since we’re trying to solve for \(u_3\) so we can think of this term as a constant we’re multiplying with:</p>
<div>
	$$
	\begin{align*}
	u_3 = c( w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2  )
	\end{align*}
	$$
</div>
<p>But we know we want \(u_3\) to be a unit vector. So we can just divide by the length of it.</p>
<div>
	$$
	\begin{align*}
	u_3 = \frac{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2}{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 \Vert}
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3</b></h4>
<p>Find an orthonormal basis for \(P_2 \in C^0([-1, 1])\). equipped with</p>
<div>
	$$
	\begin{align*}
	\langle f, g \rangle = \int_{-1}^{1} f(x)g(x) dx
	\end{align*}
	$$
</div>
<p>Choose \(\{1, x, x^2\}\). Apply Gram-Schmidt. So</p>
<div>
	$$
	\begin{align*}
	\Vert 1\Vert^2 &amp;= \langle 1, 1 \rangle = \int_{-1}^{1} 1dx = 2 \\
	u_1 &amp;= \frac{1}{\Vert 1\Vert} = \frac{1}{\sqrt{2}}
	\end{align*}
	$$
</div>
<p>Next, we’ll find \(u_2\)</p>
<div>
	$$
	\begin{align*}
	\Vert w_2 \Vert^2 &amp;= \langle x, x \rangle = \int_{-1}^{1} x^2 dx = \frac{x^2}{3} \Big|^1_{-1} = \frac{2}{3} \\
	\langle w_2, u_1 \rangle &amp;=  \int_{-1}^{1} x\frac{1}{\sqrt{2}} dx = 0 \\
	u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle u_1}{\Vert w_2 - \langle w_2, u_1 \rangle u_1 \Vert} = \frac{\sqrt{3}}{\sqrt{2}}x
	\end{align*}
	$$
</div>
<p>And finally \(u_3\)</p>
<div>
	$$
	\begin{align*}
	u_3 &amp;= \frac{w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1}{\Vert w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1 \Vert}
	\\
	&amp;= \sqrt{\frac{5}{8}}(3x^2 - 1)
	\end{align*}
	$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Fourier coefficients</b></h4>
<p>The coefficients with respect to an orthonormal spanning set that we studied last time have a special name:
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(S \subseteq V\) be an (possibly infinite) orthonormal subset. The scalars \(\langle x, u\rangle\) for \(u \in S\) are called the Fourier coefficients of \(x\) with respect to \(S\).
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<div> 
$$
\begin{align*}
V &amp;= C^0([-1,1]), \langle f,g \rangle = \int_{-1}^1 f(t)g(t)dt \\
S &amp;= \big\{\frac{1}{\sqrt{2}}\big\} \cup \{\sin n \pi t\}_{n=1}^{\infty} \cup \{\cos n \pi t \}_{n=1}^{\infty}
\end{align*}
$$
</div>
<p>We can easily check that \(S\) is an orthonormal set. Each of the two vectors is orthogonal to each other and each vector is of unit length. Find the Fourier coefficients of \(f = |t| \in C^0([-1,1])\)</p>
<div> 
$$
\begin{align*}
\langle f, \frac{1}{\sqrt{2}} \rangle &amp;= \int_{-1}^1 \frac{1}{\sqrt{2}} |t| dt = \frac{1}{\sqrt{2}} \\
\langle f, \sin n \pi t \rangle &amp;= \int_{-1}^1 \sin n \pi t |t| dt = 0 \\
\langle f, \cos n \pi t \rangle &amp;= \int_{-1}^1 \cos n \pi t |t| dt = 
\begin{cases} 0 \quad \ \ \quad \text{if $n$ even } \\ \frac{-4}{(n\pi)^2} \quad \text{if $n$ odd } \end{cases}
\end{align*}
$$
</div>
<p>When \(S = \{u_1,...,u_k\}\) is finite, then we can write</p>
<div> 
$$
\begin{align*}
x = \sum_{j=1}^{k} \langle x, u_j \rangle u_j
\end{align*}
$$
</div>
<p>But now in the infinite case, Is</p>
<div> 
$$
\begin{align*}
|t| = \frac{1}{2} = \sum_{n\text{ odd}} \frac{-4}{(n\pi)^2} \cos n \pi t
\end{align*}
$$
</div>
<p>Yes it is true but this is an infinite sum that converges to a number that is the absolute value of \(t\). This is basically the beginning of studying Fourier Analysis where any sufficiently nice function can be written as an infinite sum of sines and cosines. 
<br />
<br />
But one thing we know here is that \(S\) is not a basis for \(C^0[-1,1]\). \(|t| \neq\) finite set of elements of \(S\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition \(x, y \in V\) are orthogonal (perpendicular) if \(\langle x, y \rangle = 0\) \(S \in V\) is orthogonal if for any \(x, y\) distinct in \(S\), \(\langle x, y \rangle = 0\) \(S \subset V\) is orthonormal if it is orthogonal and for each \(x \in S, \Vert x \Vert = 1\) Remark: \(x \in V\) is a unit vector if \(\Vert x \Vert = 1\). If \(x \neq \bar{0}_V\), then \(\frac{x}{\Vert x \Vert}\) is a unit vector. This process is normalization. Example 1 $$ \begin{align*} V = C^0([0,1]), \langle f,g \rangle = \int_0^1 f(t)g(t)dt \end{align*} $$ Show \(\sin(2\pi t)\) and \(\cos(2\pi t)\) are orthogonal. To do this, we’ll compute their inner product. $$ \begin{align*} \langle \sin(2\pi t), \cos(2\pi t) \rangle &amp;= \int_0^1 \sin(2\pi t) \cos(2\pi t) \\ &amp;= \frac{\sin^2 2 \pi t}{4\pi} \Big|^1_0 = 0 \end{align*} $$ Example 2 The standard basis \(\beta = \{e_1, e_2, ..., e_n \}\) of \(\mathbf{R}^n\) is an orthonormal subset. For every distinct two vectors in \(\beta\), their inner product is zero. Moreover, for any vector \(e_i \in \beta\), \(\langle e_i, e_i \rangle = 1\). In general, $$ \begin{align*} \langle e_i, e_j \rangle &amp;= \delta_{ij} = \begin{cases} 1 \quad \text{if } i = j \\ 0 \quad \text{if } i \neq j\end{cases}\\ \end{align*} $$ In general, if \(\{v_1,...,v_k\}\) is orthonormal, then \(\langle v_i, v_j \rangle = \delta_{ij}\). The next theorem tells us why orthonormal sets are useful. Theorem Suppose \(S = \{v_1,...,v_k\} \subseteq V\) is orthonormal. If \(y \in span(S)\), then $$ \begin{align*} y = \sum_{j=1}^k \langle y, v_j \rangle v_j \end{align*} $$ So we don’t need to solve a system of linear equations to figure out the coefficients if \(y\) written with respect to \(S\). Proof: We know that \(y \in span(S)\) Therefore, we can write \(y\) as $$ \begin{align*} y = \sum_{j=1}^k a_j v_j \end{align*} $$ for scalars \(a_1,...,a_k\). We also know that the \(S\) is an orthonormal set. Taking the inner product of both sides with respect to one vector from \(S\), we see that $$ \begin{align*} \langle y, v_i \rangle &amp;= \langle \sum_{j=1}^k a_j v_j, v_i \rangle \text{ for all $i = 1,...,k$} \\ &amp;= \sum_{j=1}^k a_j \langle v_j, v_i \rangle \text{ (the inner product is linear in its first argument)} \\ &amp;= \sum_{j=1}^k a_j \delta_{ij} \\ &amp;= a_i \text{ ($\delta_{ij}$ is 1 only for $i = j$)} \end{align*} $$ Therefore, $$ \begin{align*} y = \sum_{j=1}^k \langle y, v_j \rangle v_j. \quad \blacksquare \end{align*} $$ What about orthogonal subsets, can we say anything about them? Yes! Corollary 1 If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\). If \(y \in span(S)\), then $$ \begin{align*} y = \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j \end{align*} $$ Proof: If \(S = \{v_1,...,v_k\}\) is orthogonal. We can turn this set into an orthonormal set by normalizing the set so $$ \begin{align*} \{\frac{v_1}{\Vert v_1 \Vert^2},...,\frac{v_k}{\Vert v_k \Vert^2}\} \end{align*} $$ By the previous theorem, then $$ \begin{align*} y &amp;= \sum_{j=1}^k \langle y, \frac{v_j}{\Vert v_j \Vert }\rangle \frac{v_j}{\Vert v_j \Vert} \\ &amp;= \sum_{j=1}^k \frac{\langle y, v_j \rangle}{\Vert v_j \Vert^2 } v_j. \quad \blacksquare \end{align*} $$ Corollary 2 If \(S = \{v_1,...,v_k\} \subseteq V\) is orthogonal and \(\bar{0} \notin S\), then \(S\) is linearly independent. Proof: To see that it’s linearly independent, then the only solution to the equation $$ \begin{align*} a_1v_1 + ... + a_kv_k = \bar{0}_V \end{align*} $$ is the trivial solution. But by corollary 1, if \(\bar{0}_V \in span(S)\), then we know the coefficients when it’s written relative to (S). Specifically the \(j\)’s coefficient is $$ \begin{align*} a_j = \frac{\langle \bar{0}_V, v_j \rangle}{\Vert v_j \Vert^2} = 0. \quad \blacksquare \end{align*} $$ Theorem If \(V\) is finite dimensional inner product space, then it has an orthonormal basis. This will follow from the procedure we will study next … Gram-Schmidt Process Theorem Let \(\{w_1,...,w_k\}\) be a linearly independent subset of \(V\). Set $$ \begin{align*} u_1 &amp;= \frac{w_1}{\Vert w_1 \Vert} \\ u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle}{\Vert w_2 - \langle w_2, u_1 \rangle \Vert} \\ &amp;\vdots \\ u_k &amp;= \frac{w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j}{\Vert w_k - \sum_{j=1}^{k-1} \langle w_k, u_j \rangle u_j \Vert} \end{align*} $$ Then \(\{u_1,...,u_k\}\) is orthonormal and has same span as \(\{w_1,...,w_k\}\). Proof The basic idea of the proof (by induction) is that given \(\{w_1,...,w_k\}\) is linearly independent and given than \(\{u_1, u_2\}\) is orthonormal with the same span as \(\{w_1, w_2\}\), we want \(u_3\) such that \(\{u_1, u_2, u_3\}\) is orthonormal and \(span(\{u_1, u_2, u_3\}) = span(\{w_1,w_2,w_3\})\) To show that the two spans are the same, it suffices to show that \(w_3 \in span(\{u_1, u_2, u_3\})\). In this case, we know by the theorem above what the coefficients should be: $$ \begin{align*} w_3 = \langle w_3, u_1 \rangle u_1 + \langle w_3, u_2 \rangle u_2 + \langle w_3, u_3 \rangle u_3 \end{align*} $$ Therefore, we can use the above equation to solve for \(u_3\). But we don’t want to divide by \(\langle w_3, u_3 \rangle\) since we’re trying to solve for \(u_3\) so we can think of this term as a constant we’re multiplying with: $$ \begin{align*} u_3 = c( w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 ) \end{align*} $$ But we know we want \(u_3\) to be a unit vector. So we can just divide by the length of it. $$ \begin{align*} u_3 = \frac{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2}{w_3 - \langle w_3, u_1 \rangle u_1 - \langle w_3, u_2 \rangle u_2 \Vert} \end{align*} $$ Example 3 Find an orthonormal basis for \(P_2 \in C^0([-1, 1])\). equipped with $$ \begin{align*} \langle f, g \rangle = \int_{-1}^{1} f(x)g(x) dx \end{align*} $$ Choose \(\{1, x, x^2\}\). Apply Gram-Schmidt. So $$ \begin{align*} \Vert 1\Vert^2 &amp;= \langle 1, 1 \rangle = \int_{-1}^{1} 1dx = 2 \\ u_1 &amp;= \frac{1}{\Vert 1\Vert} = \frac{1}{\sqrt{2}} \end{align*} $$ Next, we’ll find \(u_2\) $$ \begin{align*} \Vert w_2 \Vert^2 &amp;= \langle x, x \rangle = \int_{-1}^{1} x^2 dx = \frac{x^2}{3} \Big|^1_{-1} = \frac{2}{3} \\ \langle w_2, u_1 \rangle &amp;= \int_{-1}^{1} x\frac{1}{\sqrt{2}} dx = 0 \\ u_2 &amp;= \frac{w_2 - \langle w_2, u_1 \rangle u_1}{\Vert w_2 - \langle w_2, u_1 \rangle u_1 \Vert} = \frac{\sqrt{3}}{\sqrt{2}}x \end{align*} $$ And finally \(u_3\) $$ \begin{align*} u_3 &amp;= \frac{w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1}{\Vert w_3 - \langle w_3, u_2 \rangle u_2 - \langle w_3, u_1 \rangle u_1 \Vert} \\ &amp;= \sqrt{\frac{5}{8}}(3x^2 - 1) \end{align*} $$ Fourier coefficients The coefficients with respect to an orthonormal spanning set that we studied last time have a special name: Definition Let \(S \subseteq V\) be an (possibly infinite) orthonormal subset. The scalars \(\langle x, u\rangle\) for \(u \in S\) are called the Fourier coefficients of \(x\) with respect to \(S\). Example 1 $$ \begin{align*} V &amp;= C^0([-1,1]), \langle f,g \rangle = \int_{-1}^1 f(t)g(t)dt \\ S &amp;= \big\{\frac{1}{\sqrt{2}}\big\} \cup \{\sin n \pi t\}_{n=1}^{\infty} \cup \{\cos n \pi t \}_{n=1}^{\infty} \end{align*} $$ We can easily check that \(S\) is an orthonormal set. Each of the two vectors is orthogonal to each other and each vector is of unit length. Find the Fourier coefficients of \(f = |t| \in C^0([-1,1])\) $$ \begin{align*} \langle f, \frac{1}{\sqrt{2}} \rangle &amp;= \int_{-1}^1 \frac{1}{\sqrt{2}} |t| dt = \frac{1}{\sqrt{2}} \\ \langle f, \sin n \pi t \rangle &amp;= \int_{-1}^1 \sin n \pi t |t| dt = 0 \\ \langle f, \cos n \pi t \rangle &amp;= \int_{-1}^1 \cos n \pi t |t| dt = \begin{cases} 0 \quad \ \ \quad \text{if $n$ even } \\ \frac{-4}{(n\pi)^2} \quad \text{if $n$ odd } \end{cases} \end{align*} $$ When \(S = \{u_1,...,u_k\}\) is finite, then we can write $$ \begin{align*} x = \sum_{j=1}^{k} \langle x, u_j \rangle u_j \end{align*} $$ But now in the infinite case, Is $$ \begin{align*} |t| = \frac{1}{2} = \sum_{n\text{ odd}} \frac{-4}{(n\pi)^2} \cos n \pi t \end{align*} $$ Yes it is true but this is an infinite sum that converges to a number that is the absolute value of \(t\). This is basically the beginning of studying Fourier Analysis where any sufficiently nice function can be written as an infinite sum of sines and cosines. But one thing we know here is that \(S\) is not a basis for \(C^0[-1,1]\). \(|t| \neq\) finite set of elements of \(S\). References Math416 by Ely Kerman]]></summary></entry><entry><title type="html">Section 4.3: Exercise 21</title><link href="http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21.html" rel="alternate" type="text/html" title="Section 4.3: Exercise 21" /><published>2024-08-31T01:01:36-07:00</published><updated>2024-08-31T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/31/lec20-4.3-exercise-21.html"><![CDATA[<div class="ydiv">
4.3: Exercise 21
</div>
<div class="ybdiv">
Prove that if \(M \in M_{n \times n}(\mathbf{F})\) can be written in the form
	$$
	\begin{align*}
	M = \begin{pmatrix}
	A &amp; B \\
	O &amp; C
	\end{pmatrix}
	\end{align*}
	$$
where \(A\) and \(C\) are square square matrices. Prove that \(\det(M) = \det(A)\det(C)\)
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose \(A\) is of size \(k \times k\) and \(C\) is of size \(m \times m\), then we know that \(k+m=n\) and \(O\) is of size \(m \times k\). We have two cases. If \(A\) is not invertible, then we know that \(\det(A) = 0\). Moreover, the columns of \(A\) are linearly dependent and the rank of \(A\) is less than \(k\). But \(M\) contains \(k+m\) columns and since the first \(k\) columns are linearly dependent, then the rank of \(M\) is less than \(k+m\). Therefore \(\det(M) = 0 = \det(A)\det(C)\) and we are done.
<br />
<br />
Suppose that \(A\) is invertible. Then define</p>
<div>
	$$
	\begin{align*}
     P &amp;= 
 	 \begin{pmatrix}
 	A &amp; O' \\
 	O &amp; I_m
 	\end{pmatrix},
	Q = 
	\begin{pmatrix}
	I_k &amp; A^{-1}B \\
	O &amp; C
	\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Where \(O'\) is of size \(k \times m\). Note here that \(PQ = M\). We claim that \(\det(P) = \det(A)\) and \(\det(Q)=C\). Once we show this claim is true, then we can conclude that \(\det(M) = \det(P)\det(Q) = \det(A)\det(Q)\).
<br />
<br />
To show that \(\det(P)=\det(O)\), We’ll prove this by induction on \(m\).<br />
Base Case \(m=1\): \(I_m\) is of size \(1 \times 1\). \(O\) is of size \(1 \times k\). The last row of \(P\) therefore contains a row of zeros followed by 1. Computing the determinant of \(P\) by cofactor expansion along this last row yields exactly \(\det(A)\) as required.
<br />
<br />
Inductive Case: Suppose this is true for \(m-1\). We will show that this is true for \(m\). In this case \(O\) is of size \(m \times k\) and \(I_m\) is of size \(m \times m\). The last row of this matrix is also a sequence of zeros followed by a 1 by the definition of \(O\) and \(I_m\). So compute the determinant by cofactor expansion along the last row to see that \(\det(P) = 1 \det(P')\). where \(P'\) is the matrix \(P\) but with the last column and the last row. We can now apply the inductive hypothesis to conclude that \(\det(P') = \det(A)\) and therefore \(\det(P) = 1\det(A) = \det(A)\) as we wanted to show.
<br />
<br />
We can use the same inductive proof to prove that \(\det(Q) = C\). Once we do so, we are done.</p>

<p>\(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[4.3: Exercise 21 Prove that if \(M \in M_{n \times n}(\mathbf{F})\) can be written in the form $$ \begin{align*} M = \begin{pmatrix} A &amp; B \\ O &amp; C \end{pmatrix} \end{align*} $$ where \(A\) and \(C\) are square square matrices. Prove that \(\det(M) = \det(A)\det(C)\) Proof: Suppose \(A\) is of size \(k \times k\) and \(C\) is of size \(m \times m\), then we know that \(k+m=n\) and \(O\) is of size \(m \times k\). We have two cases. If \(A\) is not invertible, then we know that \(\det(A) = 0\). Moreover, the columns of \(A\) are linearly dependent and the rank of \(A\) is less than \(k\). But \(M\) contains \(k+m\) columns and since the first \(k\) columns are linearly dependent, then the rank of \(M\) is less than \(k+m\). Therefore \(\det(M) = 0 = \det(A)\det(C)\) and we are done. Suppose that \(A\) is invertible. Then define $$ \begin{align*} P &amp;= \begin{pmatrix} A &amp; O' \\ O &amp; I_m \end{pmatrix}, Q = \begin{pmatrix} I_k &amp; A^{-1}B \\ O &amp; C \end{pmatrix} \end{align*} $$ Where \(O'\) is of size \(k \times m\). Note here that \(PQ = M\). We claim that \(\det(P) = \det(A)\) and \(\det(Q)=C\). Once we show this claim is true, then we can conclude that \(\det(M) = \det(P)\det(Q) = \det(A)\det(Q)\). To show that \(\det(P)=\det(O)\), We’ll prove this by induction on \(m\). Base Case \(m=1\): \(I_m\) is of size \(1 \times 1\). \(O\) is of size \(1 \times k\). The last row of \(P\) therefore contains a row of zeros followed by 1. Computing the determinant of \(P\) by cofactor expansion along this last row yields exactly \(\det(A)\) as required. Inductive Case: Suppose this is true for \(m-1\). We will show that this is true for \(m\). In this case \(O\) is of size \(m \times k\) and \(I_m\) is of size \(m \times m\). The last row of this matrix is also a sequence of zeros followed by a 1 by the definition of \(O\) and \(I_m\). So compute the determinant by cofactor expansion along the last row to see that \(\det(P) = 1 \det(P')\). where \(P'\) is the matrix \(P\) but with the last column and the last row. We can now apply the inductive hypothesis to conclude that \(\det(P') = \det(A)\) and therefore \(\det(P) = 1\det(A) = \det(A)\) as we wanted to show. We can use the same inductive proof to prove that \(\det(Q) = C\). Once we do so, we are done.]]></summary></entry><entry><title type="html">Section 5.3: Transition Matrices</title><link href="http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices.html" rel="alternate" type="text/html" title="Section 5.3: Transition Matrices" /><published>2024-08-30T01:01:36-07:00</published><updated>2024-08-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/30/lec26-5.3-transition-matrices.html"><![CDATA[<!---------------------------------------5.12--------------------------------------------->
<div class="purdiv">
Theorem 5.12
</div>
<div class="purbdiv">
Let \(A\) be a square matrix with complex entries. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists if and only if both of the following conditions hold:
<ol style="list-style-type:lower-alpha">
	<li>Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\)</li>
	<li>If 1 is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to 1 equals the multiplicity of 1 as an eigenvalue of 1. </li>
</ol>
</div>
<p><br />
<b>Proof:</b> [TODO]
<br />
<br />
<!----------------------------------------5.13--------------------------------------------></p>
<div class="purdiv">
Theorem 5.13
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) satisfy the following two conditions
<ol type="i"> 
	<li>Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\)</li>
	<li>\(A\) is diagonalizable. </li>
</ol>
Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists
</div>
<!------------------------------------------------------------------------------------>
<p><br />
<b>Proof:</b>
<br />
<br />
Since \(A\) is diagonalizable, then we know that there exists an invertible matrix \(Q\) such that \(D = Q^{-1}AQ\). Suppose that</p>
<div> 
$$
\begin{align*}
D = 
\begin{pmatrix} 
\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>\(\lambda_1,...,\lambda_n\) are the eigenvalues of \(A\). But condition (i) requires that for each \(i\), \(\lambda_i = 1\) or \(|\lambda_i| &lt; 1\). Thus</p>
<div>
$$
\begin{align*}
\lim\limits_{m \rightarrow \infty} {\lambda_i}^m &amp;= \begin{cases} 1 \quad \text{if } \lambda_i = 1 \\ 0 \quad \text{otherwise } \end{cases}
\end{align*}
$$
</div>
<p>so</p>
<div> 
$$
\begin{align*}
D^m = 
\begin{pmatrix} 
\lambda_1^m &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_2^m &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n^m
\end{pmatrix}
\end{align*}
$$
</div>
<p>and so the sequence \(D, D^2 ...\) converges to a limit \(L\). Therefore,</p>
<div> 
$$
\begin{align*}
\lim\limits_{m \rightarrow \infty} A^m = \lim\limits_{m \rightarrow \infty} (QDQ^{-1}) = QLQ^{-1}

\end{align*}
$$
</div>
<p><br />
<!----------------------------------------5.14--------------------------------------------></p>
<div class="purdiv">
Theorem 5.14
</div>
<div class="purbdiv">
Let \(M\) be an \(n \times n\) matrix having real nonnegative entries, let \(v\) be a column vector in \(\mathbf{R}^n\) having nonnegative coordinates, and let \(u \in \mathbf{R}^n\) be the column vector in which each coordinate equals 1. Then
<ol type="a"> 
	<li>\(M\) is a transition matrix if and only if \(u^tM = u^t\);</li>
	<li>\(v\) is a probability vector if and only if \(u^tv = (1)\). </li>
</ol>
</div>
<p><br />
<b>Proof (a):</b>
<br />
<br />
\(\Rightarrow\): Suppose \(M\) is a transition matrix. Then by the definition of matrix-vector multiplication,</p>
<div> 
$$
\begin{align*}
u^tM &amp;= 
\begin{pmatrix} 
1 &amp; 1 &amp; \cdots &amp; 1
\end{pmatrix}
\begin{pmatrix} 
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{m2} \\
\end{pmatrix}
=
\begin{pmatrix}
w_1 &amp; w_2 &amp; \cdots &amp; w_n
\end{pmatrix}
\end{align*}
$$
</div>
<p>where \(w_i\) is the sum</p>
<div> 
$$
\begin{align*}
w_i &amp;= 1a_{1i} + 1a_{2i} + ... + 1a_{ni} \\
    &amp;= a_{1i} + a_{2i} + ... + a_{ni} \\
	&amp;= 1 \quad \text{(since $M$ is a transition matrix)}
\end{align*}
$$
</div>
<p><br />
\(\Leftarrow\): [TODO: same argument as \(\Rightarrow\)]
<br />
<br /><b>Proof (b):</b> [TODO: also very similar to \((a)\)]
<br />
<br />
<!-----------------------------------5.14 (Corollary)-----------------------------------------></p>
<div class="purdiv">
Theorem 5.14 (Corollary)
</div>
<div class="purbdiv">
<ol type="a"> 
	<li>The product of two \(n \times n \) transition matrices is an \(n \times n\) transition matrix. In particular, any power of a transition matrix is a transition matrix.</li>
	<li>The product of a transition matrix a probability vector is a probability vector.</li>
</ol>
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
[TODO]
<br />
<br />
<!-----------------------------------Definition-----------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
A transition matrix is called regular if some power of the matrix contains only nonzero (i.e., positive) entries.
</div>
<p><br />
<br />
<!-----------------------------------Definition-----------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\). For \(1 \leq i,j \leq n\), define \(\rho_i(A)\) to be the sum of the absolute values of the entries of row \(i\) of \(A\), and define \(\nu_j(A)\) to be equal to the sum of the absolute values of the entries of column \(j\) of \(A\). Thus
$$
\begin{align*}
\rho_i(A) = \sum_{j=1}^n |A_{ij}| \quad \text{ for $i = 1,2,...,n$}
\end{align*}
$$
and
$$
\begin{align*}
\nu_j(A) = \sum_{i=1}^n |A_{ij}| \quad \text{ for $j = 1,2,...,n$}
\end{align*}
$$
The row sum of \(A\) denoted \(\rho(A)\), and the column sum of \(A\), denoted \(\nu(A)\), are defined as
$$
\begin{align*}
\rho(A) = \max\{\rho_i(A): 1 \leq i \leq n\}
\text{ and }
\nu(A) = \max\{\nu_j(A): 1 \leq j \leq n\}
\end{align*}
$$
</div>
<p><br />
<br />
TODO: definition of Gershgorin’s circle
<br />
<br />
<!-----------------------------------5.15-----------------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Gershgorin's Circle Theorem)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\). Then every eigenvalue of \(A\) is contained in a Greshgorin disk.
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 1)
</div>
<div class="purbdiv">
Let \(\lambda\) be any eigenvalue of \(A \in M_{n \times n}(\mathbf{C})\). Then \(|\lambda| \leq \rho(A)\).
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 2)
</div>
<div class="purbdiv">
Let \(\lambda\) be any eigenvalue of \(A \in M_{n \times n}(\mathbf{C})\). Then \(|\lambda| \leq 
min\{\rho(A),\nu(A)\}\).
</div>
<p><br />
<br />
<!-----------------------------5.15 (Corollary 1)----------------------------------></p>
<div class="purdiv">
Theorem 5.15 (Corollary 3)
</div>
<div class="purbdiv">
If \(\lambda\) is an eigenvalue of a transition matrix, then \(|\lambda| \leq 1\).
</div>
<p><br />
<br />
<!--------------------------------------5.16-----------------------------------------></p>
<div class="purdiv">
Theorem 5.16
</div>
<div class="purbdiv">
Every transition matrix has 1 as an eigenvalue.
</div>
<p><br />
<br />
<!--------------------------------------5.17-----------------------------------------></p>
<div class="purdiv">
Theorem 5.17
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a matrix in which each entry is a positive real number, and let \(\lambda\) be a complex eigenvalue of \(A\) such that \(|\lambda| = \rho(A)\). Then \(\lambda = \rho(A)\) and \(\{u\}\) is a basis for \(E_{\lambda}\), where \(u \in \mathbf{C}^n\) is the column vector in which each coordinate equals 1.
</div>
<p><br />
<br />
<b>Proof</b>
[TODO: we did some version of this in class]
<br />
<br />
<!-----------------------------------5.17 (Corollary 1)--------------------------------------></p>
<div class="purdiv">
Theorem 5.17 (Corollary 1)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a matrix in which each entry is a positive, and let \(\lambda\) be an eigenvalue of \(A\) such that \(|\lambda| = \nu(A)\). Then \(\lambda = \nu(A)\) and the dimension of \(E_{\lambda}\) equals 1.
</div>
<p><br />
<br />
<!----------------------------------- 5.17 (Corollary 2) --------------------------------------></p>
<div class="purdiv">
Theorem 5.17 (Corollary 2)
</div>
<div class="purbdiv">
Let \(A \in M_{n \times n}(\mathbf{C})\) be a transition matrix in which each entry is positive, and let \(\lambda\) be an eigenvalue of \(A\) other than 1. Then \(|\lambda| &lt; 1\). Moreover, the eigenspace corresponding to the eigenvalue 1 has dimension 1.
</div>
<p><br />
<br />
Okay, so only if the transition matrix itself has positive entries (regular is not enough), then \(\lambda &lt; 1\) for eigenvalues that are not 1 AND the dimension of the eigenspace corresponding to eigenvalue 1 is 1.
<br />
<br />
<!-------------------------------------- 5.18 ----------------------------------------></p>
<div class="purdiv">
Theorem 5.18
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix, and let \(\lambda\) be an eigenvalue of \(A\). 
<ol type="a"> 
	<li>\(|\lambda| \leq 1\).</li>
	<li>If \(|\lambda| = 1\), then \(\lambda = 1\), and \(\dim(E_{\lambda}) = 1.\)</li>
</ol>
</div>
<p><br />
<br />
This is the other result I was looking for. If \(A\) is regular, then we’ll have the normal restriction of \(|\lambda| \leq 1\).
<br />
<br />
<!-------------------------------- 5.18 (Corollary) ---------------------------------></p>
<div class="purdiv">
Theorem 5.18 (Corollary)
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix that is diagonalizable. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists.
</div>
<p><br />
<br />
<!-------------------------------------- 5.19 ----------------------------------------></p>
<div class="purdiv">
Theorem 5.19
</div>
<div class="purbdiv">
Let \(A\) be a regular transition matrix, and let \(\lambda\) be an eigenvalue of \(A\). 
<ol type="a"> 
	<!--------(a)------------>
	<li>The multiplicity of 1 as an eigenvalue of \(A\) is 1.</li>
	<!--------(b)------------>
	<li>\(\lim\limits_{m \rightarrow \infty} A^m\) exists.</li>
	<!--------(c)------------>
	<li>\(L = \lim\limits_{m \rightarrow \infty} A^m\) is a transition matrix.</li>
	<!--------(d)------------>
	<li>\(AL = LA = L\)</li>
	<!--------(e)------------>
	<li>The columns of \(L\) are identical. In fact, each column of \(L\) is equal to the unique probability vector \(v\) that is also an eigenvector of \(A\) corresponding to the eigenvalue 1.</li>
	<!--------(f)------------>
	<li>For any probability vector \(w\), \(\lim\limits_{m \rightarrow \infty} (A^m w) = v.\)</li>
</ol>
</div>
<p><br />
<br /></p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.12 Let \(A\) be a square matrix with complex entries. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists if and only if both of the following conditions hold: Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\) If 1 is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to 1 equals the multiplicity of 1 as an eigenvalue of 1. Proof: [TODO] Theorem 5.13 Let \(A \in M_{n \times n}(\mathbf{C})\) satisfy the following two conditions Every eigenvalue of \(A\) is contained in \(S\) where \(S=\{\lambda \in \mathbf{C}: |\lambda| &lt; 1 \text{ or }\lambda = 1\}\) \(A\) is diagonalizable. Then \(\lim\limits_{m \rightarrow \infty} A^m\) exists Proof: Since \(A\) is diagonalizable, then we know that there exists an invertible matrix \(Q\) such that \(D = Q^{-1}AQ\). Suppose that $$ \begin{align*} D = \begin{pmatrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n \end{pmatrix} \end{align*} $$ \(\lambda_1,...,\lambda_n\) are the eigenvalues of \(A\). But condition (i) requires that for each \(i\), \(\lambda_i = 1\) or \(|\lambda_i| &lt; 1\). Thus $$ \begin{align*} \lim\limits_{m \rightarrow \infty} {\lambda_i}^m &amp;= \begin{cases} 1 \quad \text{if } \lambda_i = 1 \\ 0 \quad \text{otherwise } \end{cases} \end{align*} $$ so $$ \begin{align*} D^m = \begin{pmatrix} \lambda_1^m &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2^m &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n^m \end{pmatrix} \end{align*} $$ and so the sequence \(D, D^2 ...\) converges to a limit \(L\). Therefore, $$ \begin{align*} \lim\limits_{m \rightarrow \infty} A^m = \lim\limits_{m \rightarrow \infty} (QDQ^{-1}) = QLQ^{-1}]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5 (Corollary)</title><link href="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5 (Corollary)" /><published>2024-08-29T01:01:36-07:00</published><updated>2024-08-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/29/lec25-theorem-5.5-corollary.html"><![CDATA[<div class="purdiv">
Theorem 5.5 Corollary
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable.
</div>
<p><br />
<b>Proof:</b>
<br />
<br />
Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\)
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Corollary Let \(T\) be a linear operator on an \(n\)-dimensional vector space \(V\). If \(T\) has \(n\) distinct eigenvalues, then \(T\) is diagonalizable. Proof: Suppose that \(T\) has distinct eigenvalues \(\lambda_1, ..., \lambda_n\). For each \(i\), choose an eigenvector \(v_i\) corresponding to \(\lambda_i\). By theorem 5.5, we know that \(\{v_1,...,v_n\}\) must be linearly independent and since \(\dim(V)=n\), then this set must be a basis for (V). Since we have a basis of eigenvectors then by theorem 5.1, \(T\) must be diagonalizable. \(\ blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.2: Theorem 5.5</title><link href="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html" rel="alternate" type="text/html" title="Section 5.2: Theorem 5.5" /><published>2024-08-28T01:01:36-07:00</published><updated>2024-08-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/28/lec25-theorem-5.5.html"><![CDATA[<div class="purdiv">
Theorem 5.5
</div>
<div class="purbdiv">
Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent.
</div>
<p><br />
Proof:
<br />
<br />
By induction on \(k\). 
<br />
<br />
Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done.
<br />
<br />
Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent.
<br />
<br /> 
Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0
	\end{align}
	$$
</div>
<p>We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that</p>
<div>
	$$
	\begin{align*}
	(T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\
	\sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\
	\end{align*}
	$$
</div>
<p>But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means  that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have</p>
<div>
	$$
	\begin{align}
	\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0
	\end{align}
	$$
</div>
<p>The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). 
<br />
<br />
I really don’t like this proof!
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 5.5 Let \(T\) be a linear operator on a vector space, and let \(\lambda_1,...,\lambda_k\) be distinct eigenvalues of \(T\). For each \(i=1,2...,k\), let \(S_i\) be a finite set of eigenvectors of \(T\) corresponding to \(\lambda_i\). If each \(S_i\) is linearly independent, then \(S_1 \cup ...\cup S_k\) is linearly independent. Proof: By induction on \(k\). Base Case: If \(k = 1\), then \(S_1\) is linearly independent and we are done. Inductive Case: Suppose the hypothesis is true for \(k - 1\) and suppose now we have \(k\) distinct eigenvalues \(\lambda_1,...,\lambda_k\). For each \(i=1,2,...,k\), let \(S_i = \{v_{i1}, v_{i2}, ..., v_{in}\}\) be a linearly independent set of eigenvectors corresponding to \(\lambda_i\). We will show that \(S = S_1 \cup ... \cup S_k\) is linearly independent. Consider any scalars \(\{a_{ij}\}\) where \(i = 1,2,...,k\) and \(j = 1,2,...,n\) such that $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} = 0 \end{align} $$ We want to show that the only solution to the above equation is trivial solution. Apply \(T - \lambda_kI_n\) to both sides of the equation to see that $$ \begin{align*} (T-\lambda_kI)(\sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij}) &amp;= (T - \lambda_kI_n) 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} T-\lambda_kI(a_{ij}v_{ij}) &amp;= 0 \quad \text{ (by the linearity of $T$)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} T(a_{ij}v_{ij}) - \lambda_k a_{ij} v_{ij} &amp;= 0 \\ \sum_{i=1}^k \sum_{j=1}^{n_i} \lambda_i a_{ij}v_{ij} - \lambda_k a_{ij} v_{ij} &amp;= 0 \quad \text{ ($v_{ij}$ is an eigenvector)}\\ \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{j=1}^{n_k} a_{kj} (\lambda_k - \lambda_k) v_{kj} + \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \sum_{i=1}^{k-1} \sum_{j=1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} &amp;= 0\\ \end{align*} $$ But now we reduced the problem to \(k-1\) eigenvalues and their corresponding eigenvectors and \(S_1 \cup...\cup S_{k-1}\) is linearly independent by the induction hypothesis. This means that the scalars \(a_{ij}(\lambda_i-\lambda_k)\) in the above sum must be 0. But we’re also given that \(\lambda_1...\lambda_k\) are distinct so \(\lambda_i - \lambda_k \neq 0\) for any \(i=1,...,k-1\). Therefore we must have \(a_{ij}\) specifically be zero for all \(i=1,...,k-1\) and \(j=1,...,n\). Observe now that in the original equation we have $$ \begin{align} \sum_{i=1}^k \sum_{j=1}^{n_i} a_{ij}v_{ij} &amp;= 0 \end{align} $$ The scalars \(a_{ij}\) are 0 for all \(i = 1,...,{k-1}\). We also know that \(S_k\) is linearly independent. Therefore, \(a_{jk}\) are all 0. So \(S\) is linearly independent as we wanted to show. \(\blacksquare\). I really don’t like this proof! References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 5.1: Exercise 15</title><link href="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html" rel="alternate" type="text/html" title="Section 5.1: Exercise 15" /><published>2024-08-27T01:01:36-07:00</published><updated>2024-08-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/27/lec25-5.1-exercises-15.html"><![CDATA[<div class="ydiv">
Exercise 15
</div>
<div class="ybdiv">
For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues).
</div>
<p><br />
Proof:
<br />
<br />
Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det((A - \lambda I_n)^t) \\
	                  &amp;= \det(A^t - (\lambda I_n)^t) \\
	                  &amp;= \det(A^t - \lambda I_n).
	\end{align*}
	$$
</div>
<p>From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Exercise 15 For any square matrix \(A\), prove that \(A\) and \(A^t\) have the same characteristic polynomial (and hence the same eigenvalues). Proof: Suppose \(A\) is a square matrix of size \(n \times n\). By theorem 4.8, we know that for any matrix \(B \in M_{n\times n}\), \(\det(B) = \det(B^t)\). Therefore, $$ \begin{align*} \det(A - \lambda I_n) &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det((A - \lambda I_n)^t) \\ &amp;= \det(A^t - (\lambda I_n)^t) \\ &amp;= \det(A^t - \lambda I_n). \end{align*} $$ From this, we see that both \(A\) and \(A^t\) have the same characteristic polynomial and therefore, they must have the same eigenvalues. \(\ \blacksquare\) References: Linear Algebra 5th Edition]]></summary></entry><entry><title type="html">Section 4.3: Theorem 4.8</title><link href="http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8.html" rel="alternate" type="text/html" title="Section 4.3: Theorem 4.8" /><published>2024-08-26T01:01:36-07:00</published><updated>2024-08-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/26/lec20-theorem-4.8.html"><![CDATA[<div class="purdiv">
Theorem 4.8
</div>
<div class="purbdiv">
For any \(A \in M_{n \times n}(\mathbf{F})\), \(\det(A^t) = \det(A)\).
</div>
<p><br />
Proof:
<br />
<br />
If \(A\) is not invertible, then \(\text{rank}(A) &lt; n\). We know that \(\text{rank}(A) = \text{rank}(A^t)\) by theorem 3.6 (corollary 2). Therefore, \(A^t\) is not invertible and \(\det(A) = \det(A^t) = 0\).
<br />
<br />
Suppose now that \(A\) is invertible, then we can write \(A\) as a product of elementary matrices, \(A = E_k,E_{k-1}...,E_1\). Furthermore, we’ve proved that \(\det(E) = \det(E^t)\) for any elementary matrix. From this, notice that</p>

<div>
	$$
	\begin{align*}
	\det(A^t) &amp;= \det((E_k,E_{k-1}...,E_1)^t) \\
	          &amp;= \det(E_1^tE_2^t...E_k^t) \\
	          &amp;= \det(E_1^t)\det(E_2^t)...\det(E_k^t) \\
	          &amp;= \det(E_1)\det(E_2)...\det(E_k) \\
	          &amp;= \det(E_k)\det(E_{k-1})...\det(E_1) \\
	          &amp;= \det(E_k,E_{k-1}...,E_1) \\
			  &amp;= \det(A). \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References:</b></h4>
<ul>
<li><a href="https://www.amazon.com/Linear-Algebra-5th-Stephen-Friedberg/dp/0134860241/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Linear Algebra 5th Edition</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Theorem 4.8 For any \(A \in M_{n \times n}(\mathbf{F})\), \(\det(A^t) = \det(A)\). Proof: If \(A\) is not invertible, then \(\text{rank}(A) &lt; n\). We know that \(\text{rank}(A) = \text{rank}(A^t)\) by theorem 3.6 (corollary 2). Therefore, \(A^t\) is not invertible and \(\det(A) = \det(A^t) = 0\). Suppose now that \(A\) is invertible, then we can write \(A\) as a product of elementary matrices, \(A = E_k,E_{k-1}...,E_1\). Furthermore, we’ve proved that \(\det(E) = \det(E^t)\) for any elementary matrix. From this, notice that]]></summary></entry><entry><title type="html">Lecture 29/30: Inner Product Spaces and Norms</title><link href="http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces.html" rel="alternate" type="text/html" title="Lecture 29/30: Inner Product Spaces and Norms" /><published>2024-08-25T01:01:36-07:00</published><updated>2024-08-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/08/25/lec29-inner-product-spaces.html"><![CDATA[<p>Let \(V\) be. vector space over \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\).</p>
<div> 
$$
\begin{align*}
\mathbf{C} = \{ z=a+ib \ | \ a, b \in \mathbf{R} \}
\end{align*}
$$
</div>
<p>The complex conjugate of \(z\) is \(\bar{z} = a - ib\).</p>
<div> 
$$
\begin{align*}
z\bar{z} = (a + ib)(a - ib) = a^2 + b^2 = |z|^2
\end{align*}
$$
</div>
<!------------------------------------------------------------------------------------>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inner product \(\langle \ , \ \rangle\) on a vector \(V\) is a map
$$
\begin{align*}
\langle \ , \ \rangle : \ &amp;V \times V \rightarrow \mathbf{F} = \mathbf{R} \text{ or } \mathbf{C} \\
&amp;(x, y) \rightarrow \langle x , y \rangle
\end{align*}
$$
such that
<ol type="i">
	<li>\(\langle x+z, y \rangle = \langle x , y \rangle + \langle z , y \rangle\)</li>
	<li>\(\langle cx, y \rangle = c \langle x , y \rangle\)</li>
	<li>\(\langle x, y \rangle = \overline{\langle y, x \rangle}\). Note if \(\mathbf{F} = \mathbf{R}\), then \(\langle x, y \rangle = \langle y, x \rangle\) </li>
	<li>\(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\)</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 1</b></h4>
<p>The simplest example is \(V = \mathbf{R}^1\) (vector space over \(\mathbf{R}\)) where</p>
<div> 
$$
\begin{align*}
\langle x, y \rangle &amp;= xy
\end{align*}
$$
</div>
<p>This map satisfies the inner product properties. Notice that</p>
<ol type="i">
	<li>\(\langle x+z, y \rangle = (x+z)y = xy + zx = \langle x , y \rangle + \langle z , y \rangle \)</li>
	<li>\(\langle cx, y \rangle = cxy = c\langle x, y \rangle\)</li>
	<li>\(\langle x, y \rangle = \overline{\langle y, x \rangle} = yx = xy\). </li>
	<li>\(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\)</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 2</b></h4>
<p>We can also define this inner product</p>
<div> 
$$
\begin{align*}
\langle \langle x, y \rangle \rangle_c &amp;= c^2xy \quad c \neq 0
\end{align*}
$$
</div>
<p>which also satisfies the inner product properties (TODO)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 3: Dot Product</b></h4>
<p>Another example is \(V = \mathbf{R}^n\) over \(\mathbf{R}\) where</p>
<div> 
$$
\begin{align*}
\langle x, y \rangle &amp;= x_1y_1 + ... + x_ny_n = \sum_j x_jy_j
\end{align*}
$$
</div>
<p>which is commonly known as the dot product.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 4</b></h4>
<p>Another example is \(V = \mathbf{R}^2\) over \(\mathbf{R}\) where</p>
<div> 
$$
\begin{align*}
\langle \langle x, y \rangle \rangle &amp;= 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2
\end{align*}
$$
</div>
<p>We claim that \(\langle \langle \ , \ \rangle \rangle\) is an inner product. To see this check each property in the definition. For example for property 4, we see that</p>
<div> 
$$
\begin{align*}
\langle \langle x, x \rangle \rangle &amp;= 2x_1x_1 + x_1x_2 + x_2x_1 + x_2x_2 \\
                                     &amp;= 2x_1^2 + 2x_1x_2 + x_2^2 \\
									 &amp;= x_1^2 + (x_1 + x_2)^2 &gt; 0
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 5</b></h4>
<p>Define \(V = \mathbf{C}\) over \(\mathbf{C}\) where</p>
<div> 
$$
\begin{align*}
\langle z_1, z_2 \rangle &amp;= z_1\bar{z_2}
\end{align*}
$$
</div>
<p>Note here that if we defined the product as \(\langle z_1, z_2 \rangle = z_1z_2\). This will fail to satisfy the inner product conditions.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 6: Frobenius Inner Product</b></h4>
<p>Define \(V = \mathbf{C}^0([0,1]) = \{f: [0,1] \rightarrow \mathbf{R} \ | \ f \text{ continuous}\}\)</p>
<div> 
$$
\begin{align*}
\langle f, g \rangle &amp;= \int_0^1 f(t)g(t)dt
\end{align*}
$$
</div>
<p>is an inner product. Checking property (4)</p>
<div> 
$$
\begin{align*}
\langle f, g \rangle &amp;= \int_0^1 f(t)f(t) dt \\
                    &amp;= \int_0^1 f^2(t) dt \\
					&amp;&gt; 0 \text{ unless $f(t)=0 \ \forall t \in [0,1]$ }				
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example 7</b></h4>
<p>Define \(V = M_{n \times n}(\mathbf{R})\)</p>
<div> 
$$
\begin{align*}
\langle A, B \rangle &amp;= tr(B^tA)
\end{align*}
$$
</div>
<p>where</p>
<div> 
$$
\begin{align*}
tr: \ &amp;M_{n \times n}(\mathbf{R}) \rightarrow M_{n \times n}(\mathbf{R}) \\
&amp;C \rightarrow \sum_j C_{jj} = C_{11} + C_{22} + ... + C_{nn}
\end{align*}
$$
</div>
<p>Does this map satisfy the inner product conditions?</p>
<ol type="i">
	<!------------(i)---------------->
	<li> We want to show that \(\langle A + C, B \rangle = \langle A, B \rangle + \langle C, B \rangle\). Expand the inner product to see that
		<div> 
		$$
		\begin{align*}
		\langle A+C, B \rangle &amp;= tr(B^t(A+C)) \\
		                       &amp;= tr(B^tA + B^tC) \\
							   &amp;= tr(B^tA) + tr(B^tC) \\
							   &amp;= \langle A, B \rangle + \langle C, B \rangle 
		\end{align*}
		$$
		</div>
	</li>
	<!------------(ii)---------------->
	<li>We want to show that \(\langle cA, B \rangle = c\langle A, B \rangle\):
	<div> 
	$$
	\begin{align*}
	\langle cA, B \rangle &amp;= tr(B^t(cA)) \\
	                       &amp;= ctr(B^tA) \\
						   &amp;= c\langle A, B \rangle
	\end{align*}
	$$
	</div>
	</li>
	<!------------(iii)---------------->
	<li>We want to show that \(\langle A, B \rangle = \langle B, A \rangle\). (over \(\mathbf{R}\)). Note here that \(tr(C^t) = tr(C)\). Then
	<div> 
	$$
	\begin{align*}
	\langle A, B \rangle &amp;= tr(B^tA) \\
	                       &amp;= tr((B^tA)^t) \\
	                       &amp;= tr(A^tB) \\
						   &amp;= \langle B, A \rangle
	\end{align*}
	$$
	</div>
	</li>
	<!------------(iv)---------------->
	<li>We need to show that \(\langle A, A \rangle &gt; 0 \text{ if } A \neq \bar{0} \in M_{n \times n}(\mathbf{R})\)
		<div> 
		$$
		\begin{align*}
		\langle A, A \rangle &amp;= tr(A^tA) \\
		                       &amp;= \sum_j (A^tA)_{jj} \\
		                       &amp;= \sum_j \sum_k (A^t)_{jk} (A)_{kj} \\
		                       &amp;= \sum_j \sum_k (A)_{kj} (A)_{kj} \\
							   &amp;= \sum_j \sum_k A^2_{kj}						   
		\end{align*}
		$$
		</div>
		Note here that \(A^2_{kj} &gt; 0\) unless \(A_{1j},...A_{nj} = 0\). These are the entries of the \(j\)th column of \(A\). This means that this is zero unless \(A\) is the zero matrix.
	</li>
</ol>
<p>In fact, \(tr(B^tA) = \sum_{ij} A_{ij}B_{ij}\). the definition of \(tr(B^tA)\) is for square matrices while the definition \(\sum_{ij} A_{ij}B_{ij}\) works for any matrices. This works for an inner product on matrices.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Inner Product Spaces</b></h4>
<p>So far, we’ve seen that inner products are not unique. We can define many inner products on a given vector space. So we have the following definition to fix a specific inner product on a vector space
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
An inner product space is a vector space is a vector space \(V\) with a fixed inner product.
</div>
<p><br />
<br />
Example: (\(\mathbf{R}^2, \langle x,y \rangle = x_1y_1 + x_2y_2\)) is an inner product space different from \((\mathbf{R}^2, \langle x,y \rangle = 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2)\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Norm of a Vector</b></h4>
<p>For the rest of the lecture, we’re going to assume that we have a fixed inner product on \(V\). 
<br />
<br /></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
Let \(V, \langle \ , \ \rangle\) be an inner product space. The length (or norm) of \(v \in V\) is
		$$
		\begin{align*}
		\Vert v \Vert = \sqrt{\langle v, v \rangle}				   
		\end{align*}
		$$
</div>
<p><br />
<br />
Example \(V = \mathbf{C}^0([0,1])\), Let \(\Vert f \Vert: (\int_0^1 f(t)^2 dt)^{1/2}\). This is also called the \(L^2\)-norm.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The distance between \(x, y\) in \(V\) is \(\Vert x - y \Vert\)
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="bdiv">
Definition
</div>
<div class="bbdiv">
The sphere of radius \(r\) and center \(x \in V\) is \(\{y \in V \ | \ \Vert x - y \Vert = r \}\)
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<div class="purdiv">
Theorem
</div>
<div class="purbdiv">
For any \(x, y \in V\) and \(c \in \mathbf{F}\)
<ol type="a">
	<li>\(\Vert cx \Vert = |c|\Vert x \Vert  \)</li>
	<li>\(\Vert x \Vert = 0 \leftrightarrow x = \bar{0}_V \)</li>
	<li>\(| \langle x , y \rangle | \leq \Vert x \Vert \Vert y \Vert \). Cauchy-Schwarz</li>
	<li>\(\Vert x + y \Vert \leq \Vert x \Vert \Vert y \Vert \). The Triangle Inequality</li>
	
</ol>
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------------>
The proof for (a) and (b) follow easily. For (c). The motivation is from \(\mathbf{R}^2\) with the standard inner product.</p>
<div> 
$$
\begin{align*}
 \langle x , y \rangle  &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert  \cos(\theta) \\
| \langle x , y \rangle | &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert | \cos(\theta)|
\end{align*}
$$
</div>
<p>But we know that \(| \cos(\theta)| \leq 1\). Therefore</p>
<div> 
$$
\begin{align*}
| \langle x , y \rangle | &amp;\leq x \cdot y = \Vert x \Vert \Vert y \Vert 
\end{align*}
$$
</div>
<p>But we want to prove this in general so:
<br />
<br />
<!------------------------------------------------------------------------------------>
<b>Proof (c)</b>:
<br />
<br />
If \(y = \bar{0}_V\), then the inequality is true and we are done. So assume that \(y \neq \bar{0}_V\). If we multiply both sides by \(\frac{1}{\Vert y \Vert}\) which is a scalar, then this scalar can be factored out by property 1. Therefore, we can scale \(y\) by whatever factor we want and so let’s just assume that \(y\) has length 1 (\(\Vert y \Vert = 1\)). So it suffies to show that</p>
<div> 
$$
\begin{align*}
| \langle x , y \rangle | &amp;\leq \Vert x \Vert \\
| \langle x , y \rangle |^2 &amp;\leq \Vert x \Vert^2 \text{ (because it's easier than dealing with a squareroot)} \\
&amp;= \langle x, x \rangle 
\end{align*}
$$
</div>
<p>To show this, take the project of vector \(x\) onto vector \(y\) which has length 1. The projection is a vector \(x - \langle x , y \rangle y\). Moreover,</p>
<div> 
$$
\begin{align*}
0 &amp;\leq \Vert x - \langle x , y \rangle y \Vert^2 \text{ ( the length of any vector $\geq 0$)} \\
 &amp;= \langle x - \langle x , y \rangle y, x - \langle x , y \rangle y \rangle \\
 &amp;= \langle x, x \rangle - \langle\langle x , y \rangle y, x\rangle +  \langle x, -\langle x , y \rangle y\rangle + \langle - \langle x, y \rangle y, - \langle x, y \rangle y \rangle \text{( by property 1)} \\
 &amp;= \Vert x \Vert^2 - \langle x, y \rangle \langle y, x \rangle + \overline{- \langle x, y \rangle y, x \rangle} + \Vert -\langle x, y \rangle y \Vert^2 \\ 
 &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 -  \overline{\langle x, y \rangle} \overline{\langle y, x \rangle}  + |\langle x, y \rangle |^2 \Vert y \Vert^2 \\
 &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 - |\langle x, y \rangle |^2 + |\langle x, y \rangle |^2 \\
 &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------>
<b>Proof (d)</b>
<br /></p>
<div> 
$$
\begin{align*}
\Vert x + y \Vert^2 &amp;= \langle x + y, x + y \rangle \\
                   &amp;= \Vert x \Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2 \\
                   &amp;\leq \Vert x \Vert^2 + 2 |\langle x, y \rangle| + \Vert y \Vert^2 \text{ multiply a complex number by its conjecture to see}\\
                   &amp;\leq \Vert x \Vert^2 + 2 \Vert x \Vert \Vert y \Vert + \Vert y \Vert^2 \text { (By (c))}\\
                   &amp;= (\Vert x \Vert + \Vert y \Vert)^2 \\				   
\end{align*}
$$
</div>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>Math416 by Ely Kerman</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Let \(V\) be. vector space over \(\mathbf{F} = \mathbf{R}\) or \(\mathbf{C}\). $$ \begin{align*} \mathbf{C} = \{ z=a+ib \ | \ a, b \in \mathbf{R} \} \end{align*} $$ The complex conjugate of \(z\) is \(\bar{z} = a - ib\). $$ \begin{align*} z\bar{z} = (a + ib)(a - ib) = a^2 + b^2 = |z|^2 \end{align*} $$ Definition An inner product \(\langle \ , \ \rangle\) on a vector \(V\) is a map $$ \begin{align*} \langle \ , \ \rangle : \ &amp;V \times V \rightarrow \mathbf{F} = \mathbf{R} \text{ or } \mathbf{C} \\ &amp;(x, y) \rightarrow \langle x , y \rangle \end{align*} $$ such that \(\langle x+z, y \rangle = \langle x , y \rangle + \langle z , y \rangle\) \(\langle cx, y \rangle = c \langle x , y \rangle\) \(\langle x, y \rangle = \overline{\langle y, x \rangle}\). Note if \(\mathbf{F} = \mathbf{R}\), then \(\langle x, y \rangle = \langle y, x \rangle\) \(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\) Example 1 The simplest example is \(V = \mathbf{R}^1\) (vector space over \(\mathbf{R}\)) where $$ \begin{align*} \langle x, y \rangle &amp;= xy \end{align*} $$ This map satisfies the inner product properties. Notice that \(\langle x+z, y \rangle = (x+z)y = xy + zx = \langle x , y \rangle + \langle z , y \rangle \) \(\langle cx, y \rangle = cxy = c\langle x, y \rangle\) \(\langle x, y \rangle = \overline{\langle y, x \rangle} = yx = xy\). \(\langle x, x \rangle &gt; 0 \text{ if } x \neq \bar{0}\) Example 2 We can also define this inner product $$ \begin{align*} \langle \langle x, y \rangle \rangle_c &amp;= c^2xy \quad c \neq 0 \end{align*} $$ which also satisfies the inner product properties (TODO) Example 3: Dot Product Another example is \(V = \mathbf{R}^n\) over \(\mathbf{R}\) where $$ \begin{align*} \langle x, y \rangle &amp;= x_1y_1 + ... + x_ny_n = \sum_j x_jy_j \end{align*} $$ which is commonly known as the dot product. Example 4 Another example is \(V = \mathbf{R}^2\) over \(\mathbf{R}\) where $$ \begin{align*} \langle \langle x, y \rangle \rangle &amp;= 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2 \end{align*} $$ We claim that \(\langle \langle \ , \ \rangle \rangle\) is an inner product. To see this check each property in the definition. For example for property 4, we see that $$ \begin{align*} \langle \langle x, x \rangle \rangle &amp;= 2x_1x_1 + x_1x_2 + x_2x_1 + x_2x_2 \\ &amp;= 2x_1^2 + 2x_1x_2 + x_2^2 \\ &amp;= x_1^2 + (x_1 + x_2)^2 &gt; 0 \end{align*} $$ Example 5 Define \(V = \mathbf{C}\) over \(\mathbf{C}\) where $$ \begin{align*} \langle z_1, z_2 \rangle &amp;= z_1\bar{z_2} \end{align*} $$ Note here that if we defined the product as \(\langle z_1, z_2 \rangle = z_1z_2\). This will fail to satisfy the inner product conditions. Example 6: Frobenius Inner Product Define \(V = \mathbf{C}^0([0,1]) = \{f: [0,1] \rightarrow \mathbf{R} \ | \ f \text{ continuous}\}\) $$ \begin{align*} \langle f, g \rangle &amp;= \int_0^1 f(t)g(t)dt \end{align*} $$ is an inner product. Checking property (4) $$ \begin{align*} \langle f, g \rangle &amp;= \int_0^1 f(t)f(t) dt \\ &amp;= \int_0^1 f^2(t) dt \\ &amp;&gt; 0 \text{ unless $f(t)=0 \ \forall t \in [0,1]$ } \end{align*} $$ Example 7 Define \(V = M_{n \times n}(\mathbf{R})\) $$ \begin{align*} \langle A, B \rangle &amp;= tr(B^tA) \end{align*} $$ where $$ \begin{align*} tr: \ &amp;M_{n \times n}(\mathbf{R}) \rightarrow M_{n \times n}(\mathbf{R}) \\ &amp;C \rightarrow \sum_j C_{jj} = C_{11} + C_{22} + ... + C_{nn} \end{align*} $$ Does this map satisfy the inner product conditions? We want to show that \(\langle A + C, B \rangle = \langle A, B \rangle + \langle C, B \rangle\). Expand the inner product to see that $$ \begin{align*} \langle A+C, B \rangle &amp;= tr(B^t(A+C)) \\ &amp;= tr(B^tA + B^tC) \\ &amp;= tr(B^tA) + tr(B^tC) \\ &amp;= \langle A, B \rangle + \langle C, B \rangle \end{align*} $$ We want to show that \(\langle cA, B \rangle = c\langle A, B \rangle\): $$ \begin{align*} \langle cA, B \rangle &amp;= tr(B^t(cA)) \\ &amp;= ctr(B^tA) \\ &amp;= c\langle A, B \rangle \end{align*} $$ We want to show that \(\langle A, B \rangle = \langle B, A \rangle\). (over \(\mathbf{R}\)). Note here that \(tr(C^t) = tr(C)\). Then $$ \begin{align*} \langle A, B \rangle &amp;= tr(B^tA) \\ &amp;= tr((B^tA)^t) \\ &amp;= tr(A^tB) \\ &amp;= \langle B, A \rangle \end{align*} $$ We need to show that \(\langle A, A \rangle &gt; 0 \text{ if } A \neq \bar{0} \in M_{n \times n}(\mathbf{R})\) $$ \begin{align*} \langle A, A \rangle &amp;= tr(A^tA) \\ &amp;= \sum_j (A^tA)_{jj} \\ &amp;= \sum_j \sum_k (A^t)_{jk} (A)_{kj} \\ &amp;= \sum_j \sum_k (A)_{kj} (A)_{kj} \\ &amp;= \sum_j \sum_k A^2_{kj} \end{align*} $$ Note here that \(A^2_{kj} &gt; 0\) unless \(A_{1j},...A_{nj} = 0\). These are the entries of the \(j\)th column of \(A\). This means that this is zero unless \(A\) is the zero matrix. In fact, \(tr(B^tA) = \sum_{ij} A_{ij}B_{ij}\). the definition of \(tr(B^tA)\) is for square matrices while the definition \(\sum_{ij} A_{ij}B_{ij}\) works for any matrices. This works for an inner product on matrices. Inner Product Spaces So far, we’ve seen that inner products are not unique. We can define many inner products on a given vector space. So we have the following definition to fix a specific inner product on a vector space Definition An inner product space is a vector space is a vector space \(V\) with a fixed inner product. Example: (\(\mathbf{R}^2, \langle x,y \rangle = x_1y_1 + x_2y_2\)) is an inner product space different from \((\mathbf{R}^2, \langle x,y \rangle = 2x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2)\) The Norm of a Vector For the rest of the lecture, we’re going to assume that we have a fixed inner product on \(V\). Definition Let \(V, \langle \ , \ \rangle\) be an inner product space. The length (or norm) of \(v \in V\) is $$ \begin{align*} \Vert v \Vert = \sqrt{\langle v, v \rangle} \end{align*} $$ Example \(V = \mathbf{C}^0([0,1])\), Let \(\Vert f \Vert: (\int_0^1 f(t)^2 dt)^{1/2}\). This is also called the \(L^2\)-norm. Definition The distance between \(x, y\) in \(V\) is \(\Vert x - y \Vert\) Definition The sphere of radius \(r\) and center \(x \in V\) is \(\{y \in V \ | \ \Vert x - y \Vert = r \}\) Theorem For any \(x, y \in V\) and \(c \in \mathbf{F}\) \(\Vert cx \Vert = |c|\Vert x \Vert \) \(\Vert x \Vert = 0 \leftrightarrow x = \bar{0}_V \) \(| \langle x , y \rangle | \leq \Vert x \Vert \Vert y \Vert \). Cauchy-Schwarz \(\Vert x + y \Vert \leq \Vert x \Vert \Vert y \Vert \). The Triangle Inequality The proof for (a) and (b) follow easily. For (c). The motivation is from \(\mathbf{R}^2\) with the standard inner product. $$ \begin{align*} \langle x , y \rangle &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert \cos(\theta) \\ | \langle x , y \rangle | &amp;= x \cdot y = \Vert x \Vert \Vert y \Vert | \cos(\theta)| \end{align*} $$ But we know that \(| \cos(\theta)| \leq 1\). Therefore $$ \begin{align*} | \langle x , y \rangle | &amp;\leq x \cdot y = \Vert x \Vert \Vert y \Vert \end{align*} $$ But we want to prove this in general so: Proof (c): If \(y = \bar{0}_V\), then the inequality is true and we are done. So assume that \(y \neq \bar{0}_V\). If we multiply both sides by \(\frac{1}{\Vert y \Vert}\) which is a scalar, then this scalar can be factored out by property 1. Therefore, we can scale \(y\) by whatever factor we want and so let’s just assume that \(y\) has length 1 (\(\Vert y \Vert = 1\)). So it suffies to show that $$ \begin{align*} | \langle x , y \rangle | &amp;\leq \Vert x \Vert \\ | \langle x , y \rangle |^2 &amp;\leq \Vert x \Vert^2 \text{ (because it's easier than dealing with a squareroot)} \\ &amp;= \langle x, x \rangle \end{align*} $$ To show this, take the project of vector \(x\) onto vector \(y\) which has length 1. The projection is a vector \(x - \langle x , y \rangle y\). Moreover, $$ \begin{align*} 0 &amp;\leq \Vert x - \langle x , y \rangle y \Vert^2 \text{ ( the length of any vector $\geq 0$)} \\ &amp;= \langle x - \langle x , y \rangle y, x - \langle x , y \rangle y \rangle \\ &amp;= \langle x, x \rangle - \langle\langle x , y \rangle y, x\rangle + \langle x, -\langle x , y \rangle y\rangle + \langle - \langle x, y \rangle y, - \langle x, y \rangle y \rangle \text{( by property 1)} \\ &amp;= \Vert x \Vert^2 - \langle x, y \rangle \langle y, x \rangle + \overline{- \langle x, y \rangle y, x \rangle} + \Vert -\langle x, y \rangle y \Vert^2 \\ &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 - \overline{\langle x, y \rangle} \overline{\langle y, x \rangle} + |\langle x, y \rangle |^2 \Vert y \Vert^2 \\ &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 - |\langle x, y \rangle |^2 + |\langle x, y \rangle |^2 \\ &amp;= \Vert x \Vert^2 - |\langle x, y \rangle |^2 \end{align*} $$ Proof (d) $$ \begin{align*} \Vert x + y \Vert^2 &amp;= \langle x + y, x + y \rangle \\ &amp;= \Vert x \Vert^2 + \langle x, y \rangle + \langle y, x \rangle + \Vert y \Vert^2 \\ &amp;\leq \Vert x \Vert^2 + 2 |\langle x, y \rangle| + \Vert y \Vert^2 \text{ multiply a complex number by its conjecture to see}\\ &amp;\leq \Vert x \Vert^2 + 2 \Vert x \Vert \Vert y \Vert + \Vert y \Vert^2 \text { (By (c))}\\ &amp;= (\Vert x \Vert + \Vert y \Vert)^2 \\ \end{align*} $$]]></summary></entry></feed>