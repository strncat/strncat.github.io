<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-16T20:26:01-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">nemo’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">Lecture 13: Cosets</title><link href="http://localhost:4000/jekyll/update/2025/02/05/math417-13-cosets.html" rel="alternate" type="text/html" title="Lecture 13: Cosets" /><published>2025-02-05T00:01:36-08:00</published><updated>2025-02-05T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/05/math417-13-cosets</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/05/math417-13-cosets.html"><![CDATA[<div class="mintheaderdiv">
Definition 2.4.14
</div>
<div class="mintbodydiv">
Let \(H\) be subgroup of a group \(G\). <br />
\(gH = \{gh \ | \ h \in H\}\) is called a left coset of \(H\) in \(G\). 
<br />
\(Hg = \{hg \ | \ h \in H\}\) is called a right coset of \(H\) in \(G\).
</div>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>Example 1: Let \(D_4 = \{e, r, r^2, r^3, j, rj, r^2j, r^3j\}\). where \(r^4 = e = j^2\) and \(jr = r^{-1}j\).
<br /> Define the subgroup \(H = \langle j \rangle = \{e, j\}\). The left cosets are:</p>
<div>
$$
\begin{align*}
eH &amp;= \{e, j\} = jH \\
rH &amp;= \{r, rj\} = rjH \\
r^2H &amp;= \{r^2, r^2j\} = r^2jH \\
r^3H &amp;= \{r^3, r^3j\} = r^3jH \\
\end{align*}
$$
</div>
<p>Notice here that none of the elements overlap between the sets so this gives us a way to partition the set \(G\) into four pairwise disjoint subsets (We’ll prove this next). The right cosets are</p>
<div>
$$
\begin{align*}
He &amp;= \{e, j\} = Hj \\
Hr &amp;= \{r, jr\} = \{e, r^3j\} = Hr^3j \\
Hr^2 &amp;= \{r^2, r^2j\} = Hr^2j \\
Hr^3 &amp;= \{r^3, rj\} = Hrj \\
\end{align*}
$$
</div>
<p>Example 2: Let \(G = \mathbf{Z}\) (with addition) and take \(H = \mathbf{Z}n \leq G\) where \(n \geq 1\). <br />
The left coset is \(a + H = a + \mathbf{Z}n = \{a + kn \ | \ k \in \mathbf{Z}\}\) <br />
The left coset is \(H + a = \mathbf{Z}n + a\) which is the same as the right coset. This happens because the group is abelian!<br />
Another name for these cosets are the congruence class \([a]_n\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cosets are Pairwise Disjoint</b></h4>
<p>Next we show why these subets must be pairwise disjoint.
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (2.5.4)
</div>
<div class="peachbodydiv">
Let \(H\) be a subgroup of a group \(G\). If \(X, Y \subseteq G\) are left \(H-\)cosets, then either
<ol>
	<li>\(X \cap Y = \emptyset\).</li>
	<li>\(X = Y\).</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------>
<b>Proof</b>
<br />
We’ll prove that not (1) implies (2) (to prove an or statement). <br />
Suppose that \(g \in X \cap Y\). We want to show that \(X = Y\). Since \(X\) and \(Y\) are left cosets, then we can write \(X = xH\) and \(Y = yH\) for some \(x, y \in G\). Since \(g \in X \cap Y\), then we can write \(g = xh_1 = yh_2\) for some \(h_1, h_2 \in H\). Observe now that</p>
<div>
$$
\begin{align*}
x &amp;= gh_1^{-1} \\
  &amp;= (yh_2)h_1^{-1} \\
  &amp;= y(h_2h_1^{-1})
\end{align*}
$$
</div>
<p>Likewise</p>
<div>
$$
\begin{align*}
y &amp;= gh_2^{-1} \\
  &amp;= (xh_1)h_2^{-1} \\
  &amp;= x(h_1h_2^{-1})
\end{align*}
$$
</div>
<p>Now, let \(xh \in xH\), then</p>
<div>
$$
\begin{align*}
xh &amp;= (yh_2h_1^{-1})h \\
   &amp;= y(h_2h_1^{-1}h)
\end{align*}
$$
</div>
<p>But this implies that \(xh \in yH\). So \(xH \subseteq yH\). Likewise, let \(yh \in yH\), then</p>
<div>
$$
\begin{align*}
yh &amp;= (xh_1h_2^{-1})h \\
   &amp;= x(h_1h_2^{-1}h)
\end{align*}
$$
</div>
<p>so \(yh \in xH\) and \(yH \subseteq xH\). Therefore \(xH = yH\). \(\blacksquare\) 
<br />
<br />
Based on this, we have the following proposition
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 2.5.3
</div>
<div class="peachbodydiv">
Let \(H\) be a subgroup of a group \(G\), and let \(a\) and \(b\) be
elements of \(G\). The following conditions are equivalent:
<ol type="a">
	<li>\(a \in bH\).</li>
	<li>\(b \in aH\).</li>
	<li>\(aH = bH\).</li>
	<li>\(b^{-1}a \in H\).</li>
	<li>\(a^{-1}b \in H\).</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof (book)</b>
<br />
Suppose \((a)\) holds. Let \(a \in bH\). Then we can write \(a = bh\). Since \(H\) is a subgroup, then we know \(h^{-1} \in H\). So we can write \(b = ah^{-1}\). But \(ah^{-1}\) is in \(aH\) by the definition of a left coset which is what we wanted to show. With similar reason \((b)\) also implies \((a)\). 
<br />
<br />
For \((c)\), we want to show that \(aH \subseteq bH\) and \(bH \subseteq aH\). To show that \(bH \subseteq aH\), we want to show that for any arbitrary element \(x\) in \(bH\), that \(x\) is also in \(aH\). So consider any \(x \in bH\). By the definition of a coset, we can write \(x = bh\) for some \(h \in H\). Now suppose that \((b)\) holds and so we have \(b \in aH\). This means that we can write \(b = ah_1\) for some \(h_1 \in H\). But this means that we can write \(x = bh = ah_1h\). The product \(h_1h\) is in \(H\) because \(H\) is a subgroup. Furthermore, \(ah_1h\) must be in \(aH\) by the definition of a coset. Therefore, \(x = bh \in aH\) and so \(bH \subseteq aH\) as we wanted to show. Since \((b)\) also implies \((a)\) we can use a similar reasoning to show that \(aH \subseteq bH\).
<br />
<br />
For \((d)\) ….
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Lagrange Theorem</b></h4>
<p>Based on what we learned so far, we can now present the following important results
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(H\) be a subgroup of a group \(G\). Then all left \(H-\)cosets and right \(H-\)cosets have the same cardinality as \(H\).
</div>
<p><br />
This works well for infinite groups as well because all we need is a bijection to show they have the same cardinality.
<br />
<br />
<!------------------------------------------------------------------------>
<b>Proof</b>
<br />
We know the coset \(aH\) is constructed such that each element is appended on the left by \(a\). So naturally construct the following bijection</p>
<div>
$$
\begin{align*}
H &amp;\rightarrow aH \\
h &amp;\rightarrow ah
\end{align*}
$$
</div>
<p>This works. For any element in \(x \in aH\), the inverse is just \(a^{-1}x\). Likewise we can construct a bijection for the right coset as follows</p>
<div>
$$
\begin{align*}
H &amp;\rightarrow Ha \\
h &amp;\rightarrow ha
\end{align*}
$$
</div>
<p><br />
This leads to Lagrange Theorem.
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Lagrange Theorem
</div>
<div class="peachbodydiv">
Let \(H\) be a subgroup of a finite group \(G\). Then \(|H|\) divides \(|G|\).
</div>
<p><br />
<br />Proof&lt;/b&gt;
<br />
We know by the previous proposition that all cosets have the same cardinality as \(H\) itself. Moreover, we know that the collection of left cosets \(\{aH, a \in G\}\) partitions \(G\) into pairwise disjoint subsets. Therefore</p>
<div>
$$
\begin{align*}
|G| = \sum|aH| \\
    &amp;= \sum |H| \quad \text{(by the previous proposition)}
\end{align*}
$$
</div>
<p>From this we see that \(|G| = m|H|\) as we wanted to show. \(\ \blacksquare\)
<br />
<br />
This in turn leads to the following theorem
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Order Theorem
</div>
<div class="peachbodydiv">
Let \(H\) be a subgroup of a finite group \(G\). Then for any \(g \in G\), \(o(g) = |\langle g \rangle|\) divides \(|G|\).
</div>
<p><br />
<b>Proof</b>
[TODO]
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>More Definitions in Cosets</b></h4>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
	The index of a subgroup \(H \leq G\) is the number of left \(H-\)cosets.
	We denote the index by \([G:H]\) which is index of \(H\) in \(G\).
</div>
<p><br />
The index could be infinite but we only care when the index is finite. 
<br />
Using Lagrange’s Theorem also note that if \(G\) is finite, then \([G : H] = \frac{|G|}{|H|}\). 
<br />
Also note that \([G:H]\) can be finite even if \(G\) and \(H\) are infinte.
<br />
<br />
Example: Let \(G = \mathbf{Z}, H = \mathbf{Z_n}\) where \(n &gt; 0\). Then the index of \(\mathbf{Z}_n\) inside \(\mathbf{Z}\) is the number of left cosets which is the number of congruent classes, \([\mathbf{Z}:\mathbf{Z}_n] = n\)</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 2.4.14 Let \(H\) be subgroup of a group \(G\). \(gH = \{gh \ | \ h \in H\}\) is called a left coset of \(H\) in \(G\). \(Hg = \{hg \ | \ h \in H\}\) is called a right coset of \(H\) in \(G\). Examples Example 1: Let \(D_4 = \{e, r, r^2, r^3, j, rj, r^2j, r^3j\}\). where \(r^4 = e = j^2\) and \(jr = r^{-1}j\). Define the subgroup \(H = \langle j \rangle = \{e, j\}\). The left cosets are: $$ \begin{align*} eH &amp;= \{e, j\} = jH \\ rH &amp;= \{r, rj\} = rjH \\ r^2H &amp;= \{r^2, r^2j\} = r^2jH \\ r^3H &amp;= \{r^3, r^3j\} = r^3jH \\ \end{align*} $$ Notice here that none of the elements overlap between the sets so this gives us a way to partition the set \(G\) into four pairwise disjoint subsets (We’ll prove this next). The right cosets are $$ \begin{align*} He &amp;= \{e, j\} = Hj \\ Hr &amp;= \{r, jr\} = \{e, r^3j\} = Hr^3j \\ Hr^2 &amp;= \{r^2, r^2j\} = Hr^2j \\ Hr^3 &amp;= \{r^3, rj\} = Hrj \\ \end{align*} $$ Example 2: Let \(G = \mathbf{Z}\) (with addition) and take \(H = \mathbf{Z}n \leq G\) where \(n \geq 1\). The left coset is \(a + H = a + \mathbf{Z}n = \{a + kn \ | \ k \in \mathbf{Z}\}\) The left coset is \(H + a = \mathbf{Z}n + a\) which is the same as the right coset. This happens because the group is abelian! Another name for these cosets are the congruence class \([a]_n\). Cosets are Pairwise Disjoint Next we show why these subets must be pairwise disjoint. Proposition (2.5.4) Let \(H\) be a subgroup of a group \(G\). If \(X, Y \subseteq G\) are left \(H-\)cosets, then either \(X \cap Y = \emptyset\). \(X = Y\). Proof We’ll prove that not (1) implies (2) (to prove an or statement). Suppose that \(g \in X \cap Y\). We want to show that \(X = Y\). Since \(X\) and \(Y\) are left cosets, then we can write \(X = xH\) and \(Y = yH\) for some \(x, y \in G\). Since \(g \in X \cap Y\), then we can write \(g = xh_1 = yh_2\) for some \(h_1, h_2 \in H\). Observe now that $$ \begin{align*} x &amp;= gh_1^{-1} \\ &amp;= (yh_2)h_1^{-1} \\ &amp;= y(h_2h_1^{-1}) \end{align*} $$ Likewise $$ \begin{align*} y &amp;= gh_2^{-1} \\ &amp;= (xh_1)h_2^{-1} \\ &amp;= x(h_1h_2^{-1}) \end{align*} $$ Now, let \(xh \in xH\), then $$ \begin{align*} xh &amp;= (yh_2h_1^{-1})h \\ &amp;= y(h_2h_1^{-1}h) \end{align*} $$ But this implies that \(xh \in yH\). So \(xH \subseteq yH\). Likewise, let \(yh \in yH\), then $$ \begin{align*} yh &amp;= (xh_1h_2^{-1})h \\ &amp;= x(h_1h_2^{-1}h) \end{align*} $$ so \(yh \in xH\) and \(yH \subseteq xH\). Therefore \(xH = yH\). \(\blacksquare\) Based on this, we have the following proposition Proposition 2.5.3 Let \(H\) be a subgroup of a group \(G\), and let \(a\) and \(b\) be elements of \(G\). The following conditions are equivalent: \(a \in bH\). \(b \in aH\). \(aH = bH\). \(b^{-1}a \in H\). \(a^{-1}b \in H\). Proof (book) Suppose \((a)\) holds. Let \(a \in bH\). Then we can write \(a = bh\). Since \(H\) is a subgroup, then we know \(h^{-1} \in H\). So we can write \(b = ah^{-1}\). But \(ah^{-1}\) is in \(aH\) by the definition of a left coset which is what we wanted to show. With similar reason \((b)\) also implies \((a)\). For \((c)\), we want to show that \(aH \subseteq bH\) and \(bH \subseteq aH\). To show that \(bH \subseteq aH\), we want to show that for any arbitrary element \(x\) in \(bH\), that \(x\) is also in \(aH\). So consider any \(x \in bH\). By the definition of a coset, we can write \(x = bh\) for some \(h \in H\). Now suppose that \((b)\) holds and so we have \(b \in aH\). This means that we can write \(b = ah_1\) for some \(h_1 \in H\). But this means that we can write \(x = bh = ah_1h\). The product \(h_1h\) is in \(H\) because \(H\) is a subgroup. Furthermore, \(ah_1h\) must be in \(aH\) by the definition of a coset. Therefore, \(x = bh \in aH\) and so \(bH \subseteq aH\) as we wanted to show. Since \((b)\) also implies \((a)\) we can use a similar reasoning to show that \(aH \subseteq bH\). For \((d)\) …. Lagrange Theorem Based on what we learned so far, we can now present the following important results Proposition Let \(H\) be a subgroup of a group \(G\). Then all left \(H-\)cosets and right \(H-\)cosets have the same cardinality as \(H\). This works well for infinite groups as well because all we need is a bijection to show they have the same cardinality. Proof We know the coset \(aH\) is constructed such that each element is appended on the left by \(a\). So naturally construct the following bijection $$ \begin{align*} H &amp;\rightarrow aH \\ h &amp;\rightarrow ah \end{align*} $$ This works. For any element in \(x \in aH\), the inverse is just \(a^{-1}x\). Likewise we can construct a bijection for the right coset as follows $$ \begin{align*} H &amp;\rightarrow Ha \\ h &amp;\rightarrow ha \end{align*} $$ This leads to Lagrange Theorem. Lagrange Theorem Let \(H\) be a subgroup of a finite group \(G\). Then \(|H|\) divides \(|G|\). Proof&lt;/b&gt; We know by the previous proposition that all cosets have the same cardinality as \(H\) itself. Moreover, we know that the collection of left cosets \(\{aH, a \in G\}\) partitions \(G\) into pairwise disjoint subsets. Therefore $$ \begin{align*} |G| = \sum|aH| \\ &amp;= \sum |H| \quad \text{(by the previous proposition)} \end{align*} $$ From this we see that \(|G| = m|H|\) as we wanted to show. \(\ \blacksquare\) This in turn leads to the following theorem Order Theorem Let \(H\) be a subgroup of a finite group \(G\). Then for any \(g \in G\), \(o(g) = |\langle g \rangle|\) divides \(|G|\). Proof [TODO] More Definitions in Cosets Definition The index of a subgroup \(H \leq G\) is the number of left \(H-\)cosets. We denote the index by \([G:H]\) which is index of \(H\) in \(G\). The index could be infinite but we only care when the index is finite. Using Lagrange’s Theorem also note that if \(G\) is finite, then \([G : H] = \frac{|G|}{|H|}\). Also note that \([G:H]\) can be finite even if \(G\) and \(H\) are infinte. Example: Let \(G = \mathbf{Z}, H = \mathbf{Z_n}\) where \(n &gt; 0\). Then the index of \(\mathbf{Z}_n\) inside \(\mathbf{Z}\) is the number of left cosets which is the number of congruent classes, \([\mathbf{Z}:\mathbf{Z}_n] = n\)]]></summary></entry><entry><title type="html">Lecture 12: Homomorphism</title><link href="http://localhost:4000/jekyll/update/2025/02/04/math417-12-homomorphism.html" rel="alternate" type="text/html" title="Lecture 12: Homomorphism" /><published>2025-02-04T00:01:36-08:00</published><updated>2025-02-04T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/04/math417-12-homomorphism</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/04/math417-12-homomorphism.html"><![CDATA[<p>While isomorphisms need to be bijections, Homomorphisms do not.</p>
<div class="mintheaderdiv">
Definition 2.4.1
</div>
<div class="mintbodydiv">
A map between groups \(\varphi : G \rightarrow H\) is called a homomorphism if it preserves group multiplication, \(\varphi(g_1)(g_2) = \varphi(g_1)\varphi(g_2)\) for all \(g_1, g_2 \in G\). An endomorphism of \(G\) is a homomorphism \(\varphi: G \rightarrow G\).
</div>
<p><br />
<!----------------------------------------------------------------------------->
So Given a homomorphism, if we find that it’s a bijection, then it is an isomorphism.
<br />
<br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Some Facts about Homomorphisms</b></h4>
<!------------------------------------------------------------------------------>
<div class="peachheaderdiv">
Proposition 2.4.11
</div>
<div class="peachbodydiv">
Let \(\varphi : G \rightarrow H\) and \(\psi : G \rightarrow H\) be homomorphisms of groups.
<ol type="a">
	<li>\(\varphi(e_G) = e_H\)</li>
	<li>For each \(g \in G\), \(\varphi(g^{-1}) = (\varphi(g))^{-1}\)</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof (Book)</b>
<br />
For any \(g \in G\).</p>
<div>
	$$
	\begin{align*}
	\varphi(e_G)\varphi(g) &amp;= \varphi(e_Gg) \quad \text{(because $\varphi$ is a homomorphism)} \\
	                 &amp;= \varphi(g)
	\end{align*}
	$$
</div>
<p>So \(\varphi(e_G)\) is an identity element in \(H\) but by the uniqueness of the identity element, then \(\varphi(e_G) = e_H\). Similarly, for any \(\in G\),</p>
<div>
	$$
	\begin{align*}
	\varphi(g^{-1})\varphi(g) &amp;= \varphi(g^{-1}g) \quad \text{(because $\varphi$ is a homomorphism)} \\
	                 &amp;= \varphi(e_G) \\
					 &amp; = e_H
	\end{align*}
	$$
</div>
<p>And by the uniqueness of the inverse, \(\varphi(g^{-1}) = \varphi(g)^{-1}\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>The Image of a Homomorphism</b></h4>
<p>Given a homomorphism \(\varphi\) between two groups \(G\) and \(H\), the image of \(\varphi\) which is a subset of \(H\) is also a subgroup.
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Given some homomorphism \(\varphi: G \rightarrow H\). The image of this function
	$$
	\begin{align*}
	\varphi(G) = \{\varphi(g) \ : \ g \in G\}
	\end{align*}
	$$
is a subgroup of \(H\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
TODO
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>The Kernel of a Homomorphism</b></h4>
<p>The kernel of a group is simply all the elements such that \(\varphi(g)=e\). 
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition 2.4.14
</div>
<div class="mintbodydiv">
Let \(\varphi: G \rightarrow H\) be a homomorphism of groups. The kernel of the homomorphism \(\varphi\), denoted \(ker(\varphi)\), is \(\varphi^{-1}(e_H) = \{g \in G \ : \ \varphi(g) = e_H\}\).
</div>
<p><br />
In fact, The kernel of a group \(K = ker \ \varphi\) is a subgroup of \(G\). 
<br />
<br />
<b>Proof</b>
<br />
We will show this by showing that the kernel satisfies the subgroup properties:</p>
<ol>
	<li>We know by Proposition 2.4.11, that \(\varphi(e_G) = e_H\) so \(e_G \in K\).</li>
	<li>Closed under product: For any \(a,b \in K\), \(\varphi(a)\varphi(b) = e_He_H = e_H \in K\)</li>
	<li>Closed under inverses: For any \(a \in K\), \(\varphi(a^{-1}) = \varphi(a)^{-1} = e_H^{-1} = e_H\). Therefore,\(a^{-1} \in K\). \(\ \blacksquare\)</li>
</ol>
<p><br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p><b>Example 1</b>: Consider the following with the addition operation</p>
<div>
	$$
	\begin{align*}
	\pi \ : \ &amp;\mathbf{Z} \rightarrow \mathbf{Z}_n\\
	          &amp;a \rightarrow [a]_n
	\end{align*}
	$$
</div>
<p>\(\pi\) is a homomorphism because \([a+b] = [a] + [b]\). Remember the rule is that \(\varphi(ab) = \varphi(a)\varphi(b)\) where the first operation comes from the first group while the second operation comes from the second group. Here, both groups have addition as their binary operation. 
<br />
<br />
\(\pi\) is also surjective. Since for any element \(h \in \mathbf{Z}_n\), we have an element \(g \in \mathbf{Z}\) such that \(\pi(g) = [g]\). Moreover, we see that \(ker(\pi) = \mathbf{Z}n = \langle n \rangle\).
<br />
<br />
<!----------------------------------------------------------------------------->
<b>Example 2</b>: Suppose \(g \in G\). Let</p>
<div>
	$$
	\begin{align*}
	\varphi \ : \ &amp;\mathbf{Z} \rightarrow G\\
	          &amp;k \rightarrow g^k
	\end{align*}
	$$
</div>
<p>We want to show that \(\varphi(ab) = \varphi(a)\varphi(b)\). But we know that \(\varphi(ab) = g^{a+b} =  g^ag^b = \varphi(a)\varphi(b)\). So \(\varphi\) is a homomorphism. 
<br />
<br />
Note here that the image of \(\varphi\) is the subgroup generated by \(g\), \(\langle g \rangle\). What about the kernel of \(\varphi\)?</p>
<div class="ediv">
  $$
  \begin{equation*}
  ker(\varphi) = \begin{cases} 
  \{0\} \quad &amp;\text{if } o(g) = \infty \\ 
  \mathbf{Z}d \quad \quad &amp;\text{if } o(g) = d
  \end{cases}
  \end{equation*}
  $$
</div>
<p><br />
<!----------------------------------------------------------------------------->
<b>Example 3: </b>Consider the following</p>
<div>
	$$
	\begin{align*}
	\varphi \ : \ &amp;D_n \rightarrow S_n\\
	          &amp;V \rightarrow Sym(V)
	\end{align*}
	$$
</div>
<p>where \(V\) is the vertices of the polygon. In fact \(\varphi(D_n)\) is isomorphic to \(D_n\) and it is a subgroup of \(S_n\).<br />
<br />
<br />
<!----------------------------------------------------------------------------->
<b>Example 4: </b>Consider the following</p>
<div>
	$$
	\begin{align*}
	\varphi \ : \ &amp;S_n \rightarrow GL(\mathbf{R}^{x})\\
	          &amp;\sigma \rightarrow P_{\sigma}
	\end{align*}
	$$
</div>
<p>where \(P_{\sigma}\) is the matrix associated with the permutation \(\sigma\). For example</p>
<div>
	$$
	\begin{align*}
	(1 \ 2 \ 3) \rightarrow 
	\begin{pmatrix}
	0 &amp; 0 &amp; 1 \\
	1 &amp; 0 &amp; 0 \\
	0 &amp; 1 &amp; 0
	\end{pmatrix}
	\end{align*}
	$$
</div>
<!----------------------------------------------------------------------------->
<p><b>Example 5: </b>Consider the following</p>
<div>
	$$
	\begin{align*}
	\psi \ : \ &amp;GL(\mathbf{R}^{x}) \rightarrow \mathbf{R}^{x}\\
	          &amp;P \rightarrow \det(P)
	\end{align*}
	$$
</div>
<p>This is another homomorphism. In fact, The composition of \(\psi \circ \varphi\) is a homomorphism. That is</p>
<div>
	$$
	\begin{align*}
	\psi \circ \varphi \ : \ &amp;S_n \rightarrow \mathbf{R}^{x}\\
	          &amp;\sigma \rightarrow \det(P_{\sigma})
	\end{align*}
	$$
</div>
<p>is a homomorphism. Fact: The composition of two homomorphism is a homomorphism.
<br />
<br />
<!----------------------------------------------------------------------------->
<b>Example 6: </b>Consider \(V\) where \(V\) is a vector space. Now consider the group \((V, +)\) (throw the scalar multiplication away). This is an abelian group. Consider the linear map</p>
<div>
	$$
	\begin{align*}
	T \ : \ &amp;V \rightarrow W
	\end{align*}
	$$
</div>
<p>This is a homomorphism (recall that linear maps need to also to preserve scalar multiplication). Furthermore, \(ker T = \{v \in V \ | \ T(v) = 0\}\) (observe that the kernel is also the nullspace of \(T\)). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Normal Subgroups</b></h4>
<p>It turns out that only a certain kind of subgroups could be kernels of homomorphism.
<br />
<!-----------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
	A normal subgroup of \(G\) is a subgroup \(N \leq G\) such that \(gNg^{-1} = N\) for all \(g \in G\) where \(gNg^{-1} = \{gng^{-1} \ | \ n \in N\}\). 
</div>
<p><br />
<!----------------------------------------------------------------------------->
This operation \(x \rightarrow gxg^{-1}\) is called the conjugation of \(x\) by \(g\). The definition says that if we conjeguate all the elements of \(N\) by a fixed element \(g \in G\), then we should get \(N\) back for every \(g\). 
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
To show a subgroup \(N \leq G\) is normal, it suffies to show that \(gNg^{-1} \subseteq N\) for all \(g \in G\).
</div>
<p><br />
That is, we just need to check that each \(gNg^{-1}\) is a subset of \(N\).
<!------------------------------------------------------------------------------>
<br />
<br />
<b>Proof</b>
<br />
Given that \(gNg^{-1} \subseteq N\). We want to show that this implies that \(N \subseteq gNg^{-1}\) which means that \(gNg^{-1} = N\) which is what is required by the definition.
<br />
<br />
Let \(g \in G\) and \(n \in N\). Observe that</p>
<div>
	$$
	\begin{align*}
	n &amp;= (gg^{-1})n(gg^{-1}) \\
	  &amp;= g(g^{-1}ng)g^{-1} \\
	  &amp;= g(x)g^{-1}
	\end{align*}
	$$
</div>
<p>where \(x = g^{-1}ng\). If we show that \(gNg^{-1} \subseteq N\), then we need to show that for any \(n \in N\), this \(n\) can be written as \(n = g^{-1}ng = x\) which means that we want to show that \(x \in N\). But by the hypothesis</p>
<div>
	$$
	\begin{align*}
	 x &amp;= g^{-1}ng \\
	   &amp;= g^{-1}n(g^{-1})^{-1} \\
	   &amp;= ana^{-1} \quad \text{ (let $a = g^{-1}$ where $g^{-1} \in G$)}
	\end{align*}
	$$
</div>
<p>Since \(g^{-1}\) is just an arbitrary element in \(G\), then \(ana^{-1} \in N\) which is what we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Examples</b></h4>
<p>Take \(D_3 = \{e, r, r^2, j, rj, r^2j\}\) where \(r^3 = e = j^2\) and \(jr = r^{-1}j\). This rule also implies \(jr^k = r^{-k}j\). Now suppose we have the subgroup generated by \(j\), \(H = \langle j \rangle = \{e, j\}\). Is this a normal subgroup?
<br />
<br />
For every \(g \in G\), we want to show that \(gHg^{-1} \subseteq H\) so</p>
<div>
	$$
	\begin{align*}
	 eHe^{-1} &amp;= H. \\
	 rHr^{-1} &amp;= \{rer^{-1}, rjr^{-1}\} = \{e, r^2j\} \neq H.
	\end{align*}
	$$
</div>
<p>So we stop here. This is not a normal subgroup. Does \(D_3\) have any normal subgroups? The trivial subgroups are normal subgroups. What about any non-trivial subgroups? It turns out that \(N = \langle r \rangle = \{e,r,r^2\}\) is normal in \(D_3\). We have to check all 6 cases</p>
<div>
	$$
	\begin{align*}
	 eNe^{-1} &amp;= N \\
	 rNr^{-1} &amp;= N \\
	 r^2Nr^{-2} &amp;= N \\
	 jNr^{-1} &amp;= N \\
	 rjN(rj)^{-1} &amp;= N \\
	 r^2jN(r^2j)^{-1} &amp;= N.
	\end{align*}
	$$
</div>
<p>FACT: If \(G\) has some generating set like \(G = \langle g_1, ..., g_r \rangle\), then to check if \(H\) is normal, we just need to show that \(g_kHg_k^{-1}\) for each \(g_i\) in the generating set \(g_1, ... g_k\). So for \(D_3\), we know it is generated by \(\langle r, j \rangle\), so we can check those two.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Kernels are Normal Subgroups</b></h4>
<p>Back to homomorphisms. It turns out that kernels of homomorphisms are normal subgroups.
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(\varphi \ : \ G \rightarrow H\) is a homomorphism, then \(K = ker \varphi = \{g \in G \ | \ \varphi(g) = e\} \) is a normal subgroup.
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
We know that \(k\) is a subgroup of \(G\). We need to show that given \(g \in G\) and \(x \in K\) that \(gxg^{-1} \in K\). By definition, to check if any element is in the kernel, we need to apply the homomorphism and see if the result is the identity element. So</p>
<div>
	$$
	\begin{align*}
	 \varphi(gNg^{-1}) &amp;= \varphi(g)\varphi(x)\varphi(g)^{-1} \\
					  &amp;= \varphi(g)e\varphi(g)^{-1} \\
					  &amp;= \varphi(g)\varphi(g)^{-1} \\
					  &amp;= e.
	\end{align*}
	$$
</div>
<p>From this, we see \(gKg^{-1} \subseteq K\) for all \(g \in G\). \(\blacksquare\)
<br />
<br />
Note also that all subgroups of abelian groups are normal.
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Injectivity of Homomorphisms</b></h4>
<p>Another useful fact:
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(\varphi \ : \ G \rightarrow H\) be a homomorphism, then \(\varphi\) is injective if and only if \(K = ker(\varphi) = \{e\} \).
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
\(\Longrightarrow\): Suppose that \(\varphi\) is injective. This means that if \(\varphi(a) = \varphi(e) = e\), then \(a = e\). But this means that the only element where \(\varphi(a) = e\) is the identity element itself. [why must \(varphi(e)=e\)? I can’t remember, verify].
<br />
<br />
\(\Longleftarrow\):Now suppose that \(K = ker(\varphi) = \{e\}\). We want to show that \(\varphi\) is injective. This means that for any elements \(a, b \in G\), if \(\varphi(a) = \varphi(b)\), then we must have \(a = b\). Let \(\varphi(a) = \varphi(b) = h \in H\). We want to show that \(a = b\). Observe that</p>
<div>
	$$
	\begin{align*}
	 \varphi(a^{-1}b) &amp;= \varphi(a)^{-1}\varphi(b) \\
					  &amp;= h^{-1}h \\
					  &amp;= e_H.
	\end{align*}
	$$
</div>
<p>Since \(\varphi(a^{-1}b) = e_H\), then this implies that \(a^{-1}b \in K\). But \(K\) has only a single element which is the identity. Then \(a^{-1}b = e\) which means that \(a = b\) as desired. \(\ \blacksquare\)
<br />
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[While isomorphisms need to be bijections, Homomorphisms do not. Definition 2.4.1 A map between groups \(\varphi : G \rightarrow H\) is called a homomorphism if it preserves group multiplication, \(\varphi(g_1)(g_2) = \varphi(g_1)\varphi(g_2)\) for all \(g_1, g_2 \in G\). An endomorphism of \(G\) is a homomorphism \(\varphi: G \rightarrow G\). So Given a homomorphism, if we find that it’s a bijection, then it is an isomorphism. Some Facts about Homomorphisms Proposition 2.4.11 Let \(\varphi : G \rightarrow H\) and \(\psi : G \rightarrow H\) be homomorphisms of groups. \(\varphi(e_G) = e_H\) For each \(g \in G\), \(\varphi(g^{-1}) = (\varphi(g))^{-1}\) Proof (Book) For any \(g \in G\). $$ \begin{align*} \varphi(e_G)\varphi(g) &amp;= \varphi(e_Gg) \quad \text{(because $\varphi$ is a homomorphism)} \\ &amp;= \varphi(g) \end{align*} $$ So \(\varphi(e_G)\) is an identity element in \(H\) but by the uniqueness of the identity element, then \(\varphi(e_G) = e_H\). Similarly, for any \(\in G\), $$ \begin{align*} \varphi(g^{-1})\varphi(g) &amp;= \varphi(g^{-1}g) \quad \text{(because $\varphi$ is a homomorphism)} \\ &amp;= \varphi(e_G) \\ &amp; = e_H \end{align*} $$ And by the uniqueness of the inverse, \(\varphi(g^{-1}) = \varphi(g)^{-1}\). \(\ \blacksquare\) The Image of a Homomorphism Given a homomorphism \(\varphi\) between two groups \(G\) and \(H\), the image of \(\varphi\) which is a subset of \(H\) is also a subgroup. Proposition Given some homomorphism \(\varphi: G \rightarrow H\). The image of this function $$ \begin{align*} \varphi(G) = \{\varphi(g) \ : \ g \in G\} \end{align*} $$ is a subgroup of \(H\). Proof TODO The Kernel of a Homomorphism The kernel of a group is simply all the elements such that \(\varphi(g)=e\). Definition 2.4.14 Let \(\varphi: G \rightarrow H\) be a homomorphism of groups. The kernel of the homomorphism \(\varphi\), denoted \(ker(\varphi)\), is \(\varphi^{-1}(e_H) = \{g \in G \ : \ \varphi(g) = e_H\}\). In fact, The kernel of a group \(K = ker \ \varphi\) is a subgroup of \(G\). Proof We will show this by showing that the kernel satisfies the subgroup properties: We know by Proposition 2.4.11, that \(\varphi(e_G) = e_H\) so \(e_G \in K\). Closed under product: For any \(a,b \in K\), \(\varphi(a)\varphi(b) = e_He_H = e_H \in K\) Closed under inverses: For any \(a \in K\), \(\varphi(a^{-1}) = \varphi(a)^{-1} = e_H^{-1} = e_H\). Therefore,\(a^{-1} \in K\). \(\ \blacksquare\) Examples Example 1: Consider the following with the addition operation $$ \begin{align*} \pi \ : \ &amp;\mathbf{Z} \rightarrow \mathbf{Z}_n\\ &amp;a \rightarrow [a]_n \end{align*} $$ \(\pi\) is a homomorphism because \([a+b] = [a] + [b]\). Remember the rule is that \(\varphi(ab) = \varphi(a)\varphi(b)\) where the first operation comes from the first group while the second operation comes from the second group. Here, both groups have addition as their binary operation. \(\pi\) is also surjective. Since for any element \(h \in \mathbf{Z}_n\), we have an element \(g \in \mathbf{Z}\) such that \(\pi(g) = [g]\). Moreover, we see that \(ker(\pi) = \mathbf{Z}n = \langle n \rangle\). Example 2: Suppose \(g \in G\). Let $$ \begin{align*} \varphi \ : \ &amp;\mathbf{Z} \rightarrow G\\ &amp;k \rightarrow g^k \end{align*} $$ We want to show that \(\varphi(ab) = \varphi(a)\varphi(b)\). But we know that \(\varphi(ab) = g^{a+b} = g^ag^b = \varphi(a)\varphi(b)\). So \(\varphi\) is a homomorphism. Note here that the image of \(\varphi\) is the subgroup generated by \(g\), \(\langle g \rangle\). What about the kernel of \(\varphi\)? $$ \begin{equation*} ker(\varphi) = \begin{cases} \{0\} \quad &amp;\text{if } o(g) = \infty \\ \mathbf{Z}d \quad \quad &amp;\text{if } o(g) = d \end{cases} \end{equation*} $$ Example 3: Consider the following $$ \begin{align*} \varphi \ : \ &amp;D_n \rightarrow S_n\\ &amp;V \rightarrow Sym(V) \end{align*} $$ where \(V\) is the vertices of the polygon. In fact \(\varphi(D_n)\) is isomorphic to \(D_n\) and it is a subgroup of \(S_n\). Example 4: Consider the following $$ \begin{align*} \varphi \ : \ &amp;S_n \rightarrow GL(\mathbf{R}^{x})\\ &amp;\sigma \rightarrow P_{\sigma} \end{align*} $$ where \(P_{\sigma}\) is the matrix associated with the permutation \(\sigma\). For example $$ \begin{align*} (1 \ 2 \ 3) \rightarrow \begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \end{pmatrix} \end{align*} $$ Example 5: Consider the following $$ \begin{align*} \psi \ : \ &amp;GL(\mathbf{R}^{x}) \rightarrow \mathbf{R}^{x}\\ &amp;P \rightarrow \det(P) \end{align*} $$ This is another homomorphism. In fact, The composition of \(\psi \circ \varphi\) is a homomorphism. That is $$ \begin{align*} \psi \circ \varphi \ : \ &amp;S_n \rightarrow \mathbf{R}^{x}\\ &amp;\sigma \rightarrow \det(P_{\sigma}) \end{align*} $$ is a homomorphism. Fact: The composition of two homomorphism is a homomorphism. Example 6: Consider \(V\) where \(V\) is a vector space. Now consider the group \((V, +)\) (throw the scalar multiplication away). This is an abelian group. Consider the linear map $$ \begin{align*} T \ : \ &amp;V \rightarrow W \end{align*} $$ This is a homomorphism (recall that linear maps need to also to preserve scalar multiplication). Furthermore, \(ker T = \{v \in V \ | \ T(v) = 0\}\) (observe that the kernel is also the nullspace of \(T\)). Normal Subgroups It turns out that only a certain kind of subgroups could be kernels of homomorphism. Definition A normal subgroup of \(G\) is a subgroup \(N \leq G\) such that \(gNg^{-1} = N\) for all \(g \in G\) where \(gNg^{-1} = \{gng^{-1} \ | \ n \in N\}\). This operation \(x \rightarrow gxg^{-1}\) is called the conjugation of \(x\) by \(g\). The definition says that if we conjeguate all the elements of \(N\) by a fixed element \(g \in G\), then we should get \(N\) back for every \(g\). Proposition To show a subgroup \(N \leq G\) is normal, it suffies to show that \(gNg^{-1} \subseteq N\) for all \(g \in G\). That is, we just need to check that each \(gNg^{-1}\) is a subset of \(N\). Proof Given that \(gNg^{-1} \subseteq N\). We want to show that this implies that \(N \subseteq gNg^{-1}\) which means that \(gNg^{-1} = N\) which is what is required by the definition. Let \(g \in G\) and \(n \in N\). Observe that $$ \begin{align*} n &amp;= (gg^{-1})n(gg^{-1}) \\ &amp;= g(g^{-1}ng)g^{-1} \\ &amp;= g(x)g^{-1} \end{align*} $$ where \(x = g^{-1}ng\). If we show that \(gNg^{-1} \subseteq N\), then we need to show that for any \(n \in N\), this \(n\) can be written as \(n = g^{-1}ng = x\) which means that we want to show that \(x \in N\). But by the hypothesis $$ \begin{align*} x &amp;= g^{-1}ng \\ &amp;= g^{-1}n(g^{-1})^{-1} \\ &amp;= ana^{-1} \quad \text{ (let $a = g^{-1}$ where $g^{-1} \in G$)} \end{align*} $$ Since \(g^{-1}\) is just an arbitrary element in \(G\), then \(ana^{-1} \in N\) which is what we wanted to show. \(\ \blacksquare\) Examples Take \(D_3 = \{e, r, r^2, j, rj, r^2j\}\) where \(r^3 = e = j^2\) and \(jr = r^{-1}j\). This rule also implies \(jr^k = r^{-k}j\). Now suppose we have the subgroup generated by \(j\), \(H = \langle j \rangle = \{e, j\}\). Is this a normal subgroup? For every \(g \in G\), we want to show that \(gHg^{-1} \subseteq H\) so $$ \begin{align*} eHe^{-1} &amp;= H. \\ rHr^{-1} &amp;= \{rer^{-1}, rjr^{-1}\} = \{e, r^2j\} \neq H. \end{align*} $$ So we stop here. This is not a normal subgroup. Does \(D_3\) have any normal subgroups? The trivial subgroups are normal subgroups. What about any non-trivial subgroups? It turns out that \(N = \langle r \rangle = \{e,r,r^2\}\) is normal in \(D_3\). We have to check all 6 cases $$ \begin{align*} eNe^{-1} &amp;= N \\ rNr^{-1} &amp;= N \\ r^2Nr^{-2} &amp;= N \\ jNr^{-1} &amp;= N \\ rjN(rj)^{-1} &amp;= N \\ r^2jN(r^2j)^{-1} &amp;= N. \end{align*} $$ FACT: If \(G\) has some generating set like \(G = \langle g_1, ..., g_r \rangle\), then to check if \(H\) is normal, we just need to show that \(g_kHg_k^{-1}\) for each \(g_i\) in the generating set \(g_1, ... g_k\). So for \(D_3\), we know it is generated by \(\langle r, j \rangle\), so we can check those two. Kernels are Normal Subgroups Back to homomorphisms. It turns out that kernels of homomorphisms are normal subgroups. Proposition If \(\varphi \ : \ G \rightarrow H\) is a homomorphism, then \(K = ker \varphi = \{g \in G \ | \ \varphi(g) = e\} \) is a normal subgroup. Proof We know that \(k\) is a subgroup of \(G\). We need to show that given \(g \in G\) and \(x \in K\) that \(gxg^{-1} \in K\). By definition, to check if any element is in the kernel, we need to apply the homomorphism and see if the result is the identity element. So $$ \begin{align*} \varphi(gNg^{-1}) &amp;= \varphi(g)\varphi(x)\varphi(g)^{-1} \\ &amp;= \varphi(g)e\varphi(g)^{-1} \\ &amp;= \varphi(g)\varphi(g)^{-1} \\ &amp;= e. \end{align*} $$ From this, we see \(gKg^{-1} \subseteq K\) for all \(g \in G\). \(\blacksquare\) Note also that all subgroups of abelian groups are normal. Injectivity of Homomorphisms Another useful fact: Proposition Let \(\varphi \ : \ G \rightarrow H\) be a homomorphism, then \(\varphi\) is injective if and only if \(K = ker(\varphi) = \{e\} \). Proof \(\Longrightarrow\): Suppose that \(\varphi\) is injective. This means that if \(\varphi(a) = \varphi(e) = e\), then \(a = e\). But this means that the only element where \(\varphi(a) = e\) is the identity element itself. [why must \(varphi(e)=e\)? I can’t remember, verify]. \(\Longleftarrow\):Now suppose that \(K = ker(\varphi) = \{e\}\). We want to show that \(\varphi\) is injective. This means that for any elements \(a, b \in G\), if \(\varphi(a) = \varphi(b)\), then we must have \(a = b\). Let \(\varphi(a) = \varphi(b) = h \in H\). We want to show that \(a = b\). Observe that $$ \begin{align*} \varphi(a^{-1}b) &amp;= \varphi(a)^{-1}\varphi(b) \\ &amp;= h^{-1}h \\ &amp;= e_H. \end{align*} $$ Since \(\varphi(a^{-1}b) = e_H\), then this implies that \(a^{-1}b \in K\). But \(K\) has only a single element which is the identity. Then \(a^{-1}b = e\) which means that \(a = b\) as desired. \(\ \blacksquare\) References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 11: Dihedral Groups</title><link href="http://localhost:4000/jekyll/update/2025/02/03/math417-11-dihedral-groups.html" rel="alternate" type="text/html" title="Lecture 11: Dihedral Groups" /><published>2025-02-03T00:01:36-08:00</published><updated>2025-02-03T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/03/math417-11-dihedral-groups</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/03/math417-11-dihedral-groups.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
The Dihedral group is the group of rotational symmetries of a regular n-gon in \(\mathbf{R}^3\). It is a subgroup of the special orthogonal matrices, \(SO(3)\).
</div>
<p><br />
Before discussing the Dihedral group, we will discuss the symmetries of the disk. Why? if you inscribe a regular polygon inside a circle, observe that the symmetries of the polygon are a subset of the symmetries of the circle/disk. So it’s helpful to start there.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<h4><b>Symmetries of the Disk</b></h4>
<p>Let \(D \leq SO(3)\) be the rotational symmetries of the unit disk: \(\{(x,y,0) \ | \ x^2 + y^2 = 1 \}\). There are two types of rotations in \(D\). A rotation \(g\) around the \(z-\)axis so the \(z-\)axis remains fixed and we’re spinning the disk around it. Here \(g(e_3) = e_3\). The other type is when we flip the disk so the \(z-\)axis is now reversed. Here \(g(e_3) = -e_3\).
<br />
<!------------------------------------------------------------------------------></p>
<h4><b>Rotations Around the \(z-\)axis</b></h4>
<p>A rotation around an arbitrary angle which is \(r_{\theta} = Rot_{e_3}(\theta)\). It is always a symmetry around the unit disk. Note here that \(r_0=e\) and if we rotate by more than \(2\pi\), then it’s a rotation we’ve seen before. So \(r_{\theta + 2\pi n} = r_{\theta}\) so we need to specify \(r_{\theta}\) where \(\theta \in [0, 2\pi)]\). Note also that \(r_{\theta}^{-1} = r_{-\theta}\).
<br />
<br />
<!------------------------------------------------------------------------------></p>
<h4><b>Rotations Around the a Line in the \(xy-\)axis (Flips)</b></h4>
<p>Here the \(z-\)axis will get reversed since we’re flipping the disk by \(180^{\circ}\) (so \(e_3\) will now be \(-e_3\) facing the opposite the direction). The rotation axis itself is parallel to the \(xy\) plane. And we write \(j_{\theta} = Rot_{u_{\theta}}(\pi)\). Just draw a disk and draw any line that goes through the center. This line will be the rotation axis that is fixed and we’re flipping over it.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec11/disk.png" width="40%" class="center" /></p>
<p>The rotation vector can written as</p>
<div>
	$$
	\begin{align*}
	u_\theta = (\cos \theta)e_1 + (\sin \theta)e_2 + 0e_3
	\end{align*}
	$$
</div>
<p>This vector spans a line \(l_{\theta} = \mathbf{R}u_{\theta}\). This line is in the \(xy\) plane that goes through the vector \(u\). Note here that adding another rotation of \(\pi\) will not change the line so \(j_{\theta + \pi n} = j_{\theta}\). However, the vector \(u_{\theta}\) though will face the other direction after adding \(\pi\) and so \(u_{\theta+\pi} = -u_{\theta}\). Moreover, \(j^2_{\theta} = e\).
<br />
<br />
<!------------------------------------------------------------------------------></p>
<h4><b>Multiplication of Rotations and Flips</b></h4>
<p>To describe the group structure, we want to see what happens if we perform two rotations, two flips, a rotation followed by a flip and a flip followed by a rotation.</p>
<ul>
	<li><b>Two Rotations:</b> This case is easy, since two rotations around the \(z-\)axis (spinning around) is just another rotation so
		<div>
			$$
			\begin{align*}
			r_{\alpha}r_{\beta} = r_{\alpha+\beta}
			\end{align*}
			$$
		</div>
	</li>
	<!------ (2) ------->
	<li><b>A Flip by \(\beta\) followed by a Rotation of \(\alpha\)</b> \(r_{\alpha}j_{\beta}\): 
		<div>
			$$
			\begin{align*}
			r_{\alpha}j_{\beta} = j_{\beta+\frac{\alpha}{2}}
			\end{align*}
			$$
		</div>
		But why? We first flip the disk where \(l_{\beta}\) is fixed like before. After the flip, we rotate the disk around the \(z-\)axis by \(\alpha\) as illustrated below
		<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec11/flip.png" width="90%" class="center" /></p>
		So what we want is to compose these two operations into one and find where the fixed rotation axis will be. If you turn the third figure back to the front and now you want to use one flip to get to the same exact positions of the lines in the third figure, you'll see that the new axis will be \(u_{\beta+\frac{\alpha}{2}}\).
		<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec11/disk1.png" width="40%" class="center" /></p>
	</li>
	<!------ (3) ------->
	<li><b>A Rotation followed by a Flip</b> \(j_{\alpha}r_{\beta}\): 
		<div>
			$$
			\begin{align*}
			j_{\alpha}r_{\beta} = j_{\alpha-\frac{\beta}{2}}
			\end{align*}
			$$
		</div>
	</li>
	<li><b>Two Flips</b> \(j_{\alpha}j_{\beta}\). Now this is clearly a rotation since we're back to the same face. but we're flipping by a different axis each time. The final rotation is the difference of the two times two. (TODO: why times 2?)
		<div>
			$$
			\begin{align*}
			j_{\alpha}j_{\beta} = r_{2(\alpha-\beta)}
			\end{align*}
			$$
		</div>
	</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Elements of \(D\)</b></h4>
<p>Based on the previous list, we can now list precisely the elements of (D). We have</p>
<ul>
	<li>\(r_{\theta}\) for unique \(\theta \in [0, 2\pi)\). Noting that \(r_{\theta + 2\pi n} = r_{\theta}\)</li>
	<li>\(j_{\theta}\) for unique \(\theta \in [0, \pi)\). Noting that \(j_{\theta + \pi n} = j_{\theta}\)</li>
</ul>
<p>In addition to the multiplication table based on the previous discussion.</p>
<ul>
	<li>\(r_{\alpha}r_{\beta} = r_{\alpha+\beta}\)</li>
	<li>\(r_{\alpha}j_{\beta} = j_{\beta+\frac{\alpha}{2}}\)</li>
	<li>\(j_{\alpha}r_{\beta} = j_{\alpha-\frac{\beta}{2}}\)</li>
	<li>\(j_{\alpha}j_{\beta} = r_{2(\alpha-\beta)}\)</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Alternative Description of \(D\)</b></h4>
<p>Another description is that let \(j = j_{0}\). Then, \(j_{\theta} = r_{2\theta}j\). So every element of \(D\) can be written uniquely in one of the following two forms.</p>
<ul>
	<li>\(r_{\theta}, \theta \in [0, 2\pi]\)</li>
	<li>\(r_{\theta}j, \theta \in [0, 2\pi]\)</li>
</ul>
<p>where</p>
<ul>
	<li>\(r_{\theta + 2\pi n} = r_{\theta}\)</li>
	<li>\(r_{\alpha}r_{\beta} = r_{\alpha+\beta}\)</li>
	<li>\(r_{\alpha + \beta}\)</li>
	<li>\(j^2 = e\)</li>
	<li>\(jr_{\theta} = r_{\theta}^{-1}j\). (TODO: Show this using the previous rules)</li>
</ul>
<p>This is a complete description of \(D\).
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Dihedral Groups</b></h4>
<p>Fix \(n \geq 3\). Let \(V = \{u_{\frac{2\pi}{n}k}, k = 0,...,n-1\}\) be the vertices of a regular \(n-\)gon. Note that \(D_n \leq D \leq SO(3)\). Now, the group \(D_n\) will include specifically the symmetries of the disk that take the vertices of the polygon to themselves or another vertex in the polygon. In \(D_n\),</p>
<div>
	$$
	\begin{align*}
	r_{\frac{2\pi}{n}} = r = Rot_{\frac{2\pi}{n}}(e_3)
	\end{align*}
	$$
</div>
<p>Rotations in \(D_n\) are therefore: \(e, r, r^2, r^3, ..., r^{n-1}\) where \(r^{k+n} = r^k\).<br />
Flips in \(D_n\) are \(j, rj, r^2,...,r^{n-1}j\) since we established that \(j_{\frac{\pi}{n}k} = r_{\frac{2\pi}{n}}j_0\). In general, we’ll have \(n\) flips for an \(n-\)polygon illustrated as follows</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec11/flips.png" width="70%" class="center" /></p>

<p>Note that \(D_n\) has \(2n\) symmetries total and we have the following identities:</p>
<ul>
	<li>\(r^n = e\)</li>
	<li>\(j^2 = e\)</li>
	<li>\(jr = r^{-1}j\)</li>
	<li>\(j^2 = e\)</li>
</ul>
<p>Notice also that this group is generated by two elements so \(D_n = \langle r, j\rangle\). Overall, you will see that</p>
<div>
	$$
	\begin{align*}
	D_3 &amp;= \{e, r, r^2, j, rj, r^2j\} \\
	D_4 &amp;= \{e, r, r^2, r^3, j, rj, r^2j, r^3j\}
	\end{align*}
	$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition The Dihedral group is the group of rotational symmetries of a regular n-gon in \(\mathbf{R}^3\). It is a subgroup of the special orthogonal matrices, \(SO(3)\). Before discussing the Dihedral group, we will discuss the symmetries of the disk. Why? if you inscribe a regular polygon inside a circle, observe that the symmetries of the polygon are a subset of the symmetries of the circle/disk. So it’s helpful to start there. Symmetries of the Disk Let \(D \leq SO(3)\) be the rotational symmetries of the unit disk: \(\{(x,y,0) \ | \ x^2 + y^2 = 1 \}\). There are two types of rotations in \(D\). A rotation \(g\) around the \(z-\)axis so the \(z-\)axis remains fixed and we’re spinning the disk around it. Here \(g(e_3) = e_3\). The other type is when we flip the disk so the \(z-\)axis is now reversed. Here \(g(e_3) = -e_3\). Rotations Around the \(z-\)axis A rotation around an arbitrary angle which is \(r_{\theta} = Rot_{e_3}(\theta)\). It is always a symmetry around the unit disk. Note here that \(r_0=e\) and if we rotate by more than \(2\pi\), then it’s a rotation we’ve seen before. So \(r_{\theta + 2\pi n} = r_{\theta}\) so we need to specify \(r_{\theta}\) where \(\theta \in [0, 2\pi)]\). Note also that \(r_{\theta}^{-1} = r_{-\theta}\). Rotations Around the a Line in the \(xy-\)axis (Flips) Here the \(z-\)axis will get reversed since we’re flipping the disk by \(180^{\circ}\) (so \(e_3\) will now be \(-e_3\) facing the opposite the direction). The rotation axis itself is parallel to the \(xy\) plane. And we write \(j_{\theta} = Rot_{u_{\theta}}(\pi)\). Just draw a disk and draw any line that goes through the center. This line will be the rotation axis that is fixed and we’re flipping over it. The rotation vector can written as $$ \begin{align*} u_\theta = (\cos \theta)e_1 + (\sin \theta)e_2 + 0e_3 \end{align*} $$ This vector spans a line \(l_{\theta} = \mathbf{R}u_{\theta}\). This line is in the \(xy\) plane that goes through the vector \(u\). Note here that adding another rotation of \(\pi\) will not change the line so \(j_{\theta + \pi n} = j_{\theta}\). However, the vector \(u_{\theta}\) though will face the other direction after adding \(\pi\) and so \(u_{\theta+\pi} = -u_{\theta}\). Moreover, \(j^2_{\theta} = e\). Multiplication of Rotations and Flips To describe the group structure, we want to see what happens if we perform two rotations, two flips, a rotation followed by a flip and a flip followed by a rotation. Two Rotations: This case is easy, since two rotations around the \(z-\)axis (spinning around) is just another rotation so $$ \begin{align*} r_{\alpha}r_{\beta} = r_{\alpha+\beta} \end{align*} $$ A Flip by \(\beta\) followed by a Rotation of \(\alpha\) \(r_{\alpha}j_{\beta}\): $$ \begin{align*} r_{\alpha}j_{\beta} = j_{\beta+\frac{\alpha}{2}} \end{align*} $$ But why? We first flip the disk where \(l_{\beta}\) is fixed like before. After the flip, we rotate the disk around the \(z-\)axis by \(\alpha\) as illustrated below So what we want is to compose these two operations into one and find where the fixed rotation axis will be. If you turn the third figure back to the front and now you want to use one flip to get to the same exact positions of the lines in the third figure, you'll see that the new axis will be \(u_{\beta+\frac{\alpha}{2}}\). A Rotation followed by a Flip \(j_{\alpha}r_{\beta}\): $$ \begin{align*} j_{\alpha}r_{\beta} = j_{\alpha-\frac{\beta}{2}} \end{align*} $$ Two Flips \(j_{\alpha}j_{\beta}\). Now this is clearly a rotation since we're back to the same face. but we're flipping by a different axis each time. The final rotation is the difference of the two times two. (TODO: why times 2?) $$ \begin{align*} j_{\alpha}j_{\beta} = r_{2(\alpha-\beta)} \end{align*} $$ Elements of \(D\) Based on the previous list, we can now list precisely the elements of (D). We have \(r_{\theta}\) for unique \(\theta \in [0, 2\pi)\). Noting that \(r_{\theta + 2\pi n} = r_{\theta}\) \(j_{\theta}\) for unique \(\theta \in [0, \pi)\). Noting that \(j_{\theta + \pi n} = j_{\theta}\) In addition to the multiplication table based on the previous discussion. \(r_{\alpha}r_{\beta} = r_{\alpha+\beta}\) \(r_{\alpha}j_{\beta} = j_{\beta+\frac{\alpha}{2}}\) \(j_{\alpha}r_{\beta} = j_{\alpha-\frac{\beta}{2}}\) \(j_{\alpha}j_{\beta} = r_{2(\alpha-\beta)}\) Alternative Description of \(D\) Another description is that let \(j = j_{0}\). Then, \(j_{\theta} = r_{2\theta}j\). So every element of \(D\) can be written uniquely in one of the following two forms. \(r_{\theta}, \theta \in [0, 2\pi]\) \(r_{\theta}j, \theta \in [0, 2\pi]\) where \(r_{\theta + 2\pi n} = r_{\theta}\) \(r_{\alpha}r_{\beta} = r_{\alpha+\beta}\) \(r_{\alpha + \beta}\) \(j^2 = e\) \(jr_{\theta} = r_{\theta}^{-1}j\). (TODO: Show this using the previous rules) This is a complete description of \(D\). Dihedral Groups Fix \(n \geq 3\). Let \(V = \{u_{\frac{2\pi}{n}k}, k = 0,...,n-1\}\) be the vertices of a regular \(n-\)gon. Note that \(D_n \leq D \leq SO(3)\). Now, the group \(D_n\) will include specifically the symmetries of the disk that take the vertices of the polygon to themselves or another vertex in the polygon. In \(D_n\), $$ \begin{align*} r_{\frac{2\pi}{n}} = r = Rot_{\frac{2\pi}{n}}(e_3) \end{align*} $$ Rotations in \(D_n\) are therefore: \(e, r, r^2, r^3, ..., r^{n-1}\) where \(r^{k+n} = r^k\). Flips in \(D_n\) are \(j, rj, r^2,...,r^{n-1}j\) since we established that \(j_{\frac{\pi}{n}k} = r_{\frac{2\pi}{n}}j_0\). In general, we’ll have \(n\) flips for an \(n-\)polygon illustrated as follows]]></summary></entry><entry><title type="html">Lecture 09/10: Subgroups</title><link href="http://localhost:4000/jekyll/update/2025/02/02/math417-09-subgroups.html" rel="alternate" type="text/html" title="Lecture 09/10: Subgroups" /><published>2025-02-02T00:01:36-08:00</published><updated>2025-02-02T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/02/math417-09-subgroups</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/02/math417-09-subgroups.html"><![CDATA[<div class="mintheaderdiv">
Definition 2.2.1
</div>
<div class="mintbodydiv">
A non-empty subset \(H\) of a group \(G\) is called a subgroup if \(H\) is itself a group with the group operation inherited from \(G\). We write \(H \leq G\) to indicate that \(H\) is a subgroup of \(G\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
Example:</p>
<ul>
	<li>The group \(SO(3)\), the special orthogonal matrices is a subgroup of \(GL_3(\mathbf{R}\), the group of invertible \(3 \times 3\) matrices. Both of these are under the same operation.</li>
	<li>Non-example: \(\mathbf{Z}_n\): congruence classes modulo \(n\) with addition. We also have \(\phi(n)\): congruence classes with multiplicative inverses. This has the operation multiplication so it's not a subgroup! Even when you define \(\phi(4) = \{[1],[3]\}\) with addition, it will not satisfy the group requirements, so it's not a subgroup.</li>
</ul>
<p>Since we need to check a lot for subgroups. We have a set of necessary conditions.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
The necessary conditions for a subset \(H\) of \(G\) to be a subgroup of \(G\) are:
<ol>
	<li>\(H\) is not empty</li>
	<li>\(H\) is closed under multiplication. That is, for all elements \(h_1\) and \(h_2\) of \(H\), the products \(h_1h_2\) is also an element of \(H\)</li>
	<li>\(H\) is closed under inverses. For all \(h \in H\), the inverse \(h^{-1}\) is an element of \(H\)</li>
</ol>
</div>
<p><br />
<b>Proof</b>
<br /></p>
<ul>
	<li>\(H\) is associative. For any \(a,b,c \in H\), \((ab)c = a(bc)\). We're inheriting the same operation from \(G\) so \(H\) must be associative.</li>
	<li>\(H\) has an identity element: We know \(H\) is non-empty by (1). We also know for any \(h \in H\), \(h^{-1} \in H\) by (3). By (2), \(H\) is closed under multiplication so \(hh^{-1} \in H\) but \(hh^{-1} = e\) so the identity element is in \(H\).  </li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Consider the group \(G = S_4\) (permutations of \(\{1,2,3,4\}\)). The order of this group is \(|G| = 4\). The following are subgroups of \(S_4\):</p>
<ul>
	<li>\(H=\{ \sigma \in S_4 \ | \ \sigma(4) = 4\}\). So this subgroup fixes \(4\) in the permutation. \(|H| = 3! = 6\). In fact, it is permuting only 3 numbers and it is isomorphic to \(S_3\).</li>
	<!---------------------------->
	<li>\(H'=\{e,(1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3)\}\). It's not empty. It has the identity element. Every element is its own inverse. The one thing not too obvious is proving it is closed. This subgroup is isomorphic to the symmetry group of the rectangle. What's an isomorphism in this case? Assign \(A\) to 1, \(B\) to 2, \(C\) to 3, \(D\) to 4. Recall that the symmetries of the rectangle has the symmetries \(r_1, r_2, r_3, e\) where \(r_1\) is rotating around the \(x-\)axis. Observe now that \(r_1\)
		<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec10/rect-r1.png" width="45%" class="center" /></p>
	</li>
	rotates the vertices such that \(A\) and \(B\) switch positions and \(C\) and \(D\) switch positions. This is equivalent to the permutation \((1 \ 2)(3 \ 4)\). Following this, we'll see that \(r_2\) gets mapped to \((1 \ 4)(2 \ 3)\) and \(r_3\) gets mapped to \((1 \ 3)(2 \ 4)\).
	<!---------------------------->
	<li>\(H''=\{e,(1 \ 2 \ 3 \ 4)(4 \ 3 \ 2 \ 1), (1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3), (1 \ 3),(2 \ 4)\}\). This one is isomorphic to \(D_4\) which is the symmetries of the square.</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------------></p>
<h4><b>Generated Subgroups</b></h4>
<p>We begin with the following lemma which states that the intersection of a collection of subgroups is another subgroup.
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma (Book Proposition 2.2.8)
</div>
<div class="yellowbodydiv">
Let \(G\) be a group and let \(H_1, H_2,...,H_n\) be subgroups of \(G\). Then 
	$$
	\begin{align*}
	H = H_1 \cap H_2 \ \cap ... \cap \ H_n
	\end{align*}
	$$
is a subgroup of \(G\). More generally, if \(\{H_{\alpha}\}\) is any collection of subgroups, then \(\cap_{\alpha}\) is a subgroup.
<!------------------------------------------------------------------------------>
</div>
<p><br />
<b>Proof</b>
<br /></p>
<ol>
<li>Every \(H_i\) is a subgroup. Therefore \(e \in H_i\) for each \(i\) and thus \(e \in H\).</li>
<li>For any two elements \(a, b \in H\), we must have \(a, b \in H_i\) for each \(i\). Their product \(ab\) must also be in every \(H_i\) since each \(H_i\) is a subgroup. Therefore \(ab \in H\).</li>
<li>For any \(a \in H\), \(a\) must be in \(H_i\) for each \(i\). Therefore \(a^{-1} \in H_i \ \forall i\) and thus \(a^{-1} \in H\).</li>
</ol>
<p>Since any intersection of subgroups is another subgroups, we can then define a generated subgroups as follow
<br />
<!------------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(G\) be a group and let \(S\) be a subset of \(G\). Then
	$$
	\begin{align*}
	 \langle S \rangle = \bigcap_{\text{All } H_i \leq G \text{ such that } S \subseteq H_i}
	\end{align*}
	$$
\(\langle S \rangle\) is called <b>the subgroup generated by</b> \(S\) and it is the smallest subgroup in \(G\) that contains \(S\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
Observe that</p>
<ul>
<li>\(\langle S \rangle\) is a subgroup by the previous lemma since it is the intersection of subgroups.</li>
<li>\(S \subseteq \langle S \rangle\).</li>
<li>It is the smallest group that contains \(S\). Because for any subgroup \(K \leq G\) such that \(S \subseteq K\), we must have \(\langle S \rangle \leq K\) (why? because if \(S\) is contained in \(K\) and \(K\) is a group, then all the products of the elements of \(S\) are in \(K\). Same for the inverses. So the whole subgroup \(\langle S \rangle\) must also be contained in \(K\).)</li>
</ul>
<p>Additionally from the book:</p>
<ul>
<li>If \(S\) contains a single element, \(S = \{a\}\), then the subgroup generated by \(S\) is denoted by \(\langle a \rangle\). </li>
<li>If \(G = \langle S \rangle\), then \(G\) is generated by \(S\) or \(S\) generates \(G\)</li>
</ul>
<p>Another way to describe \(\langle S \rangle\) is take a bottom up approach to describe the elements.
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Suppose \(G\) is a group and \(S\) is a subset of \(G\). Then \(\langle S \rangle\) is equal to the set of elements in \(G\) of the form
	$$
	\begin{align*}
	 g_1g_2...g_k \quad \text{ where either } g_i \in S \text{ or } g_i^{-1} \in S \text{ for all } i.
	\end{align*}
	$$
</div>
<!------------------------------------------------------------------------------>
<p><br />
In other words, \(\langle S \rangle\) is a subgroup such that that it contains all the possible products \(g_1g_2...g_n\) which are in \(S\) or an inverse to an element of \(S\). The set that contains such products is often called the “set of words in \(S\)” in group theory.
<br />
<br />
<b>Proof</b>
<br />
Let \(T\) be the set of words \(\{e\} \cup \{g_1...g_k\}\) where \(g_i \in S\) or \(g_i^{-1} \in S\) for all \(i = 1,...,k\). We need to show that</p>
<ol>
	<li>\(T\) is a subgroup of \(G\). [This is true because \(T\) has the identity element. For any two word, the product of these is just a longer word which is also in \(T\). It is closed under the inverses because \((g_1g_2...g_k)^{-1} = g_k^{-1}...g_1^{-1}\).]</li>
	<li>\(S \subseteq T\). [This is true because all the words of length 1 make up the set \(S\). It is true by construction.]</li>
	<li>If \(K \leq G\) is a subgroup such that \(S \subseteq K\), then \(T \subseteq K\). [This is true because if \(K\) contains \(S\) then it must contain all the inverses of these words and then also the products of all of these words since it's a group (closed) so it has all of these elements and it contains \(T\) itself.]</li>
</ol>
<p>Condition (1) and (2) imply that \(\langle S \rangle \subseteq T\). Conditions (1) and (3) imply that \(T \subseteq \langle S \rangle\). Therefore, \(T = \langle S \rangle\). \(\ \blacksquare\) We’ll use this proposition a lot. 
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Lattice of Subgroups</b></h4>
<p>Consider the group \(S_3 = \{e, (1 \ 2), (1 \ 3), (2 \ 3), (1 \ 2 \ 3), (1 \ 3 \ 2)\}\). We can list its subgroups in terms of its generating subsets as follows:</p>
<ul>
	<li>\(\langle (1 \ 2) \rangle = \{e, (1 \ 2)\}\). This says that the subgroup generated by \(\{(1 \ 2)\}\) is \(\{e, (1 \ 2)\} = \langle (1 \ 2) \rangle \)</li>
	<li>\(\langle (1 \ 3) \rangle = \{e, (1 \ 3)\}\).</li>
	<li>\(\langle (2 \ 3) \rangle = \{e, (2 \ 3)\}\).</li>
	<li>\(\langle (1 \ 2 \ 3) \rangle = \{e, (1 \ 2 \ 3), (1 \ 3 \ 2)\}\) = \(\langle (1 \ 3 \ 2) \rangle\) </li>
	<li>\(S_3 = \langle (1 \ 2), (1 \ 2 \ 3) \rangle\).</li>
</ul>
<p>We can then draw this in the following figure</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec10/lattice1.png" width="55%" class="center" /></p>

<p>Notice also that 	\(S_4 = \langle (1 \ 2 \ 3 \ 4), (2 \ 4) \rangle\).
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cyclic Groups and Cyclic Subgroups</b></h4>
<p>We’ll start with the definition
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A group \(G\) is cyclic if \(G = \langle a \rangle\) for some \(a \in G\). Similarly, a subgroup \(H \leq G\) is cyclic if \(H = \langle a \rangle\) for some \(a \in G\). 
</div>
<!----------------------------------------------------------------------------->
<p><br />
In fact, we can describe all the elements in a cyclic subgroup as follows
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book 2.2.9)
</div>
<div class="peachbodydiv">
Let \(a\) be an element of a group \(G\). The subgroup \(\langle a \rangle\) generated by \(a\) is \(\{a^k \ : \ k \in \mathbf{Z}\}\).
</div>
<p><br />
Note here that this subgroup is abelian!
<br />
<!----------------------------------------------------------------------------->
<br />
<b>Proof (book)</b>
<br />
Let \(H = \{a^k \ : \ k \in \mathbf{Z}\}\). We will show that \(\langle a \rangle = H\) as follows:
<br />
<br />
\(\langle a \rangle \subseteq H\): We know that \(G\) is a group and \(H\) is subset of \(G\). We claim that it is a subgroup of \(G\). It is closed under multiplication because for any \(a^k\) and \(a^l\) in \(H\), \(a^{k+l} \in H\). It is closed under inverses because for any \(a^k \in H\), \((a^{k})^{-1} = a^{-k} \in H\). Furthermore, \(H\) contains \(a\) and since \(H\) is a subgroup, then \(H\) contains all the powers of \(a\). Therefore, \(\langle a \rangle \subseteq H\). 
<br />
<br />
\(H \subseteq \langle a \rangle\): We showed that \(H = \{a^k \ : \ k \in \mathbf{Z}\}\) is a subgroup above. It is closed under multiplication and so it contains all powers of \(a\). Therefore, \(H \subseteq \langle a \rangle\). \(\ \blacksquare\) 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose \(G = (\mathbf{Z}, + )\). For any element \(a \in \mathbf{Z}\),</p>
<div>
	$$
	\begin{align*}
	 \langle a \rangle = \{ka \ : \ k \in \mathbf{Z}\} = \mathbf{Z}a.
	\end{align*}
	$$
</div>
<ul>
<li>The set of powers of \(a\) is the set of all multiples of \(a\) since the group operation is addition. So \(a+a+a+a+a\) is the fifth power.</li>

<li>If \(a = 0\), then \(\langle a \rangle = \mathbf{Z}0 = \{0\}\)  is the trivial subgroup. </li>

<li>If \(a \neq 0\), then \(\mathbf{Z}a\) is infinite. In fact it is isomorphic to \(\mathbf{Z}\) so \(\mathbf{Z}a \approx \mathbf{Z}\). </li>

<li>Note here that \(\mathbf{Z} \neq \mathbf{Z}a\) unless \(a = \pm 1\).</li>

<li>Also note, that \(\langle a \rangle  = \langle -a \rangle\).</li>
</ul>

<p>Now consider \(a, b \in \mathbf{Z}\). Suppose we want to find the subgroup generated by the set of \(\{a,b\}\), In other words, \(\langle a,b \rangle\). This is the set of all words composed of \(a\) and \(b\) and their inverses. In fact this is the set of all integer combinations of \(a\) and \(b\)</p>
<div>
	$$
	\begin{align*}
	 \langle a, b \rangle = \{ma + nb \ : \ m,n \in \mathbf{Z}\} = I(a,b)
	\end{align*}
	$$
</div>
<p>This is also a cyclic subgroup because \(\langle a, b \rangle = I(a,b) = \mathbf{Z}d\) where \(d = gcd(a,b)\) So the gcd is the generator of this subgroup. In fact, we will see next that all subgroups of \(\mathbf{Z}\) are cyclic
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Cyclic Subgroups in \(\mathbf{Z}\)</b></h4>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
<ol>
	<li>All subgroups of \((\mathbf{Z}, +)\) are cyclic except for the trivial subgroup \(\langle 0 \rangle = \{0\}\). </li>
	<li>All subgroups are infinite.</li>
	<li>Each subgroup is generated by  a unique \(d \geq 0\).</li>
	<li>Each non-trivial subgroup is isomorphic to \(\mathbf{Z}\).</li>
</ol>
</div>
<!----------------------------------------------------------------------------->
<p><br />
<b>Proof (1)</b>
<br />
To prove that all subgroups are cyclic, we need to find a generator for each of the subgroups. We have two cases:
<br />
If \(H = \{0\}\), then \(H = \mathbf{Z}0\) and we are done.
<br />
If \(H \neq \{0\}\), then \(H\) has at least a non-zero element. By the well ordering principle of \(\mathbf{N}\), there is a smallest element \(d \in H \cap \mathbf{N}\) (\(d\) is the smallest positive element in \(H\)). We claim that \(H = \mathbf{Z}d\). 
<br />
<br />
\(\mathbf{Z}d \subseteq H\): We know \(d \in H\) and \(H\) is a group. Therefore, any multiple of \(d\) is in \(H\) since it must be closed so \(\mathbf{Z}d \subseteq H\) as required.
<br />
<br />
\(H \subseteq \mathbf{Z}d\): Suppose \(x \in H\). We want to show that \(x\) is a multiple of \(d\). Use division with remainder (\(x \div d\)) to get \(x = qd + r\) such that \(q, r \in \mathbf{Z}\) and \(0 \leq r &lt; d\). Observe now that</p>
<div>
	$$
	\begin{align*}
	r = x - qd.
	\end{align*}
	$$
</div>
<p>\(x \in H\) by assumption. \(qd \in H\) since \(d \in H\). Therefore, we must have \(r \in H\). But \(d\) is the smallest positive element in \(H\) by the hypothesis and \(r &lt; d\) so \(r\) must be zero. Therefore, \(x = qd \in \mathbf{Z}d\) and \(H = \mathbf{Z}d\) as we wanted to show. \(\ \blacksquare\). 
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Lattice of subgroups of \(\mathbf{Z}\)</b></h4>
<p>We can organize all the subgroups of \(\mathbf{Z}\) in a lattice ordered by the subset relation. For example the all the multiples of 4 are contained in the multiples of 2, so \(\mathbf{Z}4 \subset \mathbf{Z}4\) and it it goes under \(\mathbf{Z}2\) and so. This lattice is also a divisibility lattice meaning that if \(\mathbf{Z}a \subseteq \mathbf{Z}b\), then \(b \ | \ a\). For example we see that \(\mathbf{Z}6 \subset \mathbf{Z}3\) in the lattice and so this implies that \(3 \ | \ 6\).</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec1/01-1.png" width="40%" class="center" /></p>
<p>[TODO add pic]
<br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>Order of an Element</b></h4>
<p>Suppose \(G\) is a group and \(a \in G\), then we denote the order of an element by \(o(a)\). The order of \(a\) is as follows
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
\(o(a) = |\langle a \rangle| \in \mathbf{N} \cap \{\infty\} \)
</div>
<p><br />
This says that the order of an element is equal to the size of its generated subgroup \(\langle a \rangle\). But we also defined the order of an element previously as follows
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book: 2.2.17)
</div>
<div class="peachbodydiv">
	<ul>
<li>If \(o(a) = n\), then \(n\) is the least positive integer such that \(a^n = e\). Furthermore, \(\langle a \rangle = \{a^k \ : \ 0 \leq k &lt; o(a)\}\).</li>
<li>If \(o(a) = \infty\), then \(a^n \neq e \ \forall n\).</li>
</ul>
</div>
<!----------------------------------------------------------------------------->
<p><br />
The claim is that these two definitions are equivalent.
<br />
<br />
<b>Proof</b>
<br />
By definition, the cyclic subgroup generated by \(a\) is \(\langle a \rangle = \{a^k \ | \ k \in \mathbf{Z} \}\). Suppose that \(o(a) = |\langle a \rangle| = n\). Since \(\langle a \rangle\) is finite, then two powers of \(a\) must coincide. So \(a^i = a^{j}\) for some \(i &lt; j\). This means that \(a^{j-i} = e\) where \(k = j - i &gt; 0\) (side note: why? think of the integers mod 7 with addition, \(2^2 = 2+2\) and \(2^9=18\) both leave a remainder of 4. While \(2^7\) is \(e\)). By the well-ordering principle, there a least \(k\) such that \(a^k = e\), We want to show that \(k\) is equal to the order of \(\langle a \rangle\) which is \(n\). 
<br />
<br />
Now, suppose we have some power \(m\) of \(a\). Write \(m = kq + r\) where \(0 \leq r &lt; k\) and \(r = rem_k(m)\). So \(a^m = (a^k)^qa^r = a^qa^r = a^r\). This means that</p>
<div>
	$$
	\begin{align*}
	 \langle a \rangle = \{e, a, a^2, ..., a^{k-1}\}
	\end{align*}
	$$
</div>
<p>Since \(n\) is the size of the set by assumption, then \(n \leq k\). But that means that that we have two powers in the set above that must coincide where \(a\) raised to the power of their difference is the identity element. But this is a contradiction since we said that \(k\) is the smallest positive number such that \(a^k = e\). Therefore, we must have \(n = k\). So the minimality of \(k\) implies their equality. 
<br />
<br />
If \(\langle a \rangle\) is infinite, we can use the same argument to show that if \(a^k = e\) for some \(k &gt; 0\), then all elements can be written as \(\langle a \rangle = \{e, a, a^2, ..., a^{k-1}\}\) but this is a finite set which is a contradiction. \(\ \blacksquare\)
<br />
<br />
The notion of order leads to the following result
<br />
<!-----------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book 2.2.20)
</div>
<div class="peachbodydiv">
Let \(a\) be an element of a group \(G\).
<ol type="a">
	<li>If \(a\) has infinte order, then \(\langle a \rangle\) is isomorphic to \(\mathbf{Z}\)</li>
	<li>If \(a\) has finite order, then \(\langle a \rangle\) is isomorphic to the group \(\mathbf{Z}_n\)</li>
</ol>
</div>
<p><br />
<!----------------------------------------------------------------------------->
<b>Proof (book)</b>
<br />
For \((a)\), we want to show this by finding an example of an isomorphism. So define the map</p>
<div>
	$$
	\begin{align*}
	 \varphi \ : \ &amp;\mathbf{Z} \rightarrow \langle a \rangle \\
	 &amp;\varphi(k) = a^k.
	\end{align*}
	$$
</div>
<p>To show that this map is an isomorphism, we need to show that it is a bijection and also that for any two elements \(a, b \in \mathbf{Z}\), \(\varphi(ab) = \varphi(a)\varphi(b)\) (Definition 2.1.13).
<br />
<br />
To show that it is a bijection, observe that this map is surjective or onto by the definition of \(\langle a \rangle\). (Recall that that \(\langle a \rangle\) is of infinite order). It is also injective or 1-1 because all the elements of \(\langle a \rangle\) (powers of \(a\)) are distinct. Furthermore, observe that for any \(k, l \in \mathbf{Z}\)</p>
<div>
	$$
	\begin{align*}
	 \varphi(k + l) &amp;= a^{k + l}, \\
	 \varphi(k)\varphi(l) &amp;= a^k a^l = a^{k+1}.
	\end{align*}
	$$
</div>
<p>So \(\varphi\) is an isomorphism.
<br />
<br />
Similarly for \((b)\), we want to define an isomorphism. Note here that both \(\mathbf{Z}_n\) and \(\langle a \rangle\) have \(n\) elements so define the map</p>
<div>
	$$
	\begin{align*}
	 \varphi \ : \ &amp;\mathbf{Z}_n \rightarrow \langle a \rangle \\ 
	 &amp;\varphi([k]) = a^k.
	\end{align*}
	$$
</div>
<p>where \(0 \leq k \leq n-1\). In \(\mathbf{Z}_n\), This map is both injective and surjective. Furthermore, observe that \([k] + [l] = [k+l]\) but since we’re module class \(n\), then \([k+l] = [qn + r] = [0 + r] = [r]\) so</p>
<div>
	$$
	\begin{align*}
	 \varphi([k] + [l]) &amp;= a^{[k + l]} = a^{[r]}.
	\end{align*}
	$$
</div>
<p>while,</p>
<div>
	$$
	\begin{align*}
	 \varphi([k])\varphi([l]) &amp;= a^{[k]}a^{[l]} = a^{[k+l]} = a^{[r]}.
	\end{align*}
	$$
</div>
<p>Therefore \(\varphi\) is an isomorphism.
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Subgroups of \(\mathbf{Z}_{12}\)</b></h4>
<p>We can also build a lattice for the subgroups of \(\mathbf{Z}_{12}\).</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec10/z12.png" width="80%" class="center" /></p>
<p>Note that</p>
<ul>
	<li>\(\langle [1] \rangle = \mathbf{Z}_{12} = \langle [5] \rangle = \langle [7] \rangle = \langle [11] \rangle\).</li>
	<li>\(\langle [2] \rangle = \{[0],[2],[4],[6],[8],[10]\}\).</li>
	<li>\(\langle [3] \rangle = \{[0],[3],[6],[9]\}\).</li>
	<li>\(\langle [4] \rangle = \{[0],[4],[8]\}\).</li>
	<li>\(\langle [6] \rangle = \{[0],[6]\}\).</li>
	<li>\(\langle [12] \rangle = \{[0]\}\).</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Subgroups in the Finite Cyclic Group \(\mathbf{Z}_n\)</b></h4>

<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(a\) be an element of a group \(G\).
<ol>
	<li>Every subgroup of \(\mathbf{Z}_n\) is cyclic.</li>
	<li>If \(a \in \mathbf{Z}\), then \(\langle [a] \rangle = \langle [d] \rangle\) where \(d = gcd(a,n)\).</li>
	<li>If \(a, b \ | \ n\), then \(\langle [a] \rangle \subseteq \langle [b] \rangle\) if and only if \(b \ | \ a\).</li>
	<li>Every subgroup of \(\mathbf{Z}\) is \(\langle d \rangle\) for a unique \(d &gt; 0\), \(d \ | \ n\) and \(|\langle d \rangle| = \frac{n}{d}\).</li>
</ol>
</div>
<!---------------------------------------------------------------------->
<p><br />
\((2)\) simply says that the standard representative/generator of the cyclic subgroup \(\langle a \rangle\) is \(gcd(a, n)\). We can replace \(a\) with \(gcd(a,n)\). They both represent the same congruence class.<br /> \((3)\) is similar to \(\mathbf{Z}\). Recall that \(\mathbf{Z}2\) (multiples of 2) contains the subgroup \(\mathbf{Z}4\) and we concluded that \(\mathbf{Z}4 \subseteq \mathbf{Z}2\) implies that \(2 \ | \ 4\). A similar thing here.
<br />
<br />
To prove this theorem we need the following lemma:
<br />
<!----------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
Given a subset \(H \subseteq \mathbf{Z}_n\). Define
	$$
	\begin{align*}
	\tilde{H} &amp;= \{a \in \mathbf{Z} \ | \ [a]_n \in H \} \subseteq \mathbf{Z} \\
	          &amp;= \bigcup_{S \in H} S
 	\end{align*}
	$$
Then, \(H\) is a subgroup of \(\mathbf{Z}_n\) if and only if \(\tilde{H}\) is a subgroup of \(\mathbf{Z}\).
</div>
<p><br />
So \(H\) is a set of integers such that the elements are the representatives of the congruence classes in \(\mathbf{Z}_n\). The proof of this lemma is in the lecture notes.
<br />
<br />
<!---------------------------------------------------------------------->
<b>Theorem Proof</b>
<br />
For (1), Let \(H \leq \mathbf{Z}_n\) be a subgroup. We want to show that \(H\) is cyclic by proving that \(H\) is a cyclic group generated by some element. Consider \(\tilde{H}\). Since \(H\) is a subgroup, then \(\tilde{H}\) is a subgroup of \(\mathbf{Z}_n\) by the lemma. Moreover, \(\tilde{H} = \mathbf{Z}d\) for some \(d \in \mathbf{Z}\) by the previous theorem (every subgroup of \(\mathbf{Z}\) is cyclic and is generated by some \(d\)). Therefore, by the definition of \(\tilde{H}\), \([d]\) must be a congruence class in \(H\). This implies that \(\langle [d] \rangle \subseteq H\) since \(H\) is a group and all multiples of \(d\) must be in \(H\). 
<br />
<br />
To prove the other direction \(H \subseteq \langle [d] \rangle\), consider \([a] \in H\). This means that \(a \in \tilde{H}\) by the definition of \(\tilde{H}\). But \(\tilde{H} = \mathbf{Z}d\) so \(a\) is a multiple of \(d\) and we can write \(a = sd\) for some \(s \in \mathbf{Z}\). Therefore, \([a] = s[d]\) and \([a] \in \langle [d] \rangle\). Therefore, \(H = \langle d \rangle\) and \(H\) is cyclic as we wanted to show. \(\ \blacksquare\)
<br />
<br />
For (2), suppose that \(d = gcd(a,n)\). We want to show that \(\langle [a] \rangle = \langle [d] \rangle\). Since \(d\) is the gcd, then \(d\) divides both \(a\) and \(n\). Also \(d = I(a,n)\) and we can write \(d = sa + tn\) for some \(s, t \in \mathbf{Z}\). 
<br />
<br />
Since \(a \ | \ d\), then \(a = sd\) and so \([a] = s[d]\) so \(\langle [a] \rangle \subseteq \langle [d] \rangle\). Since \(d = sa + tn\), then \([d] = s[d] + t[n] = s[d]\) because \([n] = [0]\) since we’re in \(\mathbf{Z}_n\). Therefore, \(\langle [d] \rangle \subseteq \langle [a] \rangle\). So we’ve shown that \(\langle [a] \rangle = \langle [d] \rangle\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Subgroups of Finite Cyclic Groups</b></h4>
<p>In fact, not just the subgroups of \(\mathbf{Z}_n\) that are cyclic. All subgroups of any finite cyclic group \(G\) are cyclic. We can generalize the previous theorem as follows
<br />
<br /></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let G = \(\langle g \rangle\) and \(o(g) = n &lt; \infty\). Then
<ol>
	<li>Every subgroup of \(G\) is cyclic.</li>
	<li>\(\forall \ a \in \mathbf{Z}\), then \(\langle g^a \rangle = \langle g^d \rangle\) where \(d = gcd(a,n)\).</li>
	<li>If \(a, b \ | \ n\), then \(\langle g^a \rangle \subseteq \langle g^b \rangle\) if and only if \(b \ | \ a\).</li>
	<li>Every subgroup of \(G\) is of the form \(\langle g^d \rangle\) for a unique \(d &gt; 0\), \(d \ | \ n\) and it has order \(\frac{n}{d}\).</li>
</ol>
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!-----------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
	<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 2.2.1 A non-empty subset \(H\) of a group \(G\) is called a subgroup if \(H\) is itself a group with the group operation inherited from \(G\). We write \(H \leq G\) to indicate that \(H\) is a subgroup of \(G\). Example: The group \(SO(3)\), the special orthogonal matrices is a subgroup of \(GL_3(\mathbf{R}\), the group of invertible \(3 \times 3\) matrices. Both of these are under the same operation. Non-example: \(\mathbf{Z}_n\): congruence classes modulo \(n\) with addition. We also have \(\phi(n)\): congruence classes with multiplicative inverses. This has the operation multiplication so it's not a subgroup! Even when you define \(\phi(4) = \{[1],[3]\}\) with addition, it will not satisfy the group requirements, so it's not a subgroup. Since we need to check a lot for subgroups. We have a set of necessary conditions. Proposition The necessary conditions for a subset \(H\) of \(G\) to be a subgroup of \(G\) are: \(H\) is not empty \(H\) is closed under multiplication. That is, for all elements \(h_1\) and \(h_2\) of \(H\), the products \(h_1h_2\) is also an element of \(H\) \(H\) is closed under inverses. For all \(h \in H\), the inverse \(h^{-1}\) is an element of \(H\) Proof \(H\) is associative. For any \(a,b,c \in H\), \((ab)c = a(bc)\). We're inheriting the same operation from \(G\) so \(H\) must be associative. \(H\) has an identity element: We know \(H\) is non-empty by (1). We also know for any \(h \in H\), \(h^{-1} \in H\) by (3). By (2), \(H\) is closed under multiplication so \(hh^{-1} \in H\) but \(hh^{-1} = e\) so the identity element is in \(H\). Example Consider the group \(G = S_4\) (permutations of \(\{1,2,3,4\}\)). The order of this group is \(|G| = 4\). The following are subgroups of \(S_4\): \(H=\{ \sigma \in S_4 \ | \ \sigma(4) = 4\}\). So this subgroup fixes \(4\) in the permutation. \(|H| = 3! = 6\). In fact, it is permuting only 3 numbers and it is isomorphic to \(S_3\). \(H'=\{e,(1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3)\}\). It's not empty. It has the identity element. Every element is its own inverse. The one thing not too obvious is proving it is closed. This subgroup is isomorphic to the symmetry group of the rectangle. What's an isomorphism in this case? Assign \(A\) to 1, \(B\) to 2, \(C\) to 3, \(D\) to 4. Recall that the symmetries of the rectangle has the symmetries \(r_1, r_2, r_3, e\) where \(r_1\) is rotating around the \(x-\)axis. Observe now that \(r_1\) rotates the vertices such that \(A\) and \(B\) switch positions and \(C\) and \(D\) switch positions. This is equivalent to the permutation \((1 \ 2)(3 \ 4)\). Following this, we'll see that \(r_2\) gets mapped to \((1 \ 4)(2 \ 3)\) and \(r_3\) gets mapped to \((1 \ 3)(2 \ 4)\). \(H''=\{e,(1 \ 2 \ 3 \ 4)(4 \ 3 \ 2 \ 1), (1 \ 2)(3 \ 4), (1 \ 3)(2 \ 4), (1 \ 4)(2 \ 3), (1 \ 3),(2 \ 4)\}\). This one is isomorphic to \(D_4\) which is the symmetries of the square. Generated Subgroups We begin with the following lemma which states that the intersection of a collection of subgroups is another subgroup. Lemma (Book Proposition 2.2.8) Let \(G\) be a group and let \(H_1, H_2,...,H_n\) be subgroups of \(G\). Then $$ \begin{align*} H = H_1 \cap H_2 \ \cap ... \cap \ H_n \end{align*} $$ is a subgroup of \(G\). More generally, if \(\{H_{\alpha}\}\) is any collection of subgroups, then \(\cap_{\alpha}\) is a subgroup. Proof Every \(H_i\) is a subgroup. Therefore \(e \in H_i\) for each \(i\) and thus \(e \in H\). For any two elements \(a, b \in H\), we must have \(a, b \in H_i\) for each \(i\). Their product \(ab\) must also be in every \(H_i\) since each \(H_i\) is a subgroup. Therefore \(ab \in H\). For any \(a \in H\), \(a\) must be in \(H_i\) for each \(i\). Therefore \(a^{-1} \in H_i \ \forall i\) and thus \(a^{-1} \in H\). Since any intersection of subgroups is another subgroups, we can then define a generated subgroups as follow Definition Let \(G\) be a group and let \(S\) be a subset of \(G\). Then $$ \begin{align*} \langle S \rangle = \bigcap_{\text{All } H_i \leq G \text{ such that } S \subseteq H_i} \end{align*} $$ \(\langle S \rangle\) is called the subgroup generated by \(S\) and it is the smallest subgroup in \(G\) that contains \(S\). Observe that \(\langle S \rangle\) is a subgroup by the previous lemma since it is the intersection of subgroups. \(S \subseteq \langle S \rangle\). It is the smallest group that contains \(S\). Because for any subgroup \(K \leq G\) such that \(S \subseteq K\), we must have \(\langle S \rangle \leq K\) (why? because if \(S\) is contained in \(K\) and \(K\) is a group, then all the products of the elements of \(S\) are in \(K\). Same for the inverses. So the whole subgroup \(\langle S \rangle\) must also be contained in \(K\).) Additionally from the book: If \(S\) contains a single element, \(S = \{a\}\), then the subgroup generated by \(S\) is denoted by \(\langle a \rangle\). If \(G = \langle S \rangle\), then \(G\) is generated by \(S\) or \(S\) generates \(G\) Another way to describe \(\langle S \rangle\) is take a bottom up approach to describe the elements. Proposition Suppose \(G\) is a group and \(S\) is a subset of \(G\). Then \(\langle S \rangle\) is equal to the set of elements in \(G\) of the form $$ \begin{align*} g_1g_2...g_k \quad \text{ where either } g_i \in S \text{ or } g_i^{-1} \in S \text{ for all } i. \end{align*} $$ In other words, \(\langle S \rangle\) is a subgroup such that that it contains all the possible products \(g_1g_2...g_n\) which are in \(S\) or an inverse to an element of \(S\). The set that contains such products is often called the “set of words in \(S\)” in group theory. Proof Let \(T\) be the set of words \(\{e\} \cup \{g_1...g_k\}\) where \(g_i \in S\) or \(g_i^{-1} \in S\) for all \(i = 1,...,k\). We need to show that \(T\) is a subgroup of \(G\). [This is true because \(T\) has the identity element. For any two word, the product of these is just a longer word which is also in \(T\). It is closed under the inverses because \((g_1g_2...g_k)^{-1} = g_k^{-1}...g_1^{-1}\).] \(S \subseteq T\). [This is true because all the words of length 1 make up the set \(S\). It is true by construction.] If \(K \leq G\) is a subgroup such that \(S \subseteq K\), then \(T \subseteq K\). [This is true because if \(K\) contains \(S\) then it must contain all the inverses of these words and then also the products of all of these words since it's a group (closed) so it has all of these elements and it contains \(T\) itself.] Condition (1) and (2) imply that \(\langle S \rangle \subseteq T\). Conditions (1) and (3) imply that \(T \subseteq \langle S \rangle\). Therefore, \(T = \langle S \rangle\). \(\ \blacksquare\) We’ll use this proposition a lot. Lattice of Subgroups Consider the group \(S_3 = \{e, (1 \ 2), (1 \ 3), (2 \ 3), (1 \ 2 \ 3), (1 \ 3 \ 2)\}\). We can list its subgroups in terms of its generating subsets as follows: \(\langle (1 \ 2) \rangle = \{e, (1 \ 2)\}\). This says that the subgroup generated by \(\{(1 \ 2)\}\) is \(\{e, (1 \ 2)\} = \langle (1 \ 2) \rangle \) \(\langle (1 \ 3) \rangle = \{e, (1 \ 3)\}\). \(\langle (2 \ 3) \rangle = \{e, (2 \ 3)\}\). \(\langle (1 \ 2 \ 3) \rangle = \{e, (1 \ 2 \ 3), (1 \ 3 \ 2)\}\) = \(\langle (1 \ 3 \ 2) \rangle\) \(S_3 = \langle (1 \ 2), (1 \ 2 \ 3) \rangle\). We can then draw this in the following figure]]></summary></entry><entry><title type="html">Groups and Isomorphism</title><link href="http://localhost:4000/jekyll/update/2025/02/01/math417-08-groups-and-isomorphism.html" rel="alternate" type="text/html" title="Groups and Isomorphism" /><published>2025-02-01T00:01:36-08:00</published><updated>2025-02-01T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/02/01/math417-08-groups-and-isomorphism</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/02/01/math417-08-groups-and-isomorphism.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A group \((G, G \times G \rightarrow G)\) is a set with a binary operation on that set such that. 
<ol>
	<li>The operation is associative so that \((ab)c = a(bc), \forall a,b,c \in G\)</li>
	<li>There exists an identity element. \(\exists e \in G\) such that \(ae = a = ea \forall a \in G\).</li>
	<li>Every element has an inverse. \(\forall a \in G, \exists a^{-1} \in G\) such that \(aa^{-1} = e = a^{-1}a\).</li>
</ol>
Additionally, a group is commutative/abelian if \(ab = ba, \forall a, b \in G.\).
</div>
<!---------------------------------------------------------------------->
<p><br />
<br />
In addition to groups, we also have</p>
<ul> 
<li>Monoids which satisfy \((1)\) and \((2)\). </li>
<li>Semi-groups which satisfy \((1)\).</li>
<li>Magma \(G, \cdot\) with no additional properties. </li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Basic Properties of Groups</b></h4>
<p>In the next few propositions, we’ll prove that the identity element in a group is unique and similarly the inverses are unique.
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
The identity element is unique.
</div>
<p><br />
<b>Proof</b>
<br />
Let \(e\) and \(e'\) be identity elements. Then by definition for any \(x, y \in G\)</p>
<div>
	$$
	\begin{align*}
	 xe &amp;= x = ex \\
	 ye' &amp;= y = e'y
	\end{align*}
	$$
</div>
<p>If we let \(x = e'\) and let \(y = e\), then</p>
<div>
	$$
	\begin{align*}
	 e'e &amp;= e' = ee' \\
	 ee' &amp;= e = e'e
	\end{align*}
	$$
</div>
<p>From this we see that \(e = e'\) as desired. \(\ \blacksquare\)
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Inverses in a group are unique.
</div>
<p><br />
<!---------------------------------------------------------------------->
<b>Proof</b>
<br />
Let \(a \in G\) and suppose for the sake of contradiction that \(a\) has two inverses \(b\) and \(c\). That is \(ab = e = ba\) and \(ac = e = ca\). Then,</p>
<div>
	$$
	\begin{align*}
	 c = ec = (ba)c = b(ac) = be = b.
	\end{align*}
	$$
</div>
<p>Therefore, \(c = b\) which is a contradiction so the inverse must be unique. \(\ \blacksquare\)
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (2.1.2)
</div>
<div class="peachbodydiv">
Let \(G\) be a group and let \(a, b \in G\). If \(ab = e\), then \(a = b^{-1}\). Likewise, if \(ba = e\), then \(b = a^{-1}\).
</div>
<p><br />
By definition, \(b\) an inverse of \(a\) if \(ab = ba = e\) so it’s an inverse on both sides. This proposition proposes that checking only one side is enough. That is if \(ab = e\), then \(a = b^{-1}\) and \(b\) is the inverse. So what we want to show here is that given \(ba = e\), then \(b\) is the inverse of \(a\). We don’t check the other side. One side is enough to imply the other.
<br />
<br />
<!---------------------------------------------------------------------->
<b>Proof</b>
<br />
We know from the previous proposition that \(a\) has an inverse \(a^{-1} \in G\). Now consider the expression \(a^{-1}ab\). By associativity, we can reduce this expression in two ways</p>
<div>
	$$
	\begin{align*}
	 (a^{-1}a)b &amp;= a^{-1}(ab)
	\end{align*}
	$$
</div>
<p>The left hand side</p>
<div>
	$$
	\begin{align*}
	 (a^{-1}a)b &amp;= eb = b
	\end{align*}
	$$
</div>
<p>The right hand side is</p>
<div>
	$$
	\begin{align*}
	 a^{-1}(ab) &amp;= a^{-1}
	\end{align*}
	$$
</div>
<p>So \(b = a^{-1}\) and therefore, \(ba = a^{-1}a = e\). 
<br />
<br />
<!---------------------------------------------------------------------->
<b>Proof (Book)</b>
<br />
Suppose \(hg = e\), then</p>
<div>
	$$
	\begin{align*}
	 h = h(gg^{-1}) = (hg)g^{-1}= eg^{-1} = g^{-1}.
	\end{align*}
	$$
</div>
<p>Suppose now that \(gh = e\), then</p>
<div>
	$$
	\begin{align*}
	 g = g(hh^{-1}) = (gh)h^{-1} = eh^{-1} = h^{-1}. \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Corollary 2.1.3
</div>
<div class="peachbodydiv">
Let \(g\) be an element of a group \(G\). We have \(g = (g^{-1})^{-1}\)
</div>
<p><br />
<b>Proof</b>
<br />
We know \(gg^{-1} = e\). By (2.1.2) \(g = (g^{-1})^{-1}\). \(\ \blacksquare\)
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 2.1.4
</div>
<div class="peachbodydiv">
Let \(G\) be a group and let \(a,b \in G\). Then We have \((ab)^{-1} = b^{-1}a^{-1}\)
</div>
<p><br />
<b>Proof</b>
<br />
Notice that \((ab)(b^{-1}a^{-1}) = a(b((b^{-1}a^{-1})) = a((bb^{-1})a^{-1}) = a(ea^{-1}) = aa^{-1} = e\). Therefore, \((ab)^{-1} = b^{-1}a^{-1}\).
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>The Left and Right Multiplication Maps</b></h4>

<div class="peachheaderdiv">
Proposition 2.1.5
</div>
<div class="peachbodydiv">
Let \(G\) be a group and \(a \in G\). <br />
The map \(L_a: G \rightarrow G\) defined by \(L_a(x) = ax\) is a bijection. Similarly, <br />
the map \(R_a: G \rightarrow G\) defined by \(R_a(x) = xa\) is a bijection.
</div>
<!---------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
To show that the map \(L_a\) is a bijection, we need to show that \(L_a\) is both one to one and onto or equivalently that \(L_a\) has an inverse. We claim that the left multiplication by \(a^{-1}\) or \(L_{a^{-1}}\) is the inverse of \(L_a\). To see that,</p>
<div>
	$$
	\begin{align*}
	L_{a^{-1}}(L_a(x)) = a^{-1}(ax) = (a^{-1}a)x = ex = x.
	\end{align*}
	$$
</div>
<p>Similarly,</p>
<div>
	$$
	\begin{align*}
	L_a(L_{a^{-1}}(x)) = a(a^{-1}x) = (aa^{-1})x = ex = x.
	\end{align*}
	$$
</div>
<p>So \(L_{a^{-1}}\) and \(L_a\) are inverse maps so both are bijective. \(\ \blacksquare\)
<br />
<br /></p>

<!---------------------------------------------------------------------->
<div class="peachheaderdiv">
Corollary 2.1.6
</div>
<div class="peachbodydiv">
Let \(G\) be a group and let \(a\) and \(b\) be elements of \(G\). The equation \(ax = b\) has a unique solution \(x\) in \(G\). and likewise the equation \(xa = b\) has a unique solution in \(G\).
</div>
<p><br />
<b>Proof</b>
<br />
For \(ax = b\) to have a solution. The map \(L_a\) needs to be onto or surjective. For the solution to be unique, the map needs to be one to one or injective. Similarly for \(xa = b\) to have a solution, we want \(R_a\) to be a bijective. Since we proved earlier that \(L_a\) and \(R_a\) are bijections, then both equations have unique solutions. \(\ \blacksquare\)
<br />
<br /></p>

<!---------------------------------------------------------------------->
<div class="peachheaderdiv">
Corollary 2.1.7 (Cancellation)
</div>
<div class="peachbodydiv">
Suppose \(a, x, y\) are elements of a group \(G\). If \(ax = ay\), then \(x = y\). Similarly, if \(xa = ya\), then \(x = y\).
</div>
<p><br />
<b>Proof</b>
<br />
Suppose \(ax = ay\). We know that \(L_a(x) = ax\) is one to one. So for any elements \(x, y \in G\), \(ax = ay\) must imply that \(x = y\) by definition of a one to one or injective map. A similar arguments shows that if \(xa = ya\) must imply that \(x = y\) by the injectivity of \(R_a\). \(\ \blacksquare\)
<br />
<br /></p>

<!---------------------------------------------------------------------->
<div class="peachheaderdiv">
Corollary 2.1.8
</div>
<div class="peachbodydiv">
If \(G\) is a finite group, each row and each column of the multiplication table of \(G\) contains each element of \(G\) exactly once.
</div>
<p><br />
<b>Proof (book)</b>
<br />
A row in the multiplication table can be represented by a left multiplication map \(G \rightarrow G\) if you fix the element multiplied on the left. We know the left multiplication map is a bijection. Therefore every element/result must be unique and each element of \(G\) must show up in the row. Similarly, each column can be represented by a right multiplication map. The map is a bijection and so each element must be unique and shown exactly once. (TODO clean up this proof)
<br />
<br /></p>
<hr />

<p><br />
<!---------------------------------------------------------------------></p>
<h4><b>Associativity in Groups</b></h4>
<p>We know by definition that the product is associative so for all \(a, b, c \in G\), we have \((ab)c = a(bc)\). What about the product of 4 or more elements? is it associative? For example, there are five ways to group four elements</p>
<div>
	$$
	\begin{align*}
	a(b(cd)), a((bc)d), (ab)(cd), (a(bc))d, ((ab)c)d
	\end{align*}
	$$
</div>
<p>\(a(b(cd))\) and \(((ab)c)d\) follow from the definition. For the rest, see that</p>
<div>
	$$
	\begin{align*}
	a(bcd) = a(b(cd)) = (ab)(cd) = ((ab)c)d = (abc)d
	\end{align*}
	$$
</div>
<p>How many ways are there to associate an \(n\)-fold product?</p>
<div>
	$$
	\begin{align*}
	\frac{1}{2}\binom{2n - 2}{n - 1}
	\end{align*}
	$$
</div>
<p>In general this works for any number of elements. Formally,
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 2.1.19 (General Associative law)
</div>
<div class="peachbodydiv">
Let \(M\) be a set with an associative operation, \(M \times M \rightarrow M\), denoted by juxtaposition. For every \(n \geq 1\), there is a unique product \(M^n \rightarrow M\),
	$$
	\begin{align*}
	(a_1, a_2,...,a_n) \rightarrow a_1a_2...a_n,
	\end{align*}
	$$
such that
<ol type="a">
	<li> The product of one element is that element \((a) = a\).</li>
	<li> The product of two elements agrees with the given operation \((ab) = ab\).</li>
	<li>For all \(n \geq 2\), for all \(a_1,...,a_n \in M\), and for all \(1 \leq k \leq n-1\), 
		$$
		\begin{align*}
		a_1a_2...a_n = (a_1...a_k)(a_{k-1}...a_n)
		\end{align*}
		$$
	</li>
</ol>
</div>
<!---------------------------------------------------------------------->
<p><br />
<b>Proof</b>
<br />
By induction on \(n\). <br />
Base Case: For \(n \leq 3\), property \((c)\) holds by definition.
<br />
Inductive Case: Suppose this is true for all \(1 \leq r \leq n\) where a unique product of \(r\) elements satisfies the properties \((a)-(c)\) above. Suppose now that we have \(n\) elements. Fix elements \(a_1, ...,a_n \in M\). By the inductive hypothesis, the \(n-1\) products</p>
<div>
	$$
	\begin{align*}
	p_k = (a_1...a_k)(a_{k+1}...a_n),
	\end{align*}
	$$
</div>
<p>are defined since we have at most \(n-1\) elements. … [TODO]
<br />
<br /></p>
<hr />

<p><br />
<!---------------------------------------------------------------------></p>
<h4><b>General Powers</b></h4>
<p>Now, we turn into defining powers of an element in a group
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
For \(a \in G\) where \(G\) is a group and \(n \in \mathbf{Z}\). Define \(a^n \in G\) by
<ol>
	<li>\(a^0 = e\).</li>
	<li>\(a^1 = a\).</li>
	<li>\(a^{-1} \) is the inverse of \(a\).</li>
	<li>\(n \geq 1\), \(a^{n+1} = a^n a\).</li>
	<li>\(n \leq -1\), \(a^{n-1} = a^n a^{-1}\).</li>
</ol>
</div>
<!---------------------------------------------------------------------->
<p><br />
Based on this definition we have the following proposition
<br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
	$$
	\begin{align*}
	a^{m}a^{n} &amp;= a^{m + n} \\
	(a^m)^{n} &amp;= a^{mn} \\	
	\end{align*}
	$$
</div>
<p><br />
<b>Proof (1)</b>
<br />
By induction on \(n\). <br />
Base Case \((n = 1)\): \(a^m a = a^{m+1}\) by definition.
<br />
Inductive Case: Suppose the inductive hypothesis is true for \(n\). That is \(a^m a^n = a^{m+n}\). Now observe that</p>
<div>
	$$
	\begin{align*}
	a^m a^{n+1} &amp;= a^m (a^n a) \text{ (by definition)} \\
	            &amp;= (a^m a^n) a \text{ (by associativity)} \\
	            &amp;= a^{m+n} a \text{ (by the inductive hypothesis)} \\
	            &amp;= a^{m+n+1} \text{ (by definition)}. \ \blacksquare
	\end{align*}
	$$
</div>
<p><br />
<b>Proof (2)</b>
By induction on \(n\). <br />
Base Case \((n = 1)\): \((a^{m})^1 = a^{m}\).
<br />
Inductive Case: Suppose the inductive hypothesis is true for \(n\). That is \((a^m)^n = a^{mn}\). Now observe that</p>
<div>
	$$
	\begin{align*}
	(a^{m})^{n+1} &amp;= (a^{m})^{n} (a^{m})^{1} \text{ (by definition)} \\
	              &amp;= (a^{m})^{n} a^{m} \\
	              &amp;= a^{mn} a^{m}  \text{ (by the inductive hypothesis)} \\
				  &amp;= a^{mn+m}  \text{ (by the previous proof)} \\
	            &amp;= a^{m(n+1)}. \ \blacksquare
	\end{align*}
	$$
</div>

<p><br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Isomorphism</b></h4>
<p>One thing that we want to do is to compare two groups. For example take \((\mathbf{Z}_4, +)\), the symmetries of the rectangle, \((\phi(5), \cdot)\) and \((\phi(8), \cdot)\). These are all groups with exactly 4 elements. To compare two groups, we want to see if we can construct a bijection between the two groups. Formally, this is called an isomorphism as follows
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition 2.1.13
</div>
<div class="mintbodydiv">
We say two groups \(G\) and \(H\) are isomorphic if there is a bijection \(\varphi: G \rightarrow H\) such that for all \(a, b \in G\)
	$$
	\begin{align*}
	\varphi(ab) = \varphi(a)\varphi(b)	
	\end{align*}
	$$
where the first multiplication is in \(G\) while the second is in \(H\).<br />
 The map \(\varphi\) is called an isomorphism.
</div>
<p><br />
<br />
We write \(H \approx G\) for \(G\) is isomorphic to \(H\).
<br />
<br />
An an example consider the group of symmetries of the equilateral triangle \(D_3\) and the group \(S_3\) (permutations of \(\{1,2,3\}\)). Both groups contain exactly 6 elements. They turn out to be isomorphic. Observe here that if we assign the vertices \(A\) to \(1\), \(B\) to \(2\) and \(C\) to \(3\), then the rotation \(a\) flips the vertices \(B\) and \(C\). This is exactly the same as the permutation \((2 \ 3)\). Similarly, if you think about the rotation \(r\), you’ll notice that \(r\) permutes the vertices in the exact way as the permutation \((1 \ 2 \ 3)\). In fact, all permutations. Another example is the \(b\) rotation. This rotation fixes \(b\) and rotates \(a\) and \(c\). This is also the same as the permutation \((1 \ 3)\). We can map the rest of the symmetries as follows</p>
<div>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(D_3\)</td>
    <td>\(S_3\)</td>
  </tr>
  <tr>
    <td>\(id\)</td>
    <td>\(id\)</td>
  </tr>
  <tr>
    <td>\(r\)</td>
    <td>\((1 \quad 2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(a\)</td>
    <td>\((2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(r^2\)</td>
    <td>\((1 \quad 3 \quad 2)\)</td>
  </tr>
  <tr>
    <td>\(b\)</td>
    <td>\((1 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(c\)</td>
    <td>\((1 \quad 2)\)</td>
  </tr>
</table>
</div>
<p><br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
	If \(\varphi:G \rightarrow H\) is an isomorphism, then \(\varphi^{-1}: G \rightarrow H\) is also an isomorphism.
</div>
<p><br />
<b>Proof</b>
<br />
Let \(a', b' \in H\). We want to show that</p>
<div>
	$$
	\begin{align*}
	\varphi^{-1}(a'b') = \varphi^{-1}(a') \varphi^{-1}(b')
	\end{align*}
	$$
</div>
<p>Let \(a' = \varphi(a)\) and \(b' = \varphi(b)\) for some \(a, b \in G\). Since \(\varphi\) is injective then it suffices to show that</p>
<div>
	$$
	\begin{align*}
	\varphi(\varphi^{-1}(a'b')) = \varphi(\varphi^{-1}(a') \varphi^{-1}(b'))
	\end{align*}
	$$
</div>
<p>The right hand side is clearly just \(a'b'\). \(\varphi\) is an isomorphism so the left hand side becomes</p>
<div>
	$$
	\begin{align*}
	\varphi(\varphi^{-1}(a') \varphi^{-1}(b')) &amp;= 
	\varphi(\varphi^{-1}(a')) \varphi(\varphi^{-1}(b')) \\
	                          &amp;= a'b'
	\end{align*}
	$$
</div>
<p>Therefore \(\varphi^{-1}\) is an isomorphism as desired. \(\ \blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>Groups of Small Order</b></h4>
<p>The order of a group is the number of elements in it. Formally,
<br />
<br />
<!----------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition 2.1.10
</div>
<div class="mintbodydiv">
The order of a group is its size or cardinality. We will denote the order of a group \(G\) by \(|G|\).
</div>
<p><br />
<br />
One interesting thing to do is to classify all groups of a given finite order. If we do that for small sizes, we get
<!---------------------------------------------------------------------></p>
<ol>
	<li>Order 0: None because we need to at least have the identity element.</li>
	<li>Order 1: \(G = \{e\}\). Other groups of order 1 can be \(G'=\{1\}\). These two groups are isomorphic. Technically, it's a different set but to us, they're the same group. So "up to isomorphism", there is only one group of size 1.</li>
	<li>Order 2: Up to isomorphism, the only unique group is \(\mathbf{Z}_2 = \{[0], [1], [2]\}\).</li>
	<li>Order 3: Up to isomorphism, the only unique group is \(\mathbf{Z}_3\). All other groups of size 3 will be isomorphic to \(\mathbf{Z}_3\).</li>
	<li>order 4: Up to isomorphism, there are two unique groups. \(\mathbf{Z}_4 \) and \(\mathbf{Z}_2 \times \mathbf{Z}_2\) (symmetries of the rectangle). Groups of order 4 can be isomorphic to either one.</li>
	<li>Order 5: Up to isomorphism, the only unique group is \(\mathbf{Z}_5\)</li>
	<li>Order 6: Up to isomorphism, we have two unique groups. \(\mathbf{Z}_6\) and \(S_3 \approx D_3\). These two groups are not isomorphic because \(S_3\) is non-abelian while \(\mathbf{Z}_6\) is abelian. The proof for this fact is next.</li>
</ol>
<p><br />
<!----------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(G \approx H\), then \(G\) is abelian if and only if \(H\) is abelian.
</div>
<p><br />
<b>Proof (Lecture Notes)</b>
<br />
Suppose that \(\phi: G \rightarrow H\) is an isomorphism. Furthermore, let \(a, b \in G\) and \(a', b' \in H\) such that \(\phi^{-1}(a') = a\) and \(\phi^{-1}(b') = b'\). We will prove both directions of the statement.
<br />
<br />
\(\Rightarrow\): Suppose that \(G\) is abelian. We will show that \(H\) is abelian. Observe that</p>
<div>
	$$
	\begin{align*}
	\phi(ab) &amp;= \phi(a)\phi(b) = a'b' \\
	\phi(ba) &amp;= \phi(b)\phi(a) = b'a'.
	\end{align*}
	$$
</div>
<p>But we know that \(G\) is abelian and so \(ab = ba\) so \(\phi(ab) = \phi(ba)\) and therefore we must have \(a'b' = b'a'\).
<br />
<br />
\(\Leftarrow\): Now suppose that \(H\) is abelian. Then,</p>
<div>
	$$
	\begin{align*}
	\phi(ab) &amp;= \phi(a)\phi(b) = a'b' \\
	\phi(ba) &amp;= \phi(b)\phi(a) = b'a'.
	\end{align*}
	$$
</div>
<p>We know that \(a'b' = b'a'\) because \(H\) is abelian. But since \(\phi\) is injective, this implies that \(ab = ba\). (remember injective means that \(f(a)=f(b) \implies a = b\)). \(\ \blacksquare\)
<br />
<br />
Note that for this direction, we could’ve instead relied on \(\phi^{-1}\) being an isomorphism itself. and so observe that</p>
<div>
	$$
	\begin{align*}
	\phi^{-1}(a'b') &amp;= \phi^{-}(a')\phi^{-}(b') = ab \\
	\phi^{-1}(b'a') &amp;= \phi^{-}(b')\phi^{-}(a') = ba.
	\end{align*}
	$$
</div>
<p>\(H\) is abelian so \(a'b' = b'a'\) so \(\phi^{-1}(a'b') = \phi^{-1}(b'a')\) and thus \(ab = ba\).
<br />
<br />
<br /></p>
<hr />

<p><br />
<br />
<!---------------------------------------------------------------------->
Extra Notes from the book: Two groups are isomorphic means that the multiplication tables match up. In fact, not only that but the identity elements and inverses of elements match up as well. The following propositions state these facts.
<br /></p>
<div class="peachheaderdiv">
Proposition 2.1.18
</div>
<div class="peachbodydiv">
If \(\phi : G \rightarrow H\) is an isomorphism, then \(\phi(e_G) = e_H\), and for each \(g \in G\), \(\phi(g^{-1}) = \phi(g)^{-1}\)
</div>
<p><br />
<b>Proof</b>
<br />
Since \(\phi\) is an isomorphism, then we know that for each \(h \in H\), there is a \(g \in G\) such that \(\phi(g) = h\). Therefore,</p>
<div>
	$$
	\begin{align*}
	\phi(e_G)h &amp;= \phi(e_G)\phi(g) \\
	           &amp;= \phi(e_Gg) \quad \text{$\phi$ is an isomorphism}\\
			   &amp;= \phi(g) \\
			   &amp;= h \\
	\end{align*}
	$$
</div>
<p>So \(\phi(e_G)\) is an identity element. But since the identity element is unique, then \(\phi(e_G) = \phi(e_H)\).
<br />
<br />
To show that \(\phi(g^{-1}) = \phi(g)^{-1}\), see that</p>
<div>
	$$
	\begin{align*}
	\phi(g^{-1})\phi(g) &amp;= \phi(g^{-1}g) \quad \text{$\phi$ is an isomorphism} \\
	        &amp;= \phi(e_G) \\
			&amp;= e_H \quad \text{by the previous result} 	
	\end{align*}
	$$
</div>
<p>So \(\phi(g)\) is the inverse of \(\phi(g^{-1})\) or in other words \(\phi(g)^{-1} = \phi(g^{-1})\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!----------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A group \((G, G \times G \rightarrow G)\) is a set with a binary operation on that set such that. The operation is associative so that \((ab)c = a(bc), \forall a,b,c \in G\) There exists an identity element. \(\exists e \in G\) such that \(ae = a = ea \forall a \in G\). Every element has an inverse. \(\forall a \in G, \exists a^{-1} \in G\) such that \(aa^{-1} = e = a^{-1}a\). Additionally, a group is commutative/abelian if \(ab = ba, \forall a, b \in G.\). In addition to groups, we also have Monoids which satisfy \((1)\) and \((2)\). Semi-groups which satisfy \((1)\). Magma \(G, \cdot\) with no additional properties. Basic Properties of Groups In the next few propositions, we’ll prove that the identity element in a group is unique and similarly the inverses are unique. Proposition The identity element is unique. Proof Let \(e\) and \(e'\) be identity elements. Then by definition for any \(x, y \in G\) $$ \begin{align*} xe &amp;= x = ex \\ ye' &amp;= y = e'y \end{align*} $$ If we let \(x = e'\) and let \(y = e\), then $$ \begin{align*} e'e &amp;= e' = ee' \\ ee' &amp;= e = e'e \end{align*} $$ From this we see that \(e = e'\) as desired. \(\ \blacksquare\) Proposition Inverses in a group are unique. Proof Let \(a \in G\) and suppose for the sake of contradiction that \(a\) has two inverses \(b\) and \(c\). That is \(ab = e = ba\) and \(ac = e = ca\). Then, $$ \begin{align*} c = ec = (ba)c = b(ac) = be = b. \end{align*} $$ Therefore, \(c = b\) which is a contradiction so the inverse must be unique. \(\ \blacksquare\) Proposition (2.1.2) Let \(G\) be a group and let \(a, b \in G\). If \(ab = e\), then \(a = b^{-1}\). Likewise, if \(ba = e\), then \(b = a^{-1}\). By definition, \(b\) an inverse of \(a\) if \(ab = ba = e\) so it’s an inverse on both sides. This proposition proposes that checking only one side is enough. That is if \(ab = e\), then \(a = b^{-1}\) and \(b\) is the inverse. So what we want to show here is that given \(ba = e\), then \(b\) is the inverse of \(a\). We don’t check the other side. One side is enough to imply the other. Proof We know from the previous proposition that \(a\) has an inverse \(a^{-1} \in G\). Now consider the expression \(a^{-1}ab\). By associativity, we can reduce this expression in two ways $$ \begin{align*} (a^{-1}a)b &amp;= a^{-1}(ab) \end{align*} $$ The left hand side $$ \begin{align*} (a^{-1}a)b &amp;= eb = b \end{align*} $$ The right hand side is $$ \begin{align*} a^{-1}(ab) &amp;= a^{-1} \end{align*} $$ So \(b = a^{-1}\) and therefore, \(ba = a^{-1}a = e\). Proof (Book) Suppose \(hg = e\), then $$ \begin{align*} h = h(gg^{-1}) = (hg)g^{-1}= eg^{-1} = g^{-1}. \end{align*} $$ Suppose now that \(gh = e\), then $$ \begin{align*} g = g(hh^{-1}) = (gh)h^{-1} = eh^{-1} = h^{-1}. \ \blacksquare \end{align*} $$ Corollary 2.1.3 Let \(g\) be an element of a group \(G\). We have \(g = (g^{-1})^{-1}\) Proof We know \(gg^{-1} = e\). By (2.1.2) \(g = (g^{-1})^{-1}\). \(\ \blacksquare\) Proposition 2.1.4 Let \(G\) be a group and let \(a,b \in G\). Then We have \((ab)^{-1} = b^{-1}a^{-1}\) Proof Notice that \((ab)(b^{-1}a^{-1}) = a(b((b^{-1}a^{-1})) = a((bb^{-1})a^{-1}) = a(ea^{-1}) = aa^{-1} = e\). Therefore, \((ab)^{-1} = b^{-1}a^{-1}\). The Left and Right Multiplication Maps]]></summary></entry><entry><title type="html">Lecture 06/07: Modular Arithmetic</title><link href="http://localhost:4000/jekyll/update/2025/01/30/math417-07-modular-arithmetic.html" rel="alternate" type="text/html" title="Lecture 06/07: Modular Arithmetic" /><published>2025-01-30T00:01:36-08:00</published><updated>2025-01-30T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/30/math417-07-modular-arithmetic</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/30/math417-07-modular-arithmetic.html"><![CDATA[<!------------------------------------------------------------------------------>
<div class="mintheaderdiv">
Definition (Book Definition 1.7.1)
</div>
<div class="mintbodydiv">
Given integers \(a\) and \(b\), and a natural number \(n\), we say that "\(a\) is congruent to \(b\) modulo \(n\)" and we write 
$$
\begin{align*}
a \equiv b \bmod n \quad \text{or} \quad a \equiv_n b
\end{align*}
$$
if \(a - b\) is divisible by \(n\) or \(n \ | \ a - b\). So there exists some \(t \in \mathbf{Z}\) such that \(a - b = tn\)
</div>
<!------------------------------------------------------------------------------>
<p><br />
<br />
Example: \((1 \bmod 7) = (8 \bmod 7) = 1\). Therefore, \(1 \equiv_7 8\).
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
For each integer \(a\), write
$$
\begin{align*}
[a] &amp;= [a]_n = \{x \in \mathbf{Z} \ | \ x \equiv a \bmod n\} \subseteq \mathbf{Z} \\
    &amp;= \{ a + ny \ | \ y \in \mathbf{Z} \}
\end{align*}
$$
The set \([a]\) is called the residue class or congruence class of \(a\) modulo \(n\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
<br />
Example: \([2]_6 = \{2 + 6y \ | \ y \in \mathbf{Z}\} = \{ ...,-10,-4,2,8,14,20,... \}\)
<br />
\([2]_6 = [8]_6 = [-10]_6 = [602]_6 = ....\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Remainder Function</b></h4>
<p>Another important definition that we need is the following
<br />
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
	\(rem_n \ : \ \mathbf{Z} \rightarrow \{0,1,2,...,n-1\}\) is the remainder function after dividing by \(n\).
	<br />
	\(rem_n(a) = r\) is the unique remainder of \(a \div n\) such that \(0 \leq r &lt; n\) and \(a = qn + r\) for some \(q \in \mathbf{Z}\). Note that \(a - r\) is divisible by \(n\) or \(a \equiv_n r\).
</div>
<!------------------------------------------------------------------------------>
<p><br />
It’s important to note that the remainder is in the same congruence class as \(a\). In fact \([a]_n \cap \{0,...n-1\} = \{r\}\). Usually the remainder \(r\) is the standard/canonical name for the congruence class. So for example, we usually don’t write \([602]_6\) but write \([2]_6\). But we don’t have to put it in canonical form.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Properties of Congruence</b></h4>
<p>Congruence is an equivalence relation. The following properties show this.
<br />
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma (Book 1.7.2)
</div>
<div class="yellowbodydiv">
	Properties of Congruence:
<ol>
	<li>Reflexive: For all \(a \in \mathbf{Z}\), \(a \equiv a \bmod n\).</li>
	<li>Symmetric: For all \(a, b \in \mathbf{Z}\), \(a \equiv b \bmod n\) if and only if \(b \equiv a \bmod n\).</li>
	<li>Transitive: For all \(a, b, c \in \mathbf{Z}\), if \(a \equiv b \bmod n\) and \(b \equiv c \bmod n\), then \(a \equiv c \bmod n\).</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof (book)</b>
<br />
For \((a)\), \(a - a = 0\) is divisible by \(n\). For \((b)\), if \(a - b\) is divisible by \(n\), then \(b - a\) is also divisible by \(n\) and vice versa. For \((c)\), if \(a - b\) is divisible by \(n\) and \(b - c\) is divisible by \(4n\), then \((a - b) + (b - c) = a - c\) is also divisible by \(n\). \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------------>
Based on these properties, we have the following proposition
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Proposition (Book Lemma 1.7.3)
</div>
<div class="yellowbodydiv">
For \(a, b \in \mathbf{Z}\), the following are equivalent:
<ol type="a">
	<li>\(a \equiv b \bmod n\).</li>
	<li>\([a]_n = [b]_n\).</li>
	<li>\(\text{rem}_n(a) = \text{rem}_n(b)\).</li>
	<li>\([a]_n \cap [b]_n \neq \emptyset\).</li>
</ol>
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof (Book):</b>
<br />
\((a) \implies (b)\):
<br />
Suppose \(a \equiv b \bmod n\). We want to show that \([a]_n = [b]_n\). 
<br />
\([a]_n \subseteq [b]_n\): Let \(c \in \mathbf{Z}\). If \(c \equiv a \bmod n\), then \(c \equiv b \bmod n\) by Lemma 1.7.2 (c). Therefore \([a]_n \subseteq [b]_n\).
<br />
\([b]_n \subseteq [a]_n\): If \(c \equiv b \bmod n\), then \(c \equiv a \bmod n\) and so \([a] = [b]\) as required.
<br />
<br />
\((b) \implies (c)\):
<br />
By definition, \(\text{rem}_n(x)\) is the unique element of \([x]\) that lies inside \(\{0,1,...,n-1\}\). So if \([a]_n=[b]_n\), then it must be the same element.
<br />
<br />
\((c) \implies (d)\):
<br />
\((d)\) is an immediate application of \((c)\)
<br />
<br />
\((d) \implies (a)\):
Suppose that \([a]_n \cap [b]_n \neq \emptyset\). Let \(c \in [a]_n \cap [b]_n\). Then \(a \equiv c \bmod n\) and \(b \equiv c \bmod n\). But this implies that \(a \equiv b \bmod n\). \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Modular Arithmetic</b></h4>
<p>The following lemma establishes how modular arithmetic is done.
<br /></p>
<div class="yellowheaderdiv">
Proposition (Book Lemma 1.7.5)
</div>
<div class="yellowbodydiv">
Let \(a, a', b, b'\) be integers with \(a \equiv a' \bmod n\) and \(b \equiv b' \bmod n\). Then 
$$
\begin{align*}
a + b &amp;\equiv a' + b' \bmod n \\
ab &amp;\equiv a'b' \bmod n
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
[TODO]
<br />
<br />
<!------------------------------------------------------------------------------>
We can now use these modular arithmetic properties to define algebraic structures on a set.
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
	$$
	\begin{align*}
	\mathbf{Z}_n &amp;= \{ \text{ set of congruence classes mod } n \} \\
	             &amp;= \{ [a]_n \ | \ a \in \mathbf{Z} \} \\
				 &amp;= \{ [0]_n, [1]_n,...,[n-1]_n \ | \ a \in \mathbf{Z} \} \\
	\end{align*}
	$$
</div>
<p><br />
<br />
<!------------------------------------------------------------------------------>
So now we can use the operations we defined previously to turn this set into a commutative ring.
<br />
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Define operations \(+\), \(\cdot\) on \(\mathbf{Z}_n\) by
	$$
	\begin{align*}
	[a]_n + [b]_n &amp;= [a + b]_n \\
	[a]_n[b]_n &amp;= [ab]_n
	\end{align*}
	$$
</div>
<p><br />
\((\mathbf{Z}_n,+,\cdot)\) is commutative ring with identity while \((\mathbf{Z}_n,\cdot)\) is a commutative monoid. \([1]\) is the identity element.
<br />
<br />
Only some elements \([a] \in \mathbf{Z}_n\) have a multiplicative inverse. (such that \([a][b] = [1] = [b][a]\)). The question is when do we have a multiplicative inverse? The answer is in the following proposition.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(n \geq 1, a \in \mathbf{Z}\). Then \([a]\) has a multiplicative inverse in \(\mathbf{Z}\) if and only if \(gcd(a,n) = 1\).
</div>
<p><br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Suppose that \(gcd(a,n)=1\). Then there exists \(r, s \in \mathbf{Z}\) such that \(ar + ns = 1\). Re-writing this equation, we observe that</p>
<div>
$$
\begin{align*}
ar + ns = 1 &amp;\Longleftrightarrow ar = 1 + (-s)n \quad \text{ (so $ar$ and 1 differ by a multiple of $n$)}\\
             &amp;\Longleftrightarrow ar = 1 \bmod n \\
			 &amp;\Longleftrightarrow [a][r] = 1. \ \blacksquare \\
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------>
We will use \(\phi(n) = \{x \in \mathbf{Z}_n \ | \ x \text{ has a multiplicative inverse}\} \in \mathbf{Z}_n\). 
<br />
\((\phi(n))\) is a commutative group.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Binomial Theorem</b></h4>
<p><br />
Next we have the binomial theorem which we need to prove a proposition which will then lead to Fermat’s Little Theorem.
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem (Binomial Theorem)
</div>
<div class="yellowbodydiv">
	\(a, b \in \mathbf{R}, n \in \mathbf{N}\). Then
	$$
	\begin{align*}
	(a + b)^n = \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^k
	\end{align*}
	$$
	where \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) for \(a \leq k \leq n\)
</div>
<p><br />
<b>Proof</b>
<br />
Use Pascal’s Identity: \(\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}\)… TODO
<br />
<br />
As a consequence of the binomial theorem, we have the next proposition
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(p\) be prime. For all \(a, b \in \mathbf{Z}\):
	$$
	\begin{align*}
	(a + b)^p \equiv a^p + b^p \bmod p
	\end{align*}
	$$
</div>
<p><br />
<br />
First observe that \((a+b)^5 = a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + b^5\). So those middle terms all have coefficients divisible by 5. Therefore, they’ll go away if we apply mod \(5\). This happens when \(p\) is prime.
<br />
<br />
<!------------------------------------------------------------------------------>
<b>Proof</b>
<br />
Using the binomial theorem, we can first expand the sum \((a+b)^p\) as follows</p>
<div>
	$$
	\begin{align*}
	(a + b)^p &amp;= \sum_{k=0}^{n} \binom{p}{k} a^{p-k} b^k \\
	          &amp;= a^{p} + \binom{p}{1} a^{p-1} b + \binom{p}{2} a^{p-2} b^2 + ... + b^k \\
	\end{align*}
	$$
</div>
<p>What we need to show is that \(\binom{p}{k} \equiv 0 \bmod p\) if \(0 &lt; k &lt; p\). If we show this, then what’s left is the first and last terms only. Observe that</p>
<div>
	$$
	\begin{align*}
	\binom{p}{k} &amp;= \frac{p!}{k!(p-k)!} \\
	\binom{p}{k} k! (p-k)! &amp;= p! \\
	\end{align*}
	$$
</div>
<p>\(p! = p(p-1)...1\). So \(p \ | \ p!\). But \(p! = \binom{p}{k} k! (p-k)!\) so \(p\) divides this whole product. Since \(p\) is prime, then \(p\) must divide one of the factors. Now observe that \(p\) can’t divide \(k!\) since \(k &lt; p\). \(p\) doesn’t divide \((p - k)!\) either since \(p - k &lt; p\). Therefore, \(p\) must divide \(\binom{p}{k}\) as desired. \(\ \blacksquare\). 
<!------------------------------------------------------------------------------>
<br />
<br />
A consequence of this proof is Fermat’s Little Theorem
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Fermat's Little Theorem</b></h4>
<p>Next, we will prove Fermat’s Little Theorem.
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
	Let \(p\) be prime, \(a \in \mathbf{Z}\)
	<ol>
		<li>\(a^p \equiv a \bmod p\)</li>
		<li>If \(p \nmid a\), then \(a^{p-1} \equiv 1 \bmod p\)</li>
	</ol>
</div>
<!------------------------------------------------------------------------------>
<p><br />
<b>Proof of (1)</b>
<br /></p>
<div class="proofdiv">
By Induction on \(a\) for \(a \geq 1\).
<br />
Base Case (\(a = 1\)): \(1^p \equiv 1 \bmod p\) and so we're done.
<br />
<br />
Inductive Case (\(a &gt; 1\)): 
Assume it is true for \(a\). We will show that it is true for \(a+1\). 
<div>
	$$
	\begin{align*}
	(a + 1)^p &amp;\equiv a^p + 1^p \quad \text{(By the previous proposition)} \\
	          &amp;\equiv a + 1 \quad \text{(By the inductive hypothesis)}
	\end{align*}
	$$
</div>
</div>
<!------------------------------------------------------------------------>
<p><br />
Note that if \(a = 0\), then \(0^p = 0\). 
<br />
<!------------------------------------------------------------------------>
For \(a \leq 0\), we can use downward induction.</p>
<div class="proofdiv">
By Induction on \(a\) for \(a &lt; 0\).
<br />
Base Case (\(a = -1\)):  We need to show that \((-1)^p \equiv -1 \bmod p\). \(p\) is prime so we have two cases. If \(p = 2\), then \((-1)^2 = 1 = -1 \bmod 2\). If \(p\) is odd, then \((-1)^p = -1\).
<br />
<br />
Inductive Case (\(a &lt; -1\)): We want to show that \(a^p \equiv a\) implies \((a - 1)^p = a - 1\).
</div>
<p><br />
<!------------------------------------------------------------------------>
<b>Proof of (2)</b>
<br />
We are given that \(a \nmid p\). From part (1), we know that \(a^p \equiv a \bmod p\). So \(p \ | \ a^p - a = a(a^{p-1} - 1)\). But since \(p\) is prime and since it doesn’t divide \(p\), then it must divide \(a^{p-1} - 1\). 
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Two-Prime Fermat</b></h4>
<p>There is a more generalized version of Fermat’s Little Theorem. We will use Fermat’s theorem to prove it.
<br />
<br /></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
	Let \(p, q\) be distinct primes and let \(n = pq\). Let \(m\) be
	$$
	\begin{align*}
	   m = lcm(p-1, q-1) = \frac{(p-1)(q-1)}{gcd(p-1,q-1)}
	\end{align*}
	$$
If \(a \in \mathbf{Z}\), \(h \in \mathbf{N}\) such that \(h \equiv 1 \bmod m\), then \(a^h \equiv a \bmod n\)
</div>
<!------------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
We know that \(h \equiv 1 \bmod m\) so \(h = 1 + tm\) for some \(t \in \mathbf{Z}\). We want to show that \(a^h \equiv a \bmod n\). In other words, we want to show that \(n \ | \ a^h - a\) which means that \(pq \ | \ a^h - a\). We can write \(a^h - a\) as follows</p>
<div>
	$$
	\begin{align*}
	   a^h - a &amp;= a^{1+tm} - a \\
	           &amp;= a(a^{tm} - 1).
	\end{align*}
	$$
</div>
<p>Therefore, want to show that \(pq\) divides \(a(a^{tm} - 1)\). Lecture 5 Proposition (Corollary 1.6.17 in the book) states that if \(a\) and \(b\) are relatively prime, \(a \ | \ n\) and \(b \ | \ n\), \(ab \ | \ n\). We’re given that \(p\) and \(q\) are distinct primes so they are relatively prime. So the goal is to prove that \(p\) divides \(a(a^{tm} - 1)\) and \(q\) divides \(a(a^{tm} - 1)\) to conclude that the product \(pq\) divides \(a(a^{tm} - 1)\).
<br />
<br />
To start, we want to show that \(p \ | \ a(a^{tm} - 1)\). But since \(p\) is prime, then it will have to divide \(a\) or \(a^{tm} - 1\). So we need to show that either \(p \ | \ a\) or \(p \ | \ a^{tm} - 1\). So suppose that \(p \nmid a\). We claim that \(p \ | \ a^{tm} - 1\). Recall that \(m = lcm(p-1, q-1)\). So \(m\) is a multiple of \(p-1\) and we can write \(m = (p - 1)s\) for some \(s \in \mathbf{N}\). So now we can write \(a^{tm}\) as follows</p>
<div>
	$$
	\begin{align*}
	   a^{tm} &amp;= a^{ts(p-1)} \\
	              &amp;= (a^{p-1})^{ts}.
	\end{align*}
	$$
</div>
<p>Since we assumed that \(p \nmid a\). Then we can use Fermat’s Little Theorem (part 2) to conclude that \(a^{p-1} \equiv 1 \bmod p\). Now we can use modular arithmetic to simplify the original expression.</p>
<div>
	$$
	\begin{align*}
	   (a^{p-1})^{ts} &amp;\equiv 1^{ts} \bmod p \text{ (because } a^{p-1} \equiv 1 \bmod p) \\
	                  &amp;\equiv 1 \bmod p.
	\end{align*}
	$$
</div>
<p>This implies that \(a^{tm} - 1\) must be divisible by \(p\) as we wanted to show. So \(p\) divides the product \(a(a^{tm} - 1)\). With a similar argument, we can show this for \(q\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>RSA Cryptosystem</b></h4>
<p>This encryption method is based on the two prime fermat theorem. It is widely used to encrypt many of the transactions that happen on the internet.
<br />
<br />
Symmetric Encryption:
We have some plain text (\(x \in \mathbf{Z}_n\)) that we want to encrypt. We have a function that takes a key (\(e \in \mathbf{Z}\)) to encrypt the plain text and turn it to encrypted text (\(y \in \mathbf{Z}_n\)). To decrypt it back, we have to use the same key again to turn it to plain text. The flaw in this method is that the encryption and decryption keys are the same and will need to be shared somehow. That’s why we have an alternative:
<br />
<br />
Asymmetric Encryption:
This is the same proces except that now we have a decryption key \((d \in \mathbf{Z})\). So now we have a pair of keys \((e, d)\). You will broadcast \(e\) so that anyone can encrypt a message and send it to you, but you are the only with the decryption key. The goal here is to design the pair such that one one can deduce \(d\) from \(e\). So how to design such a pair?
<br />
<br />
One way to implement this idea is the following (RSA):</p>
<ol>
	<li>Pick a prime number \(p,q\) large (100s of digits)</li>
	<li>\(n = pq\)</li>
	<li>\(m = lcm(p-1,q-1)\)</li>
	<li>Choose \(1 &lt; d, e &lt; m\) such that \(de \equiv 1 \bmod m\). You can first pick \(d\) and then you can pick a multiplicative inverse of \(d\) to be \(e\) using the Euclidean Algorithm.</li>
</ol>
<p>So now, you take the plain test \(x\) and raise it to the \(e\)th power. \(x^e\) is the encrypted text. \(e\) is the encryption key. The output \(y = x^e\) is the encrypted text. To decrypt it, raise the encrypted text to the \(d\)th power so \(y^d = x^{ed}\) and so now we have the following equation</p>
<div>
	$$
	\begin{align*}
	  x^{ed} \equiv x \bmod pq
	\end{align*}
	$$
</div>
<p>This is Two-prime Fermat.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition (Book Definition 1.7.1) Given integers \(a\) and \(b\), and a natural number \(n\), we say that "\(a\) is congruent to \(b\) modulo \(n\)" and we write $$ \begin{align*} a \equiv b \bmod n \quad \text{or} \quad a \equiv_n b \end{align*} $$ if \(a - b\) is divisible by \(n\) or \(n \ | \ a - b\). So there exists some \(t \in \mathbf{Z}\) such that \(a - b = tn\) Example: \((1 \bmod 7) = (8 \bmod 7) = 1\). Therefore, \(1 \equiv_7 8\). Definition For each integer \(a\), write $$ \begin{align*} [a] &amp;= [a]_n = \{x \in \mathbf{Z} \ | \ x \equiv a \bmod n\} \subseteq \mathbf{Z} \\ &amp;= \{ a + ny \ | \ y \in \mathbf{Z} \} \end{align*} $$ The set \([a]\) is called the residue class or congruence class of \(a\) modulo \(n\). Example: \([2]_6 = \{2 + 6y \ | \ y \in \mathbf{Z}\} = \{ ...,-10,-4,2,8,14,20,... \}\) \([2]_6 = [8]_6 = [-10]_6 = [602]_6 = ....\) The Remainder Function Another important definition that we need is the following Definition \(rem_n \ : \ \mathbf{Z} \rightarrow \{0,1,2,...,n-1\}\) is the remainder function after dividing by \(n\). \(rem_n(a) = r\) is the unique remainder of \(a \div n\) such that \(0 \leq r &lt; n\) and \(a = qn + r\) for some \(q \in \mathbf{Z}\). Note that \(a - r\) is divisible by \(n\) or \(a \equiv_n r\). It’s important to note that the remainder is in the same congruence class as \(a\). In fact \([a]_n \cap \{0,...n-1\} = \{r\}\). Usually the remainder \(r\) is the standard/canonical name for the congruence class. So for example, we usually don’t write \([602]_6\) but write \([2]_6\). But we don’t have to put it in canonical form. Properties of Congruence Congruence is an equivalence relation. The following properties show this. Lemma (Book 1.7.2) Properties of Congruence: Reflexive: For all \(a \in \mathbf{Z}\), \(a \equiv a \bmod n\). Symmetric: For all \(a, b \in \mathbf{Z}\), \(a \equiv b \bmod n\) if and only if \(b \equiv a \bmod n\). Transitive: For all \(a, b, c \in \mathbf{Z}\), if \(a \equiv b \bmod n\) and \(b \equiv c \bmod n\), then \(a \equiv c \bmod n\). Proof (book) For \((a)\), \(a - a = 0\) is divisible by \(n\). For \((b)\), if \(a - b\) is divisible by \(n\), then \(b - a\) is also divisible by \(n\) and vice versa. For \((c)\), if \(a - b\) is divisible by \(n\) and \(b - c\) is divisible by \(4n\), then \((a - b) + (b - c) = a - c\) is also divisible by \(n\). \(\ \blacksquare\) Based on these properties, we have the following proposition Proposition (Book Lemma 1.7.3) For \(a, b \in \mathbf{Z}\), the following are equivalent: \(a \equiv b \bmod n\). \([a]_n = [b]_n\). \(\text{rem}_n(a) = \text{rem}_n(b)\). \([a]_n \cap [b]_n \neq \emptyset\). Proof (Book): \((a) \implies (b)\): Suppose \(a \equiv b \bmod n\). We want to show that \([a]_n = [b]_n\). \([a]_n \subseteq [b]_n\): Let \(c \in \mathbf{Z}\). If \(c \equiv a \bmod n\), then \(c \equiv b \bmod n\) by Lemma 1.7.2 (c). Therefore \([a]_n \subseteq [b]_n\). \([b]_n \subseteq [a]_n\): If \(c \equiv b \bmod n\), then \(c \equiv a \bmod n\) and so \([a] = [b]\) as required. \((b) \implies (c)\): By definition, \(\text{rem}_n(x)\) is the unique element of \([x]\) that lies inside \(\{0,1,...,n-1\}\). So if \([a]_n=[b]_n\), then it must be the same element. \((c) \implies (d)\): \((d)\) is an immediate application of \((c)\) \((d) \implies (a)\): Suppose that \([a]_n \cap [b]_n \neq \emptyset\). Let \(c \in [a]_n \cap [b]_n\). Then \(a \equiv c \bmod n\) and \(b \equiv c \bmod n\). But this implies that \(a \equiv b \bmod n\). \(\ \blacksquare\) Modular Arithmetic The following lemma establishes how modular arithmetic is done. Proposition (Book Lemma 1.7.5) Let \(a, a', b, b'\) be integers with \(a \equiv a' \bmod n\) and \(b \equiv b' \bmod n\). Then $$ \begin{align*} a + b &amp;\equiv a' + b' \bmod n \\ ab &amp;\equiv a'b' \bmod n \end{align*} $$ Proof [TODO] We can now use these modular arithmetic properties to define algebraic structures on a set. Definition $$ \begin{align*} \mathbf{Z}_n &amp;= \{ \text{ set of congruence classes mod } n \} \\ &amp;= \{ [a]_n \ | \ a \in \mathbf{Z} \} \\ &amp;= \{ [0]_n, [1]_n,...,[n-1]_n \ | \ a \in \mathbf{Z} \} \\ \end{align*} $$ So now we can use the operations we defined previously to turn this set into a commutative ring. Definition Define operations \(+\), \(\cdot\) on \(\mathbf{Z}_n\) by $$ \begin{align*} [a]_n + [b]_n &amp;= [a + b]_n \\ [a]_n[b]_n &amp;= [ab]_n \end{align*} $$ \((\mathbf{Z}_n,+,\cdot)\) is commutative ring with identity while \((\mathbf{Z}_n,\cdot)\) is a commutative monoid. \([1]\) is the identity element. Only some elements \([a] \in \mathbf{Z}_n\) have a multiplicative inverse. (such that \([a][b] = [1] = [b][a]\)). The question is when do we have a multiplicative inverse? The answer is in the following proposition. Proposition Let \(n \geq 1, a \in \mathbf{Z}\). Then \([a]\) has a multiplicative inverse in \(\mathbf{Z}\) if and only if \(gcd(a,n) = 1\). Proof Suppose that \(gcd(a,n)=1\). Then there exists \(r, s \in \mathbf{Z}\) such that \(ar + ns = 1\). Re-writing this equation, we observe that $$ \begin{align*} ar + ns = 1 &amp;\Longleftrightarrow ar = 1 + (-s)n \quad \text{ (so $ar$ and 1 differ by a multiple of $n$)}\\ &amp;\Longleftrightarrow ar = 1 \bmod n \\ &amp;\Longleftrightarrow [a][r] = 1. \ \blacksquare \\ \end{align*} $$ We will use \(\phi(n) = \{x \in \mathbf{Z}_n \ | \ x \text{ has a multiplicative inverse}\} \in \mathbf{Z}_n\). \((\phi(n))\) is a commutative group. Binomial Theorem Next we have the binomial theorem which we need to prove a proposition which will then lead to Fermat’s Little Theorem. Theorem (Binomial Theorem) \(a, b \in \mathbf{R}, n \in \mathbf{N}\). Then $$ \begin{align*} (a + b)^n = \sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^k \end{align*} $$ where \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) for \(a \leq k \leq n\) Proof Use Pascal’s Identity: \(\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}\)… TODO As a consequence of the binomial theorem, we have the next proposition Proposition Let \(p\) be prime. For all \(a, b \in \mathbf{Z}\): $$ \begin{align*} (a + b)^p \equiv a^p + b^p \bmod p \end{align*} $$ First observe that \((a+b)^5 = a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + b^5\). So those middle terms all have coefficients divisible by 5. Therefore, they’ll go away if we apply mod \(5\). This happens when \(p\) is prime. Proof Using the binomial theorem, we can first expand the sum \((a+b)^p\) as follows $$ \begin{align*} (a + b)^p &amp;= \sum_{k=0}^{n} \binom{p}{k} a^{p-k} b^k \\ &amp;= a^{p} + \binom{p}{1} a^{p-1} b + \binom{p}{2} a^{p-2} b^2 + ... + b^k \\ \end{align*} $$ What we need to show is that \(\binom{p}{k} \equiv 0 \bmod p\) if \(0 &lt; k &lt; p\). If we show this, then what’s left is the first and last terms only. Observe that $$ \begin{align*} \binom{p}{k} &amp;= \frac{p!}{k!(p-k)!} \\ \binom{p}{k} k! (p-k)! &amp;= p! \\ \end{align*} $$ \(p! = p(p-1)...1\). So \(p \ | \ p!\). But \(p! = \binom{p}{k} k! (p-k)!\) so \(p\) divides this whole product. Since \(p\) is prime, then \(p\) must divide one of the factors. Now observe that \(p\) can’t divide \(k!\) since \(k &lt; p\). \(p\) doesn’t divide \((p - k)!\) either since \(p - k &lt; p\). Therefore, \(p\) must divide \(\binom{p}{k}\) as desired. \(\ \blacksquare\). A consequence of this proof is Fermat’s Little Theorem Fermat's Little Theorem Next, we will prove Fermat’s Little Theorem. Theorem Let \(p\) be prime, \(a \in \mathbf{Z}\) \(a^p \equiv a \bmod p\) If \(p \nmid a\), then \(a^{p-1} \equiv 1 \bmod p\) Proof of (1) By Induction on \(a\) for \(a \geq 1\). Base Case (\(a = 1\)): \(1^p \equiv 1 \bmod p\) and so we're done. Inductive Case (\(a &gt; 1\)): Assume it is true for \(a\). We will show that it is true for \(a+1\). $$ \begin{align*} (a + 1)^p &amp;\equiv a^p + 1^p \quad \text{(By the previous proposition)} \\ &amp;\equiv a + 1 \quad \text{(By the inductive hypothesis)} \end{align*} $$ Note that if \(a = 0\), then \(0^p = 0\). For \(a \leq 0\), we can use downward induction. By Induction on \(a\) for \(a &lt; 0\). Base Case (\(a = -1\)): We need to show that \((-1)^p \equiv -1 \bmod p\). \(p\) is prime so we have two cases. If \(p = 2\), then \((-1)^2 = 1 = -1 \bmod 2\). If \(p\) is odd, then \((-1)^p = -1\). Inductive Case (\(a &lt; -1\)): We want to show that \(a^p \equiv a\) implies \((a - 1)^p = a - 1\). Proof of (2) We are given that \(a \nmid p\). From part (1), we know that \(a^p \equiv a \bmod p\). So \(p \ | \ a^p - a = a(a^{p-1} - 1)\). But since \(p\) is prime and since it doesn’t divide \(p\), then it must divide \(a^{p-1} - 1\). Two-Prime Fermat There is a more generalized version of Fermat’s Little Theorem. We will use Fermat’s theorem to prove it. Theorem Let \(p, q\) be distinct primes and let \(n = pq\). Let \(m\) be $$ \begin{align*} m = lcm(p-1, q-1) = \frac{(p-1)(q-1)}{gcd(p-1,q-1)} \end{align*} $$ If \(a \in \mathbf{Z}\), \(h \in \mathbf{N}\) such that \(h \equiv 1 \bmod m\), then \(a^h \equiv a \bmod n\) Proof We know that \(h \equiv 1 \bmod m\) so \(h = 1 + tm\) for some \(t \in \mathbf{Z}\). We want to show that \(a^h \equiv a \bmod n\). In other words, we want to show that \(n \ | \ a^h - a\) which means that \(pq \ | \ a^h - a\). We can write \(a^h - a\) as follows $$ \begin{align*} a^h - a &amp;= a^{1+tm} - a \\ &amp;= a(a^{tm} - 1). \end{align*} $$ Therefore, want to show that \(pq\) divides \(a(a^{tm} - 1)\). Lecture 5 Proposition (Corollary 1.6.17 in the book) states that if \(a\) and \(b\) are relatively prime, \(a \ | \ n\) and \(b \ | \ n\), \(ab \ | \ n\). We’re given that \(p\) and \(q\) are distinct primes so they are relatively prime. So the goal is to prove that \(p\) divides \(a(a^{tm} - 1)\) and \(q\) divides \(a(a^{tm} - 1)\) to conclude that the product \(pq\) divides \(a(a^{tm} - 1)\). To start, we want to show that \(p \ | \ a(a^{tm} - 1)\). But since \(p\) is prime, then it will have to divide \(a\) or \(a^{tm} - 1\). So we need to show that either \(p \ | \ a\) or \(p \ | \ a^{tm} - 1\). So suppose that \(p \nmid a\). We claim that \(p \ | \ a^{tm} - 1\). Recall that \(m = lcm(p-1, q-1)\). So \(m\) is a multiple of \(p-1\) and we can write \(m = (p - 1)s\) for some \(s \in \mathbf{N}\). So now we can write \(a^{tm}\) as follows $$ \begin{align*} a^{tm} &amp;= a^{ts(p-1)} \\ &amp;= (a^{p-1})^{ts}. \end{align*} $$ Since we assumed that \(p \nmid a\). Then we can use Fermat’s Little Theorem (part 2) to conclude that \(a^{p-1} \equiv 1 \bmod p\). Now we can use modular arithmetic to simplify the original expression. $$ \begin{align*} (a^{p-1})^{ts} &amp;\equiv 1^{ts} \bmod p \text{ (because } a^{p-1} \equiv 1 \bmod p) \\ &amp;\equiv 1 \bmod p. \end{align*} $$ This implies that \(a^{tm} - 1\) must be divisible by \(p\) as we wanted to show. So \(p\) divides the product \(a(a^{tm} - 1)\). With a similar argument, we can show this for \(q\). RSA Cryptosystem This encryption method is based on the two prime fermat theorem. It is widely used to encrypt many of the transactions that happen on the internet. Symmetric Encryption: We have some plain text (\(x \in \mathbf{Z}_n\)) that we want to encrypt. We have a function that takes a key (\(e \in \mathbf{Z}\)) to encrypt the plain text and turn it to encrypted text (\(y \in \mathbf{Z}_n\)). To decrypt it back, we have to use the same key again to turn it to plain text. The flaw in this method is that the encryption and decryption keys are the same and will need to be shared somehow. That’s why we have an alternative: Asymmetric Encryption: This is the same proces except that now we have a decryption key \((d \in \mathbf{Z})\). So now we have a pair of keys \((e, d)\). You will broadcast \(e\) so that anyone can encrypt a message and send it to you, but you are the only with the decryption key. The goal here is to design the pair such that one one can deduce \(d\) from \(e\). So how to design such a pair? One way to implement this idea is the following (RSA): Pick a prime number \(p,q\) large (100s of digits) \(n = pq\) \(m = lcm(p-1,q-1)\) Choose \(1 &lt; d, e &lt; m\) such that \(de \equiv 1 \bmod m\). You can first pick \(d\) and then you can pick a multiplicative inverse of \(d\) to be \(e\) using the Euclidean Algorithm. So now, you take the plain test \(x\) and raise it to the \(e\)th power. \(x^e\) is the encrypted text. \(e\) is the encryption key. The output \(y = x^e\) is the encrypted text. To decrypt it, raise the encrypted text to the \(d\)th power so \(y^d = x^{ed}\) and so now we have the following equation $$ \begin{align*} x^{ed} \equiv x \bmod pq \end{align*} $$ This is Two-prime Fermat. References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 06: Least Common Multiple</title><link href="http://localhost:4000/jekyll/update/2025/01/29/math417-06-lcm.html" rel="alternate" type="text/html" title="Lecture 06: Least Common Multiple" /><published>2025-01-29T00:01:36-08:00</published><updated>2025-01-29T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/29/math417-06-lcm</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/29/math417-06-lcm.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
A least common multiple (LCM) is \(m \in \mathbf{Z_{\geq 0}}\) such that:
<ol type="a">
	<li>\(a \ | \ m\) and \(b \ | \ m\)</li>
	<li>If \(n \in \mathbf{Z}\), \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\).</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
In fact the LCM is unique if it exists. To prove it’s existence we’ll need the following definition
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="mintheaderdiv">
Definition (Well Ordering Principle)
</div>
<div class="mintbodydiv">
A non-empty subset of \(\mathbf{N}\) has a smallest element
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
Next, we’ll prove the existence of the LCM.
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
The LCM always exists
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Let \(a, b \in \mathbf{Z}\). Let \(J = \mathbf{Z}a \cap \mathbf{Z}b\). We want to show that \(J\) itself is a set of multiples so we can write \(J = \mathbf{Z}m\) for some \(m \in \mathbf{Z}_{\geq 0}\). This means that \(m\) must be the LCM of \(a\) and \(b\) by definition. [Why? If \(J = \mathbf{Z}m\), then \(m\) is the least element in \(J\), all the other elements are multiples of \(m\). We have \(a \ | \ m\) and \(b \ | \ m\) because \(J\) also contains common multiples of \(a\) and \(b\). And for any \(n \in \mathbf{Z}\), if \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\). This is true because \(m\) is the smallest of all these common multiples.] We will use division by remainder to prove this!
<br />
<br />
Observe that \(J\) is closed under addition and subtraction. So if \(c, d \in J\), then any integer combination of \(c\) and \(d\) will be in \(J\). So for any \(m, n \in \mathbf{Z}\),  \(cm + dn \in J\).
<br />
<br />
We have two cases:
<br />
Case 1: If either \(a = 0\) or \(b = 0\), then \(J = \{0\} = \mathbf{Z}0\).
<br />
<br />
Case 2: If \(a \neq 0\) and \(b \neq 0\), then \(J\) is the set of common multiples of \(\mathbf{Z}a\) and \(\mathbf{Z}b\). Consider \(J \cap \mathbf{N}\). This is the set of positive common multiples. This set is not empty because it will at least include \(|ab|\). So \(J \cap \mathbf{N}\) is a non-empty subset of \(\mathbf{N}\). By the well-ordering principle, there is a smallest number in \(J \cap \mathbf{N}\). Let \(m \in J \cap \mathbf{N}\) be the smallest common multiple. (so we’re setting \(m\) to be the smallest positive common multiple of \(\mathbf{Z}a\) and \(\mathbf{Z}b\))
<br />
<br />
We claim that \(J = \mathbf{Z}m\) (the set of multiples of \(m\)). We will show this by proving that \(J \subseteq \mathbf{Z}m\) and that \(\mathbf{Z}m \subseteq J\). 
<br />
<br />
\(\mathbf{Z}m \subseteq J\): This is true since \(m \in J\) and we showed earlier that \(m\) is closed under addition and subtraction so any multiple of \(m\) is also in \(J\) and therefore \(\mathbf{Z}m \subseteq J\).
<br />
<br />
\(J \subseteq \mathbf{Z}m\): Suppose \(x \in J = \mathbf{Z}a \cap \mathbf{Z}b\). Consider what happens when we divide \(x\) by \(m\). We can write \(x = qm + r\). with \(q, r \in \mathbf{Z}\) and \(0 \leq r &lt; m\). Re-write the equation as</p>
<div>
$$
\begin{align*}
r = x - qm.
\end{align*}
$$
</div>
<p>Observe now that \(x \in J\) by assumption. \(qm\) is a multiple of \(m\) so it’s in \(J\). So their difference is also in \(J\) which means that \(r \in J\). If \(r = 0\), \(x = qm \in \mathbf{Z}m\) and we’re done. If \(r \neq 0\), then \(r \in J\), then \(r \in J \cap \mathbf{N}\) but also \(r &lt; m\). But that’s a contradiction because \(m\) is the smallest element in \(J \cap \mathbf{N}\) and so this case doesn’t happen. So \(J = \mathbf{Z}m\) as desired. \(\ \blacksquare\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Consequences of the LCM</b></h4>
<p>The next few propositions are some consequences of the LCM.
<br />
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(d = gcd(a,b) = 1\), then \(m = lcm(a,b) = ab\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
We are given that \(d = gcd(a,b) = 1\). This means that \(a\) and \(b\) are relatively prime. Condition 1 is true since \(a \ | \ ab\) and \(b \ | \ ab\). For condition two of the LCM definition, let \(n \in \mathbf{Z}\) such that \(a \ | \ n\) and \(b \ | \ n\). By the proposition from lecture 5, \(ab \ | \ n\). (Proposition: If \(a\) and \(b\) are relatively prime, then if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | \ n\)). \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Let \(e\) be a common divisor of \(a\) and \(b\) so \(e \ | \ a\) and \(e \ | \ b\). Then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\). 
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
[TODO] \(x\) is a common divisor of \(\frac{a}{e}\) and \(\frac{b}{e}\) if and only if \(xe\) is a common divisor of \(a\) and \(b\). 
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(a, b \in \mathbf{N}\), \(d = gcd(a,b)\) and \(m = lcm(a,b)\), then \(m = \frac{ab}{d}\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
We need the previous two propositions</p>
<ol>
	<li>If \(d = 1\), then \(m = lcm(a,b) = ab\)</li>
	<li>If \(e \ | \ a\) and \(e \ | \ b\), then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\)</li>
</ol>
<p>If we divide both \(a\) and \(b\) by \(d\), their gcd will become 1 so \(gcd(\frac{a}{d}, \frac{b}{d}) = 1\). Since the gcd is 1, then \(lcm(\frac{a}{d}, \frac{b}{d}) = \frac{b}{d}\frac{a}{d}\) by part \((1)\). So now let \(e = d\), so by \((2)\),</p>
<div>
$$
\begin{align*}
\frac{1}{d} lcm(a, b) &amp;= lcm(\frac{a}{d}, \frac{b}{d}) \\
\frac{m}{d}  &amp;= \frac{b}{d}\frac{a}{d} \\
m &amp;= \frac{ab}{d} \\
\end{align*}
$$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Pairwise Relatively Prime</b></h4>
<p><br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
\(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime if \(gcd(a_i,a_j) = 1\) for all \(1 \leq i,j \leq k, i \neq j\)
</div>
<p><br />
<br />
Some facts based this:
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
If \(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime, then \(a_k,b = a_1,...,a_{k-1}\) are relatively prime so \(gcd(a_k,b) = 1\). 
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose for the sake of contradiction that \(a_k\) and \(b\) are not relatively prime so they have a common divisor. Let \(p\) be a prime that is a common divisor of \(a_k\) and \(b\). So \(p \ | \ b\) which means that \(p \ | \ a_1a_2...a_{k-1}\). But since \(p\) is prime, then by the proposition from the previous lecture, \(p\) must divide one of the factors in \(a_1a_2...a_{k-1}\). Let \(p\) divide \(a_i\) for some \(1 \leq i \leq k - 1\). However \(p \ | \ p_k\). This means that \(gcd(a_i,a_k) \neq 1\). This is a contradiction since we assumed that \(a_1,...,a_k\) are pairwise relatively prime. Therefore, we must have \(a_k\) and \(b\) be relatively prime. \(\ \blacksquare\)
<br />
<br />
The consequence of this lemma is the following
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(a_1, a_2,...,a_k\) are pairwise relatively prime and if \(a_i \ | \ n\) for all \(i = 1,...,k\), then \(a_1a_2,...,a_k \ | \ n\) 
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
By Induction on \(k\) 
<br />
Base Case: If \(k=1\), then there is nothing to prove. 
<br />
If \(k = 2\), then \(gcd(a_1,a_2)=1\). If \(a_1 \ | \ n\) and \(a_1 \ | \ n\), then by the proposition from lecture 5, we must have \(ab \ | \ n\).
<br />
<br />
Inductive Case \(k \geq 3\): <br />
Let \(b = a_1a_2...a_{k-1}\).  By the previous lemma we know that \(gcd(a_k,b) = 1\). Also by the inductive hypothesis if \(a_1,...,a_{k-1} \ | \ n\), then \(a_1...a_{k-1} = b \ | \ n\). but \(gcd(a_k,b) = 1\) so \(a_k\) and \(b\) are relatively prime. Since they are relatively prime and they both divide \(n\), then \(ba_k = a_1...a_{k-1}a_k\) also divides \(n\) as we wanted to show. \(\ \blacksquare\)</p>

<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition A least common multiple (LCM) is \(m \in \mathbf{Z_{\geq 0}}\) such that: \(a \ | \ m\) and \(b \ | \ m\) If \(n \in \mathbf{Z}\), \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\). In fact the LCM is unique if it exists. To prove it’s existence we’ll need the following definition Definition (Well Ordering Principle) A non-empty subset of \(\mathbf{N}\) has a smallest element Next, we’ll prove the existence of the LCM. Proposition The LCM always exists Proof Let \(a, b \in \mathbf{Z}\). Let \(J = \mathbf{Z}a \cap \mathbf{Z}b\). We want to show that \(J\) itself is a set of multiples so we can write \(J = \mathbf{Z}m\) for some \(m \in \mathbf{Z}_{\geq 0}\). This means that \(m\) must be the LCM of \(a\) and \(b\) by definition. [Why? If \(J = \mathbf{Z}m\), then \(m\) is the least element in \(J\), all the other elements are multiples of \(m\). We have \(a \ | \ m\) and \(b \ | \ m\) because \(J\) also contains common multiples of \(a\) and \(b\). And for any \(n \in \mathbf{Z}\), if \(a \ | \ n\) and \(b \ | \ n\), then \(m \ | \ n\). This is true because \(m\) is the smallest of all these common multiples.] We will use division by remainder to prove this! Observe that \(J\) is closed under addition and subtraction. So if \(c, d \in J\), then any integer combination of \(c\) and \(d\) will be in \(J\). So for any \(m, n \in \mathbf{Z}\), \(cm + dn \in J\). We have two cases: Case 1: If either \(a = 0\) or \(b = 0\), then \(J = \{0\} = \mathbf{Z}0\). Case 2: If \(a \neq 0\) and \(b \neq 0\), then \(J\) is the set of common multiples of \(\mathbf{Z}a\) and \(\mathbf{Z}b\). Consider \(J \cap \mathbf{N}\). This is the set of positive common multiples. This set is not empty because it will at least include \(|ab|\). So \(J \cap \mathbf{N}\) is a non-empty subset of \(\mathbf{N}\). By the well-ordering principle, there is a smallest number in \(J \cap \mathbf{N}\). Let \(m \in J \cap \mathbf{N}\) be the smallest common multiple. (so we’re setting \(m\) to be the smallest positive common multiple of \(\mathbf{Z}a\) and \(\mathbf{Z}b\)) We claim that \(J = \mathbf{Z}m\) (the set of multiples of \(m\)). We will show this by proving that \(J \subseteq \mathbf{Z}m\) and that \(\mathbf{Z}m \subseteq J\). \(\mathbf{Z}m \subseteq J\): This is true since \(m \in J\) and we showed earlier that \(m\) is closed under addition and subtraction so any multiple of \(m\) is also in \(J\) and therefore \(\mathbf{Z}m \subseteq J\). \(J \subseteq \mathbf{Z}m\): Suppose \(x \in J = \mathbf{Z}a \cap \mathbf{Z}b\). Consider what happens when we divide \(x\) by \(m\). We can write \(x = qm + r\). with \(q, r \in \mathbf{Z}\) and \(0 \leq r &lt; m\). Re-write the equation as $$ \begin{align*} r = x - qm. \end{align*} $$ Observe now that \(x \in J\) by assumption. \(qm\) is a multiple of \(m\) so it’s in \(J\). So their difference is also in \(J\) which means that \(r \in J\). If \(r = 0\), \(x = qm \in \mathbf{Z}m\) and we’re done. If \(r \neq 0\), then \(r \in J\), then \(r \in J \cap \mathbf{N}\) but also \(r &lt; m\). But that’s a contradiction because \(m\) is the smallest element in \(J \cap \mathbf{N}\) and so this case doesn’t happen. So \(J = \mathbf{Z}m\) as desired. \(\ \blacksquare\). Consequences of the LCM The next few propositions are some consequences of the LCM. Proposition If \(d = gcd(a,b) = 1\), then \(m = lcm(a,b) = ab\) Proof We are given that \(d = gcd(a,b) = 1\). This means that \(a\) and \(b\) are relatively prime. Condition 1 is true since \(a \ | \ ab\) and \(b \ | \ ab\). For condition two of the LCM definition, let \(n \in \mathbf{Z}\) such that \(a \ | \ n\) and \(b \ | \ n\). By the proposition from lecture 5, \(ab \ | \ n\). (Proposition: If \(a\) and \(b\) are relatively prime, then if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | \ n\)). \(\ \blacksquare\) Proposition Let \(e\) be a common divisor of \(a\) and \(b\) so \(e \ | \ a\) and \(e \ | \ b\). Then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\). Proof [TODO] \(x\) is a common divisor of \(\frac{a}{e}\) and \(\frac{b}{e}\) if and only if \(xe\) is a common divisor of \(a\) and \(b\). Proposition If \(a, b \in \mathbf{N}\), \(d = gcd(a,b)\) and \(m = lcm(a,b)\), then \(m = \frac{ab}{d}\) Proof We need the previous two propositions If \(d = 1\), then \(m = lcm(a,b) = ab\) If \(e \ | \ a\) and \(e \ | \ b\), then \(lcm(\frac{a}{e}, \frac{b}{e}) = \frac{1}{e} lcm(a,b)\) If we divide both \(a\) and \(b\) by \(d\), their gcd will become 1 so \(gcd(\frac{a}{d}, \frac{b}{d}) = 1\). Since the gcd is 1, then \(lcm(\frac{a}{d}, \frac{b}{d}) = \frac{b}{d}\frac{a}{d}\) by part \((1)\). So now let \(e = d\), so by \((2)\), $$ \begin{align*} \frac{1}{d} lcm(a, b) &amp;= lcm(\frac{a}{d}, \frac{b}{d}) \\ \frac{m}{d} &amp;= \frac{b}{d}\frac{a}{d} \\ m &amp;= \frac{ab}{d} \\ \end{align*} $$ Pairwise Relatively Prime Definition \(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime if \(gcd(a_i,a_j) = 1\) for all \(1 \leq i,j \leq k, i \neq j\) Some facts based this: Lemma If \(a_1,...,a_k \in \mathbf{Z}\) are pairwise relatively prime, then \(a_k,b = a_1,...,a_{k-1}\) are relatively prime so \(gcd(a_k,b) = 1\). Proof Suppose for the sake of contradiction that \(a_k\) and \(b\) are not relatively prime so they have a common divisor. Let \(p\) be a prime that is a common divisor of \(a_k\) and \(b\). So \(p \ | \ b\) which means that \(p \ | \ a_1a_2...a_{k-1}\). But since \(p\) is prime, then by the proposition from the previous lecture, \(p\) must divide one of the factors in \(a_1a_2...a_{k-1}\). Let \(p\) divide \(a_i\) for some \(1 \leq i \leq k - 1\). However \(p \ | \ p_k\). This means that \(gcd(a_i,a_k) \neq 1\). This is a contradiction since we assumed that \(a_1,...,a_k\) are pairwise relatively prime. Therefore, we must have \(a_k\) and \(b\) be relatively prime. \(\ \blacksquare\) The consequence of this lemma is the following Proposition If \(a_1, a_2,...,a_k\) are pairwise relatively prime and if \(a_i \ | \ n\) for all \(i = 1,...,k\), then \(a_1a_2,...,a_k \ | \ n\) Proof By Induction on \(k\) Base Case: If \(k=1\), then there is nothing to prove. If \(k = 2\), then \(gcd(a_1,a_2)=1\). If \(a_1 \ | \ n\) and \(a_1 \ | \ n\), then by the proposition from lecture 5, we must have \(ab \ | \ n\). Inductive Case \(k \geq 3\): Let \(b = a_1a_2...a_{k-1}\). By the previous lemma we know that \(gcd(a_k,b) = 1\). Also by the inductive hypothesis if \(a_1,...,a_{k-1} \ | \ n\), then \(a_1...a_{k-1} = b \ | \ n\). but \(gcd(a_k,b) = 1\) so \(a_k\) and \(b\) are relatively prime. Since they are relatively prime and they both divide \(n\), then \(ba_k = a_1...a_{k-1}a_k\) also divides \(n\) as we wanted to show. \(\ \blacksquare\)]]></summary></entry><entry><title type="html">Lecture 05: Greatest Common Divisor</title><link href="http://localhost:4000/jekyll/update/2025/01/28/math417-05-gcd.html" rel="alternate" type="text/html" title="Lecture 05: Greatest Common Divisor" /><published>2025-01-28T00:01:36-08:00</published><updated>2025-01-28T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/28/math417-05-gcd</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/28/math417-05-gcd.html"><![CDATA[<div class="mintheaderdiv">
Definition 1.6.8
</div>
<div class="mintbodydiv">
A natural number \(d\) is the greatest common divisor of integers \(a\) and \(b\) if
<ol type="a">
	<li>\(d\) is a common divisor. So \(d\) divides \(a\) and \(b\)</li>
	<li>Every common divisor \(e\) also divides \(d\). In other words, if \(e \ | \ a\) and \(e \ | \ b\), then \(e \ | \ d\)</li>
</ol>
</div>
<!------------------------------------------------------------------------>
<p><br />
If the greatest common exists, then it is unique. Why? suppose \(d\) and \(d'\) are both greatest common divisors. Then by definition, \(d \ | \ d'\) and \(d' \ | \ d\) because every common divisor divides the gcd and they each must divide each other. But this means that \(d = \pm 'd\) by the divisibility properties from last lecture. But also by definition, \(d\) is non-negative and so \(d = 'd\). \(\ \blacksquare\)
<br />
<br />
Note that in the book \(a\) and \(b\) are nonzero in the definition. With this definition, the set of divisors of \(0\) is \(\mathbf{Z}\). If \(a = 0\) and \(b \neq 0\), then \(|a|\) is the gcd of \(a\) and \(0\). If both \(a\) and \(b\) are zero, then \(0\) is the gcd of \(0\) and \(0\). 
<br />
<br />
<!------------------------------------------------------------------------>
Before addressing the question of whether the greatest common divisor exists, we’ll to define one more thing
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(a, b \in \mathbf{Z}\). An integer combination of \(a\) and \(b\) is any integer of the form
$$
\begin{align*}
I(a,b) = \{ra + sb \ | \ r, s \in \mathbf{Z}\}.
\end{align*}
$$
</div>
<!------------------------------------------------------------------------>
<p><br />
For example \(I(4,6) = \{4s + 6t \ | \ s, t \in \mathbf{Z}\}\). If we let \(s = -1\) and \(t = 1\), then \(2 \in I(4,6)\). In fact, this set produces all of the even integers. It includes all multiples of 2. In other words, we can also write that \(I(4,6) = \mathbf{Z}2\). In fact, this turns out to always be true, the set of integer combinations of two integers is also the set of multiples of a number and that number is the greatest common divisor! even when one of the integers is \(0\). So yes the GCD exists and can even be computed. Before formally proving its existence, we’ll present the way it can be computed next.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Euclidean Algorithm</b></h4>
<p>Let \(\mathbf{Z}^2 = \{(a,b), a,b \in \mathbf{Z}\}\). 
<br />
Define \(F: \mathbf{Z}^2 \rightarrow \mathbf{Z}^2\) by</p>
<div class="ediv">
  $$
  \begin{equation*}
  F(m,n) = \begin{cases} (n,r), r = rem_n(m) \quad &amp;\text{if } n \neq 0 \\ (|m|,0) \quad \quad &amp;\text{if } n = 0\end{cases}
  \end{equation*}
  $$
</div>
<p><br />
\(rem_n(m)\) is the remainder of \(m \div n\) (recall that we can write \(m = qn + r\) where \(0 \leq r &lt; |n|)\)).
<br />
<br />
<b>Euclidean Algorithm:</b> Iterate \(F\) until stable.
<br />
<br />
Example:</p>
<div>
  $$
  \begin{align*}
  (42, -24) \xrightarrow{F} (-24, 18) \xrightarrow{F} (18, 12) \xrightarrow{F} (12, 6) \xrightarrow{F} (6, 0) \xrightarrow{F} (6, 0)
  \end{align*}
  $$
</div>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The GCD Theorem</b></h4>
<p>So now we have an algorithm to compute the GCD algorithm. Next, we will prove that it does compute the GCD and so the GCD does exist.
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Let \(a, b \in \mathbf{Z}\). Then
<ol>
	<li>For \(a, b\) have a GCD \(d \geq 0\).</li>
	<li>\(I(a,b) = \mathbf{Z}d\).</li>
	<li>\(d\) is computed by the Euclidean algorithm.</li>
</ol>
</div>
<p><br />
We’ll start from (3) and then prove that the answer produced by the algorithm is in fact an integer combination of the original input (statement 2). Finally, we’ll show that this just means that we have computed the GCD (statement). To do all of this we will also need the following lemmas
<br />
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
If \(a, b \in I(c,d)\), then \(I(a,b) \subseteq I(c,d)\)
</div>
<p><br />
<b>Proof</b>: Homework Problem
<br />
<br />
<!-------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Lemma
</div>
<div class="yellowbodydiv">
If \(F(m,n) = (a,b)\), then \(I(m,n) = I(a,b)\)
</div>
<p><br />
<b>Proof</b>:
<br />
We have two cases:
<br />
<br />
Case 1: \(n \neq 0\). In this case, \(F(m,n) = (n, r)\) where \(r\) is the remainder after dividing \(m\) by \(n\). So \(m = qn + r\) where \(0 \leq r &lt; |n|\). We want to show that the integer combinations of \(m\) and \(n\) is the same as the integer combinations of \(n\) and \(r\), that is, \(I(m,n) = I(n,r)\). To do this, we will show that \(I(m,n) \subseteq I(n,r)\) and \(I(n,r) \subseteq I(m,n)\). To show that \(I(m,n) \subseteq I(n,r)\), observe that \(m\) is an integer combination of \(n\) and \(r\) because we can write \(m\) as</p>
<div>
  $$
  \begin{align*}
  m &amp;= qn + 1r.  
  \end{align*}
  $$
</div>
<p>Similarly, \(n\) is also an integer combination of \(n\) and \(r\) because we can write \(n\) as</p>
<div>
  $$
  \begin{align*}
  n &amp;= 1n + 0r.
  \end{align*}
  $$
</div>
<p>Since \(m\) and \(n\) can both be written as integer combinations of \(n\) and \(r\), that is \(m \in I(n,r)\) and \(n \in I(n,r)\), then by the previous lemma,  \(I(m,n) \subseteq I(n,r)\). To see that \(I(m,n) \subseteq I(n,r)\), observe that we can write \(n\) as \(n = 0m + 1n\) so \(n \in I(m,n)\). Similarly, \(r = 1m + (-q)m\) so \(r \in I(m,n)\). Then, by the previous lemma, \(I(m,n) \subseteq I(n,r)\). Therefore, \(I(m,n) = I(n,r)\). 
<br />
<br />
Case 2: \(n = 0\). In this case, \(F(m,n) = F(m,0) = (|m|,0)\). In this case the integer combinations of \(0\) and \(m\) are just multiples of \(m\) so \(I(m,0) = \mathbf{Z}m\). But this is the same as the integer combinations of \(|m|\) and \(0\) so \(\mathbf{Z}m = I(|m|,0) = I(m,0)\) which is what we wanted to show. \(\blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Proof of the GCD Theorem</b></h4>
<p>We iterate the algorithm until \(n = 0\) and \(F(m,n) = (|m|,0)\). By the previous lemma we saw that \(I(m,n) = \mathbf{Z}d\) for some \(d \geq 0\). 
<br />
<br />
<b>Claim:</b> \(d\) is a GCD.<br /></p>
<ol>
	<li>\(m,n \in I(m,n) = \mathbf{Z}d\). So \(m\) and \(n\) are both multiples of \(d\) and d divides both of them. \(d \ | \ m\) and \(d \ | \ n\).</li>
	<li>So now for condition 2, suppose \(e\) is a common divisor of \(m\) and \(n\). We want to show that \(e\) divides \(d\). Since \(e \ | \ m\) and \(e \ | \ n\), then \(m\) and \(n\) are both multiples of \(e\). So we can write \(m = eu\) and \(n = ev\) for some \(u,v \in \mathbf{Z}\). 
	<br />
	<br />
	But we know that any integer combination of \(m\) and \(n\) is a multiple of \(d\). So we can write for some \(r, s \in \mathbf{Z}\) 
	<div>
	  $$
	  \begin{align*}
	  d &amp;= rm + sn \\
	   &amp;= reu + sev \\
	   &amp;= e(ru + sv).
	  \end{align*}
	  $$
	</div>
	So \(e\) must divide \(d\).
</li>
</ol>
<p>Therefore, \(d = gcd(m,n)\) as desired. \(\ \blacksquare\) 
<br />
Note that the Euclidean Algorithm gives a method for computing \(r, s \in \mathbf{Z}\), so \(d = rm + sn\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>GCD Example</b></h4>
<p>TODO … all I have now is this</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec05/1.png" width="55%" class="center" /></p>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Relatively Prime Integers</b></h4>
<p>Next, we’ll see how the GCD is used in the definition of relatively prime numbers.
<br /></p>
<div class="mintheaderdiv">
Definition (Book Definition 1.6.14)
</div>
<div class="mintbodydiv">
\(a, b \in \mathbf{Z}\) are relatively prime if gcd\((a,b) = 1\).
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
For example \(4\) and \(9\) are relatively prime. gcd\((4,9)=1\). 
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book Proposition 1.6.15)
</div>
<div class="peachbodydiv">
\(a, b\) are relatively prime if and only if
$$
\begin{align*}
1 = ra + sb
\end{align*}
$$
for some \(r,s \in \mathbf{Z}\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
\(\Rightarrow:\) If \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). By the GCD Theorem, this means that \(I(a,b) = 1\) and so we’re done.
<br />
\(\Leftarrow:\) If \(1 = ra + sb\) for some \(r,s \in \mathbf{Z}\), then this means that \(1 \in I(a,b)\). But \(I(a,b)\) is also the set of multiples of some integer \(m\). But since \(1\) is in the set, then it must contains all multiples of \(1\). Therefore, we must have \(1 = I(a,b)\). By the GCD Theorem, \(1\) is therefore the gcd of \(a\) and \(b\) and so \(a\) and \(b\) are relatively prime. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition (Book Corollary 1.6.17)
</div>
<div class="peachbodydiv">
If \(a, b\) are relatively prime and if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | n\).
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose \(a\) and \(b\) are relatively prime. We are given that \(a \ | \ n\) so \(n = au\) for some \(u \in \mathbf{Z}\). Similarly, \(b \ | \ n\) and so \(n = bv\) for some \(v \in \mathbf{Z}\). Since \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). This means that \(I(a,b) = 1\) and we can write \(1 = ra + sb\) for some integers \(s, r \in \mathbf{Z}\). Multiply this equation by \(b\) as follows</p>
<div>
$$
\begin{align*}
1 &amp;= ra + sb \\
n &amp;= n(ra + sb) \\
  &amp;= nra + nsb \\
  &amp;= (bv)ra + (au)sb \\
  &amp;= (ab)vr + (ab)us \\
  &amp;= (ab)(vr + us). \\
\end{align*}
$$
</div>
<p>From this we see that \(ab \ | \ n\) as desired. \(\ \blacksquare\)
<br />
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(p\) is prime and \(p \ | \ ab\), then either \(p \ | \ a\) or \(p \ | \ b\)
</div>
<!------------------------------------------------------------------------>
<p><br />
<b>Proof</b>
<br />
Suppose that \(p \ | \ ab\). We’ll show that if \(p \nmid a\), then \(p \ | \ b\). Since \(p\) is prime and \(p \nmid a\), then gcd\((p,a)=1\). By Proposition (1.6.15), this implies that \(1 = pr + as\) for some \(r, s \in \mathbf{Z}\). Multiply this equation by \(b\) to see that</p>
<div>
$$
\begin{align*}
1 &amp;= pr + as \\
b &amp;= b(pr + as) \\
b &amp;= p(br) + (ab)s \\
\end{align*}
$$
</div>
<p>Clearly \(p\) divides the first term. \(p\) also divides the second term by the assumption we’re given. Therefore, \(p\) must divide \(b\) as we wanted to show. \(\ \blacksquare\)
<br />
<br />
Fact: If \(p \ | \ a_1a_2...a_k\), then \(p\) divides at least one of the factors. So \(p \ | \ a_i\) for some \(i \in \{1,...,k\}\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition 1.6.8 A natural number \(d\) is the greatest common divisor of integers \(a\) and \(b\) if \(d\) is a common divisor. So \(d\) divides \(a\) and \(b\) Every common divisor \(e\) also divides \(d\). In other words, if \(e \ | \ a\) and \(e \ | \ b\), then \(e \ | \ d\) If the greatest common exists, then it is unique. Why? suppose \(d\) and \(d'\) are both greatest common divisors. Then by definition, \(d \ | \ d'\) and \(d' \ | \ d\) because every common divisor divides the gcd and they each must divide each other. But this means that \(d = \pm 'd\) by the divisibility properties from last lecture. But also by definition, \(d\) is non-negative and so \(d = 'd\). \(\ \blacksquare\) Note that in the book \(a\) and \(b\) are nonzero in the definition. With this definition, the set of divisors of \(0\) is \(\mathbf{Z}\). If \(a = 0\) and \(b \neq 0\), then \(|a|\) is the gcd of \(a\) and \(0\). If both \(a\) and \(b\) are zero, then \(0\) is the gcd of \(0\) and \(0\). Before addressing the question of whether the greatest common divisor exists, we’ll to define one more thing Definition Let \(a, b \in \mathbf{Z}\). An integer combination of \(a\) and \(b\) is any integer of the form $$ \begin{align*} I(a,b) = \{ra + sb \ | \ r, s \in \mathbf{Z}\}. \end{align*} $$ For example \(I(4,6) = \{4s + 6t \ | \ s, t \in \mathbf{Z}\}\). If we let \(s = -1\) and \(t = 1\), then \(2 \in I(4,6)\). In fact, this set produces all of the even integers. It includes all multiples of 2. In other words, we can also write that \(I(4,6) = \mathbf{Z}2\). In fact, this turns out to always be true, the set of integer combinations of two integers is also the set of multiples of a number and that number is the greatest common divisor! even when one of the integers is \(0\). So yes the GCD exists and can even be computed. Before formally proving its existence, we’ll present the way it can be computed next. The Euclidean Algorithm Let \(\mathbf{Z}^2 = \{(a,b), a,b \in \mathbf{Z}\}\). Define \(F: \mathbf{Z}^2 \rightarrow \mathbf{Z}^2\) by $$ \begin{equation*} F(m,n) = \begin{cases} (n,r), r = rem_n(m) \quad &amp;\text{if } n \neq 0 \\ (|m|,0) \quad \quad &amp;\text{if } n = 0\end{cases} \end{equation*} $$ \(rem_n(m)\) is the remainder of \(m \div n\) (recall that we can write \(m = qn + r\) where \(0 \leq r &lt; |n|)\)). Euclidean Algorithm: Iterate \(F\) until stable. Example: $$ \begin{align*} (42, -24) \xrightarrow{F} (-24, 18) \xrightarrow{F} (18, 12) \xrightarrow{F} (12, 6) \xrightarrow{F} (6, 0) \xrightarrow{F} (6, 0) \end{align*} $$ The GCD Theorem So now we have an algorithm to compute the GCD algorithm. Next, we will prove that it does compute the GCD and so the GCD does exist. Theorem Let \(a, b \in \mathbf{Z}\). Then For \(a, b\) have a GCD \(d \geq 0\). \(I(a,b) = \mathbf{Z}d\). \(d\) is computed by the Euclidean algorithm. We’ll start from (3) and then prove that the answer produced by the algorithm is in fact an integer combination of the original input (statement 2). Finally, we’ll show that this just means that we have computed the GCD (statement). To do all of this we will also need the following lemmas Lemma If \(a, b \in I(c,d)\), then \(I(a,b) \subseteq I(c,d)\) Proof: Homework Problem Lemma If \(F(m,n) = (a,b)\), then \(I(m,n) = I(a,b)\) Proof: We have two cases: Case 1: \(n \neq 0\). In this case, \(F(m,n) = (n, r)\) where \(r\) is the remainder after dividing \(m\) by \(n\). So \(m = qn + r\) where \(0 \leq r &lt; |n|\). We want to show that the integer combinations of \(m\) and \(n\) is the same as the integer combinations of \(n\) and \(r\), that is, \(I(m,n) = I(n,r)\). To do this, we will show that \(I(m,n) \subseteq I(n,r)\) and \(I(n,r) \subseteq I(m,n)\). To show that \(I(m,n) \subseteq I(n,r)\), observe that \(m\) is an integer combination of \(n\) and \(r\) because we can write \(m\) as $$ \begin{align*} m &amp;= qn + 1r. \end{align*} $$ Similarly, \(n\) is also an integer combination of \(n\) and \(r\) because we can write \(n\) as $$ \begin{align*} n &amp;= 1n + 0r. \end{align*} $$ Since \(m\) and \(n\) can both be written as integer combinations of \(n\) and \(r\), that is \(m \in I(n,r)\) and \(n \in I(n,r)\), then by the previous lemma, \(I(m,n) \subseteq I(n,r)\). To see that \(I(m,n) \subseteq I(n,r)\), observe that we can write \(n\) as \(n = 0m + 1n\) so \(n \in I(m,n)\). Similarly, \(r = 1m + (-q)m\) so \(r \in I(m,n)\). Then, by the previous lemma, \(I(m,n) \subseteq I(n,r)\). Therefore, \(I(m,n) = I(n,r)\). Case 2: \(n = 0\). In this case, \(F(m,n) = F(m,0) = (|m|,0)\). In this case the integer combinations of \(0\) and \(m\) are just multiples of \(m\) so \(I(m,0) = \mathbf{Z}m\). But this is the same as the integer combinations of \(|m|\) and \(0\) so \(\mathbf{Z}m = I(|m|,0) = I(m,0)\) which is what we wanted to show. \(\blacksquare\) Proof of the GCD Theorem We iterate the algorithm until \(n = 0\) and \(F(m,n) = (|m|,0)\). By the previous lemma we saw that \(I(m,n) = \mathbf{Z}d\) for some \(d \geq 0\). Claim: \(d\) is a GCD. \(m,n \in I(m,n) = \mathbf{Z}d\). So \(m\) and \(n\) are both multiples of \(d\) and d divides both of them. \(d \ | \ m\) and \(d \ | \ n\). So now for condition 2, suppose \(e\) is a common divisor of \(m\) and \(n\). We want to show that \(e\) divides \(d\). Since \(e \ | \ m\) and \(e \ | \ n\), then \(m\) and \(n\) are both multiples of \(e\). So we can write \(m = eu\) and \(n = ev\) for some \(u,v \in \mathbf{Z}\). But we know that any integer combination of \(m\) and \(n\) is a multiple of \(d\). So we can write for some \(r, s \in \mathbf{Z}\) $$ \begin{align*} d &amp;= rm + sn \\ &amp;= reu + sev \\ &amp;= e(ru + sv). \end{align*} $$ So \(e\) must divide \(d\). Therefore, \(d = gcd(m,n)\) as desired. \(\ \blacksquare\) Note that the Euclidean Algorithm gives a method for computing \(r, s \in \mathbf{Z}\), so \(d = rm + sn\). GCD Example TODO … all I have now is this Relatively Prime Integers Next, we’ll see how the GCD is used in the definition of relatively prime numbers. Definition (Book Definition 1.6.14) \(a, b \in \mathbf{Z}\) are relatively prime if gcd\((a,b) = 1\). For example \(4\) and \(9\) are relatively prime. gcd\((4,9)=1\). Proposition (Book Proposition 1.6.15) \(a, b\) are relatively prime if and only if $$ \begin{align*} 1 = ra + sb \end{align*} $$ for some \(r,s \in \mathbf{Z}\) Proof \(\Rightarrow:\) If \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). By the GCD Theorem, this means that \(I(a,b) = 1\) and so we’re done. \(\Leftarrow:\) If \(1 = ra + sb\) for some \(r,s \in \mathbf{Z}\), then this means that \(1 \in I(a,b)\). But \(I(a,b)\) is also the set of multiples of some integer \(m\). But since \(1\) is in the set, then it must contains all multiples of \(1\). Therefore, we must have \(1 = I(a,b)\). By the GCD Theorem, \(1\) is therefore the gcd of \(a\) and \(b\) and so \(a\) and \(b\) are relatively prime. \(\ \blacksquare\) Proposition (Book Corollary 1.6.17) If \(a, b\) are relatively prime and if \(a \ | \ n\) and \(b \ | \ n\), then \(ab \ | n\). Proof Suppose \(a\) and \(b\) are relatively prime. We are given that \(a \ | \ n\) so \(n = au\) for some \(u \in \mathbf{Z}\). Similarly, \(b \ | \ n\) and so \(n = bv\) for some \(v \in \mathbf{Z}\). Since \(a\) and \(b\) are relatively prime, then by definition gcd\((a,b)=1\). This means that \(I(a,b) = 1\) and we can write \(1 = ra + sb\) for some integers \(s, r \in \mathbf{Z}\). Multiply this equation by \(b\) as follows $$ \begin{align*} 1 &amp;= ra + sb \\ n &amp;= n(ra + sb) \\ &amp;= nra + nsb \\ &amp;= (bv)ra + (au)sb \\ &amp;= (ab)vr + (ab)us \\ &amp;= (ab)(vr + us). \\ \end{align*} $$ From this we see that \(ab \ | \ n\) as desired. \(\ \blacksquare\) Proposition If \(p\) is prime and \(p \ | \ ab\), then either \(p \ | \ a\) or \(p \ | \ b\) Proof Suppose that \(p \ | \ ab\). We’ll show that if \(p \nmid a\), then \(p \ | \ b\). Since \(p\) is prime and \(p \nmid a\), then gcd\((p,a)=1\). By Proposition (1.6.15), this implies that \(1 = pr + as\) for some \(r, s \in \mathbf{Z}\). Multiply this equation by \(b\) to see that $$ \begin{align*} 1 &amp;= pr + as \\ b &amp;= b(pr + as) \\ b &amp;= p(br) + (ab)s \\ \end{align*} $$ Clearly \(p\) divides the first term. \(p\) also divides the second term by the assumption we’re given. Therefore, \(p\) must divide \(b\) as we wanted to show. \(\ \blacksquare\) Fact: If \(p \ | \ a_1a_2...a_k\), then \(p\) divides at least one of the factors. So \(p \ | \ a_i\) for some \(i \in \{1,...,k\}\) References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 04: Integers</title><link href="http://localhost:4000/jekyll/update/2025/01/27/math417-04-integers.html" rel="alternate" type="text/html" title="Lecture 04: Integers" /><published>2025-01-27T00:01:36-08:00</published><updated>2025-01-27T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/27/math417-04-integers</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/27/math417-04-integers.html"><![CDATA[<p>Some facts about the integers</p>
<ul>
	<li>\((\mathbf{Z}, +)\) is a commutative group. \(+\) is associative and commutative. The identity element is \(0\) and each element \(a\)'s inverse is \(-a\).</li>
	<li>\((\mathbf{Z}, \cdot)\) is a commutative monoid. \(\cdot\) is associative and commutative. The identity element is \(1\). (No inverses required)</li>
	<li>Distributive Law: \(a(b+c) = (ab) + (ac), \forall a,b,c \in \mathbf{Z}\). (side note: the list of properties so far for reference imply that \(\mathbf{Z}\) is a commutative ring with unit.</li>
	<li>\(\mathbf{Z} / \{0\}\) is closed under multiplication. In other words, if \(a \neq, b \neq 0\), then \(ab \neq 0\) We can also re-write this by taking its contrapositive so \(ab = 0\) implies that \(a = 0\) or \(b = 0\).</li>
	<li>\(\mathbf{N} = \mathbf{Z}_{&gt;0}\) is closed under \(+\) and \(\cdot\).</li>
	<li>If \(a, b \in \mathbf{Z} / \{0\}\), then \( |ab| \geq \max\{|a|,|b|\} \). (Note that \(a &gt; b\) means that \(a - b \in \mathbf{Z}_{&gt;0}\).)</li>
</ul>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Divisibility</b></h4>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(a, b \in \mathbf{Z}\). \(a\) <b>divides</b> \(b\) (write \(a | b\)) if there exists \(m \in \mathbf{Z}\) such that \(am = b\).
<br />
(We also say that \(a\) is a factor of \(b\) or \(b\) is a multiple of \(a\)).
<br />
In fact, \(a|b\) if and only if \(\frac{b}{a} \in \mathbf{Z} \text{if $a \neq 0$}\).
</div>
<p><br />
The factors or divisors of \(6\) for example are \(\{\pm 1, \pm 2, \pm 3, \pm 6\}\) while the divisors of \(0\) are all of \(\mathbf{Z}\). We also have the set of all multiplies of integer \(a\) so if \(a\) is 7, then the set is \(\{...,-14,-7,0,7,14,21,...\}\). Notation: we will denote the set of all multiples as</p>
<div> 
$$
\begin{align*}
\mathbf{Z}a = \{na \ | \ n \in \mathbf{Z}\}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Divisibility Properties</b></h4>
<p>The following are some well known propositions about divisibility.</p>
<ol>
	<li>If \(a | b\) and \(b | a\), then \(b \in \{\pm a\}\)</li>
	<li>If \(a | 1\), then \(a \in \{\pm 1\}\)</li>
	<li>If \(a | b\) and \(b | c\), then \(a | c\)</li>
	<li>If \(a | b\) and \(a | c\), then \(a | (b + c)\)</li>
	<li>If \(a | b\) and \(a | c\), then \(a | (mb + nc) \quad \forall m, n \in \mathbf{Z}\)</li>
</ol>
<p>Note that</p>
<div> 
$$
\begin{align*}
a | b \Leftrightarrow b \in \mathbf{Z}a \Leftrightarrow \mathbf{Z}b \subseteq \mathbf{Z}c
\end{align*}
$$
</div>
<p><br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Prime Numbers</b></h4>
<div class="mintheaderdiv">
Definition 1.6.3
</div>
<div class="mintbodydiv">
A natural number is prime if (i) \(n &lt; 1\) and (ii) the only positive divisors of \(n\) are \(\{1, n\}\).
</div>
<p><br />
<br />
<!-------------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition 1.6.4
</div>
<div class="peachbodydiv">
Every \(n \in \mathbf{N}\) greater than 1 is equal to a product of primes (at least one).
</div>
<p><br />
<!------------------------------------------------------------------------------->
<b>Proof</b><br />
By Induction on \(n \geq 2\). 
<br />
Base Case: \(n = 2:\) 2 is a prime so it’s a product of one prime.
<br />
<br />
Inductive Case \(n &gt; 2\): 
<br />
Suppose that inductive hypothesis is true for any number \(r\) where \(2 \leq r &lt; n\). Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of primes and we’re done. Otherwise, \(n\) is not a prime and has divisors other than \(1\) or itself. Let these divisors be \(a\) and \(b\) so that \(n = ab\). \(a\) and \(b\) are not \(n\) or \(1\) so we know that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of primes. Therefore, \(n = ab\) is also a product of primes. \(\ \blacksquare\) 
<br />
<br />
<!------------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem 1.6.6 (Euclid)
</div>
<div class="yellowbodydiv">
There are infinitely many prime numbers.
</div>
<p><br />
<b>Proof</b>
<br />
Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1 \in \mathbf{N}\). We know that \(p\) is greater than any of the primes \(p_1,p_2,...,p_k\). Observe now that none of these primes \(p_1, p_2,...,p_k\) can divide \(p\). Why? because if any of these primes did (say it was \(p_i\)), then this means that \(p_i \ | \ n\). But \(p_i\) also divides the product \(p_1p_2...p_k\). By the last fact of integers above, then \(p_i \ | \ (n - p_1p_2...p_k)\). So \(p_i \ | \ 1\). But that’s impossible so \(p_i\) can’t divide \(n\). But by Proposition 1.6.4, \(p\) must be a product of primes. Therefore, there must be another prime or \(p\) itself is a prime. This is a contradiction and so we can conclude that there are infinitely many primes. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Divison with Remainder</b></h4>
<p>The way we learn division is this. Given \(a, d \in \mathbf{Z}, d &gt; 0\), then \(\frac{a}{d} = q + \frac{r}{d}\). The quotient, \(q \in \mathbf{Z}\) and the remainder \(r \in \mathbf{Z}\). Moreover, \(\frac{r}{d} \in [0,1)\). Stated differently,
<br /></p>
<div class="peachheaderdiv">
Proposition 1.6.7
</div>
<div class="peachbodydiv">
Given integers \(a\) and \(d \in \mathbf{Z}\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that 
$$
\begin{align*}
a = qd + r
\end{align*}
$$
where \(0 \leq r &lt; d\). 
</div>
<!------------------------------------------------------------------------>
<p><br />
<br />
<b>Proof: (from my own notes (reading the book))</b>
<br />
<br />
We are given \(a\) and \(d\) such that \(d \geq 1\). <br />
We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\).
<br />
<br />
We have two cases:
<br />
\(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\).<br />
Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that</p>
<div> 
$$
\begin{align*}
(a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\
a &amp;= q'd + d + r \\
a &amp;= q'(d + 1) + r 
\end{align*}
$$
</div>
<p>And we are done.
<br />
<br />
Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done.
<br />
Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So</p>
<div> 
$$
\begin{align*}
-a &amp;= q'd + r' \\
a &amp;= -q'd - r' \\
a &amp;= -q'd - d + d - r' \\
a &amp;= (-q' - 1)d + (d - r') \\
  &amp;= (-q' - 1)d + (d - r').
\end{align*}
$$
</div>
<p>So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done.
<br />
<br />
To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that</p>
<div> 
$$
\begin{align*}
(q - q')d = r - r'
\end{align*}
$$
</div>
<p>But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\)
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Some facts about the integers \((\mathbf{Z}, +)\) is a commutative group. \(+\) is associative and commutative. The identity element is \(0\) and each element \(a\)'s inverse is \(-a\). \((\mathbf{Z}, \cdot)\) is a commutative monoid. \(\cdot\) is associative and commutative. The identity element is \(1\). (No inverses required) Distributive Law: \(a(b+c) = (ab) + (ac), \forall a,b,c \in \mathbf{Z}\). (side note: the list of properties so far for reference imply that \(\mathbf{Z}\) is a commutative ring with unit. \(\mathbf{Z} / \{0\}\) is closed under multiplication. In other words, if \(a \neq, b \neq 0\), then \(ab \neq 0\) We can also re-write this by taking its contrapositive so \(ab = 0\) implies that \(a = 0\) or \(b = 0\). \(\mathbf{N} = \mathbf{Z}_{&gt;0}\) is closed under \(+\) and \(\cdot\). If \(a, b \in \mathbf{Z} / \{0\}\), then \( |ab| \geq \max\{|a|,|b|\} \). (Note that \(a &gt; b\) means that \(a - b \in \mathbf{Z}_{&gt;0}\).) Divisibility Definition Let \(a, b \in \mathbf{Z}\). \(a\) divides \(b\) (write \(a | b\)) if there exists \(m \in \mathbf{Z}\) such that \(am = b\). (We also say that \(a\) is a factor of \(b\) or \(b\) is a multiple of \(a\)). In fact, \(a|b\) if and only if \(\frac{b}{a} \in \mathbf{Z} \text{if $a \neq 0$}\). The factors or divisors of \(6\) for example are \(\{\pm 1, \pm 2, \pm 3, \pm 6\}\) while the divisors of \(0\) are all of \(\mathbf{Z}\). We also have the set of all multiplies of integer \(a\) so if \(a\) is 7, then the set is \(\{...,-14,-7,0,7,14,21,...\}\). Notation: we will denote the set of all multiples as $$ \begin{align*} \mathbf{Z}a = \{na \ | \ n \in \mathbf{Z}\} \end{align*} $$ Divisibility Properties The following are some well known propositions about divisibility. If \(a | b\) and \(b | a\), then \(b \in \{\pm a\}\) If \(a | 1\), then \(a \in \{\pm 1\}\) If \(a | b\) and \(b | c\), then \(a | c\) If \(a | b\) and \(a | c\), then \(a | (b + c)\) If \(a | b\) and \(a | c\), then \(a | (mb + nc) \quad \forall m, n \in \mathbf{Z}\) Note that $$ \begin{align*} a | b \Leftrightarrow b \in \mathbf{Z}a \Leftrightarrow \mathbf{Z}b \subseteq \mathbf{Z}c \end{align*} $$ Prime Numbers Definition 1.6.3 A natural number is prime if (i) \(n &lt; 1\) and (ii) the only positive divisors of \(n\) are \(\{1, n\}\). Proposition 1.6.4 Every \(n \in \mathbf{N}\) greater than 1 is equal to a product of primes (at least one). Proof By Induction on \(n \geq 2\). Base Case: \(n = 2:\) 2 is a prime so it’s a product of one prime. Inductive Case \(n &gt; 2\): Suppose that inductive hypothesis is true for any number \(r\) where \(2 \leq r &lt; n\). Now, take \(n\). We have two cases. If \(n\) is a prime number, then \(n\) is a product of primes and we’re done. Otherwise, \(n\) is not a prime and has divisors other than \(1\) or itself. Let these divisors be \(a\) and \(b\) so that \(n = ab\). \(a\) and \(b\) are not \(n\) or \(1\) so we know that \(1 &lt; a &lt; n\) and \(1 &lt; b &lt; n\). We can now apply the inductive hypothesis to conclude that both \(a\) and \(b\) can be written as a product of primes. Therefore, \(n = ab\) is also a product of primes. \(\ \blacksquare\) Theorem 1.6.6 (Euclid) There are infinitely many prime numbers. Proof Suppose for the sake of contradiction that there are finitely many primes \(p_1,p_2,...,p_k\). Consider \(p = p_1p_2...p_k + 1 \in \mathbf{N}\). We know that \(p\) is greater than any of the primes \(p_1,p_2,...,p_k\). Observe now that none of these primes \(p_1, p_2,...,p_k\) can divide \(p\). Why? because if any of these primes did (say it was \(p_i\)), then this means that \(p_i \ | \ n\). But \(p_i\) also divides the product \(p_1p_2...p_k\). By the last fact of integers above, then \(p_i \ | \ (n - p_1p_2...p_k)\). So \(p_i \ | \ 1\). But that’s impossible so \(p_i\) can’t divide \(n\). But by Proposition 1.6.4, \(p\) must be a product of primes. Therefore, there must be another prime or \(p\) itself is a prime. This is a contradiction and so we can conclude that there are infinitely many primes. \(\ \blacksquare\) Divison with Remainder The way we learn division is this. Given \(a, d \in \mathbf{Z}, d &gt; 0\), then \(\frac{a}{d} = q + \frac{r}{d}\). The quotient, \(q \in \mathbf{Z}\) and the remainder \(r \in \mathbf{Z}\). Moreover, \(\frac{r}{d} \in [0,1)\). Stated differently, Proposition 1.6.7 Given integers \(a\) and \(d \in \mathbf{Z}\) with \(d \geq 1\), there exists unique integers \(q\) and \(r\) such that $$ \begin{align*} a = qd + r \end{align*} $$ where \(0 \leq r &lt; d\). Proof: (from my own notes (reading the book)) We are given \(a\) and \(d\) such that \(d \geq 1\). We want to find unique integers \(q\) and \(r\) such that \(r &lt; d\). We have two cases: \(a \geq 0\): If \(d &gt; a\), then we just take \(q = 0\) and \(r = a\). Otherwise, suppose that \(d \leq a\). By Induction on a. Assume that for all non-negative integers smaller than \(a\), we can find such integers. In particular, suppose it holds for \(a - d\), then there exists integers \(q'\) and \(r\) such that $$ \begin{align*} (a - d) &amp;= q'd + r \quad (\text{where } 0 \leq r &lt; d) \\ a &amp;= q'd + d + r \\ a &amp;= q'(d + 1) + r \end{align*} $$ And we are done. Case \(a &gt; 0\): If \(a\) is divisible by \(d\), then there exists integer \(q\) such that \(a = qd\). So we can set \(r = 0\) and we are done. Otherwise, \(-a &gt; 0\) so by the first case (\(a \geq 0\)) there exists integers \(q'\) and \(r'\) such that \(-a = q'd + r'\) with \(0 &lt; r' &lt; d\). So $$ \begin{align*} -a &amp;= q'd + r' \\ a &amp;= -q'd - r' \\ a &amp;= -q'd - d + d - r' \\ a &amp;= (-q' - 1)d + (d - r') \\ &amp;= (-q' - 1)d + (d - r'). \end{align*} $$ So \(q = (-q' - 1)\) and \(r = d - r'\) with \(0 &lt; d - r' &lt; d\) and we are done. To show that \(q\) and \(r\) are unique, suppose for the sake of contradiction that they are not. Therefore suppose that \(a = qd + r = q'd + r'\) where \(0 \leq r,r' &lt; d\). Subtracting the equations, we see that $$ \begin{align*} (q - q')d = r - r' \end{align*} $$ But this means that \(r - r'\) is divisible by d. However \(|r - r'| \leq \max\{r,r'\} &lt; d\), so we must have \(|r - r'| = 0\). This implies that \((q' - q)d = 0\) and so \(q' = q\) as we wanted to show. \(\ \blacksquare\) References MATH417 by Charles Rezk Algebra: Abstract and Concrete by Frederick M. Goodman]]></summary></entry><entry><title type="html">Lecture 03: Permutation Groups, Cycle Decomposition</title><link href="http://localhost:4000/jekyll/update/2025/01/26/math417-03-permutation-groups-cycle-decomposition.html" rel="alternate" type="text/html" title="Lecture 03: Permutation Groups, Cycle Decomposition" /><published>2025-01-26T00:01:36-08:00</published><updated>2025-01-26T00:01:36-08:00</updated><id>http://localhost:4000/jekyll/update/2025/01/26/math417-03-permutation-groups-cycle-decomposition</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/01/26/math417-03-permutation-groups-cycle-decomposition.html"><![CDATA[<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(X\) be a set. Define the <b>permutation</b> of \(X\) as a bijection from the set to itself (\(f: X \rightarrow X\)). Let \(Sym(X)\) be the set of all bijections from \(X\) to itself (\(X \rightarrow X\)).
</div>
<!------------------------------------------------------------------------>
<p><br />
\(Sym(X)\) equipped with the composition of functions operation is a group. To see this observe that</p>
<ul>
<li>\(id: X \rightarrow X, id(x) = x\) is the identity element.</li>
<li>Composition of functions is associative.</li>
<li>The composition of two permutations is another permutation.</li>
<li>Bijections have inverses and the inverse of a bijection is another bijection.</li>
</ul>
<p>This group is often denoted by the <b>Permutation Group</b> or more often the <b>Symmetric Group</b>.
<br />
<br />
The standard example for the case of finite sets is the standard set \(X = \{1,2,...,n\}\). The symmetric group has a special name called \(S_n\),</p>
<div>
$$
\begin{align*}
S_n = Sym(\{1,2,...,n\})
\end{align*}
$$
</div>
<p>The size of \(S_n\) is \(|S_n| = n!\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Two Line Notation</b></h4>
<p>Let \(\rho, \psi \in S_4\), then we’ll write</p>
<div>
	$$
	\begin{align*}
	 \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix}
	\end{align*}
	$$
</div>
<p>shows that \(\rho(1) = 1\), \(\rho(2) = 4\), \(\rho(3) = 3\) and \(\rho(4) = 2\). Similarly,</p>
<div>
	$$
	\begin{align*}
	 \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix}
	\end{align*}
	$$
</div>
<p>shows that \(\psi(1) = 4\), \(\psi(2) = 2\), \(\psi(3) = 1\) and \(\psi(4) = 3\). Finally,</p>
<div>
	$$
	\begin{align*}
	 \rho\psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 2 &amp; 4 &amp; 1 &amp; 3\end{pmatrix} \in S_4
	\end{align*}
	$$
</div>
<p>so \(\rho\psi(1) = 2\), \(\rho\psi(2) = 4\), \(\rho\psi(3) = 1\) and \(\rho\psi(4) = 3\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Cycle Notation</b></h4>
<p>We can decompose the cycles in each permutation above. For example for the first permutation</p>
<div>
	$$
	\begin{align*}
	 \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix}
	\end{align*}
	$$
</div>
<p>We can decompose this permutation into the following</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec03/1.png" width="20%" class="center" /></p>
<p>In this permutation, we have a cycle of length \(2\) For the second permutation</p>
<div>
	$$
	\begin{align*}
	 \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix}
	\end{align*}
	$$
</div>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/abstract-algebra/lec03/2.png" width="20%" class="center" /></p>
<p>we have a cycle of length \(3\). 
<br />
<br />
To describe a cycle, we can use <b>cycle notation</b> which is defined as follows
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Let \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) where \(a_1,...,a_k \in X\) are distinct. \(\sigma\) is defined by
	$$
	\begin{align*}
	 \sigma(a_i) &amp;= a_{i+1}, \quad i = 1,...,k-1 \\
	 \sigma(a_k) &amp;= a_1 \\
	 \sigma(x) &amp;= x, \quad \text{if} x \notin \{a_1,...,a_k\} \\
	\end{align*}
	$$
</div>
<p><br />
So we can write the first permutation with cycle notation as \((2,4)\) only since by definition for the other elements \(\sigma(x)=x\), while the second permutation can be written as \((1,4,3)\). 
<br />
<br />
What about the following example?</p>
<div>
	$$
	\begin{align*}
	 \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\ 3 &amp; 5 &amp; 4 &amp; 1 &amp; 2\end{pmatrix}
	\end{align*}
	$$
</div>
<p>Note here that we have two disjoint cycles. \((1 \quad 4 \quad 3)\) where \(1\) goes to \(3\) and \(3\) goes to \(4\) and \(4\) goes back to \(1\) and \((2 \quad 5)\) where \(2\) goes to \(5\) and \(5\) goes back to \(2\). This permutation can be written as</p>
<div>
	$$
	\begin{align*}
	 (1 \quad 3 \quad 4)(2 \quad 5)
	\end{align*}
	$$
</div>
<!------------------------------------------------------------------------>
<h4><b>Facts</b></h4>
<p>Some facts about the cycle notation</p>
<ul>
<li>\((1) = (2) = ... (n) = id\). All of these represent the identity permutations.</li>
<li>The order of the elements in a cycle matters up to "cyclic permutation". \((1 \quad 2 \quad 3 \quad 4) = (2 \quad 3 \quad 4 \quad 1) \neq (1 \quad 3 \quad 4 \quad 2)\)</li>
<li>For the inverse of a permutation, we just reverse the order of elements so \((a_1 \quad a_2 \quad ... \quad a_k)^{-1} = (a_k \quad a_{k-1} \quad ... \quad a_1)\)</li>
<li>Two cycles \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) and \(\tau = (b_1 \quad b_{2} \quad ... \quad b_l) \in Sym(X)\) are disjoint if \(\{a_1,a_2,...,a_k\} \cap \{b_1,b_2,...,b_l\} = \emptyset\). Moreover, if the two sets are disjoint, then \(\sigma \circ \tau = \tau \circ \sigma\). This means that the permutation above that we described with \((1 \quad 3 \quad 4)(2 \quad 5)\) can also be written as \((2 \quad 5)(1 \quad 3 \quad 4)\)</li>
</ul>
<p><br />
<!------------------------------------------------------------------------></p>
<div class="yellowheaderdiv">
Theorem
</div>
<div class="yellowbodydiv">
Every non-id element in \(Sym(X)\) where \(X\) is finite can be written as a product of pairwise disjoint cycles (of length \(\geq 2\) uniquely up to re-ordering.
</div>
<p><br />
Suppose we have \(\sigma = (1 \quad 3)(2 \quad 7 \quad 8)(4 \quad 5 \quad 6)(9) \in S_9\). Any of the cycles in this notation are pairwise disjoint. We have 3 disjoint cycles where \(9\) is fixed.
<br />
<br />
The proof for this theorem is in the class notes.
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Cycle Type</b></h4>
<p>We can classify permutations of a finite set into groups corresponding to the number of cycles of various lengths in their cycle decomposition.
<br />
<br />
For example for \(S_2\), we have two elements and so we have two permutations</p>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(1 + 1\)</td>
    <td>\(id = (1)(2)\)</td>
  </tr>
  <tr>
    <td>\(2\)</td>
    <td>\((1 \quad 2)\)</td>
  </tr>
</table>
<p>For \(S_3\),</p>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(1 + 1 + 1\)</td>
    <td>\(id = (1)(2)(3)\)</td>
  </tr>
  <tr>
    <td>\(2 + 1\)</td>
    <td>\((1 \quad 2),(1 \quad 3),(2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(3\)</td>
    <td>\((1 \quad 2 \quad 3),(1 \quad 3 \quad 2)\)</td>
  </tr>
</table>
<p>For \(S_4\),</p>
<table style="max-width: 500px; margin: 20px auto;">
  <tr>
    <td>\(1 + 1 + 1 + 1\)</td>
    <td>\(id = (1)(2)(3)(4)\)</td>
  </tr>
  <tr>
    <td>\(2 + 1 + 1\)</td>
    <td>\((1 \quad 2),(1 \quad 3),(1 \quad 4),(2 \quad 3),(2 \quad 4),(3 \quad 4)\)</td>
  </tr>
  <tr>
    <td>\(2 + 2\)</td>
    <td>\((1 \quad 2)(3 \quad 4),(1 \quad 3)(2 \quad 4),(1 \quad 4)(2 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(3 + 1\)</td>
    <td>\((1 \quad 2 \quad 3),(1 \quad 2 \quad 4)\),\((1 \quad 3 \quad 4),(2 \quad 3 \quad 4),(1 \quad 3 \quad 2)\),\((1 \quad 4 \quad 2),(1 \quad 4 \quad 3),(2 \quad 4 \quad 3)\)</td>
  </tr>
  <tr>
    <td>\(4\)</td>
    <td>\((1 \quad 2 \quad 3 \quad 4),(1 \quad 2 \quad 4 \quad 3)\),\((1 \quad 3 \quad 2 \quad 4),(1 \quad 3 \quad 4 \quad 2)\),\((1 \quad 4 \quad 2 \quad 3),(1 \quad 4 \quad 3 \quad 2)\)</td>
  </tr>
</table>
<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>The Order of an Element</b></h4>
<p>(Start of lecture 4). The first concept that we will talk about is the order of an element in a group.
<br /></p>
<div class="mintheaderdiv">
Definition
</div>
<div class="mintbodydiv">
Suppose we have a group \((G, \cdot)\) and let \(a \in G\). The order of \(a\) is the smallest positive integer \(n\) such that \(a^n = e\) (or infinite). \(\text{order}(a) \in \mathbf{N} \cup \{\infty\}\).
</div>
<!------------------------------------------------------------------------>
<p><br />
For example let \(\sigma = (1 \quad 2 \quad 3 \quad 4) \in S_5\) The order of \(\sigma\), \(\text{order}(\sigma)\) is \(4\). This is because it will take \(\sigma^4\) will finally get us back to the identity permutation. In fact that the order of a \(k-\)cycle is \(k\).
<br />
<br />
What about \(\tau =  (1 \quad 2)(3 \quad 4 \quad 5)\)? \(\text{order}(\tau) = 6\) because we have to iterate the operation \(6\) times to get to the identity element. Specifically, we have two disjoint cycles and since they are disjoint, then they operate independently. So we need to find \(n\) such that both of the cycles will return to the identity permutation</p>
<div>
$$
\begin{align*}
(1 \quad 2)^n &amp;= e \\
(3 \quad 4 \quad 5)^n &amp;= e
\end{align*}
$$
</div>
<p>This \(n\) must be then the least common multiple of \(2\) and \(3\) which is \(6\).
<br />
<br />
What about \(\nu =  (1 \quad 2)(2 \quad 3 \quad 4)\)? Note here that the two cycles are not disjoint. It’s not clear here what it would be so drawing this permutation might make this very obvious.</p>

<p>[TODO:PIC]</p>

<p>From this we see that \(\text{order}(\nu) = 4\).
<br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>Parity of a Permutation</b></h4>
<p>The second concept that we want to talk about is the parity of a permutation but first we’ll start with the following proposition.
<br /></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
Every permutation of a finite set is equal to some product of transpositions (a transposition is  2-cycle) not necessarily disjoint.
</div>
<p><br />
<!------------------------------------------------------------------------>
For example we can write \((1 \quad 2 \quad 3 \quad 4)\) as \((1 \quad 2)(2 \quad 3)(3 \quad 4)\). 
<br />
<br />
Next, we have the following proposition
<br />
<!------------------------------------------------------------------------></p>
<div class="peachheaderdiv">
Proposition
</div>
<div class="peachbodydiv">
If \(\sigma\) is a permutation of a finite set and if \(\sigma = \tau_1\tau_2...\tau_k = \nu_1\nu_2...\nu_l\) where each of \(\tau_i\) and \(\nu_j\) are transpositions, then either \(k,l\) are both even, or are both odd. i.e. \((-1)^k = (-1)^l\)
</div>
<!------------------------------------------------------------------------>
<p><br />
Because of this we can classify permutations as even or odd. We also get the fact that composing two even permutations is another even permutations and composing an even permutation with an odd permutation is an odd permutation.
<br />
<br />
<!------------------------------------------------------------------------>
<b>Proof</b> (there is an alternative proof in the notes)
<br />
Define a function \(sgn: S_n \rightarrow \{\pm 1\}\) such that</p>
<ol>
	<li>\(sgn(id) = +1\)</li>
	<li>\(sgn(\sigma \circ \tau) = sgn(\sigma)sgn(\tau)\)</li>
	<li>If \(\tau\) is a transposition, then \(sgn(\tau) = -1\)</li>
</ol>
<p>So now given a permutation \(\sigma \in S_n\), there is a corresponding permutation matrix \(A_{\sigma} \in GL_{n}(\mathbf{R})\). \(A_{\sigma}\) has the standard basis vectors but permuted according to the permutation \(\sigma\). For example</p>
<div>
$$
\begin{align*}
A_{(1 \quad 2 \quad 3)}
&amp;= 
\begin{pmatrix}
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{pmatrix}
\end{align*}
$$
</div>
<p>So</p>
<div>
$$
\begin{align*}
A_{\sigma}
&amp;= 
\begin{pmatrix}
e_{\sigma(1)} &amp; e_{\sigma(2)} &amp; ... &amp; e_{\sigma(n)}
\end{pmatrix}
\end{align*}
$$
</div>
<p>where \(e_{\sigma(k)}\) is the standard column vector \(e_k\) permuted according to \(\sigma(k)\). 
<br />
<br />
The permutation matrix \(A_{\sigma}\) is useful in that left multiplication by \(A_{\sigma}\) permutes the subset \(\{e_1,...,e_n\} \in \mathbf{R}^n\) according to \(\sigma\). So</p>
<div>
$$
\begin{align*}
A_{\sigma}e_k = e_{\sigma(k)}
\end{align*}
$$
</div>
<p>This actually leads to a formula</p>
<div>
$$
\begin{align*}
A_{\sigma}A_{\tau} = A_{\sigma \circ \tau}
\end{align*}
$$
</div>
<p>This is because</p>
<div>
$$
\begin{align*}
A_{\sigma}A_{\tau}e_k = A_{\sigma}e_{\tau(k)} = e_{\sigma(\tau(k))} \leftrightarrow A_{\sigma \circ \tau}e_k = e_{(\sigma \circ \tau)(k)}
\end{align*}
$$
</div>
<p>So now define: \(sgn(\sigma) = \det(A_{\sigma})\) where \(sgn: S_n \rightarrow \{\pm 1\}\). This function satisfies the three properties we defined for the \(sgn\) function above.</p>
<ol>
	<li>\(sgn(id) = +1\). This is true because the permutation matrix for the identity permutation is the identity matrix.</li>
	<li>\(sgn(\sigma \circ \tau) = sgn(\sigma)sgn(\tau)\). This follows from the product property of the determinant. \(\det(AB) = \det(A)\det(B)\).</li>
	<li>If \(\tau\) is a transposition, then \(sgn(\tau) = -1\). This also follows from the fact that interchanging any two columns from a matrix results in switching the sign of the determinant.</li>
</ol>
<p><br />
<br /></p>
<hr />

<p><br />
<!------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
<li>MATH417 by Charles Rezk</li>
<li><a href="https://homepage.divms.uiowa.edu/~goodman/algebrabook.dir/algebrabook.html">Algebra: Abstract and Concrete by Frederick M. Goodman</a></li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Definition Let \(X\) be a set. Define the permutation of \(X\) as a bijection from the set to itself (\(f: X \rightarrow X\)). Let \(Sym(X)\) be the set of all bijections from \(X\) to itself (\(X \rightarrow X\)). \(Sym(X)\) equipped with the composition of functions operation is a group. To see this observe that \(id: X \rightarrow X, id(x) = x\) is the identity element. Composition of functions is associative. The composition of two permutations is another permutation. Bijections have inverses and the inverse of a bijection is another bijection. This group is often denoted by the Permutation Group or more often the Symmetric Group. The standard example for the case of finite sets is the standard set \(X = \{1,2,...,n\}\). The symmetric group has a special name called \(S_n\), $$ \begin{align*} S_n = Sym(\{1,2,...,n\}) \end{align*} $$ The size of \(S_n\) is \(|S_n| = n!\). Two Line Notation Let \(\rho, \psi \in S_4\), then we’ll write $$ \begin{align*} \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix} \end{align*} $$ shows that \(\rho(1) = 1\), \(\rho(2) = 4\), \(\rho(3) = 3\) and \(\rho(4) = 2\). Similarly, $$ \begin{align*} \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix} \end{align*} $$ shows that \(\psi(1) = 4\), \(\psi(2) = 2\), \(\psi(3) = 1\) and \(\psi(4) = 3\). Finally, $$ \begin{align*} \rho\psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 2 &amp; 4 &amp; 1 &amp; 3\end{pmatrix} \in S_4 \end{align*} $$ so \(\rho\psi(1) = 2\), \(\rho\psi(2) = 4\), \(\rho\psi(3) = 1\) and \(\rho\psi(4) = 3\). Cycle Notation We can decompose the cycles in each permutation above. For example for the first permutation $$ \begin{align*} \rho = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 1 &amp; 4 &amp; 3 &amp; 2\end{pmatrix} \end{align*} $$ We can decompose this permutation into the following In this permutation, we have a cycle of length \(2\) For the second permutation $$ \begin{align*} \psi = \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 3\end{pmatrix} \end{align*} $$ we have a cycle of length \(3\). To describe a cycle, we can use cycle notation which is defined as follows Definition Let \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) where \(a_1,...,a_k \in X\) are distinct. \(\sigma\) is defined by $$ \begin{align*} \sigma(a_i) &amp;= a_{i+1}, \quad i = 1,...,k-1 \\ \sigma(a_k) &amp;= a_1 \\ \sigma(x) &amp;= x, \quad \text{if} x \notin \{a_1,...,a_k\} \\ \end{align*} $$ So we can write the first permutation with cycle notation as \((2,4)\) only since by definition for the other elements \(\sigma(x)=x\), while the second permutation can be written as \((1,4,3)\). What about the following example? $$ \begin{align*} \begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\ 3 &amp; 5 &amp; 4 &amp; 1 &amp; 2\end{pmatrix} \end{align*} $$ Note here that we have two disjoint cycles. \((1 \quad 4 \quad 3)\) where \(1\) goes to \(3\) and \(3\) goes to \(4\) and \(4\) goes back to \(1\) and \((2 \quad 5)\) where \(2\) goes to \(5\) and \(5\) goes back to \(2\). This permutation can be written as $$ \begin{align*} (1 \quad 3 \quad 4)(2 \quad 5) \end{align*} $$ Facts Some facts about the cycle notation \((1) = (2) = ... (n) = id\). All of these represent the identity permutations. The order of the elements in a cycle matters up to "cyclic permutation". \((1 \quad 2 \quad 3 \quad 4) = (2 \quad 3 \quad 4 \quad 1) \neq (1 \quad 3 \quad 4 \quad 2)\) For the inverse of a permutation, we just reverse the order of elements so \((a_1 \quad a_2 \quad ... \quad a_k)^{-1} = (a_k \quad a_{k-1} \quad ... \quad a_1)\) Two cycles \(\sigma = (a_1 \quad a_2 \quad ... \quad a_k)\) and \(\tau = (b_1 \quad b_{2} \quad ... \quad b_l) \in Sym(X)\) are disjoint if \(\{a_1,a_2,...,a_k\} \cap \{b_1,b_2,...,b_l\} = \emptyset\). Moreover, if the two sets are disjoint, then \(\sigma \circ \tau = \tau \circ \sigma\). This means that the permutation above that we described with \((1 \quad 3 \quad 4)(2 \quad 5)\) can also be written as \((2 \quad 5)(1 \quad 3 \quad 4)\) Theorem Every non-id element in \(Sym(X)\) where \(X\) is finite can be written as a product of pairwise disjoint cycles (of length \(\geq 2\) uniquely up to re-ordering. Suppose we have \(\sigma = (1 \quad 3)(2 \quad 7 \quad 8)(4 \quad 5 \quad 6)(9) \in S_9\). Any of the cycles in this notation are pairwise disjoint. We have 3 disjoint cycles where \(9\) is fixed. The proof for this theorem is in the class notes. Cycle Type We can classify permutations of a finite set into groups corresponding to the number of cycles of various lengths in their cycle decomposition. For example for \(S_2\), we have two elements and so we have two permutations \(1 + 1\) \(id = (1)(2)\) \(2\) \((1 \quad 2)\) For \(S_3\), \(1 + 1 + 1\) \(id = (1)(2)(3)\) \(2 + 1\) \((1 \quad 2),(1 \quad 3),(2 \quad 3)\) \(3\) \((1 \quad 2 \quad 3),(1 \quad 3 \quad 2)\) For \(S_4\), \(1 + 1 + 1 + 1\) \(id = (1)(2)(3)(4)\) \(2 + 1 + 1\) \((1 \quad 2),(1 \quad 3),(1 \quad 4),(2 \quad 3),(2 \quad 4),(3 \quad 4)\) \(2 + 2\) \((1 \quad 2)(3 \quad 4),(1 \quad 3)(2 \quad 4),(1 \quad 4)(2 \quad 3)\) \(3 + 1\) \((1 \quad 2 \quad 3),(1 \quad 2 \quad 4)\),\((1 \quad 3 \quad 4),(2 \quad 3 \quad 4),(1 \quad 3 \quad 2)\),\((1 \quad 4 \quad 2),(1 \quad 4 \quad 3),(2 \quad 4 \quad 3)\) \(4\) \((1 \quad 2 \quad 3 \quad 4),(1 \quad 2 \quad 4 \quad 3)\),\((1 \quad 3 \quad 2 \quad 4),(1 \quad 3 \quad 4 \quad 2)\),\((1 \quad 4 \quad 2 \quad 3),(1 \quad 4 \quad 3 \quad 2)\) The Order of an Element (Start of lecture 4). The first concept that we will talk about is the order of an element in a group. Definition Suppose we have a group \((G, \cdot)\) and let \(a \in G\). The order of \(a\) is the smallest positive integer \(n\) such that \(a^n = e\) (or infinite). \(\text{order}(a) \in \mathbf{N} \cup \{\infty\}\). For example let \(\sigma = (1 \quad 2 \quad 3 \quad 4) \in S_5\) The order of \(\sigma\), \(\text{order}(\sigma)\) is \(4\). This is because it will take \(\sigma^4\) will finally get us back to the identity permutation. In fact that the order of a \(k-\)cycle is \(k\). What about \(\tau = (1 \quad 2)(3 \quad 4 \quad 5)\)? \(\text{order}(\tau) = 6\) because we have to iterate the operation \(6\) times to get to the identity element. Specifically, we have two disjoint cycles and since they are disjoint, then they operate independently. So we need to find \(n\) such that both of the cycles will return to the identity permutation $$ \begin{align*} (1 \quad 2)^n &amp;= e \\ (3 \quad 4 \quad 5)^n &amp;= e \end{align*} $$ This \(n\) must be then the least common multiple of \(2\) and \(3\) which is \(6\). What about \(\nu = (1 \quad 2)(2 \quad 3 \quad 4)\)? Note here that the two cycles are not disjoint. It’s not clear here what it would be so drawing this permutation might make this very obvious.]]></summary></entry></feed>