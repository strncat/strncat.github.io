<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-10-03T21:32:39-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">strncat’s notebook</title><subtitle>personal study notes</subtitle><entry><title type="html">The Virtual World</title><link href="http://localhost:4000/jekyll/update/2023/10/03/virtual-world.html" rel="alternate" type="text/html" title="The Virtual World" /><published>2023-10-03T01:01:36-07:00</published><updated>2023-10-03T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/10/03/virtual-world</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/10/03/virtual-world.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>The Virtual World</b></h4>
<p>Our goal is to mimic human vision in a virtual world. We do this by:</p>
<ul>
  <li>Creating a virtual camera and place it somewhere and point it at something.</li>
  <li>Putting a film that contains pixels with RGB values between 0 and 255 into the camera.</li>
  <li>Placing some objects in the world. (geometric modeling, transformations, text mapping)</li>
  <li>Putting lights in the scene.</li>
  <li>And Finally taking the picture. The light will bounce around the room hitting the objects and back hitting the camera onto the virtual film.
<br />
<br />
<!------------------------------------------------------------------------------------></li>
</ul>
<h4><b>Pupil</b></h4>
<p>What is the role of the pupil in the eye? So we know that the lights goes off every point of an object in every direction and this is why we all see the same point on the same object. We looked at this in the previous lecture. What we didn’t discuss is that without the pupil, this light that is bouncing from every single point on the object, is actually hitting every single cone (or pixel in the camera). The image that we’ll see is just going to be a blurred image averaging all these points. We won’t see any details.
<br />
<br />
The reason why we don’t see everything as a blurry image is the pupil (or the aperture in the camera). The pupil’s rol is to restrict the light bouncing from all these points on every single object. The pupil will only allow some point, enough to see clearly. Moreover, when the pupil gets small, we’d get a sharper image and as the pupil gets larger, we’d see a blurred image. But the pupil (and the aperture) can’t be too small because otherwise the light will bend as it gets through the tiny point due to the particle/wave length duality. Therefore, it has to be small enough but not too small.
<br />
<br />
THe Camera mimic the human eyes but instead of cones we have mechanical pixels and instead of a pupil, we have an aperture. Cameras have also comlex lens system. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Pinhole Camera</b></h4>
<p>To model the virtual world camera we use a pinhole camera. Since this is a virtual world, then we won’t have the light bending issue (particle/wave duality) and we can just make the aperture a single point. So now it’s really simple. Light will go through this one point from an object to a pixel. However these images can’t do stuff like depth of field (not blurry as you go farther) but that’s fine.
<br />
<br />
Additionally:</p>
<ul>
  <li>object occlude things behind the</li>
  <li>more distant objects subtend smaller visual angels and appear smaller</li>
  <li>The image will be an upside version of the real world object
<br />
<br />
To solve this third issue and make the camera more efficient:</li>
  <li>We’ll move the film out in front of the pinhole so that the image is not upside down.</li>
  <li>We’ll only render objects further away from the camera than the film plane (of width w and height h).</li>
  <li>We’ll though add a clipping plane for effiency so we don’t every single object (knowing that we won’t render it)</li>
  <li>The volume between the film (front clipping plane) and the back clipping plane is called the <b>viewing frustum</b>.
<br />
<br />
<!------------------------------------------------------------------------------------></li>
</ul>
<h4><b>Objects in the Virtual World</b></h4>
<ul>
  <li>We’ll model the objects with 3D points. Each object will be a collection of points.</li>
  <li>Objects are created in a reference space that we call the “object space”.</li>
  <li>Once objects are created, we’ll place the objects into the scene. This space is called the “world space”. It’s important to know that we don’t have geometry in the actual space. Instead, we have code that tell us where to place the objects around us via some rigid body motion such as Rotations (3 ways to rotate) and Translate (3 ways to translate) so a total of 6 degrees of freedom. We can also scale the object. But why do it this way? why not just integrate an object such as a lamp in the virtual world? Because it’s a waste! if the lamp consisted of 100k vertices and the scene needed 100 of these, then we’ll have 10 million vertices just for lamps! Instead we can just create one lamp and then the transformations that we described to place this lamp 100 times around the scene.</li>
  <li>When we take a virtual picture, points on the object are projected on the 2d film plane which we refer to as a “scene space”. This projection is nonlinear and the source of undesireable distortion.
<!------------------------------------------------------------------------------------></li>
</ul>
<h4><b>References</b></h4>
<p>These are my notes based on attending the lecture on light from CS148 with professor Ron Fedkiw. I’ve modified the notes and also included other notes. They are not official or anything like that. Additionally I’ve used the following sources:</p>
<ul>
<li>
<a href="https://www.amazon.com/Fundamentals-Computer-Graphics-Steve-Marschner/dp/1482229390">Fundamentals of Computer Graphics</a>
</li>
</ul>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The Virtual World Our goal is to mimic human vision in a virtual world. We do this by: Creating a virtual camera and place it somewhere and point it at something. Putting a film that contains pixels with RGB values between 0 and 255 into the camera. Placing some objects in the world. (geometric modeling, transformations, text mapping) Putting lights in the scene. And Finally taking the picture. The light will bounce around the room hitting the objects and back hitting the camera onto the virtual film. Pupil What is the role of the pupil in the eye? So we know that the lights goes off every point of an object in every direction and this is why we all see the same point on the same object. We looked at this in the previous lecture. What we didn’t discuss is that without the pupil, this light that is bouncing from every single point on the object, is actually hitting every single cone (or pixel in the camera). The image that we’ll see is just going to be a blurred image averaging all these points. We won’t see any details. The reason why we don’t see everything as a blurry image is the pupil (or the aperture in the camera). The pupil’s rol is to restrict the light bouncing from all these points on every single object. The pupil will only allow some point, enough to see clearly. Moreover, when the pupil gets small, we’d get a sharper image and as the pupil gets larger, we’d see a blurred image. But the pupil (and the aperture) can’t be too small because otherwise the light will bend as it gets through the tiny point due to the particle/wave length duality. Therefore, it has to be small enough but not too small. THe Camera mimic the human eyes but instead of cones we have mechanical pixels and instead of a pupil, we have an aperture. Cameras have also comlex lens system. Pinhole Camera To model the virtual world camera we use a pinhole camera. Since this is a virtual world, then we won’t have the light bending issue (particle/wave duality) and we can just make the aperture a single point. So now it’s really simple. Light will go through this one point from an object to a pixel. However these images can’t do stuff like depth of field (not blurry as you go farther) but that’s fine. Additionally: object occlude things behind the more distant objects subtend smaller visual angels and appear smaller The image will be an upside version of the real world object To solve this third issue and make the camera more efficient: We’ll move the film out in front of the pinhole so that the image is not upside down. We’ll only render objects further away from the camera than the film plane (of width w and height h). We’ll though add a clipping plane for effiency so we don’t every single object (knowing that we won’t render it) The volume between the film (front clipping plane) and the back clipping plane is called the viewing frustum. Objects in the Virtual World We’ll model the objects with 3D points. Each object will be a collection of points. Objects are created in a reference space that we call the “object space”. Once objects are created, we’ll place the objects into the scene. This space is called the “world space”. It’s important to know that we don’t have geometry in the actual space. Instead, we have code that tell us where to place the objects around us via some rigid body motion such as Rotations (3 ways to rotate) and Translate (3 ways to translate) so a total of 6 degrees of freedom. We can also scale the object. But why do it this way? why not just integrate an object such as a lamp in the virtual world? Because it’s a waste! if the lamp consisted of 100k vertices and the scene needed 100 of these, then we’ll have 10 million vertices just for lamps! Instead we can just create one lamp and then the transformations that we described to place this lamp 100 times around the scene. When we take a virtual picture, points on the object are projected on the 2d film plane which we refer to as a “scene space”. This projection is nonlinear and the source of undesireable distortion. References These are my notes based on attending the lecture on light from CS148 with professor Ron Fedkiw. I’ve modified the notes and also included other notes. They are not official or anything like that. Additionally I’ve used the following sources: Fundamentals of Computer Graphics]]></summary></entry><entry><title type="html">Cross Product</title><link href="http://localhost:4000/jekyll/update/2023/10/02/cross-product.html" rel="alternate" type="text/html" title="Cross Product" /><published>2023-10-02T01:01:36-07:00</published><updated>2023-10-02T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/10/02/cross-product</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/10/02/cross-product.html"><![CDATA[<p>This post is just me taking notes while watching the 10th and 11th lecture of the <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a> which is the best reference on Linear Algebra ever. I might add other notes from other places as I go.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Introduction</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/corssproduct/00.png" width="70%" class="center" /></p>

<p>The cross product of two vectors $v$ and $w$ is the area of the parallelogram that they span out. This cross product can be negative however and again we depend here on the orientation. If v was on the right of w then the cross product is positive. On the other hand, if $v$ was on the left, then the product is negative. To easily remember this, think of $\widehat{i}$ and $\widehat{j}$. if you take the cross product in order, $\widehat{i} \times \widehat{j}$ (meaning that $\widehat{i}$ is on the right of \widehat{j}$), then the product is positive. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing the Cross Product</b></h4>
<p>How do we compute the cross product without calculating the area? This is where the determinant comes in. 
Remember from the determinant lesson, we wanted to measure how much a transformation will squish or stretch some area and for that we learned that we can compute the determinant of the transformation to find out this stretching/scaling factor. So now …. (drum rolls) … take these vectors $v$ and $w$ and arrange them as columns in a matrix. This matrix now is a transformation that represents where the basis vectors $widehat{i}$ and $\widehat{j}$ will land. But remember that if we started with $\widehat{i}$ and $\widehat{j}$ themselves, then that initial area of the unit square is only 1. So now after applying the transformation, we will get the scaling factor by which this area grew or shrank and since the initial area is 1, the scaling factor is just the area.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/corssproduct/00.png" width="70%" class="center" /></p>

<p>If the order of $v$ and $w$ flipped from their initial order in the matrix, then we know that the area will be negative.</p>

<h4><b>More</b></h4>
<p>Some things to notice are if the vectors are perpendicular to each other than the area is larger. Another thing is that if scale one of the vectors then the cross product is also scaled by that amount. So 3v x w = 3 (v x w).</p>

<!------------------------------------------------------------------------------------>
<h4><b>Example</b></h4>
<p>Suppose we have the vectors $v=(-3,1)$ and $w=(2,1)$.</p>

<div>
$$
\begin{align*}
\begin{bmatrix}
-3 &amp; 2 \\
1 &amp; 1
\end{bmatrix}
=
-3*1 - 2*1 = -5.
\end{align*}
$$
</div>
<p>Notice that we had $v$ first in the matrix but in the above figure we see that $v$ is now on the left of $w$ and so the area is negative as we expect.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Three Dimensions</b></h4>
<p>But …. this isn’t really a cross product. A cross product produces a new vector. In three dimensions, we will take two vectors and their cross product will be a new vector. Take two vectors and consider the parallelogram that they span. Suppose now that this area is 2.5. The new vector that we get from the cross product is a vector whose direction is prependicular to that parallelogram and its length is equal to the area of the parallelogram which is 2.5. Which perpendicular direction? We use the right hand rule. Put the forefinger of your right hand in the direction of $v$ and stick out the middle finger in the direction of $w$. Then where the thumb is pointing is the direction of the cross product. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing the Cross Product in Three Dimensions</b></h4>
<p>We do this by putting $v$ and $w$ as the second and third columns in the matrix</p>
<div>
$$
\begin{align*}
det ()
\begin{bmatrix}
\widehat{i} &amp; v_1 &amp; w_1 \\
\widehat{j} &amp; v_2 &amp; w_2 \\
\widehat{k} &amp; v_3 &amp; w_3
\end{bmatrix}
)
=
\widehat{i} (v_2w_3 - w_2v_3) - \widehat{j} (v_1w_3 - w_1v_3) + \widehat{k} (v_1w_2 - w_1v_2)
\end{align*}
$$
</div>
<p>Students are often told that the new vector is the linear combination of these basis vectors above whose length is the area of the parallelogram and whose direction is followed from applying the right hand rule. But this isn’t notational and putting the basis vectors there isn’t random. To understand where this comes from.</p>

<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing the Cross Product in Three Dimensions</b></h4>

<p>So what’s up with adding $\widehat{i}$, $\widehat{j}$ and $\widehat{k}$ as a column in the matrix? Why did we do that?</p>

<!------------------------------------------------------------------------------------>
<h4><b>References</b></h4>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is just me taking notes while watching the 10th and 11th lecture of the Essence of Linear Algebra by 3Blue1Brown which is the best reference on Linear Algebra ever. I might add other notes from other places as I go. Introduction]]></summary></entry><entry><title type="html">Working with Light</title><link href="http://localhost:4000/jekyll/update/2023/10/01/light.html" rel="alternate" type="text/html" title="Working with Light" /><published>2023-10-01T01:01:36-07:00</published><updated>2023-10-01T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/10/01/light</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/10/01/light.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>Introduction</b></h4>
<p>What is Light? Light consists of photos that travel along straight paths until they hit a surface. Photons carry some energy represented by its wavelength. A large amount of photons of different energies will together represent a specturm. The sun emits electromagnetic radiation across a wide specturm of wavelengths that includes both visible and not-visible light to the human eye. This includes radio waves, microwaves, visible light, ultraviolet etc. These forms of radiation differ in their wavelengths and frequencies. When lights hits an object, this light is either</p>

<ul>
  <li>Absorbed: This means that the object just absorbed and transformed these photons into some other energy (maybe heat for example). We don’t care about these photos because they’re not visible to us.</li>
  <li>Reflected: The light that didn’t get absorbed can be reflected. This light then will hit our eyes and we then will perceive color. For example, If the car is red, this means a lot of the red photons are bouncing off and all the other photons are being absorbed.</li>
</ul>

<p>Light can also pass through the material (maybe the object is made of glass) and can get bent, scattered and so on. When the light hits the eyes, the brain will create a signal and send it to the brain. The brain then will create an image based on these signals. So color doesn’t really exist. It’s just the brain’s interpretation of the signal generated after the photos reached the back of our eyes. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Physics of Light</b></h4>
<p>The visible spectrum of the full electromagnetic spectrum of light is really tiny (refer to the slides), but that’s all our eyes care about. Either the wavelengths or the frequencies are usually used to differenciate them but in Graphics, we mostly care about wavelengths.
<br />
<br />
Light carries multiple wavelengths. How did we figure out the wavelight that makes us see red? From experiments, we’ve combined different colored lights and observed the results. This led to discovering the <b>relative power distributions</b> (TODO: add figure).
<br />
<br />
What happens if we shine a red colored light and another green colored light? We will see that the energy adds up. In fact, we’ll have the figure on the right where the peak is now is at 600 instead (yellow light). These two green and red distributions are added together ($E = E_1(\lambda) + E_2(\lambda)$). What if we shine green, red and blue light? We get white light!
<br />
<br />
What happens if we add red, blue and green paint together? do we get white paint? no! we see black/brown paint. This is because we’re now subtracting light. Why is that? How come 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Additive vs. Subtractive Color Spaces</b></h4>
<p>So why is paint subtractive? why didn’t we add the energies up like when we shined multiple light sources together? With paint, the pigments in the paint absorb certain wavelengths of light. These absorbed wavelengths are then subtracted from the incoming light.
<br />
<br />
Where do we find additive color spaces? Displays are one major example. The monitor uses three color lights that get shined into our eyes. It is additive. Suppose that we’ve shine a light source on multiple surfaces:</p>
<ul>
	<li>If the surface is red, then it's going to absorb all the light except for red which will bounce.</li>
	<li>If the surface is white, then white (all three) is going to bounce</li>
	<li>If the surface is black, then all light will be absorbed and nothing is bounced. </li>
</ul>
<p>In general, we absorb the stuff that we don’t reflect. So the color something is what’s being reflected and everything else is being absorbed. We can actually plot this reflected light energy.</p>
<div>
$$
\begin{align*}
$Reflected(\lambda)=E(\lambda)r(\lambda) = E(\lambda)(1-a(\lambda))$.
\end{align*}
$$
</div>
<p>where $r(\lambda)+a(\lambda)=1$ (light is either reflected or absorbed!). This means is that you can’t know what color an object is. By creating a color for the object, you have to know what kind of light you’re shining on it.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Sensor Absorbtion (How does the eye take in the light?)</b></h4>

<p>The sensor (whether a sensor or the eye) absorbs the light cominng in (per unit time) and creates a signal (per unit time). This amount of current that we generate is all we know and nothing else. The cones in our eyes work the same way. To compute the signal power (energy per power), we need to take the integral over $a(\lambda)$ which is the absorbtion.</p>
<div>
$$
\begin{align*}
A = \int E(\lambda)a(\lambda)d\lambda
\end{align*}
$$
</div>
<p>Again, this light comes in to our eyes. We know all these wavelenghts and their frequencies. We then want to sum all the wavelengths of the energy coming in. Specifically we want to sum only the portion of it that is getting absorbed by our eyes. $A$ is the total enery that the sensor picked up from the light which is going to be used to generate the current. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Human Eye</b></h4>
<p>The human eye has 3 types of cones sensors and 1 type of rod sensors. Protiens in the cone/rods absorb the incoming photons changing the cell’s membrane potenial, thus creating a signal that gets interpreted by the brain. The cones are more optimized for seeing during the day while the rods’ signals are over-saturated. During the night, the rods are more optimized for seeing and the cones’ signals are under-saturated. For the rods for example, seeing during the day is like as if we stared directly into the sun! they’re not optimized for sunlight!</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/graphics/light/cones.png" width="70%" class="center" /></p>
<p>Each cone and rod cell has a specific absorbtion (response) function. This absorbtion function gives is what tells us how good a certain cell at absorbing certain wavelengths. As we’ve expected, one of the cones is really good at the “red” wavelength, one is good at “blue” and the last is good at “green”. The rod cell is really good at green. This is one reason why humans areally good at seeing green.
<br />
<br />
Let’s look at an example. Suppose we shine a light that has a single emission of a specific wavelength that is equal to 520. We then shine this light at a human eye. What will happen? The red cone will absorb very little of the light (let it be A). The green blue cone, a little more. Let that amount be B. Finally, the green cone absorbs of it (let it be C). So, since C is the biggest number, then the brain will see green! but wait, light coming into our eyes is not coming as a single wavelength. It comes with different wavelengths and frenquiences. So how does the eye interpret all of us? We’ll integrate over the absorbtion function for each cell and get the three numbers $A$, $B$ and $C$.</p>
<div>
$$
\begin{align*}
A = \int E(\lambda)a(\lambda)d\lambda
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Trichromatic Theory</b></h4>
<p>The previous example implies that we really only need 3 “colors” to re-create/trick the brain into seeing the same color again and this is exactly what scientists did. The idea is that if you have three lasers (single emission of a specific wavelength eg R=700, G=546 and B=436). Then a human is given an object with a “color”. We’ll then use the laser combinations to match that same exact “color” and the human mistakenly believes it’s the same color! This works again because each of the three cones can only send one signal (so 3 dimensional basis). This is why we only use red, green and blue lights in our computers, printers etc. So for example, based on the trichromatic theory, each pixel in an image will only need three numbers!<br />
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>3D Color Spaces</b></h4>
<p>Since we only need three colors, we can just work in three dimensions and we don’t need the enitre specturm. So we have this huge amount of light hitting the camera but once it hits the camera, the camera is going to have three cones and will knock this light down to only three values. So simple.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/graphics/light/color-cube.png" width="70%" class="center" /></p>
<p>To construct this three dimensional space of colors, map each primary color (red, green, blue) to the unit distance along the $x,y$ and $z$ axes. Black is at $(0,0,0)$ and White is at $(1,1,1)$. The resulting RGB cube represents all possible color. 
<br />
<br />
Another color space that can be construced is the The cylindrical <b>HSV</b> color space. This is a color space based on hue, saturation and value. saturation is the intesity for a partical color. value is lightness or darkness of a particular color.
<br />
<br />
Another color space is the Luminace and Chrominance (YUV). This color space was used back in black and white televison sets. Why? Because the “Y” component holds most of the spatial details of the image. Therefore, we can agressively compress the other components $U$ and $V$ since we don’t care about color and still maintain a good quality.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Mapping between Color Spaces</b></h4>
<p>To go back and forth between these color spaces is easy! we just need to find the transformation to take us from one to the other since we’re all working in $R^3$. One can map from YUV to RGB using the following matrix (TODO: ADD THE MATRIX from the Slides).
Notice in the matrix that 0.58 percent of Y is coming from the green channel. This emphasizes the point that if you want to see more detail then you want to emphasize the green light. Does this mean that we need to change important signs to be colored green because it contains more details? No…. it doesn’t matter what the eyes see. After all it’s a signal that is sent to the brain. What matters is what the brain does with the signal. The brain reacts faster to red for example than yellow or green. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Spatial Resolution</b></h4>
<p>The cones in the eyes have resolution as well. TODO …
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<p>These are my notes based on attending the lecture on light from CS148 with professor Ron Fedkiw. I’ve modified the notes and also included other notes. They are not official or anything like that. Additionally I’ve used the following sources:</p>
<ul>
<li>
<a href="https://www.amazon.com/Fundamentals-Computer-Graphics-Steve-Marschner/dp/1482229390">Fundamentals of Computer Graphics</a>
</li>
</ul>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Introduction What is Light? Light consists of photos that travel along straight paths until they hit a surface. Photons carry some energy represented by its wavelength. A large amount of photons of different energies will together represent a specturm. The sun emits electromagnetic radiation across a wide specturm of wavelengths that includes both visible and not-visible light to the human eye. This includes radio waves, microwaves, visible light, ultraviolet etc. These forms of radiation differ in their wavelengths and frequencies. When lights hits an object, this light is either]]></summary></entry><entry><title type="html">Parametric Representation of an Equation</title><link href="http://localhost:4000/jekyll/update/2023/09/30/parametric-representation.html" rel="alternate" type="text/html" title="Parametric Representation of an Equation" /><published>2023-09-30T01:01:36-07:00</published><updated>2023-09-30T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/30/parametric-representation</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/30/parametric-representation.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>Introduction</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/parametric/00.png" width="70%" class="center" /></p>
<p>We’ve already seen the implicit representation in the previous post. This time, we will focus on the parametric representation. The parametric representation describes the line or curve using one or more new variables. This variable is commonly denoted as “t”. For example, we might want to describe a curve with a specific start point, an end point and furthermore we might a want a general independent parameter that once we specify, then we get back a point $(x,y)$ on the curve. More importantly, some curves can be represented with the explicit representations becuase we can’t describe $y$ as a function of $x$! See this for example. $x=1.5$ has two possible $y$ values!</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/parametric/01.png" width="50%" class="center" /></p>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Example</b></h4>
<p>Suppose we’re given this line in its implicit form,</p>
<div>
$$
\begin{align*}
f(x,y) &amp;= x - 2y + 2 = 0.
\end{align*}
$$
</div>
<p>The parametric representation will be the following</p>
<div>
$$
\begin{align*}
x &amp;= 2t \\
y &amp;= t + 1
\end{align*}
$$
</div>
<p>It will be clear later on how we derived these terms but for now let’s make sure that we can substitute multiple values of $t$ to see that these points are indeed on the line. You can try $t=0$ to get $(x=0, y=1)$ and see that it indeed satisfies the implicit equation. But now ended up with two functions. Is there a way to simplify this? Yes, we can wrap everything in one function if we used vectors instead! So let $v = [x, y]$ be a vector and then re-write the previous two equations such that</p>
<div>
$$
\begin{align*}
v = 
\begin{bmatrix}
2 \\
1
\end{bmatrix}
t +
\begin{bmatrix}
0 \\
1
\end{bmatrix}
\end{align*}
$$
</div>
<p>It looks really good but how do we derive these constants? This part is pretty cool. Take two points on the line. For example $p_0=(0,1)$ and $p_1=(2,2)$. and now we can see that we can represent this line as with the initial point $p_0$ and then the direction of the line using $p_1-p_0$ so,</p>
<div>
$$
\begin{align*}
v = p_0 + (p_1 - p_0)t.
\end{align*}
$$
</div>
<p>At $t=0$, we’re at $p_0$. If we’re at $t=1$, we’re at $p_1$. And we can still substitute $t$ values to go beyond this range. We can also re-write this to clean it and replace $v$ with $f(t)$ since this is now just a function of $t$,</p>
<div>
$$
\begin{align*}
f(t) = (p_1 - p_0)t + p_0.
\end{align*}
$$
</div>
<p>But there is one more way to write this! re-arrange the terms such that,</p>
<div>
$$
\begin{align*}
f(t) = (1-t)p_0 + tp_1
\end{align*}
$$
</div>
<p>So really, it’s like we’re interpolating between the two points. This notation is also nicer since the two points appear once each!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>
<a href="https://www.youtube.com/watch?v=6jjLSkp0Y7I&amp;list=PLplnkTzzqsZTfYh4UbhLGpI5kGd5oW_Hh&amp;index=10&amp;t=535s">Intro to Graphics 09 - Curves (Part 1) by Cem Yuksel</a>
</li>
<li>
<a href="https://www.amazon.com/Fundamentals-Computer-Graphics-Steve-Marschner/dp/1482229390">Fundamentals of Computer Graphics</a>
</li>
</ul>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Introduction We’ve already seen the implicit representation in the previous post. This time, we will focus on the parametric representation. The parametric representation describes the line or curve using one or more new variables. This variable is commonly denoted as “t”. For example, we might want to describe a curve with a specific start point, an end point and furthermore we might a want a general independent parameter that once we specify, then we get back a point $(x,y)$ on the curve. More importantly, some curves can be represented with the explicit representations becuase we can’t describe $y$ as a function of $x$! See this for example. $x=1.5$ has two possible $y$ values! Example Suppose we’re given this line in its implicit form, $$ \begin{align*} f(x,y) &amp;= x - 2y + 2 = 0. \end{align*} $$ The parametric representation will be the following $$ \begin{align*} x &amp;= 2t \\ y &amp;= t + 1 \end{align*} $$ It will be clear later on how we derived these terms but for now let’s make sure that we can substitute multiple values of $t$ to see that these points are indeed on the line. You can try $t=0$ to get $(x=0, y=1)$ and see that it indeed satisfies the implicit equation. But now ended up with two functions. Is there a way to simplify this? Yes, we can wrap everything in one function if we used vectors instead! So let $v = [x, y]$ be a vector and then re-write the previous two equations such that $$ \begin{align*} v = \begin{bmatrix} 2 \\ 1 \end{bmatrix} t + \begin{bmatrix} 0 \\ 1 \end{bmatrix} \end{align*} $$ It looks really good but how do we derive these constants? This part is pretty cool. Take two points on the line. For example $p_0=(0,1)$ and $p_1=(2,2)$. and now we can see that we can represent this line as with the initial point $p_0$ and then the direction of the line using $p_1-p_0$ so, $$ \begin{align*} v = p_0 + (p_1 - p_0)t. \end{align*} $$ At $t=0$, we’re at $p_0$. If we’re at $t=1$, we’re at $p_1$. And we can still substitute $t$ values to go beyond this range. We can also re-write this to clean it and replace $v$ with $f(t)$ since this is now just a function of $t$, $$ \begin{align*} f(t) = (p_1 - p_0)t + p_0. \end{align*} $$ But there is one more way to write this! re-arrange the terms such that, $$ \begin{align*} f(t) = (1-t)p_0 + tp_1 \end{align*} $$ So really, it’s like we’re interpolating between the two points. This notation is also nicer since the two points appear once each! References Intro to Graphics 09 - Curves (Part 1) by Cem Yuksel Fundamentals of Computer Graphics]]></summary></entry><entry><title type="html">Dot Product</title><link href="http://localhost:4000/jekyll/update/2023/09/29/dot-product.html" rel="alternate" type="text/html" title="Dot Product" /><published>2023-09-29T01:01:36-07:00</published><updated>2023-09-29T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/29/dot-product</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/29/dot-product.html"><![CDATA[<p>This post is just me taking notes while watching the 9th lecture of the <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a> which is the best reference on Linear Algebra ever. I might add other notes from other places as I go.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Introduction</b></h4>
<p>It is easy to just compute the dot product numerically. We just multiply the x-coordinate of both vectors followed by multiplying the y-coordinate of the vector and so on. We then add all the terms together. Here is an example:</p>
<div>
$$
\begin{align*}
\begin{bmatrix}
6 \\
1 \\
2 
\end{bmatrix}
\dot
\begin{bmatrix}
4 \\
-3 \\
-1 
\end{bmatrix}
=
6*4 + 1*-3 + 2*-1 = 24 - 3 - 2 = 19.
\end{align*}
$$
</div>
<p>But luckily the dot product has also a nice geometric meaning show below. The dot product $\vec{v}*\vec{w}$ is taking the vector $w$ and projecting it on the line that goes the origin and through the tip of the vector $\vec{v}$ and then multiplying the length of this projected vector by the length of vector $v$ itself!</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/dotproduct/00.png" width="70%" class="center" /></p>
<p>However if the projected vector is pointing in the opposite direction of $v$, then this product is actually negative.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/dotproduct/00.png" width="70%" class="center" /></p>
<p>So to summarize, if both vectors are pointing in the same direction, then generally the dot product will be positive, if they’re perpendicular to each other, the product is zero and if they’re pointing in the opposite direction of each other then the product is negative. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Does the Order Matter?</b></h4>
<p>Weirdly enough, the order doesn’t matter. so if you instead project $v$ onto $w$, take the length of the porjected v and multiply it by the length of w, the product is still the same!</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/inverse/00.png" width="90%" class="center" /></p>

<p>Why doesn’t the order matter? One way to think about this is suppose we let $v$ and $w$ have the same length. In this case, you can see that whether we project $v$ on $w$ or $w$ on $v$, we’ll end up with the same exact dot product. 
<br />
<br />
Suppose now that we scale only one of them by 2 for example. So $w$ and $2v$.</p>
<ul>
  <li>If we project $w$ onto $v$, then we’ll multiply this projected w length by the length of v which is now twice it’s original length and we will have</li>
</ul>
<div>
$$
\begin{align*}
(2v) \cdot w = 2 (v \cdot w).
\end{align*}
$$
</div>
<p>This happens because scaling v has no effect on the length of the projected w vector onto v. Now suppose, we’re projecting $2v$ on to $w$ instead. This time, the length of the projection is the thing that will get scaled instead. 2v is getting projected onto w and so the dot product will again be the same. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>More Intuition</b></h4>
<p>We’ve described the numerical way to calculate a dot product and then the geometric interpretation. But why would these things be related at all? How can multiplying these coordinates and then adding them together be related to projecting vectors onto each other?
<br />
<br />
To understand this, Let’s start by considerings linear transformations from high dimensions into one-dimension (the numbers line). We know linear transformations means that parallel lines stay parallel and evenly spaced (formally, (L(v+w) = L(v) + L(w), L(cv) = cL(v)))). So when we take a line of evenly spaced dots, A linear transformation will keep these dots evenly spaced. We also know that a linear transformation is completely determined by where it takes $\widehat{i}$ and $\widehat{j}$. 
<br />
<br />
Now suppose that $v = (4, 3)$ and suppose we have a linear transformation $L$ that takes $\widehat{i}$ to 1 and $\widehat{j}$ to -2. We can re-write $v$ as a linear combination of $\widehat{i}$ and $\widehat{j}$ yields $\vec{v} = 4\widehat{i} + 3\widehat{j}$.</p>

<p>[pic]</p>

<p>After applying $L$, $v$ will be 4 times the place where $\widehat{i}$ lands and 3 times the place where $\widehat{j}$ lands. This means that $L(v) = 4<em>1 - 2</em>3 = -2$.</p>

<p>[pic]
<br />
<br />
Numerically, this is done by matrix multiplication,</p>
<div>
$$
\begin{align*}
\begin{bmatrix}
1 &amp; -2
\end{bmatrix}
\begin{bmatrix}
4 \\
3 
\end{bmatrix}
=
1*4 - 2*3 = -2
\end{align*}
$$
</div>
<p>This looks very familiar to how we compute a dot product essentially! The only difference is that the 1x2 matrix is usually written as a 2x1 vector instead! There is a connection between linear transformations that take vectors to numbers and vectors themselves! hmm
<br />
<br />
Take the numbers line and place it diagonally in a 2d plane where the number 0 will sit exactly at the origin. Let $u$ be a 2d vector such that its tip sits at the number 1 on the numbers line. Suppose now we have a bunch of 2d vectors in the plane. Project these vectors right onto the diagonal number line. We can call this a function that takes 2d vectors and projects them on the numbers line. What’s more is that this function is linear. A line of evenly spaced dots that gets projected results in these projected dots as evenly spaced on the numbers line as well. Since this is a linear function, then we must have a matrix that describes this function/transformation. This matrix must be of size 1x2 since it takes a vector and outputs a single number. To figure out this matrix, we need to find out where $\widehat{i}$ and $\widehat{j}$ have landed on the numbers line.</p>

<p>[pic with i, j and u]</p>

<p>We know that $\widehat{i}$ and $\widehat{j}$ are both unit vectors. Notice here that projecting $\widehat{i}$ onto the line that goes through $\widehat{u}$ looks symmetric to projecting $\widehat{u}$ onto on the the line that goes through $\widehat{i}$ (the x-axis). This is because both $\widehat{i}$ and $\widehat{u}$ are unit vectors. Projecting $\widehat{u}$ on the x-axis is just the x-coordinate $u_x$ of $\widehat{u}$. So now pay attention. Since we just said that the projection will be of the same length whether we we project $\widehat{u}$ on $\widehat{i}$  or $\widehat{i}$  onto $\widehat{u}$, this means that projecting $\widehat{i}$ onto the numbers line, will be at exactly the point $u_x$. Similarly, projecting $\widehat{j}$ onto the numbers line will land exactly at $u_y$! 
<br />
<br />
So now we know exactly where both $\widehat{i}$ and $\widehat{j}$ land and the matrix is [u_x u_y]. And so now FOR ANY VECTOR, computing the project of it onto the numbers line, is just a matter of multiplying the vector by the matrix [u_x u_y].
<br />
<br />
So suppose v =[v_x, v_y], then project this vector on the numbers line is like multiplying [v_x, v_y] * [u_x, u_y]. This looks EXACTLY like a dot product.</p>
<div>
$$
\begin{align*}
\begin{bmatrix}
u_x &amp; u_y
\end{bmatrix}
\begin{bmatrix}
x \\
y 
\end{bmatrix}
=
u_x*x + u_y*y
\end{align*}
$$
</div>
<p>and</p>
<div>
$$
\begin{align*}
\begin{bmatrix}
u_x \\ 
u_y
\end{bmatrix}
\begin{bmatrix}
x \\
y 
\end{bmatrix}
=
u_x*x + u_y*y
\end{align*}
$$
</div>
<p>This is why taking the dot product with a unit vector can be interpreted as projecting a vector into the span of that unit vector and taking the length.</p>

<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is just me taking notes while watching the 9th lecture of the Essence of Linear Algebra by 3Blue1Brown which is the best reference on Linear Algebra ever. I might add other notes from other places as I go. Introduction It is easy to just compute the dot product numerically. We just multiply the x-coordinate of both vectors followed by multiplying the y-coordinate of the vector and so on. We then add all the terms together. Here is an example: $$ \begin{align*} \begin{bmatrix} 6 \\ 1 \\ 2 \end{bmatrix} \dot \begin{bmatrix} 4 \\ -3 \\ -1 \end{bmatrix} = 6*4 + 1*-3 + 2*-1 = 24 - 3 - 2 = 19. \end{align*} $$ But luckily the dot product has also a nice geometric meaning show below. The dot product $\vec{v}*\vec{w}$ is taking the vector $w$ and projecting it on the line that goes the origin and through the tip of the vector $\vec{v}$ and then multiplying the length of this projected vector by the length of vector $v$ itself! However if the projected vector is pointing in the opposite direction of $v$, then this product is actually negative. So to summarize, if both vectors are pointing in the same direction, then generally the dot product will be positive, if they’re perpendicular to each other, the product is zero and if they’re pointing in the opposite direction of each other then the product is negative. Does the Order Matter? Weirdly enough, the order doesn’t matter. so if you instead project $v$ onto $w$, take the length of the porjected v and multiply it by the length of w, the product is still the same!]]></summary></entry><entry><title type="html">Implicit Representation of an Equation</title><link href="http://localhost:4000/jekyll/update/2023/09/28/implicit-representation.html" rel="alternate" type="text/html" title="Implicit Representation of an Equation" /><published>2023-09-28T01:01:36-07:00</published><updated>2023-09-28T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/28/implicit-representation</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/28/implicit-representation.html"><![CDATA[<!------------------------------------------------------------------------------------>
<h4><b>The implicit Representation of a Line</b></h4>
<p>Consider the following equation for a line that goes through the origin,</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/implicit/00.png" width="60%" class="center" /></p>
<p>This line equation ($y = 2x$) is also sometimes referred to as the explicit representation of the line. We can also represent this line using a representation called the <b>implicit representation</b>. We can do so by re-arranging the terms such that</p>
<div>
$$
\begin{align*}
y - 2x = 0.
\end{align*}
$$
</div>
<p>The implicit representation describes the relationship between the variables $x$ and $y$. It describes all the points $(x,y)$ that satisfies the equation $y - 2x = 0$. We don’t have an explicit solution. We just know the relationship between the variables that will need to be satisfied. In general, the implicit representation of a line takes the form</p>
<div>
$$
\begin{align*}
f(x, y) &amp;= 0 \\
ax + by + c &amp;= 0.
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The implicit Representation of a Circle</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/math/implicit/circle.png" width="60%" class="center" /></p>
<p>Moving on from the simple example of a line. Let’s study the circle example. This function returns a real value and for any any point $(x,y)$, if $f(x,y)$ is zero, then the point is on the circle. If it’s not zero, then it’s not on the circle. The equation of a circle can be written as</p>
<div>
$$
\begin{align*}
f(x,y) &amp;= (x - x_c)^2 + (y - y_c)^2 - r^2 = 0.
\end{align*}
$$
</div>
<p>where $(x_c, y_c)$ is a point and $r$ is a nonzero real number. Any point that satifies this equation is on the circle of center $(x_c, y_c)$ and radius $r$. We can re-write the equation using vectors to simplify the notation. if we let $c=(x_c, y_c)$ and $p=(x,y)$, then</p>
<div>
$$
\begin{align*}
f(x,y) = (p - c) \cdot (p - c) - r^2 = 0.
\end{align*}
$$
</div>
<p>We know that $(p - c) \cdot (p - c)$ is just the length of this vector squared and so</p>
<div>
$$
\begin{align*}
f(x,y) = \left\lVert p - c \right\rVert^2 - r^2 = 0.
\end{align*}
$$
</div>
<p>This just means that the points $p$ on the circle are those with a distance $r$ from the center of the circle at $c$.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<ul>
	<li>
<a href="https://www.youtube.com/watch?v=6jjLSkp0Y7I&amp;list=PLplnkTzzqsZTfYh4UbhLGpI5kGd5oW_Hh&amp;index=10&amp;t=535s">Intro to Graphics 09 - Curves (Part 1) by Cem Yuksel</a>
</li>
<li>
<a href="https://www.amazon.com/Fundamentals-Computer-Graphics-Steve-Marschner/dp/1482229390">Fundamentals of Computer Graphics</a>
</li>
</ul>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[The implicit Representation of a Line Consider the following equation for a line that goes through the origin, This line equation ($y = 2x$) is also sometimes referred to as the explicit representation of the line. We can also represent this line using a representation called the implicit representation. We can do so by re-arranging the terms such that $$ \begin{align*} y - 2x = 0. \end{align*} $$ The implicit representation describes the relationship between the variables $x$ and $y$. It describes all the points $(x,y)$ that satisfies the equation $y - 2x = 0$. We don’t have an explicit solution. We just know the relationship between the variables that will need to be satisfied. In general, the implicit representation of a line takes the form $$ \begin{align*} f(x, y) &amp;= 0 \\ ax + by + c &amp;= 0. \end{align*} $$ The implicit Representation of a Circle Moving on from the simple example of a line. Let’s study the circle example. This function returns a real value and for any any point $(x,y)$, if $f(x,y)$ is zero, then the point is on the circle. If it’s not zero, then it’s not on the circle. The equation of a circle can be written as $$ \begin{align*} f(x,y) &amp;= (x - x_c)^2 + (y - y_c)^2 - r^2 = 0. \end{align*} $$ where $(x_c, y_c)$ is a point and $r$ is a nonzero real number. Any point that satifies this equation is on the circle of center $(x_c, y_c)$ and radius $r$. We can re-write the equation using vectors to simplify the notation. if we let $c=(x_c, y_c)$ and $p=(x,y)$, then $$ \begin{align*} f(x,y) = (p - c) \cdot (p - c) - r^2 = 0. \end{align*} $$ We know that $(p - c) \cdot (p - c)$ is just the length of this vector squared and so $$ \begin{align*} f(x,y) = \left\lVert p - c \right\rVert^2 - r^2 = 0. \end{align*} $$ This just means that the points $p$ on the circle are those with a distance $r$ from the center of the circle at $c$. References Intro to Graphics 09 - Curves (Part 1) by Cem Yuksel Fundamentals of Computer Graphics]]></summary></entry><entry><title type="html">Inverse Matrices, Column Space, Rank and Null Space</title><link href="http://localhost:4000/jekyll/update/2023/09/27/system-of-linear-equations.html" rel="alternate" type="text/html" title="Inverse Matrices, Column Space, Rank and Null Space" /><published>2023-09-27T01:01:36-07:00</published><updated>2023-09-27T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/27/system-of-linear-equations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/27/system-of-linear-equations.html"><![CDATA[<p>This post is just me taking notes while watching the 7th lecture of the <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a> which is the best reference on Linear Algebra ever. I might add other notes from other places as I go.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Introduction</b></h4>
<p>One of the main uses of linear algebra is that it lets us solve a linear system of equations. So if we’re given for example the following set of equations,</p>
<div>
$$
\begin{align*}
6x - 3y + 2z = 7 \\
x + 2y + 5z = 0 \\
2x - 8y - z = -2
\end{align*}
$$
</div>
<p>We can group all the coefficients in a matrix $A$, all the variables in a vector $\vec{x}$ and all the constants in a vector $\vec{v}$. We can then write,</p>
<div>
$$
\begin{align*}
A\vec{x} &amp;= \vec{y} \\
\begin{bmatrix}
6 &amp; -3 &amp; 2 \\
1 &amp; 2 &amp; 5 \\
2 &amp; -8 &amp; -1 
\end{bmatrix}
&amp;=
\begin{bmatrix}
x \\ y \\ z
\end{bmatrix}
=
\begin{bmatrix}
7 \\ 0 \\ -2
\end{bmatrix}
\end{align*}
$$
</div>
<p>This should look very familiar! The matrix $A$ corresponds to some linear transformation so solving $Ax=v$ means we’re looking for a vector $x$ which after applying the transformation $A$, lands on the vector $v$!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>A 2D Example</b></h4>
<p>Let’s start with a simple example in two dimensions such that the matrix $A$ is a 2x2 matrix. Suppose we have two equations and two unknowns. The equations are</p>
<div>
$$
\begin{align*}
2x + 2y = 4 \\
1x + 3y = -1
\end{align*}
$$
</div>
<p>and after re-arranging everything, we’ll have</p>
<div>
$$
\begin{align*}
A\vec{x} &amp;= \vec{y} \\
\begin{bmatrix}
2 &amp; 2 \\
1 &amp; 3
\end{bmatrix}
&amp;=
\begin{bmatrix}
x \\ y
\end{bmatrix}
=
\begin{bmatrix}
-4 \\ -1
\end{bmatrix}
\end{align*}
$$
</div>
<p>The solution to this system depends on whether A squishes all of the space into a lower dimension or whether it leaves everything still in two dimensions where it started. One way to determine this to use what we learned in the last’s lecture and compute the determinant of this matrix.</p>
<ul>
	 <li>If it's zero, then we know, we're squished to a lower dimension.</li>
	 <li>If it's non-zero, then we know we have a unique solution.</li>
 </ul>
<p>We’ll analyze each case seperately next.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Case 1: The Determinant is not zero (The Inverse of a Transformation)</b></h4>
<p>In case 1, the determinant is not zero. This means the area is not zero and we’re not squished to a lower dimension. Therefore there will be only one vector $x$ such that $Ax = v$. To find it, we really want to play the transformation in reverse going from $v$ to $x$ (imagine playing it slowly from $\vec{v}$ back to $\vec{x}$). In other words, we want the inverse transformation applied to $v$. The inverse of a transformation $A$ is called the <b>inverse of $A$</b> and is denoted by $A^{-1}$. $A^{-1}$ is the unique transformation with the property that if you first apply $A$ and then follow it with $A^{-1}$, you end up back where you started. And we know that applying multiple transformations is captured algebraically with matrix multiplication. So $AA^{-1}$ corresponds to the transformation that does nothing! The transformation that does nothing is called the <b>identity</b> transformation. It leaves both $\widehat{i}$ and $\widehat{j}$ unchanged so $I = matrix (1,0,0,1)$.</p>
<div>
$$
\begin{align*}
I &amp;=
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\end{align*}
$$
</div>
<p>So now to find the unique solution, we can just apply the inverse to both sides of the equation</p>
<div>
$$
\begin{align*}
A\vec{x} &amp;= \vec{v} \\
A^{-1}A\vec{x} &amp;= A^{-1}\vec{v} \\
\vec{x} &amp;= A^{-1}\vec{v}
\end{align*}
$$
</div>
<p>Again it’s important to stress the geometric meaning of this equation. We’re playing the transformation $A$ in reverse from $v$ and following it back to $x$.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Case 2: The Determinant is Zero</b></h4>
<p>So we know when the determinant is zero, this means that the two dimensions are squished onto a lower dimension. So there is no inverse because we can’t unsquish a line to turn it into a plane. It is though still possible that a solution exist even when the determinant is zero. It’s just when the whole space is squished into a line, we have to lucky that the vector $v$ lives somewhere on that line. (green means there is a solution and yellow indicates no solution exists)</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/inverse/00.png" width="90%" class="center" /></p>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Rank</b></h4>
<p>If we take a 3x3 transformation in 3 dimensions where the determinant is zero, we will notice that some of these zero determinant cases feel a lot more restrictive than others. It seems that it will be harder to find a solution if we’re squished to a line versus for example getting squished to a plane. This leads us to develop new terminology to distiguish these cases. We’ll say the transformation that squishes us to a 1 dinmensional line has <b>rank 1</b>. If we’re squished to 2 dimensions, then the tranformation has rank 2. 
<br />
<br />
So really <b>the rank is the number of dimensions in the output of a transformation.</b> So in the 3x3 case, rank 3 is the best rank the matrix can reach and it means that the basis vectors continue to span the full three dimensions of space and the determinant is not zero!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Column Space</b></h4>
<p>The set of all possible outputs of the 3x3 transformation is called the <b>column space of $A$</b> of the transformation whether it’s a line, a plane or 3D space. 
<br />
<br />
Why did we call it the column space? We know that each column in the matrix $A$ refers to where a vector from the basis vectors have landed. For example the first column is where $\widehat{i}$ has landed, the second is where $\widehat{j}$ has landed and so on. The <b>span</b> of those transformed basis vectors gives you all possible outputs. In other words, the <b>column space</b> is the <b>span of the columns</b> of your matrix. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Null Space</b></h4>
<p>Another important thing is that the zero vector will always be included in the column space since linear transformations must keep the origin fixed in place. If the matrix is full rank, then the zero vector is the only vector that lands at the origin. But for other non full rank matrics, we can have a bunch of vectors land on zero. These vectors are called the <b>null space</b> or the <b>kernel</b> of your matrix. It’s the set of all vectors that become null (land on the zero vector). 
<br />
<br />
If we’re trying to solve $A\vec{x}=\vec{0}$, then the null space gives you all of the possible solutions to the equation.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is just me taking notes while watching the 7th lecture of the Essence of Linear Algebra by 3Blue1Brown which is the best reference on Linear Algebra ever. I might add other notes from other places as I go. Introduction One of the main uses of linear algebra is that it lets us solve a linear system of equations. So if we’re given for example the following set of equations, $$ \begin{align*} 6x - 3y + 2z = 7 \\ x + 2y + 5z = 0 \\ 2x - 8y - z = -2 \end{align*} $$ We can group all the coefficients in a matrix $A$, all the variables in a vector $\vec{x}$ and all the constants in a vector $\vec{v}$. We can then write, $$ \begin{align*} A\vec{x} &amp;= \vec{y} \\ \begin{bmatrix} 6 &amp; -3 &amp; 2 \\ 1 &amp; 2 &amp; 5 \\ 2 &amp; -8 &amp; -1 \end{bmatrix} &amp;= \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 7 \\ 0 \\ -2 \end{bmatrix} \end{align*} $$ This should look very familiar! The matrix $A$ corresponds to some linear transformation so solving $Ax=v$ means we’re looking for a vector $x$ which after applying the transformation $A$, lands on the vector $v$! A 2D Example Let’s start with a simple example in two dimensions such that the matrix $A$ is a 2x2 matrix. Suppose we have two equations and two unknowns. The equations are $$ \begin{align*} 2x + 2y = 4 \\ 1x + 3y = -1 \end{align*} $$ and after re-arranging everything, we’ll have $$ \begin{align*} A\vec{x} &amp;= \vec{y} \\ \begin{bmatrix} 2 &amp; 2 \\ 1 &amp; 3 \end{bmatrix} &amp;= \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} -4 \\ -1 \end{bmatrix} \end{align*} $$ The solution to this system depends on whether A squishes all of the space into a lower dimension or whether it leaves everything still in two dimensions where it started. One way to determine this to use what we learned in the last’s lecture and compute the determinant of this matrix. If it's zero, then we know, we're squished to a lower dimension. If it's non-zero, then we know we have a unique solution. We’ll analyze each case seperately next. Case 1: The Determinant is not zero (The Inverse of a Transformation) In case 1, the determinant is not zero. This means the area is not zero and we’re not squished to a lower dimension. Therefore there will be only one vector $x$ such that $Ax = v$. To find it, we really want to play the transformation in reverse going from $v$ to $x$ (imagine playing it slowly from $\vec{v}$ back to $\vec{x}$). In other words, we want the inverse transformation applied to $v$. The inverse of a transformation $A$ is called the inverse of $A$ and is denoted by $A^{-1}$. $A^{-1}$ is the unique transformation with the property that if you first apply $A$ and then follow it with $A^{-1}$, you end up back where you started. And we know that applying multiple transformations is captured algebraically with matrix multiplication. So $AA^{-1}$ corresponds to the transformation that does nothing! The transformation that does nothing is called the identity transformation. It leaves both $\widehat{i}$ and $\widehat{j}$ unchanged so $I = matrix (1,0,0,1)$. $$ \begin{align*} I &amp;= \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \end{align*} $$ So now to find the unique solution, we can just apply the inverse to both sides of the equation $$ \begin{align*} A\vec{x} &amp;= \vec{v} \\ A^{-1}A\vec{x} &amp;= A^{-1}\vec{v} \\ \vec{x} &amp;= A^{-1}\vec{v} \end{align*} $$ Again it’s important to stress the geometric meaning of this equation. We’re playing the transformation $A$ in reverse from $v$ and following it back to $x$. Case 2: The Determinant is Zero So we know when the determinant is zero, this means that the two dimensions are squished onto a lower dimension. So there is no inverse because we can’t unsquish a line to turn it into a plane. It is though still possible that a solution exist even when the determinant is zero. It’s just when the whole space is squished into a line, we have to lucky that the vector $v$ lives somewhere on that line. (green means there is a solution and yellow indicates no solution exists) Rank If we take a 3x3 transformation in 3 dimensions where the determinant is zero, we will notice that some of these zero determinant cases feel a lot more restrictive than others. It seems that it will be harder to find a solution if we’re squished to a line versus for example getting squished to a plane. This leads us to develop new terminology to distiguish these cases. We’ll say the transformation that squishes us to a 1 dinmensional line has rank 1. If we’re squished to 2 dimensions, then the tranformation has rank 2. So really the rank is the number of dimensions in the output of a transformation. So in the 3x3 case, rank 3 is the best rank the matrix can reach and it means that the basis vectors continue to span the full three dimensions of space and the determinant is not zero! Column Space The set of all possible outputs of the 3x3 transformation is called the column space of $A$ of the transformation whether it’s a line, a plane or 3D space. Why did we call it the column space? We know that each column in the matrix $A$ refers to where a vector from the basis vectors have landed. For example the first column is where $\widehat{i}$ has landed, the second is where $\widehat{j}$ has landed and so on. The span of those transformed basis vectors gives you all possible outputs. In other words, the column space is the span of the columns of your matrix. Null Space Another important thing is that the zero vector will always be included in the column space since linear transformations must keep the origin fixed in place. If the matrix is full rank, then the zero vector is the only vector that lands at the origin. But for other non full rank matrics, we can have a bunch of vectors land on zero. These vectors are called the null space or the kernel of your matrix. It’s the set of all vectors that become null (land on the zero vector). If we’re trying to solve $A\vec{x}=\vec{0}$, then the null space gives you all of the possible solutions to the equation. References Essence of Linear Algebra by 3Blue1Brown]]></summary></entry><entry><title type="html">Determinants</title><link href="http://localhost:4000/jekyll/update/2023/09/26/determinants.html" rel="alternate" type="text/html" title="Determinants" /><published>2023-09-26T01:01:36-07:00</published><updated>2023-09-26T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/26/determinants</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/26/determinants.html"><![CDATA[<p>This post is just me taking notes while watching the 6th lecture of the <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a> which is the best reference on Linear Algebra ever. I might add other notes from other places as I go.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>The Determinant</b></h4>
<p>We’ve talked about linear transformations previously. Some squish the space and some strech the space (while of course keeping the grid lines parallel and evenly spaced and the origin in the same place). How can we measure the amount of streching? Given an area bounded by some region, how much will its area grow by after applying some transformation? For example in the below figure, we can see that the shaded area on the left has area $A = 1$. After the transformation was applied, this area grew by a factor of 4 so the new area is now $4*A$. So the transformation scaled the area by a factor of 4.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/00.png" width="80%" class="center" /></p>
<p>The important fact here is that knowing how much the area of that one single unit square changes is enough to know how much any area will change in the space! This follows from the fact that the parallel grid lines remain parallel and evenly spaced. This scaling factor by which a linear transformation changes any area is called the <b>determinant</b>
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>More Examples</b></h4>
<p>To re-iterate, the determinant is the scaling factor by which a linear transformation changes any area. And it applies to any area because a linear transformation keeps the parallel lines parallel and evenly spaced. Let’s look at some example:</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/01.png" width="80%" class="center" /></p>

<p>Here, we know that the determinant of that transformation (aka, the factor by which any area would scale) is 0.5. Let the area be $A$ before the transformation. The new area after the transformation will be $0.5*A$ So the determinant is 0.5.</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/02.png" width="80%" class="center" /></p>

<p>This is another example where the area is completely squished and it’s just zero. This means that the area of any region would become also 0 after applying this specific transformation. This is really important because it means that checking if the determinant of a given matrix is 0, will give away wether or not the transformation associated with that matrix squishes everything into a smaller dimension.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Negative Areas?</b></h4>
<p>Unfortunately, the determinat can be negative! so what would scaling an area by a negative amount even mean? This has to do with the concept of orientation. One way to think about this is to notice that some transformations flip the space (like flipping a sheet of paper). Transformation that flip the space are said to “invert the orientation of space”. (video has beautiful animations for this!)</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/03.png" width="90%" class="center" /></p>
<p>Another way to think about this is in terms of $\widehat{i}$ and $\widehat{j}$. Notice that in the above figure and before applying the tranformation, $\widehat{j}$ is to the left of $\widehat{i}$. After applying $L$, $\widehat{j}$ is now on the right of $\widehat{i}$. Whenever the orientation of the space is inverted, the determinant will be negative. But regardless of the sign of the determinant, the absolute value will still determine the scaling factor by which the area of any region will change! 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Three Dimensions</b></h4>
<p>What about the determinat of a linear transformation in 3 dimensions! This time it will tell us how much the <b>volume</b> of a region will get scaled by! Instead of focusing on the single 1x1 square in dimensions, now we can focus on the 1x1x1 cube and observe what happens to it after the transformation.
<br />
<br />
What about negative determinants? What does orientation mean here? We will follow the right hand rule to determine the orientation. Point the forefinger of your right hand in the direction of $\widehat{i}$, your middle finger in the direction of $\widehat{j}$ and your thumb in the direction of $\widehat{k}$. If your thumb stays in the same direction after the transformation, then the orientation hasn’t changed and the determinant will be positive. If instead, you can only do that with your left hand, then the orientation is flipped and the determinant is negative.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Computing the Determinant</b></h4>
<p>We compute the determinant numerically using the following</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/04.png" width="50%" class="center" /></p>
<p>What’s the intuition behind this? Suppose that the terms $c$ and $d$ are zero and so in the above, the determinant will just the product $ad$, shown below</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/05.png" width="55%" class="center" /></p>

<p>In this case $a$ will tell us how much $\widehat{i}$ is stretched in the $x$ direction and the term $d$ will tell us how much $\widehat{j}$ is stretched in the $y$ direction show below,</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/06.png" width="50%" class="center" /></p>

<p>This should make sense since now $a*d$ is also the area of the rectangle of what the unit square has turned into. What if only $b$ or $c$ are zero and not both! Numerically, the determinant is still $ad$ and so it doesn’t change but visually it’s the area of the parallelogram with base $a$ and height $d$ as show below,</p>

<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/determinants/07.png" width="55%" class="center" /></p>
<p>What if none of the terms are zero? there is a crazy diagram in the video that shows where the term is derived from exactly. Each piece of the area will be mapped to one of the terms in the crazy equation.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is just me taking notes while watching the 6th lecture of the Essence of Linear Algebra by 3Blue1Brown which is the best reference on Linear Algebra ever. I might add other notes from other places as I go. The Determinant We’ve talked about linear transformations previously. Some squish the space and some strech the space (while of course keeping the grid lines parallel and evenly spaced and the origin in the same place). How can we measure the amount of streching? Given an area bounded by some region, how much will its area grow by after applying some transformation? For example in the below figure, we can see that the shaded area on the left has area $A = 1$. After the transformation was applied, this area grew by a factor of 4 so the new area is now $4*A$. So the transformation scaled the area by a factor of 4. The important fact here is that knowing how much the area of that one single unit square changes is enough to know how much any area will change in the space! This follows from the fact that the parallel grid lines remain parallel and evenly spaced. This scaling factor by which a linear transformation changes any area is called the determinant More Examples To re-iterate, the determinant is the scaling factor by which a linear transformation changes any area. And it applies to any area because a linear transformation keeps the parallel lines parallel and evenly spaced. Let’s look at some example:]]></summary></entry><entry><title type="html">Matrix Multiplication As Composition</title><link href="http://localhost:4000/jekyll/update/2023/09/25/matrix-multiplication-as-composition.html" rel="alternate" type="text/html" title="Matrix Multiplication As Composition" /><published>2023-09-25T01:01:36-07:00</published><updated>2023-09-25T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/25/matrix-multiplication-as-composition</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/25/matrix-multiplication-as-composition.html"><![CDATA[<p>This post is just me taking notes while watching <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a> which is the best reference on Linear Algebra ever. I might add other notes from other places as I go.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformations Recap</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/00.png" width="50%" class="center" /></p>
<p>Linear transformations are functions with vectors as inputs and vectors as outputs. We described them visually and saw how they change the vector space such that grid lines remain parallel and evenly spaced and where the origin remains fixed. We also saw that a linear transformation is completely determined by where the besis vectors of the space. For two dimensions, this means that we just need to know where $\widehat{i}$ and $\widehat{j}$ land in order to figure out where any other vector will land. And this is because any vector in two dimensions can be written as a linear combination of the two basis vectors $\widehat{i}$ and $\widehat{j}$.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/01a.png" width="40%" class="center" /></p>
<p>Suppose we want to find out where the vector $v=(3,2)$ will land after some linear transformation. We can multiply the x-coordinate of $v$ by the transformed $\widehat{i}$ coordinates and similarly, multiply the y-coordinate of $v$ by the coordinates of the transformed $\widehat{j}$.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/01.png" width="65%" class="center" /></p>
<p>To write things numerically, the convection is to arrange the coordinates of these vectors as columns in a 2x2 matrix and then to put the matrix to the left of the vector (just like functions!)</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/02.png" width="100%" class="center" /></p>
<p><br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Multiple Linear Transformations</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/03.png" width="100%" class="center" /></p>
<p>What if we wanted to apply not just one linear transformations but multiple linear transformations? For example, suppose we want to apply a rotation followed by a shear transformation (see above). It turns out that the end result is still another linear transformation recorded in the following matrix,</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/04.png" width="25%" class="center" /></p>
<p>This new linear transformations is called the <b>composition</b> of the two transformations that we applied (rotation + shear).
One way to think about the new matrix is as follows: if we were to take some vector and then pump it through the rotation and then the shear. The long way to compute where it ends up is first to multiply this vector by the rotation matrix on the left and then whatever you get and multiply that on the left by the shear matrix.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/05.png" width="60%" class="center" /></p>
<p>This is what it means to apply a rotation then a shear to a given victor and whatever we get should exactly the same as applying this vector by the new composition matrix.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/06.png" width="90%" class="center" /></p>
<p>Since this new matrix is supposed to capture this overall effect of applying a rotation and then a shear, it should be reasonable to call this new matrix as the “product” of the original two matrices.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/07.png" width="60%" class="center" /></p>
<p>One thing to always remember is that the multiplying two matrices like the above has the geometric meaning of applying one transformation then another. And the reason why we apply the most right matrix first is just following the function notation In $f(g(x))$, we apply g and then $f$.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Multiplying Matrices Numerically</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/08.png" width="60%" class="center" /></p>
<p>Let’s carry out the multiplication numerically without visualizing it. First we will need to know where $\widehat{i}$ will land. By definition we know that after applying $M_1$ above, the new coordinates of $\widehat{i}$ are captured by the first column of the $M_1$ matrix and similarly for $\widehat{j}$. (Recall that the linear transformation matrix records the coordinates of where the two basis vectors have landed).</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/09.png" width="60%" class="center" /></p>
<p>Let’s see what happens after applying the transformation $M_2$ on the vector (0,1). This can be done by multiplying the x-coordinate of that vector by the first column of $M_2$ and then similarly multiply the y-coordinate by the second column. (similar to what we learned in the previous video/post)</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/10.png" width="60%" class="center" /></p>
<p>The resulting vector is the first column of the composition matrix below!</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/11.png" width="60%" class="center" /></p>
<p>Next, we will apply $M_2$ on the transformed $\widehat{j}$ vector similar to what we did above for $\widehat{i}$</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/12.png" width="60%" class="center" /></p>
<p>And the resulting vector will be the second column of the new composition matrix!</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/4-matrix-compsition/13.png" width="60%" class="center" /></p>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Properties of Matrix Multiplication</b></h4>
<p>Does the order matter? Does $M_1M_2 = M_2M_1$? We can see this by visualizing the transformations. Take a vector and rotate it with the rotation from the above example and then apply the shear from the previous example as well. Take the same vector and apply shear transformation this time before applying the rotation. Do we get the same vector? no. It is so easy to see this with transformations rathar than figuring out this numerically!
<br />
<br />
What about associativity? Does $(AB)C=A(BC)$? Carrying this out numerically, it is not intuitive and messy! Visualizing this with transformations, it is actually really easy to see that it is true!
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>3D</b></h4>
<p>The same concept applies to 3 dimesions and the video “chapter 5: Three dimensional linear transformations” has really great animations to show different transformations in 3D.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is just me taking notes while watching Essence of Linear Algebra by 3Blue1Brown which is the best reference on Linear Algebra ever. I might add other notes from other places as I go. Linear Transformations Recap Linear transformations are functions with vectors as inputs and vectors as outputs. We described them visually and saw how they change the vector space such that grid lines remain parallel and evenly spaced and where the origin remains fixed. We also saw that a linear transformation is completely determined by where the besis vectors of the space. For two dimensions, this means that we just need to know where $\widehat{i}$ and $\widehat{j}$ land in order to figure out where any other vector will land. And this is because any vector in two dimensions can be written as a linear combination of the two basis vectors $\widehat{i}$ and $\widehat{j}$. Suppose we want to find out where the vector $v=(3,2)$ will land after some linear transformation. We can multiply the x-coordinate of $v$ by the transformed $\widehat{i}$ coordinates and similarly, multiply the y-coordinate of $v$ by the coordinates of the transformed $\widehat{j}$. To write things numerically, the convection is to arrange the coordinates of these vectors as columns in a 2x2 matrix and then to put the matrix to the left of the vector (just like functions!) Multiple Linear Transformations What if we wanted to apply not just one linear transformations but multiple linear transformations? For example, suppose we want to apply a rotation followed by a shear transformation (see above). It turns out that the end result is still another linear transformation recorded in the following matrix, This new linear transformations is called the composition of the two transformations that we applied (rotation + shear). One way to think about the new matrix is as follows: if we were to take some vector and then pump it through the rotation and then the shear. The long way to compute where it ends up is first to multiply this vector by the rotation matrix on the left and then whatever you get and multiply that on the left by the shear matrix. This is what it means to apply a rotation then a shear to a given victor and whatever we get should exactly the same as applying this vector by the new composition matrix. Since this new matrix is supposed to capture this overall effect of applying a rotation and then a shear, it should be reasonable to call this new matrix as the “product” of the original two matrices. One thing to always remember is that the multiplying two matrices like the above has the geometric meaning of applying one transformation then another. And the reason why we apply the most right matrix first is just following the function notation In $f(g(x))$, we apply g and then $f$. Multiplying Matrices Numerically Let’s carry out the multiplication numerically without visualizing it. First we will need to know where $\widehat{i}$ will land. By definition we know that after applying $M_1$ above, the new coordinates of $\widehat{i}$ are captured by the first column of the $M_1$ matrix and similarly for $\widehat{j}$. (Recall that the linear transformation matrix records the coordinates of where the two basis vectors have landed). Let’s see what happens after applying the transformation $M_2$ on the vector (0,1). This can be done by multiplying the x-coordinate of that vector by the first column of $M_2$ and then similarly multiply the y-coordinate by the second column. (similar to what we learned in the previous video/post) The resulting vector is the first column of the composition matrix below! Next, we will apply $M_2$ on the transformed $\widehat{j}$ vector similar to what we did above for $\widehat{i}$ And the resulting vector will be the second column of the new composition matrix! Properties of Matrix Multiplication Does the order matter? Does $M_1M_2 = M_2M_1$? We can see this by visualizing the transformations. Take a vector and rotate it with the rotation from the above example and then apply the shear from the previous example as well. Take the same vector and apply shear transformation this time before applying the rotation. Do we get the same vector? no. It is so easy to see this with transformations rathar than figuring out this numerically! What about associativity? Does $(AB)C=A(BC)$? Carrying this out numerically, it is not intuitive and messy! Visualizing this with transformations, it is actually really easy to see that it is true! 3D The same concept applies to 3 dimesions and the video “chapter 5: Three dimensional linear transformations” has really great animations to show different transformations in 3D. References Essence of Linear Algebra by 3Blue1Brown]]></summary></entry><entry><title type="html">Linear Transformations</title><link href="http://localhost:4000/jekyll/update/2023/09/22/linear-transformations.html" rel="alternate" type="text/html" title="Linear Transformations" /><published>2023-09-22T01:01:36-07:00</published><updated>2023-09-22T01:01:36-07:00</updated><id>http://localhost:4000/jekyll/update/2023/09/22/linear-transformations</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/09/22/linear-transformations.html"><![CDATA[<p>This post is just me taking notes while watching <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a> which is the best reference on Linear Algebra ever. I might add other notes from other places as I go.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Transformations</b></h4>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/01.png" width="60%" class="center" /></p>
<p>So what does transformation mean? A transformation takes a vector as an input and outputs another vector so it’s just a function. So why didn’t we call it a function? This is because the word transformation suggests movement. We’re imagining that this input vector is moving over to the output vector. Of course, when transforming vectors, it is much nicer to use the trick from last time where we think of the vector as the point where its tip sits on. This way it is easier to visulize transforming many vectors all at once.
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Linear Transformations</b></h4>
<p>Transformations can be pretty complex but Linear Algebra is restricted to only linear transformations. Visually speaking a transformation is linear if it has two properties:</p>
<ul>
	<li>All lines must remain lines without getting curved.</li>
	<li>The origin is fixed in place.</li>
</ul>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Describing Linear Transformations Numerically</b></h4>
<p>How do we describe a linear transformation? It turns out that all we need to do is to record where the basis vectors will land and then everything else will just follow. Why is this? From the previous post, we know can write any vector as a linear combination of the basis vector of the vector space. Suppose $v = \begin{bmatrix}3 &amp; -2\end{bmatrix}$, then we know that we can write $v$ in terms of $\widehat{i}$ and $\widehat{j}$ as shown below.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/02.png" width="45%" class="center" /></p>
<p>Writing this again, we have</p>
<div>
$$
\begin{align*}
v &amp;= 3\widehat{i} - 2\widehat{j} \\
\begin{bmatrix}
3 \\ -2
\end{bmatrix}
&amp;=
3
\begin{bmatrix}
1 \\ 0
\end{bmatrix}
-
2
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
\end{align*}
$$
</div>
<p>After applying the transformation on $\widehat{i}$ and $\widehat{j}$, we can now find out where $v$ will land by writing $v$ as a linear combination of the transformed vectors $\widehat{i}$ and $\widehat{j}$.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/03.png" width="60%" class="center" /></p>
<p>and now we can just know where $v$ will land based on where $\widehat{i}$ and $\widehat{j}$ landed after the tranformation. We can write,</p>
<div>
$$
\begin{align*}
\text{transformed (} v) &amp;= 3\text{ (transformed }\widehat{i}) - 2 \text{ (transformed }\widehat{j}) \\
&amp;=
3
\begin{bmatrix}
1 \\ -1
\end{bmatrix}
-
2
\begin{bmatrix}
1 \\ 1
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
1 \\ -5
\end{bmatrix}
\end{align*}
$$
</div>
<p><br />
<!------------------------------------------------------------------------------------></p>
<h4><b>Representing the transformation with a Matrix</b></h4>
<p>So from the above we saw how a two dimensional linear transformation is completely described by four numbers. The two coordinates for where $\widehat{i}$ lands and the two coordinates for where $\widehat{j}$ lands and this is pretty cool! Here is a cooler thing. We can package these coordinates into a two-by-two grid of numbers (a Matrix!)</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/04.png" width="40%" class="center" /></p>
<p>The first column will be where $\widehat{i}$ landed and the second column will be where $\widehat{j}$ landed. So now when we apply the transformation to some vector $v=(x,y)$, what do we get?</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/05.png" width="40%" class="center" /></p>
<p>We’ll multiply the $x$ coordinate with the transformed $\widehat{i}$ vector and similary multiply the $y$ coordinate with the transformed $\widehat{j}$ coordinate.</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/06.png" width="70%" class="center" /></p>

<p>and now we can re-arrange the matrix position to put it on the left of the vector below,</p>
<p style="text-align:center;"><img src="http://localhost:4000/assets/linear-algebra/linear-transformations/07.png" width="100%" class="center" /></p>
<p>And this is way more fun than just memeorizing how matrix multipcation works. These matrix columns are just the transformed versions of your basis vectors and the resulting vector is just a linear combinations of these two vectors. 
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>More Example Transformations</b></h4>
<p>TODO: Rotations and Shear. (the video shows really beatiful animation for the examples)
<br />
<br />
<!------------------------------------------------------------------------------------></p>
<h4><b>References</b></h4>
<p><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra by 3Blue1Brown</a>
<br />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post is just me taking notes while watching Essence of Linear Algebra by 3Blue1Brown which is the best reference on Linear Algebra ever. I might add other notes from other places as I go. Transformations So what does transformation mean? A transformation takes a vector as an input and outputs another vector so it’s just a function. So why didn’t we call it a function? This is because the word transformation suggests movement. We’re imagining that this input vector is moving over to the output vector. Of course, when transforming vectors, it is much nicer to use the trick from last time where we think of the vector as the point where its tip sits on. This way it is easier to visulize transforming many vectors all at once. Linear Transformations Transformations can be pretty complex but Linear Algebra is restricted to only linear transformations. Visually speaking a transformation is linear if it has two properties: All lines must remain lines without getting curved. The origin is fixed in place. Describing Linear Transformations Numerically How do we describe a linear transformation? It turns out that all we need to do is to record where the basis vectors will land and then everything else will just follow. Why is this? From the previous post, we know can write any vector as a linear combination of the basis vector of the vector space. Suppose $v = \begin{bmatrix}3 &amp; -2\end{bmatrix}$, then we know that we can write $v$ in terms of $\widehat{i}$ and $\widehat{j}$ as shown below. Writing this again, we have $$ \begin{align*} v &amp;= 3\widehat{i} - 2\widehat{j} \\ \begin{bmatrix} 3 \\ -2 \end{bmatrix} &amp;= 3 \begin{bmatrix} 1 \\ 0 \end{bmatrix} - 2 \begin{bmatrix} 0 \\ 1 \end{bmatrix} \end{align*} $$ After applying the transformation on $\widehat{i}$ and $\widehat{j}$, we can now find out where $v$ will land by writing $v$ as a linear combination of the transformed vectors $\widehat{i}$ and $\widehat{j}$. and now we can just know where $v$ will land based on where $\widehat{i}$ and $\widehat{j}$ landed after the tranformation. We can write, $$ \begin{align*} \text{transformed (} v) &amp;= 3\text{ (transformed }\widehat{i}) - 2 \text{ (transformed }\widehat{j}) \\ &amp;= 3 \begin{bmatrix} 1 \\ -1 \end{bmatrix} - 2 \begin{bmatrix} 1 \\ 1 \end{bmatrix} \\ &amp;= \begin{bmatrix} 1 \\ -5 \end{bmatrix} \end{align*} $$ Representing the transformation with a Matrix So from the above we saw how a two dimensional linear transformation is completely described by four numbers. The two coordinates for where $\widehat{i}$ lands and the two coordinates for where $\widehat{j}$ lands and this is pretty cool! Here is a cooler thing. We can package these coordinates into a two-by-two grid of numbers (a Matrix!) The first column will be where $\widehat{i}$ landed and the second column will be where $\widehat{j}$ landed. So now when we apply the transformation to some vector $v=(x,y)$, what do we get? We’ll multiply the $x$ coordinate with the transformed $\widehat{i}$ vector and similary multiply the $y$ coordinate with the transformed $\widehat{j}$ coordinate.]]></summary></entry></feed>